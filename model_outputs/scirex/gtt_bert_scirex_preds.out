{
    "0": {
        "doctext": "document : deep label distribution learning with label ambiguity con ##vo ##lu ##tion ##al neural networks ( con ##vn ##ets ) have achieved excellent recognition performance in various visual recognition tasks . a large labeled training set is one of the most important factors for its success . however , it is difficult to collect sufficient training images with precise labels in some domains such as apparent age estimation , head pose estimation , multi - label classification and semantic segment ##ation . fortunately , there is ambiguous information among labels , which makes these tasks different from traditional classification . based on this observation , we convert the label of each image into a discrete label distribution , and learn the label distribution by mini ##mi ##zing a ku ##ll ##back - lei ##bler diver ##gence between the predicted and ground - truth label distributions using deep con ##vn ##ets . the proposed dl ##dl ( deep label distribution learning ) method effectively utilizes the label ambiguity in both feature learning and class ##ifier learning , which help prevent the network from over - fitting even when the training set is small . experimental results show that the proposed approach produces significantly better results than state - of - the - art methods for age estimation and head pose estimation . at the same time , it also improves recognition performance for multi - label classification and semantic segment ##ation tasks . label distribution , deep learning , age estimation , head pose estimation , semantic segment ##ation . section : introduction con ##vo ##lu ##tion ##al neural networks ( con ##vn ##ets ) have achieved state - of - the - art performance on various visual recognition tasks such as image classification , object detection and semantic segment ##ation . the availability of a huge set of training images is one of the most important factors for their success . however , it is difficult to collect sufficient training images with una ##mb ##ig ##uous labels in domains such as age estimation , head pose estimation , multi - label classification and semantic segment ##ation . therefore , exploit ##ing deep learning methods with limited samples and ambiguous labels has become an attractive yet challenging topic . why is it difficult to collect a large and accurately labeled training set ? firstly , it is difficult ( even for domain experts ) to provide exact labels to some tasks . for example , the pixels close to object boundaries are very difficult to label for ann ##ota ##tors in semantic segment ##ation . in addition , pixel labeling is a time - consuming task that may limit the amount of training samples . another example is that people ' s apparent age and head pose is difficult to describe with an accurate number . secondly , it is very hard to gather complete and sufficient data . for example , it is difficult to build an age data ##set covering people from 1 to 85 years old , and ensure that every age in this range has enough associated images . similar difficulties arise in head pose estimation , where head poses are usually collected at a small set of angles with a 10 or 15 inc ##rem ##ent . thus , the publicly available age , head pose and semantic segment ##ation data ##set ##s are small scale compared to those in image classification tasks . these aforementioned small data ##set ##s have a common characteristic , i . e . , label ambiguity , which refers to the uncertainty among the ground - truth labels . on one hand , label ambiguity is una ##vo ##ida ##ble in some applications . we usually predict another person ' s age in a way like \" around 25 \" , which indicates using not only 25 , but also neighboring ages to describe the face . and , different people may have different guess ##es towards the same face . similar situations also hold for other types of tasks . the labels of pixels at object boundaries are difficult to ann ##ota ##te because of the inherent ambiguity of these pixels in semantic segment ##ation . on the other hand , label ambiguity can also happen if we are not confident in the labels we provide for an image . in the multi - label classification task , some objects are clearly visible but difficult to recognize . this type of objects are ann ##ota ##ted as difficult in the pascal visual object classes ( vo ##c ) classification challenge , e . g . , the chair in the third image of the first row in fig . [ reference ] . [ age estimation ] [ head pose estimation ] [ multi - label classification ] [ semantic segment ##ation ] there are two main types of labeling methods : single - label recognition ( sl ##r ) and multi - label recognition ( ml ##r ) . sl ##r assumes one image or pixel has one label and ml ##r assumes that one image or pixel may be assigned multiple labels . both sl ##r and ml ##r aim to answer the question of which labels can be used to describe an image or pixel , but they can not describe the label ambiguity associated with it . label ambiguity will help improve recognition performance if it can be reasonably exploited . in order to utilize label correlation ( which may be considered as a consequence of label ambiguity in some applications ) , gen ##g et al . proposed a label distribution learning ( ld ##l ) approach for age estimation and head pose estimation . recently , some improvements of ld ##l have been proposed . xi ##ng et al . proposed two algorithms named ld ##log ##it ##bo ##ost and ao ##so - ld ##log ##it ##bo ##ost to learn general models to relax the maximum entropy model in traditional ld ##l methods . furthermore , he et al . generated age label distributions through weighted linear combination of the input image ' s label and its context - neighboring samples . however , these methods are sub ##op ##ti ##mal because they only utilize the correlation of neighboring labels in class ##ifier learning , but not in learning the visual representations . deep con ##vn ##ets have natural advantages in feature learning . existing con ##vn ##et framework ##s can be viewed as classification and regression models based on different optimization objective functions . in many cases , the soft ##max loss and loss are used in deep con ##vn ##et models for classification and regression problems , respectively . the soft ##max loss maximize ##s the estimated probability of the ground - truth class without considering other classes , and the loss minimize ##s the squared difference between the estimated values of the network and the ground - truth . these methods have achieved satisfactory performance in some domains such as image classification , human pose estimation and object detection . however , existing deep learning methods can not utilize the label ambiguity information . moreover , a well - known fact is that learning a good con ##vn ##et requires a lot of images . in order to solve the issues mentioned above , we convert both traditional sl ##r and ml ##r problems to label distribution learning problems . every instance is assigned a discrete label distribution according to its ground - truth . the label distribution can naturally describe the ambiguous information among all possible labels . through deep label distribution learning , the training instances associated with each class label is significantly increased without actually increase the number of the total training examples . fig . [ reference ] intuitive ##ly shows four examples of label distribution for different recognition tasks . then , we utilize a deep con ##vn ##et to learn the label distribution in both feature learning and class ##ifier learning . since we learn label distribution with deep con ##vn ##ets , we call our method dl ##dl : deep label distribution learning . the benefits of dl ##dl are summarized as follows : dl ##dl is an end - to - end learning framework which utilizes the label ambiguity in both feature learning and class ##ifier learning ; dl ##dl not only achieve ##s more robust performance than existing classification and regression methods , but also effectively relax ##es the requirement for large amount of training images , e . g . , a training face image with ground - truth label 25 is also useful for predicting faces at age 24 or 26 ; dl ##dl ( only single model without ensemble ) achieve ##s better performance than the state - of - the - art methods on age and head pose estimation tasks . dl ##dl also improves the performance for multi - label classification and semantic segment ##ation . the rest of this paper is organized as follows . we first review the related work in section [ reference ] . then , section [ reference ] proposes the dl ##dl framework , including the dl ##dl problem definition , dl ##dl theory , label distribution construction and training details . after that , the experiments are reported in section [ reference ] . finally , section [ reference ] presents discussions and the conclusion is given in section [ reference ] . section : related work in the past two decades , many efforts have been devoted to visual recognition , including at least image classification , object detection , semantic segment ##ation , and facial attribute ( apparent age and head pose ) estimation . these works can be divided into two streams . earlier research was mainly based on hand - crafted features , while more recent ones are usually deep learning methods . in this section , we briefly review these related approaches . methods based on hand - crafted features usually include two stages . the first stage is feature extraction . the second stage learns models for recognition , detection or estimation using these features . sv ##m , random forest and neural networks have commonly been used during the learning stage . in addition , gen ##g et al . proposed the label distribution learning approach to utilize the correlation among adjacent labels , which further improved performance on age estimation and head pose estimation . although important progresses have been made with these features , the hand - crafted features render them sub ##op ##ti ##mal for particular tasks such as age or head pose estimation . more recently , learning feature representation has shown great advantages . for example , lu et al . tried to learn cost - sensitive local binary features for age estimation . deep learning has substantially improved upon the state - of - the - art in image classification , object detection , semantic segment ##ation and many other vision tasks . in many cases , the soft ##max loss is used in deep models for classification . besides classification , deep con ##vn ##ets have also been trained for regression tasks such as head pose estimation and facial landmark detection . in regression problems , the training procedure usually opt ##imi ##zes a squared loss function . satisfactory performance has also been obtained by using tu ##key ' s bi ##weight function in human pose estimation . in terms of model architecture , deep con ##vn ##et models which use deeper architecture and smaller con ##vo ##lu ##tion filters ( e . g . , v ##gg - nets and v ##gg - face ) are very powerful . nevertheless , these deep learning methods do not make use of the presence of label ambiguity in the training set , and usually require a large amount of training data . a latest approach , in inception - v ##3 , is based on label smoothing ( l ##s ) . instead of only using the ground - truth label , they utilize a mixture of the ground - truth label and a uniform distribution to regular ##ize the class ##ifier . however , l ##s is limited to the uniform distribution among labels rather than mining labels ' ambiguous information . we believe that label ambiguity is too important to ignore . if we make good use of the ambiguity , we expect the required number of training images for some tasks could be effectively reduced . in this paper , we focus on how to exploit the label ambiguity in deep con ##vn ##ets . age and head pose estimation from still face images are suitable applications of the proposed research . in addition , we also extend our works to multi - label classification and semantic segment ##ation . section : the proposed dl ##dl approach in this section , we firstly give the definition of the dl ##dl problem . then , we present the dl ##dl theory . next , we propose the construction methods of label distribution for different recognition tasks . finally , we briefly introduce the dl ##dl architecture and training details . sub ##section : the deep label distribution learning problem given an input image , we are interested in est ##imating a category output ( e . g . , age or head pose angles ) . for two input images and with ground - truth labels and , and are supposed to be similar to each other if the correlation of and is strong , and vice versa . for example , the correlation between faces aged 32 and 33 should be stronger than that between faces aged 32 and 64 , in terms of facial details that reflect the age ( e . g . , skin smooth ##ness ) . in other words , we expect high correlation among input images with similar outputs . the label distribution learning approach exploited such correlation ##s in the machine learning phase , but used features that are extracted ignoring these correlation ##s . the proposed dl ##dl approach , however , is an end - to - end deep learning method which utilizes such correlation information in both feature learning and class ##ifier learning . we will also extend dl ##dl to handle other types of label ambiguity beyond correlation . to fulfill this goal , instead of output ##ting a single value for an input , dl ##dl quan ##ti ##zes the range of possible values into several labels . for example , in age estimation , it is reasonable to assume that , and it is a common practice to estimate integer values for ages . thus , we can define the set as the ordered label set for age estimation . the task of dl ##dl is then to predict a label distribution , where is the estimated probability that should be predicted to be years old . by est ##imating an entire label distribution , the deep learning machine is forced to take care of the ambiguity among labels . specifically , the input space of our framework is , where , and are the height , width , and number of channels of the input image , respectively . dl ##dl predict ##s a label distribution vector , where is the label set defined for a specific task ( e . g . , the above ) . we assume is complete , i . e . , any possible value has a corresponding member in . a training data set with instances is then denoted as . we use bold ##face lower ##case letters like to denote vectors , and the - th element of is denoted as . the goal of dl ##dl is to directly learn a conditional probability mass function from , where is the parameters in the framework . sub ##section : deep label distribution learning given an instance with label distribution , we assume that is the activation of the last fully connected layer in a deep con ##vn ##et . we use a soft ##max function to turn these activation ##s into a probability distribution , that is , given a training data set , the goal of dl ##dl is to find to generate a distribution that is similar to . there are different criteria to measure the similarity or distance between two distributions . for example , if the ku ##ll ##back - lei ##bler ( k ##l ) diver ##gence is used as the measurement of the similarity between the ground - truth and predicted label distribution , then the best parameter is determined by thus , we can define the loss function as : st ##och ##astic gradient descent is used to minimize the objective function e ##q . [ reference ] . for any and , and the derivative of soft ##max ( e ##q . [ reference ] ) is well known , as where is 1 if , and 0 otherwise . according to the chain rule , for any fixed , we have thus , the derivative of with respect to is once is learned , the label distribution of any new instance can be generated by a forward run of the network . if the expected class label is a single one , dl ##dl outputs , where prediction with multiple labels is also allowed , which could be a set where is a pre ##de ##fine ##d threshold . if the expected output is a real number , dl ##dl predict ##s the expectation of , as where . this indicates that dl ##dl is suitable for both classification and regression tasks . sub ##section : label distribution construction the ground - truth label distribution is not available in most existing data ##set ##s , which must be generated under proper assumptions . a desirable label distribution must satisfy some basic principles : ( 1 ) should be a probability distribution . thus , we have and . ( 2 ) the probability values should have difference among all possible labels associated with an image . in other words , a less ambiguous category must be assigned high probability and those more ambiguous labels must have low pro ##ba ##bilities . in this section , we propose the way to construct label distributions for age estimation , head pose estimation , multi - label classification and semantic segment ##ation . for age estimation , we assume that the pro ##ba ##bilities should concentrate around the ground - truth age . thus , we quan ##ti ##ze to get using a normal distribution . for example , the apparent age of a face is labeled by hundreds of users . the ground - truth ( including a mean and a standard deviation ) is calculated from all the votes . for this problem , we find the range of the target ( e . g . , ) , quan ##ti ##ze it into a complete and ordered label set , where is the label set size and are all possible predictions for . a label distribution is then , where is the probability that ( i . e . , for ) . since we use equal step size in quan ##ti ##zing , the normal p . d . f . ( probability density function ) is a natural choice to generate the ground - truth from and : where . fig . [ reference ] shows a face and its corresponding label distribution . for problems where is unknown , we will show that a reasonably chosen also works well in dl ##dl . for head pose estimation , we need to jointly estimate pitch and ya ##w angles . thus , learning joint distribution is also necessary in dl ##dl . suppose the label set is , where is a pair of values . that is , we want to learn the joint distribution of two variables . then , the label distribution can be represented by an matrix , whose - th element is . for example , when we use two angles ( pitch and ya ##w ) to describe a head pose , is a pair of pitch and ya ##w angles . given an instance with ground - truth mean and co ##var ##iance matrix , we calculate its label distribution as where . in the above , we assume , that is , the co ##var ##iance matrix is diagonal . fig . [ reference ] shows a joint label distribution with head pose and . for multi - label classification , a multi - label image always contains at least one object of the class of interest . there are usually multiple labels for an image . these labels are grouped into three different levels , including positive , negative and difficult in the pascal vo ##c data ##set . a label is positive means an image contains objects from that category , and negative otherwise . difficult indicates that an object is clearly visible but difficult to recognize . existing multi - label methods often view difficult as negative , which leads to the loss of useful information . it is not reasonable either if we simply treat difficult as positive . therefore , a nature choice is to use label ambiguity . we define different pro ##ba ##bilities for different types of labels , as for positive , difficult and negative labels , respectively . furthermore , an normal ##ization is applied to ensure : where equals , or if the label is positive , difficult or negative , respectively . the label distribution is shown for a multi - label image in fig . [ reference ] . for semantic segment ##ation , we need to label a pixel as belonging to one class if it is a pixel inside an object of that class , or as the background otherwise . let denote the ann ##ota ##tion of the - th pixel , where ( assuming there are categories and 0 for background ) . fully con ##vo ##lu ##tion ##al networks ( fc ##n ) have been an effective solution to this task . in fc ##n , a ground - truth label means that and for all . however , it is very difficult to specify ground - truth labels for pixels close to object boundaries , because labels of these pixels are inherently ambiguous . we propose a mechanism to describe the label ambiguity in the boundaries . considering a ga ##uss ##ian kernel matrix , we replace the original label distribution with , as where , , is the kernel size , and are pad ##ding and stride sizes . in our experiment , we set , and , and the generated label distribution is fig . [ reference ] gives the semantic label distribution for a bird image which shows that the ambiguity is encoded in the label distributions . sub ##section : the dl ##dl architecture and training details we use a deep con ##vn ##et and a training set to learn a as the estimation of . the structure of our network is based on popular deep models such as z ##f - net and v ##gg - nets . the z ##f - net consists five con ##vo ##lu ##tion layers , followed by three fully connected layers . the v ##gg - nets architecture includes 16 or 19 layers . we modify the last fully connected layer ' s output based on the task and replace the original soft ##max loss function with the k ##l loss function . in addition , we use the parameter re ##lu for z ##f - net . in our network , the input is an order three tensor and the output may be a vector ( age estimation and multi - label classification ) , a matrix ( head pose estimation ) or a tensor ( semantic segment ##ation ) . in this paper , we train the deep models in two ways : training from scratch . for z ##f - net , the initial ##ization is performed randomly , based on a ga ##uss ##ian distribution with zero mean and 0 . 01 standard deviation , and bias ##es are initial ##ized to zero . the coefficient of the parameter re ##lu is initial ##ized to 0 . 25 . the drop ##out is applied to the last two fully connected layers with rate 0 . 5 . the coefficient of weight decay is set to . optimization is done by st ##och ##astic gradient descent ( sg ##d ) using mini - batch ##es of 128 and the momentum coefficient is 0 . 9 . the initial learning rate is set to 0 . 01 . the total number of epoch ##s is about 20 . fine - tuning . three pre - trained models including v ##gg - nets ( 16 - layers and 19 - layers ) and v ##gg - face ( 16 - layers ) are used to fine - tune for different tasks . we remove these pre - trained models ' classification layer and loss layer , and put in our label distribution layer which is initial ##ized by the ga ##uss ##ian distribution and the k ##l loss layer . the learning rates of the con ##vo ##lu ##tion ##al layers , the first two fully - connected layers and the label distribution layer are initial ##ized as 0 . 001 , 0 . 001 and 0 . 01 , respectively . we fine - tune all layers by back propagation through the whole net using mini - batch ##es of 32 . the total number of epoch ##s is about 10 for age estimation and 20 for multi - label classification . section : experiments we evaluate dl ##dl on four tasks , i . e . , age estimation , head pose estimation , multi - label classification and semantic segment ##ation . our implementation is based on mat ##con ##vn ##et . all our experiments are carried out on a n ##vid ##ia k ##40 gp ##u with 12 gb of onboard memory . sub ##section : age estimation data ##set ##s . two age estimation data ##set ##s are used in our experiments . the first is mor ##ph , which is one of the largest publicly available age data ##set ##s . there are 55 , 134 face images from more than 13 , 000 subjects . ages range from 16 to 77 . since no train / test split is provided , 10 - fold cross - validation is used for mor ##ph . the second data ##set is from the apparent age estimation competition , the first competition track of the icc ##v cha ##lea ##rn lap 2015 workshop . compared with mor ##ph , this data ##set ( cha ##lea ##rn ) consists of images collected in the wild , without any position , illumination or quality restriction . the only condition is that each image contains only one face . the data ##set has 4 , 69 ##9 images , and is split into 2 , 47 ##6 training ( train ) , 1 , 136 validation ( val ) and 1 , 08 ##7 testing ( test ) images . the apparent age ( i . e . , how old does this person look like ) of each image is labeled by multiple individuals . the age of face images range from 3 to 85 . for each image , its mean age and the corresponding standard deviation are given . since the ground - truth for test images are not published , we train on the train split and evaluate on the val split of cha ##lea ##rn images . baseline ##s . to demonstrate the effectiveness of dl ##dl , we firstly consider two related methods as baseline ##s : con ##vn ##et + l ##s ( k ##l ) and con ##vn ##et + ld ( - di ##v ) . the former uses label smoothing ( l ##s ) as ground - truth and k ##l diver ##gence as loss function . the latter uses label distribution ( ld ) as ground - truth and diver ##gence as loss function , which is in addition , we also compare dl ##dl with the following baseline methods : bf ##gs - ld ##l gen ##g et al . proposed the label distribution learning approach ( ii ##s - ll ##d ) for age and head pose estimation . they used traditional image features . to further improve ii ##s - ll ##d , gen ##g et al . proposed a bf ##gs - ld ##l algorithm by using the effective quasi - newton optimization method bf ##gs . c - con ##vn ##et classification con ##vn ##ets have obtained very competitive performance in various computer vision tasks . z ##f - net and v ##gg - net are popular models which use the soft ##max loss . we replace the image ##net - specific 1000 - way classification in these modes with the label set . r - con ##vn ##et con ##vn ##ets are also successively trained for regression tasks . in r - con ##vn ##et , the ground - truth label ( age and pose angle ) is projected into the range by the mapping , where and are the maximum and minimum values in the training label set . during prediction , the r - con ##vn ##et regression result is reverse mapped to get . to speed up convergence , the last fully connected layer is followed a hyper ##bolic tangent activation function , which maps to . the squared , and - ins loss functions are used in r - con ##vn ##et . implementation details . we use the same prep ##ro ##ces ##sing pipeline for all compared methods , including face detection , facial key points detection and face alignment , as shown in fig [ reference ] . we employ the d ##pm model to detect the main facial region . then , the detected face is fed into cascade ##d con ##vo ##lu ##tion networks to get the five facial key points , including the left / right eye centers , nose tip and left / right mouth corners . finally , based on these facial points , we align the face to the upright pose . data aug ##ment ##ation are only applied to the training images for cha ##lea ##rn . for one color input training image , we generate its gray - scale version , and left - right flip both color and gray - scale versions . thus , every training image turns into 4 images . [ input ] [ detection ] [ facial points ] [ alignment ] we define for both data ##set ##s . the label distribution of each image is generated using e ##q . [ reference ] . the mean is provided in both mor ##ph and cha ##lea ##rn . the standard deviation , however , is provided in cha ##lea ##rn but not in mor ##ph . we simply set in mor ##ph . experiments for different methods are conducted under the same data splits . 1 used 80 % of mor ##ph images for training and 20 % for evaluation ; 2 used additional external face images ( i . e . , im ##db - wi ##ki ) ; 3 used pre - trained model ( i . e . , v ##gg - nets or v ##gg - face ) . evaluation criteria . mean absolute error ( mae ) and cumulative score ( cs ) are used to evaluate the performance of age estimation . mae is the average difference between the predicted and the real age : where and are the estimated and ground - truth age of the - th testing image , respectively . cs is defined as the accuracy rate of correct estimation : where is the number of correct estimation , i . e . , testing images that satisfy . in our experiment , . in addition , a special measurement ( named - error ) is defined by the cha ##lea ##rn competition , computed as results . table [ reference ] lists results on both data ##set ##s . the upper part shows results in the literature . the middle part shows the baseline results . the lower part shows the results of the proposed approach . the first term in the parent ##hesis behind each method is the loss function corresponding to the method . max or ex ##p represent predicting according to e ##q . [ reference ] or [ reference ] , respectively . since cross - validation is used in mor ##ph , we also provide its standard deviation ##s . [ cha ##lea ##rn ] [ mor ##ph ] [ afl ##w ] [ sub ##fi ##gur ##e ] label ##form ##at = empty [ 40 ] [ 19 ] [ 62 ] [ 23 ] [ 38 ] [ 24 ] [ 26 ] [ 66 ] [ 52 ] [ 22 ] [ red ##39 . 69 ] [ red ##19 . 29 ] [ red ##6 ##1 . 61 ] [ red ##22 . 94 ] [ red ##37 . 87 ] [ red ##24 . 27 ] [ red ##25 . 40 ] [ blue ##60 . 17 ] [ blue ##35 . 06 ] [ blue ##28 . 55 ] from table [ reference ] , we can see that dl ##dl consistently out ##per ##forms baseline ##s and other published methods . the difference between dl ##dl ( k ##l , max ) and its competitor c - con ##vn ##et ( soft ##max , max ) is 0 . 51 on mor ##ph . this gap is more than 6 times the sum of their standard deviation ##s ( 0 . 03 + 0 . 05 ) , showing statistical ##ly significant differences . the advantage of dl ##dl over r - con ##vn ##et , c - con ##vn ##et and con ##vn ##et + l ##s suggests that learning label distribution is advantage ##ous in deep end - to - end models . dl ##dl has much better results than bf ##gs - ld ##l , which shows that the learned deep features are more powerful than manually designed ones . compared to con ##vn ##et + ld ( - di ##v ) , dl ##dl ( k ##l ) achieve ##s lower mae on both data ##set ##s . it indicates that k ##l - diver ##gence is better than - diver ##gence for measuring the similarity of two distributions in this context . we find that c - con ##vn ##et and r - con ##vn ##et are not stable . the r - con ##vn ##et ( ) method , although being the second best method for cha ##lea ##rn , is inferior to c - con ##vn ##et ( soft ##max , ex ##p ) for mor ##ph . in addition , we also find that e ##q . [ reference ] is better than e ##q . [ reference ] in many cases , which suggests that e ##q . [ reference ] is more suitable than e ##q . [ reference ] for age estimation . fine - tuning dl ##dl . instead of training dl ##dl from scratch , we also fine - tune the network of v ##gg - face . on the small scale cha ##lea ##rn data ##set , the mae of dl ##dl is reduced from 5 . 34 to 3 . 51 , yielding a significant improvement . the - error of dl ##dl is reduced from 0 . 44 to 0 . 31 , which is close to the best competition result 0 . 28 on the validation set . in , external training images ( 260 , 282 additional external training images with real age ann ##ota ##tion ) were used . dl ##dl only uses the cha ##lea ##rn data ##set ' s 2 , 47 ##6 training images and is the best among cha ##lea ##rn teams that do not use external data . in the competition , the best external - data - free - error is 0 . 48 , which is worse than dl ##dl ' s . however , the idea in to use external data is useful for further reducing dl ##dl ' s estimation error . fig . [ reference ] and fig . [ reference ] show the cs curves on cha ##lea ##rn and mor ##ph data ##set ##s . at every error level , our dl ##dl fine - tuned v ##gg - face always achieve ##s the best accuracy among all methods . it is noteworthy that the cs curves of dl ##dl ( k ##l , max ) and con ##vn ##et ( - di ##v , max ) are very close to that of the dl ##dl + v ##gg - face ( k ##l , max ) on mor ##ph even without lots of external data and very deep model . this observation supports the idea that using dl ##dl can achieve competitive performance even with limited training samples . in fig . [ reference ] , we show some examples of face images from the cha ##lea ##rn validation set and predicted label distributions by dl ##dl ( k ##l , ex ##p ) . in many cases , our solution is able to accurately predict the apparent age of faces . failures may come from two causes . the first is the failure to detect or align the face . the second is some extreme conditions of face images such as o ##cc ##lusion , low resolution , heavy makeup and old photos . sub ##section : head pose estimation data ##set ##s . we use three data ##set ##s in head pose estimation : pointing ' 04 , b ##ju ##t - 3d and ann ##ota ##ted facial landmarks in the wild ( afl ##w ) . in them , head pose is determined by two angles : pitch and ya ##w . pointing ' 04 disc ##ret ##izes the pitch into 9 angles and the ya ##w into 13 angles . when the pitch angel is or , the ya ##w angle is always set to . thus , there are 93 poses in total . the head images are taken from 15 different human subjects in two different time periods , resulting in images . b ##ju ##t - 3d contains 500 3d faces ( 250 male and 250 female people ) , acquired by a cyber ##ware laser scanner in an engineered environment . 9 pitch angles and 13 ya ##w angles are used . there are in total 93 poses in this data ##set , similar to that in pointing ' 04 . therefore , face images are obtained . unlike pointing ' 04 and b ##ju ##t - 3d , the afl ##w is a real - world face database . head pose is coarse ##ly obtained by fitting a mean 3d face with the po ##sit algorithm . the data ##set contains about 24 ##k faces in real - world images . we select 23 , 40 ##9 faces to ensure pitch and ya ##w angles within . implementation details . the head region is provided by bound ##ing box ann ##ota ##tions in pointing ' 04 and afl ##w . the b ##ju ##t - 3d does not contain background regions . therefore , we will not perform any prep ##ro ##ces ##sing . in dl ##dl , we set in pointing ' 04 and in b ##ju ##t - 3d for constructing label distributions . for afl ##w , ground - truth of head pose angles are given as real numbers . ground - truth ( pitch and ya ##w ) angles are divided from to in steps of , so we get ( pitch , ya ##w ) pair category labels . we set for afl ##w . since the discrete jeffrey ' s diver ##gence is used in ld ##l , we implement bf ##gs - ld ##l with the ku ##ll ##back - lei ##bler diver ##gence . all experiments are performed under the same setting , including data splits , input size and network architecture . to valid ##ate the effectiveness of dl ##dl for head pose estimation , we use the same baseline ##s as age estimation . our experiments show that e ##q . [ reference ] has lower accuracy than e ##q . [ reference ] . hence , we use e ##q . [ reference ] in this section . evaluation criteria . three types of prediction values are evaluated : pitch , ya ##w , and pitch + ya ##w , where pitch + ya ##w jointly estimates the pitch and ya ##w angles . two different measurements are used , which is mae ( e ##q . [ reference ] ) and classification accuracy ( acc ) . when we treat different poses as different classes , acc measures the pose class classification accuracy . in particular , the mae of pitch + ya ##w is calculated as the euclidean distance between the predicted ( pitch , ya ##w ) pair and the ground - truth pair ; the acc of pitch + ya ##w is calculated by regarding each ( pitch , ya ##w ) pair as a class . for r - con ##vn ##et , we only report its mae but not acc , because its predicted value are continuous real numbers . all methods are tested with 5 - fold cross validation for pointing ' 04 and b ##ju ##t - 3d following . for afl ##w , 15 , 56 ##1 face images are randomly chosen for training , and the remaining 7 , 84 ##8 for evaluation . the setup is similar to the recent literature ( 14 , 000 images for training and the rest 7 , 04 ##1 images for testing ) . [ sub ##fi ##gur ##e ] label ##form ##at = empty [ ( + 77 , - 4 ) ] [ ( - 16 , - 1 ) ] [ ( - 1 , - 30 ) ] [ ( + 30 , + 8 ) ] [ ( + 4 , - 4 ) ] [ ( - 36 , + 13 ) ] [ ( - 87 , - 3 ) ] [ ( - 61 , - 58 ) ] [ ( + 63 , + 12 ) ] [ ( + 80 , - 27 ) ] [ red ( + 75 , - 3 ) ] [ red ( - 15 , 0 ) ] [ red ( - 3 , - 27 ) ] [ red ( + 27 , + 6 ) ] [ red ( + 6 , - 3 ] [ red ( - 39 , + 15 ] [ red ( - 87 , 0 ) ] [ blue ( - 3 , - 12 ) ] [ blue ( + 21 , + 18 ] [ blue ( + 45 , - 15 ) ] results . tables [ reference ] , [ reference ] and [ reference ] show results on pointing ' 04 , b ##ju ##t - 3d and afl ##w , respectively . pointing ' 04 is small scale with only 2 , 79 ##0 images . we observe that bf ##gs - ld ##l ( with hand - crafted features ) has much lower mae and much higher accuracy than deep learning methods c - con ##vn ##et , r - con ##vn ##et and con ##vn ##et + l ##s . one reasonable conjecture is that c - con ##vn ##et , r - con ##vn ##et and con ##vn ##et + l ##s are not well - learned with only small number of training images . dl ##dl , however , successfully learns the head pose . for example , its accuracy for pitch + ya ##w is 73 . 15 ( and c - con ##vn ##et is only 42 . 97 ) . that is , dl ##dl is able to perform deep learning with few training images , while c - con ##vn ##et r - con ##vn ##et and con ##vn ##et + l ##s have failed for this task . on b ##ju ##t - 3d and afl ##w which have enough training data , we observe that many deep learning methods show higher performance than bf ##gs - ld ##l . dl ##dl achieve ##s the best performance : it has much lower mae and higher accuracy than other methods . another observation is also worth mentioning . although r - con ##vn ##et is better than c - con ##vn ##et when label is dense such as age estimation and head pose estimation on afl ##w , it is obviously worse than c - con ##vn ##et on b ##ju ##t - 3d and pointing ' 04 for head pose estimation which have sparse labels . in other words , the performance of c - con ##vn ##et and r - con ##vn ##et are not very robust , while the proposed method consistently achieve ##s excellent performance . fig . [ reference ] shows the pitch + ya ##w cs curves on the afl ##w data ##set . there is an obvious gap between dl ##dl and baseline methods at every error level . fig . [ reference ] shows the predicted label distributions for different head poses on the afl ##w testing set using the dl ##dl model . our approach can estimate head pose with low errors but may fail under some extreme conditions . it is noteworthy that dl ##dl may produce more incorrect estimation ##s when both ya ##w and pitch are large ( e . g . , ) . the reason might be that there are much fewer training examples for large angles than for other angles . sub ##section : multi - label classification data ##set ##s . we evaluate our approach for multi - label classification on the pascal vo ##c data ##set : pascal vo ##c ##200 ##7 and vo ##c ##20 ##12 . there are 9 , 96 ##3 and 22 , 53 ##1 images in them , respectively . each image is ann ##ota ##ted with one or several labels , corresponding to 20 object categories . these images are divided into three subset ##s including train , val and test sets . we train on the train ##val set and evaluate on the test set . the evaluation metric is average precision ( ap ) and mean average precision ( map ) , comply ##ing with the pascal challenge protocols . we denote our methods as images - fine - tuning - dl ##dl ( if - dl ##dl ) and proposals - fine - tuning - dl ##dl ( p ##f - dl ##dl ) when con ##vn ##ets are fine - tuned by images and proposals of images , respectively . details of these two variants are explained later in this section . we compare the proposed approaches with the following methods : v ##gg + sv ##m [ ] . this method densely extracted 4 , 09 ##6 dimensional con ##vn ##et features at the penultimate layer of v ##gg - nets pre - trained on image ##net . these features from different scales ( smallest image side ) were aggregate ##d by average pool ##ing . then , these averaged features from two networks ( \" net - d \" containing 16 layers and \" net - e \" containing 19 layers ) were further fused by stack ##ing . finally , normal ##ized the resulting image features and used these features to train a linear sv ##m class ##ifier for multi - label classification . hc ##p [ ] . hc ##p proposed to solve the multi - label object recognition task by extract ##ing object proposals from the images . the method used image label and square loss to fine - tune a pre - trained con ##vn ##et . then , bing or edge ##box ##es was used to extract object proposals , which were used to fine - tune the con ##vn ##et again . finally , scores of these proposals were max - poole ##d to obtain the prediction . fe ##v + l ##v [ ] . this approach transformed the multi - label object recognition problem into a multi - class multi - instance learning problem . two views ( label view and feature view ) were extracted for each proposal of images . then , these two views were encoded by a fisher vector for each image . if - v ##gg - \u2113 ##2 and if - v ##gg - k ##l . we fine - tune the v ##gg - nets with square loss and multi - label cross - entropy loss and use them as our if - dl ##dl ' s baseline ##s . they are trained using the same setting . implementation details . according to the ground - truth labels , we set different pro ##ba ##bilities for all possible labels on pascal vo ##c data ##set . in our experiments , , , . finally , similar to label smoothing , a uniform distribution is added to , where . if - dl ##dl . following , each training image is individually res ##cal ##ed by randomly sampling in the range [ 256 , 512 ] . we randomly crop patches from these res ##ized images . we also adjust the pool ##ing kernel in the pool ##5 layer from to . max - pool ##ing and av ##g - pool ##ing are used at pool ##5 to train two con ##vn ##ets . we obtain four con ##vn ##et models thought fine - tuning \" net - d \" and \" net - e \" . at the prediction stage , the smaller side of each image is scaled to a fixed length . each scaled image is fed to the fine - tuned con ##vn ##ets to obtain the 20 - dim probability outputs . these probability outputs from different scales and different models are averaged to form the final prediction . p ##f - dl ##dl . following , we further fine - tune if - dl ##dl models with proposals of images to boost performance . for each training image , we employ edge ##box ##es to produce a set of proposal bound ##ing boxes which are grouped into clusters by the normal ##ized cut algorithm . for each cluster , the top proposals with higher predict ##ive scores generated by edge ##box ##es are res ##ized into square shapes ( i . e . , ) . as a result , we can obtain proposals for an image . finally , these res ##ized proposals are fed into a fine - tuned if - dl ##dl model to obtain prediction scores and these scores are fused by max - pool ##ing to form the prediction distribution of the image . this process can be learned by using an end - to - end way . in our implementation , we set and at the training and the prediction stage , respectively . similar to if - dl ##dl , we also average fuse prediction scores of different models to generate the final prediction . results . in table [ reference ] , we compare single model results ( average ap of all classes ) on vo ##c ##200 ##7 . our p ##f - dl ##dl defeats all the other methods . compared with fe ##v + l ##v , 1 . 7 % improvement can be achieved by p ##f - dl ##dl even without using the bound ##ing box ann ##ota ##tion . compared with hc ##p - v ##gg , our p ##f - dl ##dl can achieve 92 . 3 % map , which is significantly higher than their 90 . 9 % . this further indicates that it is very important to learn a label distribution . table [ reference ] and [ reference ] report details of all experimental results on vo ##c ##200 ##7 and vo ##c ##20 ##12 , respectively . it can be seen that if - dl ##dl out ##per ##forms if - v ##gg - by 1 . 1 % for vo ##c ##200 ##7 and 1 . 3 % for vo ##c ##20 ##12 , which indicates that the k ##l loss function is more suitable than loss for measuring the similarity of two label distributions . furthermore , if - dl ##dl improves if - v ##gg - k ##l for about 0 . 2 - 0 . 3 points in map , which suggests that learning a label distribution is beneficial . more importantly , p ##f - dl ##dl can achieve 93 . 4 % for vo ##c ##200 ##7 and 92 . 4 % for vo ##c ##20 ##12 in map when we average fuse output scores of four p ##f - dl ##dl models . our framework shows good performance especially for scene categories such as \" chair \" , ' table \" and \" sofa \" . although p ##f - dl ##dl significantly out ##per ##forms if - dl ##dl in map , p ##f - dl ##dl has higher computational cost than if - dl ##dl on both training and testing stages . since if - dl ##dl does not need region proposals or bound ##ing box information , it may be effectively and efficiently implemented for practical multi - label application such as multi - label image retrieval . it is also possible that by adopting new techniques ( such as the region proposal method using gate ##d unit in , which has higher accuracy that ours on vo ##c tasks ) , the accuracy of our dl ##dl methods can be further improved . [ cha ##lea ##rn ] [ mor ##ph ] [ pointing ' 04 ] [ afl ##w pitch ] [ afl ##w ya ##w ] bf ##gs - ld ##l dl ##dl sub ##section : semantic segment ##ation data ##set ##s . we employ the pascal vo ##c ##20 ##11 segment ##ation data ##set and the semantic boundaries data ##set ( sb ##d ) for training the proposed dl ##dl . there are 2 , 224 images ( 1 , 112 for training and 1 , 112 for testing ) with pixel labels for 20 semantic categories in vo ##c ##20 ##11 . sb ##d contains 11 , 355 ann ##ota ##ted images ( 8 , 98 ##4 for training and 2 , 37 ##1 for testing ) from hari ##hara ##n et al . . following fc ##n , we train dl ##dl using the union set ( 8 , 82 ##5 images ) of sb ##d and vo ##c ##20 ##11 training images . we evaluate the proposed approach on vo ##c ##20 ##11 ( 1 , 112 ) and vo ##c ##20 ##12 ( 1 , 45 ##6 ) test images . evaluation criteria . the performance is measured in terms of mean i ##u ( intersection over union ) , which is the most widely used metric in semantic segment ##ation . we keep the same settings as fc ##n including training images and model structure . the main change is that we employ k ##l diver ##gence as the loss function based on label distribution ( e ##q . [ reference ] ) . note that although we transform the ground - truth to label distribution in the training process , our evaluation rely only on ground - truth label . recently , conditional random field ( cr ##f ) has been broadly used in many state - of - the - art semantic segment ##ation systems . we optional ##ly employ a fully connected cr ##f to ref ##ine the predicted category score maps using the default parameters of . results . table [ reference ] gives the performance of dl ##dl - 8 ##s and dl ##dl - 8 ##s - cr ##f on the test images of vo ##c ##20 ##11 and vo ##c ##20 ##12 and compares it to the well - known fc ##n - 8 ##s . dl ##dl - 8 ##s improves the mean i ##u of fc ##n - 8 ##s form 62 . 7 % to 64 . 9 % on vo ##c ##20 ##11 . on vo ##c ##20 ##12 , dl ##dl - 8 ##s leads to an improvement of 2 . 3 points in mean i ##u . dl ##dl achieve ##s better results than fc ##n , which suggests it is important to improve the segment ##ation performance using label ambiguity . in addition , the cr ##f further improve performance of dl ##dl - 8 ##s , offering a 2 . 6 % absolute increase in mean i ##u both on vo ##c ##20 ##11 and vo ##c ##20 ##12 . fig . [ reference ] shows four semantic segment ##ation examples from the vo ##c ##20 ##11 validation images using fc ##n - 8 ##s , dl ##dl - 8 ##s and dl ##dl - 8 ##s - cr ##f . we can see that dl ##dl - 8 ##s can successfully segment some small objects ( e . g . , car and bicycle ) and particularly improve the segment ##ation of object boundaries ( e . g . , horse ' s leg and plant ' s leaves ) , but fc ##n - 8 ##s does not . dl ##dl - 8 ##s may fail , e . g . , it sees a flower ##pot as a pot ##ted plant in the fourth row in fig . [ reference ] . furthermore , compared to dl ##dl - 8 ##s , dl ##dl - 8 ##s - cr ##f is able to ref ##ine coarse pixel - level label predictions to produce sharp boundaries and fine - grain ##ed segment ##ations ( e . g . , plant ' s leaves ) . [ sub ##fl ##oat ] label ##form ##at = empty position = top [ image ] [ fc ##n - 8 ##s ] [ dl ##dl - 8 ##s ] [ dl ##dl - 8 ##s + cr ##f ] [ ground - truth ] [ cha ##lea ##rn ] [ mor ##ph ] [ b ##ju ##t - 3d ] [ afl ##w ] section : discussions in this section , we try to understand the general ##ization performance of dl ##dl through feature visual ##ization , and to analyze why dl ##dl can achieve high accuracy with limited training data . in addition , a study of the hyper - parameter is also provided . feature visual ##ization . we visual ##ize the model features in a low - dimensional space . early layers learn low - level features ( e . g . , edge and corner ) and latter layers learn high level features ( e . g . , shapes and objects ) in a deep con ##vn ##et . hence , we extract the penultimate layer features ( 4 , 09 ##6 - dimensional ) on mor ##ph , cha ##lea ##rn , pointing ' 04 and afl ##w validation sets . to obtain the 2 - dimensional em ##bed ##ding ##s of the extracted high dimensional features , we employ a popular dimension reduction algorithm t - s ##ne . the low - dimensional em ##bed ##ding ##s of validation images from the above four data ##set ##s are shown in fig . [ reference ] . the first row shows the 2 - dim em ##bed ##ding ##s of hand - crafted features ( bi ##f for mor ##ph and cha ##lea ##rn , hog for pointing ' 04 and afl ##w ) and the second row shows that of the dl ##dl features . these figures are colored by their semantic category . it can be observed that clear semantic cluster ##ings ( old or young for age data ##set ##s , left or right , up or down for head pose data ##set ##s ) appear in deep features but do not in hand - crafted features . reduce over - fitting . dl ##dl can effectively reduce over - fitting when the training set is small . this effect can be explained by the label ambiguity . considering an input sample with one single label . in traditional deep con ##vn ##et , and for all . in dl ##dl , the label distribution contains many non zero ##s elements . the diversity of labels helps reduce over - fitting . moreover , the objective function ( e ##q . [ reference ] ) of dl ##dl can be re ##written as in e ##q . [ reference ] , the first term is the tradition con ##vn ##et loss function . the second term maximize the log - likelihood of the ambiguous labels . unlike existing data aug ##ment ##ation techniques such as random crop ##ping on the images , dl ##dl aug ##ments data on the label side . in fig . [ reference ] , mae is shown as a function of the number of epoch ##s on two age data ##set ##s ( cha ##lea ##rn and mor ##ph ) and two head pose data ##set ##s ( b ##ju ##t - 3d and afl ##w ) . on cha ##lea ##rn and afl ##w , c - con ##ven ##et ( soft ##max ) achieve ##s the lowest training mae , but produces the highest validation mae . in particular , the validation mae increases after the 8th epoch on cha ##lea ##rn . similar phenomenon is observed on afl ##w . this fact shows that over - fitting happens in c - con ##vn ##et when the number of training images is small . although there are 15 , 56 ##1 training images in afl ##w , each category contains on average ##ly 4 training images since there are 3 , 72 ##1 categories . accelerate convergence . we further analyze the convergence performance of dl ##dl , c - con ##vn ##et and r - con ##vn ##et . we can observe that the training mae is reduced very slowly at the beginning of training using c - con ##vn ##et and r - con ##ven ##et in many cases as shown in fig . [ reference ] . on the contrary , the mae of dl ##dl reduces quickly . robust performance . one notable observation is that c - con ##vn ##et and r - con ##ven ##et is unstable . fig . [ reference ] shows the mae for pitch + ya ##w , a complicated estimation of the joint distribution . this is a very sparse label set because the interval of adjacent class ( pitch or ya ##w ) is . r - con ##vn ##et has difficulty in est ##imating this output , yielding errors that are roughly 20 times higher than dl ##dl and c - con ##vn ##et . on the other hand , c - con ##vn ##et easily fall into over - fitting when there are not enough training data ( e . g , fig . [ reference ] and fig . [ reference ] ) . the proposed dl ##dl is more am ##ena ##ble to small data ##set ##s or sparse labels than c - con ##vn ##et and r - con ##vn ##et . analyze the hyper - parameter . dl ##dl ' s performance may be affected by the label distribution . here , we take age estimation ( mor ##ph ) and head pose estimation ( pointing ' 04 ) for examples . is a common hyper - parameter in these tasks if it is not provided in the ground - truth . we have empirical ##ly set in mor ##ph , and in pointing ' 04 in our experiments . in order to study the impact of , we test dl ##dl with different values , changing from 0 to 3 with 0 . 5 interval . fig . [ reference ] shows the mae performance on mor ##ph and pointing ' 04 with different . we can see that a proper is important for low mae . but generally speaking , a value that is close to the interval between neighboring labels is a good choice . because the shape of all curves are v - shape like , it is also very convenient to find an optimal value using the cross - validation strategy . section : conclusion we observe that current deep con ##vn ##ets can not successfully learn good models when there are not enough training data and / or the labels are ambiguous . we propose dl ##dl , a deep label distribution learning framework to solve this issue by exploit ##ing label ambiguity . in dl ##dl , each image is labeled by a label distribution , which can utilize label ambiguity in both feature learning and class ##ifier learning . dl ##dl consistently improves the network training process in our experiments , by preventing it from over - fitting when the training set is small . we empirical ##ly showed that dl ##dl produces robust and competitive performances than traditional classification or regression deep models on several popular visual recognition tasks . however , constructing a reasonable label distribution is still challenging due to the diversity of label space for different recognition tasks . it is an interesting direction to extend dl ##dl to more recognition problems by constructing different label distributions . bibliography : references [ ] bin - bin gao received the b . s . and m . s . degrees in applied mathematics in 2010 and 2013 , respectively . he is currently pursuing the ph . d . degree in the department of computer science and technology , nanjing university , china . his research interests include computer vision and machine learning . { ieee ##bio ##graphy } [ ] chao xi ##ng received the b . s . degree in software engineering from southeast university , china , in 2014 . he is currently a postgraduate student in the school of computer science and engineering at southeast university , china . his research interests include pattern recognition , machine learning , and data mining . { ieee ##bio ##graphy } [ ] chen - wei xi ##e received his b . s . degree from southeast university , china , in 2015 . he is currently a postgraduate student in the department of computer science and technology , nanjing university , china . his research interests include computer vision and machine learning . { ieee ##bio ##graphy } [ ] jian ##xin wu ( m ' 09 ) received the b . s . and m . s . degrees in computer science from nanjing university , and the ph . d . degree in computer science from the georgia institute of technology . he was an assistant professor with the nan ##yang technological university , singapore . he is currently a professor with the department of computer science and technology , nanjing university , china , and is associated with the national key laboratory for novel software technology , china . his current research interests include computer vision and machine learning . he has served as an area chair for cv ##pr 2017 and icc ##v 2015 , a senior pc member for aaa ##i 2017 and aaa ##i 2016 , and an associate editor of pattern recognition journal . { ieee ##bio ##graphy } [ ] xi ##n gen ##g ( m ' 13 ) received the b . s . and m . s . degrees in computer science from nanjing university , china , in 2001 and 2004 , respectively , and the ph . d degree from dea ##kin university , australia in 2008 . he joined the school of computer science and engineering at southeast university , china , in 2008 , and is currently a professor and vice dean of the school . he has authored over 50 referee ##d papers , and he holds five patents in these areas . his research interests include pattern recognition , machine learning , and computer vision .",
        "pred_seq": "[SEP] label ambiguity [SEP] [SEP] head estimation [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "label ambiguity"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "head pose estimation"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "chalearn lap 2015",
                        "chalearn"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "optimization objective functions",
                        "mean absolute error",
                        "mae"
                    ]
                ],
                "Task": [
                    [
                        "age estimation",
                        "age"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "morph"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "optimization objective functions",
                        "mean absolute error",
                        "mae"
                    ]
                ],
                "Task": [
                    [
                        "age estimation",
                        "age"
                    ]
                ]
            }
        ]
    },
    "1": {
        "doctext": "in this work we explore recent advances in rec ##urrent neural networks for large scale language modeling , a task central to language understanding . we extend current models to deal with two key challenges present in this task : corp ##ora and vocabulary sizes , and complex , long term structure of language . we perform an exhaust ##ive study on techniques such as character con ##vo ##lu ##tion ##al neural networks or long - short term memory , on the one billion word bench ##mark . our best single model significantly improves state - of - the - art per ##plex ##ity from 51 . 3 down to 30 . 0 ( whilst reducing the number of parameters by a factor of 20 ) , while an ensemble of models sets a new record by improving per ##plex ##ity from 41 . 0 down to 23 . 7 . we also release these models for the nl ##p and ml community to study and improve upon . section : introduction language modeling ( l ##m ) is a task central to natural language processing ( nl ##p ) and language understanding . models which can accurately place distributions over sentences not only en ##code complex ##ities of language such as grammatical structure , but also di ##sti ##ll a fair amount of information about the knowledge that a corp ##ora may contain . indeed , models that are able to assign a low probability to sentences that are grammatical ##ly correct but unlikely may help other tasks in fundamental language understanding like question answering , machine translation , or text sum ##mar ##ization . l ##ms have played a key role in traditional nl ##p tasks such as speech recognition , machine translation , or text sum ##mar ##ization . often ( although not always ) , training better language models improves the underlying metric ##s of the downstream task ( such as word error rate for speech recognition , or b ##le ##u score for translation ) , which makes the task of training better l ##ms valuable by itself . further , when trained on vast amounts of data , language models compact ##ly extract knowledge encoded in the training data . for example , when trained on movie sub ##titles , these language models are able to generate basic answers to questions about object colors , facts about people , etc . lastly , recently proposed sequence - to - sequence models employ conditional language models as their key component to solve diverse tasks like machine translation or video generation . deep learning and rec ##urrent neural networks ( rn ##ns ) have fueled language modeling research in the past years as it allowed researchers to explore many tasks for which the strong conditional independence assumptions are un ##real ##istic . despite the fact that simpler models , such as n - grams , only use a short history of previous words to predict the next word , they are still a key component to high quality , low per ##plex ##ity l ##ms . indeed , most recent work on large scale l ##m has shown that rn ##ns are great in combination with n - grams , as they may have different strengths that complement n - gram models , but worse when considered in isolation . we believe that , despite much work being devoted to small data sets like the penn tree bank ( pt ##b ) , research on larger tasks is very relevant as over ##fi ##tting is not the main limitation in current language modeling , but is the main characteristic of the pt ##b task . results on larger corp ##ora usually show better what matters as many ideas work well on small data sets but fail to improve on larger data sets . further , given current hardware trends and vast amounts of text available on the web , it is much more straightforward to tackle large scale modeling than it used to be . thus , we hope that our work will help and mo ##tiv ##ate researchers to work on traditional l ##m beyond pt ##b - for this purpose , we will open - source our models and training recipes . we focused on a well known , large scale l ##m bench ##mark : the one billion word bench ##mark data set . this data set is much larger than pt ##b ( one thousand fold , 800 ##k word vocabulary and 1b words training data ) and far more challenging . similar to image ##net , which helped advance computer vision , we believe that releasing and working on large data sets and models with clear bench ##marks will help advance language modeling . the contributions of our work are as follows : we explored , extended and tried to un ##ify some of the current research on large scale l ##m . specifically , we designed a soft ##max loss which is based on character level cnn ##s , is efficient to train , and is as precise as a full soft ##max which has orders of magnitude more parameters . our study yielded significant improvements to the state - of - the - art on a well known , large scale l ##m task : from 51 . 3 down to 30 . 0 per ##plex ##ity for single models whilst reducing the number of parameters by a factor of 20 . we show that an ensemble of a number of different models can bring down per ##plex ##ity on this task to 23 . 7 , a large improvement compared to current state - of - art . we share the model and recipes in order to help and mo ##tiv ##ate further research in this area . in section [ reference ] we review important concepts and previous work on language modeling . section [ reference ] presents our contributions to the field of neural language modeling , emphasizing large scale rec ##urrent neural network training . sections [ reference ] and [ reference ] aim at exhaust ##ively describing our experience and understanding throughout the project , as well as em ##pl ##acing our work relative to other known approaches . section : related work in this section we describe previous work relevant to the approaches discussed in this paper . a more detailed discussion on language modeling research is provided in . sub ##section : language models language modeling ( l ##m ) has been a central task in nl ##p . the goal of l ##m is to learn a probability distribution over sequences of symbols pertaining to a language . much work has been done on both para ##metric ( e . g . , log - linear models ) and non - para ##metric approaches ( e . g . , count - based l ##ms ) . count - based approaches ( based on statistics of n - grams ) typically add smoothing which account for unseen ( yet possible ) sequences , and have been quite successful . to this extent , kn ##ese ##r - ne ##y smoothed 5 - gram models are a fairly strong baseline which , for large amounts of training data , have challenged other para ##metric approaches based on neural networks . most of our work is based on rec ##urrent neural networks ( rn ##n ) models which retain long term depend ##encies . to this extent , we used the long - short term memory model which uses a ga ##ting mechanism to ensure proper propagation of information through many time steps . much work has been done on small and large scale rn ##n - based l ##ms . the architecture ##s that we considered in this paper are represented in figure [ reference ] . in our work , we train models on the popular one billion word bench ##mark , which can be considered to be a medium - sized data set for count - based l ##ms but a very large data set for n ##n - based l ##ms . this regime is most interesting to us as we believe learning a very good model of human language is a complex task which will require large models , and thus large amounts of data . further advances in data availability and computational resources helped our study . we argue this leap in scale enabled tremendous advances in deep learning . a clear example found in computer vision is image ##net , which enabled learning complex vision models from large amounts of data . a crucial aspect which we discuss in detail in later sections is the size of our models . despite the large number of parameters , we try to minimize computation as much as possible by adopting a strategy proposed in of projecting a relatively big rec ##urrent state space down so that the matrices involved remain relatively small , yet the model has large memory capacity . sub ##section : con ##vo ##lu ##tion ##al em ##bed ##ding models there is an increased interest in incorporating character - level inputs to build word em ##bed ##ding ##s for various nl ##p problems , including part - of - speech tag ##ging , par ##sing and language modeling . the additional character information has been shown useful on relatively small bench ##mark data sets . the approach proposed in builds word em ##bed ##ding ##s using bid ##ire ##ction ##al l ##st ##ms over the characters . the rec ##urrent networks process sequences of characters from both sides and their final state vectors are con ##cate ##nated . the resulting representation is then fed to a neural network . this model achieved very good results on a part - of - speech tag ##ging task . in , the words characters are processed by a 1 - d cnn with max - pool ##ing across the sequence for each con ##vo ##lu ##tion ##al feature . the resulting features are fed to a 2 - layer highway network , which allows the em ##bed ##ding to learn semantic representations . the model was evaluated on small - scale language modeling experiments for various languages and matched the best results on the pt ##b data set despite having 60 % fewer parameters . sub ##section : soft ##max over large vo ##ca ##bular ##ies assign ##ing probability distributions over large vo ##ca ##bular ##ies is computational ##ly challenging . for modeling language , maxim ##izing log - likelihood of a given word sequence leads to opt ##imi ##zing cross - entropy between the target probability distribution ( e . g . , the target word we should be predicting ) , and our model predictions . generally , predictions come from a linear layer followed by a soft ##max non - linear ##ity : where is the log ##it corresponding to a word . the log ##it is generally computed as an inner product where is a context vector and is a \" word em ##bed ##ding \" for . the main challenge when is very large ( in the order of one million in this paper ) is the fact that computing all inner products between and all em ##bed ##ding ##s becomes prohibit ##ively slow during training ( even when exploit ##ing matrix - matrix multiplication ##s and modern gp ##us ) . several approaches have been proposed to cope with the scaling issue : importance sampling , noise contrast ##ive estimation ( nc ##e ) , self normal ##izing partition functions or hierarchical soft ##max - they all offer good solutions to this problem . we found importance sampling to be quite effective on this task , and explain the connection between it and nc ##e in the following section , as they are closely related . section : language modeling improvements rec ##urrent neural networks based l ##ms employ the chain rule to model joint pro ##ba ##bilities over word sequences : where the context of all previous words is encoded with an l ##st ##m , and the probability over words uses a soft ##max ( see figure [ reference ] ( a ) ) . sub ##section : relationship between noise contrast ##ive estimation and importance sampling as discussed in section [ reference ] , a large scale soft ##max is necessary for training good l ##ms because of the vocabulary size . a hierarchical soft ##max employs a tree in which the probability distribution over words is deco ##mp ##osed into a product of two pro ##ba ##bilities for each word , greatly reducing training and inference time as only the path specified by the hierarchy needs to be computed and updated . choosing a good hierarchy is important for obtaining good results and we did not explore this approach further for this paper as sampling methods worked well for our setup . sampling approaches are only useful during training , as they propose an approximation to the loss which is cheap to compute ( also in a distributed setting ) - however , at inference time one still has to compute the normal ##ization term over all words . noise contrast ##ive estimation ( nc ##e ) proposes to consider a sur ##rogate binary classification task in which a class ##ifier is trained to disc ##rim ##inate between true data , or samples coming from some arbitrary distribution . if both the noise and data distributions were known , the optimal class ##ifier would be : where is the binary random variable indicating whether comes from the true data distribution , is the number of negative samples per positive word , and and are the data and noise distribution respectively ( we dropped any dependency on previous words for notation ##al simplicity ) . it is easy to show that if we train a log ##istic class ##ifier where is the log ##istic function , then , is a good approximation of ( is a log ##it which e . g . an l ##st ##m l ##m compute ##s ) . the other technique , which is based on importance sampling ( is ) , proposes to directly approximate the partition function ( which comprises a sum over all words ) with an estimate of it through importance sampling . though the methods look superficial ##ly similar , we will derive a similar sur ##rogate classification task akin to nc ##e which arrives at is , showing a strong connection between the two . suppose that , instead of having a binary task to decide if a word comes from the data or from the noise distribution , we want to identify the words coming from the true data distribution in a set , comprised of noise samples and one data distribution sample . thus , we can train a multi ##class loss over a multi ##no ##mia ##l random variable which maximize ##s , assuming w . l . o . g . that is always the word coming from true data . by bay ##es rule , and ignoring terms that are constant with respect to , we can write : and , following a similar argument than for nc ##e , if we define then is a good approximation of . note that the only difference between nc ##e and is is that , in nc ##e , we define a binary classification task between true or noise words with a log ##istic loss , whereas in is we define a multi ##class classification problem with a soft ##max and cross entropy loss . we hope that our derivation helps clarify the similarities and differences between the two . in particular , we observe that is , as it opt ##imi ##zes a multi ##class classification task ( in contrast to solving a binary task ) , may be a better choice . indeed , the updates to the log ##its with is are tied whereas in nc ##e they are independent . sub ##section : cnn soft ##max the character - level features allow for a smooth ##er and compact para ##met ##rization of the word em ##bed ##ding ##s . recent efforts on small scale language modeling have used cnn character em ##bed ##ding ##s for the input em ##bed ##ding ##s . although not as straightforward , we propose an extension to this idea to also reduce the number of parameters of the soft ##max layer . recall from section [ reference ] that the soft ##max compute ##s a log ##it as where is a context vector and the word em ##bed ##ding . instead of building a matrix of ( whose rows correspond to ) , we produce with a cnn over the characters of as - we call this a cnn soft ##max . we used the same network architecture to dynamic ##ally generate the soft ##max word em ##bed ##ding ##s without sharing the parameters with the input word - em ##bed ##ding sub - network . for inference , the vectors can be pre ##com ##puted , so there is no computational complexity increase w . r . t . the regular soft ##max . we note that , when using an importance sampling loss such as the one described in section [ reference ] , only a few log ##its have non - zero gradient ( those corresponding to the true and sampled words ) . with a soft ##max where are independently learned word em ##bed ##ding ##s , this is not a problem . but we observed that , when using a cnn , all the log ##its become tied as the function mapping from to is quite smooth . as a result , a much smaller learning rate had to be used . even with this , the model lacks capacity to differentiate between words that have very different meanings but that are spelled similarly . thus , a reasonable compromise was to add a small correction factor which is learned per word , such that : where is a matrix projecting a low - dimensional em ##bed ##ding vector back up to the dimensional ##ity of the projected l ##st ##m hidden state of . this amounts to adding a bottle ##neck linear layer , and brings the cnn soft ##max much closer to our best result , as can be seen in table [ reference ] , where adding a 128 - dim correction halves the gap between regular and the cnn soft ##max . aside from a big reduction in the number of parameters and incorporating morphological knowledge from words , the other benefit of this approach is that out - of - vocabulary ( o ##ov ) words can easily be scored . this may be useful for other problems such as machine translation where handling out - of - vocabulary words is very important . this approach also allows parallel training over various data sets since the model is no longer explicitly para ##met ##rized by the vocabulary size - or the language . this has shown to help when using byte - level input em ##bed ##ding ##s for named entity recognition , and we hope it will enable similar gains when used to map onto words . sub ##section : char l ##st ##m predictions the cnn soft ##max layer can handle arbitrary words and is much more efficient in terms of number of parameters than the full soft ##max matrix . it is , though , still considerably slow , as to evaluate per ##plex ##ities we need to compute the partition function . a class of models that solve this problem more efficiently are character - level l ##st ##ms . they make predictions one character at a time , thus allowing to compute pro ##ba ##bilities over a much smaller vocabulary . on the other hand , these models are more difficult to train and seem to perform worse even in small tasks like pt ##b . most likely this is due to the sequences becoming much longer on average as the l ##st ##m reads the input character by character instead of word by word . thus , we combine the word and character - level models by feeding a word - level l ##st ##m hidden state into a small l ##st ##m that predict ##s the target word one character at a time ( see figure [ reference ] ( c ) ) . in order to make the whole process reasonably efficient , we train the standard l ##st ##m model until convergence , freeze its weights , and replace the standard word - level soft ##max layer with the aforementioned character - level l ##st ##m . the resulting model scales independently of vocabulary size - both for training and inference . however , it does seem to be worse than regular and cnn soft ##max - we are hopeful that further research will enable these models to replace fixed vocabulary models whilst being computational ##ly attractive . section : experiments all experiments were run using the tensor ##flow system , with the exception of some older models which were used in the ensemble . sub ##section : data set the experiments are performed on the 1b word bench ##mark data set introduced by , which is a publicly available bench ##mark for measuring progress of statistical language modeling . the data set contains about 0 . 8 ##b words with a vocabulary of 79 ##34 ##7 ##1 words , including sentence boundary markers . all the sentences are shuffled and the duplicate ##s are removed . the words that are out of vocabulary ( o ##ov ) are marked with a special un ##k token ( there are approximately 0 . 3 % such words ) . sub ##section : model setup the typical measure used for reporting progress in language modeling is per ##plex ##ity , which is the average per - word log - probability on the hold ##out data set : . we follow the standard procedure and sum over all the words ( including the end of sentence symbol ) . we used the 1b word bench ##mark data set without any pre - processing . given the shuffled sentences , they are input to the network as a batch of independent streams of words . whenever a sentence ends , a new one starts without any pad ##ding ( thus maxim ##izing the o ##cc ##up ##ancy per batch ) . for the models that consume characters as inputs or as targets , each word is fed to the model as a sequence of character id ##s of pre ##es ##pe ##ci ##fied length ( see figure [ reference ] ( b ) ) . the words were processed to include special begin and end of word token ##s and were padded to reach the expected length . i . e . if the maximum word length was 10 , the word \" cat \" would be transformed to \" $ cat ^ \" due to the cnn model . in our experiments we found that limiting the maximum word length in training to 50 was sufficient to reach very good results while 32 was clearly insufficient . we used 256 characters in our vocabulary and the non - as ##ci ##i symbols were represented as a sequence of bytes . sub ##section : model architecture we evaluated many variations of rn ##n l ##m architecture ##s . these include the dimensional ##ities of the em ##bed ##ding layers , the state , projection sizes , and number of l ##st ##m layers to use . exhaust ##ively trying all combinations would be extremely time consuming for such a large data set , but our findings suggest that l ##st ##ms with a projection layer ( i . e . , a bottle ##neck between hidden states as in ) trained with truncated bp ##tt for 20 steps performed well . following we use drop ##out before and after every l ##st ##m layer . the bias ##es of l ##st ##m forget gate were initial ##ized to 1 . 0 . the size of the models will be described in more detail in the following sections , and the choices of hyper - parameters will be released as open source upon publication . for any model using character em ##bed ##ding cnn ##s , we closely follow the architecture from . the only important difference is that we use a larger number of con ##vo ##lu ##tion ##al features of 40 ##9 ##6 to give enough capacity to the model . the resulting em ##bed ##ding is then linear ##ly transformed to match the l ##st ##m projection sizes . this allows it to match the performance of regular word em ##bed ##ding ##s but only uses a small fraction of parameters . sub ##section : training procedure the models were trained until convergence with an ada ##grad opt ##imi ##zer using a learning rate of 0 . 2 . in all the experiments the rn ##ns were un ##roll ##ed for 20 steps without ever reset ##ting the l ##st ##m states . we used a batch size of 128 . we clip the gradient ##s of the l ##st ##m weights such that their norm is bounded by 1 . 0 . using these hyper - parameters we found large l ##st ##ms to be relatively easy to train . the same learning rate was used in almost all of the experiments . in a few cases we had to reduce it by an order of magnitude . unless otherwise stated , the experiments were performed with 32 gp ##u workers and as ##yn ##ch ##ron ##ous gradient updates . further details will be fully specified with the code upon publication . training a model for such large target vocabulary ( 79 ##34 ##7 ##1 words ) required to be careful with some details about the approximation to full soft ##max using importance sampling . we used a large number of negative ( or noise ) samples : 81 ##9 ##2 such samples were drawn per step , but were shared across all the target words in the batch ( 256 ##0 total , i . e . 128 times 20 un ##roll ##ed steps ) . this results in multi ##ply ##ing ( 256 ##0 x 102 ##4 ) times ( 102 ##4 x ( 81 ##9 ##2 + 1 ) ) ( instead of ( 256 ##0 x 102 ##4 ) times ( 102 ##4 x 79 ##34 ##7 ##1 ) ) , i . e . about 100 - fold less computation . section : results and analysis in this section we sum ##mar ##ize the results of our experiments and do an in - depth analysis . table [ reference ] contains all results for our models compared to previously published work . table [ reference ] shows previous and our own work on ensembles of models . we hope that our encouraging results , which improved the best per ##plex ##ity of a single model from 51 . 3 to 30 . 0 ( whilst reducing the model size considerably ) , and set a new record with ensembles at 23 . 7 , will enable rapid research and progress to advance language modeling . for this purpose , we will release the model weights and recipes upon publication . sub ##section : size matters un ##sur ##pr ##ising ##ly , size matters : when training on a very large and complex data set , fitting the training data with an l ##st ##m is fairly challenging . thus , the size of the l ##st ##m layer is a very important factor that influences the results , as seen in table [ reference ] . the best models are the largest we were able to fit into a gp ##u memory . our largest model was a 2 - layer l ##st ##m with 81 ##9 ##2 + 102 ##4 dimensional rec ##urrent state in each of the layers . increasing the em ##bed ##ding and projection size also helps but causes a large increase in the number of parameters , which is less desirable . lastly , training an rn ##n instead of an l ##st ##m yields poorer results ( about 5 per ##plex ##ity worse ) for a comparable model size . sub ##section : regular ##ization importance as shown in table [ reference ] , using drop ##out improves the results . to our surprise , even relatively small models ( e . g . , single layer l ##st ##m with 204 ##8 units projected to 512 dimensional outputs ) can over - fit the training set if trained long enough , eventually yielding hold ##out set degradation . using drop ##out on non - rec ##urrent connections largely mit ##igate ##s these issues . while over - fitting still occurs , there is no more need for early stopping . for models that had 40 ##9 ##6 or less units in the l ##st ##m layer , we used 10 % drop ##out probability . for larger models , 25 % was significantly better . even with such regular ##ization , per ##plex ##ities on the training set can be as much as 6 points below test . in one experiment we tried to use a smaller vocabulary comprising of the 100 , 000 most frequent words and found the difference between train and test to be smaller - which suggests that too much capacity is given to rare words . this is less of an issue with character cnn em ##bed ##ding models as the em ##bed ##ding ##s are shared across all words . sub ##section : importance sampling is data efficient table [ reference ] shows the test per ##plex ##ities of nc ##e vs is loss after a few epoch ##s of 204 ##8 unit l ##st ##m with 512 projection . the is objective significantly improves the speed and the overall performance of the model when compared to nc ##e . sub ##section : word em ##bed ##ding ##s vs character cnn replacing the em ##bed ##ding layer with a para ##met ##rized neural network that process characters of a given word allows the model to consume arbitrary words and is not restricted to a fixed vocabulary . this property is useful for data sets with conversation ##al or informal text as well as for morphological ##ly rich languages . our experiments show that using character - level em ##bed ##ding ##s is feasible and does not de ##grade performance - in fact , our best single model uses a character cnn em ##bed ##ding . an additional advantage is that the number of parameters of the input layer is reduced by a factor of 11 ( though training speed is slightly worse ) . for inference , the em ##bed ##ding ##s can be pre ##com ##puted so there is no speed penalty . overall , the em ##bed ##ding of the best model is para ##met ##rized by 72 m weights ( down from 820 m weights ) . table [ reference ] shows a few examples of nearest neighbor em ##bed ##ding ##s for some out - of - vocabulary words when character cnn ##s are used . sub ##section : smaller models with cnn soft ##max even with character - level em ##bed ##ding ##s , the model is still fairly large ( though much smaller than the best competing models from previous work ) . most of the parameters are in the linear layer before the soft ##max : 820 m versus a total of 1 . 04 ##b parameters . in one of the experiments we froze the word - l ##st ##m after convergence and replaced the soft ##max layer with the cnn soft ##max sub - network . without any fine - tuning that model was able to reach 39 . 8 per ##plex ##ity with only 293 m weights ( as seen in table [ reference ] ) . as described in section [ reference ] , adding a \" correction \" word em ##bed ##ding term alleviate ##s the gap between regular and cnn soft ##max . indeed , we can trade - off model size versus per ##plex ##ity . for instance , by adding 100 m weights ( through a 128 dimensional bottle ##neck em ##bed ##ding ) we achieve 35 . 8 per ##plex ##ity ( see table [ reference ] ) . to contrast with the cnn soft ##max , we also evaluated a model that replaces the soft ##max layer with a smaller l ##st ##m that predict ##s one character at a time ( see section [ reference ] ) . such a model does not have to learn long depend ##encies because the base l ##st ##m still operates at the word - level ( see figure [ reference ] ( c ) ) . with a single - layer l ##st ##m of 102 ##4 units we reached 49 . 0 test per ##plex ##ity , far below the best model . in order to make the comparisons more fair , we performed a very expensive marginal ##ization over the words in the vocabulary ( to rule out words not in the dictionary which the character l ##st ##m would assign some probability ) . when doing this marginal ##ization , the per ##plex ##ity improved a bit down to 47 . 9 . sub ##section : training speed we used 32 tesla k ##40 gp ##us to train our models . the smaller version of the l ##st ##m model with 204 ##8 units and 512 projections needs less than 10 hours to reach below 45 per ##plex ##ity and after only 2 hours of training the model beats previous state - of - the art on this data set . the best model needs about 5 days to get to 35 per ##plex ##ity and 10 days to 32 . 5 . the best results were achieved after 3 weeks of training . see table [ reference ] for more details . sub ##section : ensembles we averaged several of our best models and we were able to reach 23 . 7 test per ##plex ##ity ( more details and results can be seen in table [ reference ] ) , which is more than 40 % improvement over previous work . interesting ##ly , including the best n - gram model reduces the per ##plex ##ity by 1 . 2 point even though the model is rather weak on its own ( 67 . 6 per ##plex ##ity ) . most previous work had to either ensemble with the best n - gram model ( as their rn ##n only used a limited output vocabulary of a few thousand words ) , or use n - gram features as additional input to the rn ##n . our results , on the contrary , suggest that n - grams are of limited benefit , and suggest that a carefully trained l ##st ##m l ##m is the most competitive model . sub ##section : l ##st ##ms are best on the tail words figure [ reference ] shows the difference in log pro ##ba ##bilities between our best model ( at 30 . 0 per ##plex ##ity ) and the kn - 5 . as can be seen from the plot , the l ##st ##m is better across all the bucket ##s and significantly out ##per ##forms kn - 5 on the rare words . this is encouraging as it seems to suggest that l ##st ##m l ##ms may fare even better for languages or data sets where the number of rare words is larger than traditional n - gram models . sub ##section : samples from the model to qu ##ali ##tative ##ly evaluate the model , we sampled many sentences . we discarded short and politically incorrect ones , but the sample shown below is otherwise \" raw \" ( i . e . , not hand picked ) . the samples are of high quality - which is not a surprise , given the per ##plex ##ities attained - but there are still some occasional mistakes . sentences generated by the ensemble ( about 26 per ##plex ##ity ) : with even more new technologies coming onto the market quickly during the past three years , an increasing number of companies now must tackle the ever - changing and ever - changing environmental challenges online . < s > check back for updates on this breaking news story . < s > about 800 people gathered at he ##ver castle on long beach from noon to 2 pm , three to four times that of the funeral [UNK] . < s > we are aware of written instructions from the copyright holder not to , in any way , mention rosenberg ' s negative comments if they are relevant as indicated in the documents , \" e ##bay said in a statement . < s > it is now known that coffee and ca ##cao products can do no harm on the body . < s > yuri z ##hir ##kov was in attendance at the stamford bridge at the start of the second half but neither dr ##og ##ba nor mal ##oud ##a was able to push on through the barcelona defence . section : discussion and conclusions in this paper we have shown that rn ##n l ##ms can be trained on large amounts of data , and out ##per ##form competing models including carefully tuned n - grams . the reduction in per ##plex ##ity from 51 . 3 to 30 . 0 is due to several key components which we studied in this paper . thus , a large , regular ##ized l ##st ##m l ##m , with projection layers and trained with an approximation to the true soft ##max with importance sampling performs much better than n - grams . unlike previous work , we do not require to inter ##pol ##ate both the rn ##n l ##m and the n - gram , and the gains of doing so are rather marginal . by exploring recent advances in model architecture ##s ( e . g . l ##st ##ms ) , exploit ##ing small character cnn ##s , and by sharing our findings in this paper and accompanying code and models ( to be released upon publication ) , we hope to inspire research on large scale language modeling , a problem we consider crucial towards language understanding . we hope for future research to focus on reasonably sized data ##set ##s taking inspiration from recent advances seen in the computer vision community thanks to efforts such as image ##net . section : acknowledge ##ments we thank ci ##pr ##ian che ##lb ##a , il ##ya su ##tsk ##ever , and the google brain team for their help and discussions . we also thank ko ##ray ka ##vu ##k ##cu ##og ##lu for his help with the manuscript . bibliography : references",
        "pred_seq": "[SEP] [SEP] b score [SEP] large modeling [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [
                    [
                        "bleu score"
                    ]
                ],
                "Task": [
                    [
                        "large scale language modeling"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "one billion word benchmark",
                        "1b words training data",
                        "1b word benchmark data set"
                    ]
                ],
                "Method": [
                    [
                        "lstm",
                        "lstm lm",
                        "lstm predictions",
                        "wordlevel lstm hidden state",
                        "lstm model",
                        "characterlevel lstm",
                        "lstms",
                        "lstm layer",
                        "lstm forget gate",
                        "2layer lstm",
                        "single layer lstm",
                        "2048 unit lstm",
                        "wordlstm",
                        "singlelayer lstm of 1024 units",
                        "character lstm",
                        "lstm lms",
                        "regularized lstm lm"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "language modeling",
                        "lm",
                        "lms",
                        "language models",
                        "large scale lm",
                        "modeling language"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "one billion word benchmark",
                        "1b words training data",
                        "1b word benchmark data set"
                    ]
                ],
                "Method": [
                    [
                        "lstm lm",
                        "lstm",
                        "lstm predictions",
                        "wordlevel lstm hidden state",
                        "lstm model",
                        "characterlevel lstm",
                        "lstms",
                        "lstm layer",
                        "lstm forget gate",
                        "2layer lstm",
                        "single layer lstm",
                        "2048 unit lstm",
                        "wordlstm",
                        "singlelayer lstm of 1024 units",
                        "character lstm",
                        "lstm lms",
                        "regularized lstm lm"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "language modeling",
                        "lm",
                        "lms",
                        "language models",
                        "large scale lm",
                        "modeling language"
                    ]
                ]
            }
        ]
    },
    "2": {
        "doctext": "br ##id ##ging sal ##ien ##cy detection to weakly supervised object detection based on self - paced curriculum learning section : abstract weakly - supervised object detection ( wo ##d ) is a challenging problems in computer vision . the key problem is to simultaneously in ##fer the exact object locations in the training images and train the object detectors , given only the training images with weak image - level labels . intuitive ##ly , by sim ##ulating the selective attention mechanism of human visual system , sal ##ien ##cy detection technique can select attractive objects in scenes and thus is a potential way to provide useful prior ##s for wo ##d . however , the way to adopt sal ##ien ##cy detection in wo ##d is not trivial since the detected sal ##ien ##cy region might be possibly highly ambiguous in complex cases . to this end , this paper first comprehensive ##ly analyze ##s the challenges in applying sal ##ien ##cy detection to wo ##d . then , we make one of the earliest efforts to bridge sal ##ien ##cy detection to wo ##d via the self - paced curriculum learning , which can guide the learning procedure to gradually achieve faithful knowledge of multi - class objects from easy to hard . the experimental results demonstrate that the proposed approach can successfully bridge sal ##ien ##cy detection and wo ##d tasks and achieve the state - of - the - art object detection results under the weak supervision . section : introduction 1 object detection is one of the most fundamental yet ##chal ##len ##ging problems in computer vision community . the most recent breakthrough was achieved by gi ##rs ##hic ##k et al . , who trained the con ##vo ##lu ##tion ##al neural network ( cnn ) by using large amount of human labelled bound ##ing boxes to learn the powerful feature representations and object class ##ifiers . despite their success , the problem of object detection is still under - addressed in practice due to the heavy burden of labeling the training samples . essentially , in this big data era , humans more desire intelligent machines which are capable of automatically discovering the intrinsic patterns from the cheap ##ly and massive ##ly collected weakly * the corresponding author labeled images . thus weakly supervised object detection ( wo ##d ) systems have been gaining more interests recently . the key problem in wo ##d is how to extract the exact object local ##izations and train the corresponding object detectors from the weakly labelled training images . in such chicken - egg problem , most methods ( including the proposed one ) usually use the alternative learning strategy that first provides some coarse estimation to initial ##ize the potential object locations and then gradually train the object detectors and update object locations jointly . in this paper , we leverage sal ##ien ##cy detection to initial ##ize the potential object locations due to the following reasons : 1 ) sal ##ien ##cy detection ; [ reference ] [ reference ] aims at sim ##ulating the selective attention mechanism of human visual system to automatically select sub - regions ( usually the regions containing objects of interest ) in image scenes . thus , it can be readily utilized to provide useful prior ##s to estimate the potential object local ##izations and fit well to the investigated task . 2 ) some recent sal ##ien ##cy detection methods such as [ reference ] and [ reference ] can process much faster than the prior ##s , e . g . , intra - class similarity [ reference ] , inter - class variance [ reference ] , and distance mapping relation [ reference ] , adopted in the existing wo ##d systems . 3 ) several existing works , e . g . , , have attempted to apply sal ##ien ##cy detection techniques to wo ##d . however they still have not sufficiently explore the intrinsic bridge between these two tasks , which mo ##tiv ##ates us to clarify the insight ##ful relationship between these two tasks and further develop powerful learning regime to bridge them . essentially , although it sounds reasonable to apply sal ##ien ##cy detection to wo ##d , the way to bridge these two tasks is not trivial . the main problem is that sal ##ien ##cy detection is formulated as category - free models which only distinguish attractive regions from the image background while irrelevant to the concrete object category . thus , as shown in fig . 1 , in the images only containing one category of objects ( considered as \" easy \" images ) , the objects can be captured by sal ##ien ##cy detection methods easily and associated with the corresponding image label properly . whereas in the images weakly labelled as containing multiple categories of objects ( considered as \" hard \" images ) , objects in all of categories will have the pro ##ba ##bilities to attract the human attention and their corresponding locations are also hard for sal ##ien ##cy models to identify , which largely increases the ambiguity when considering to apply the obtained sal ##ient detection results to initial ##izing the training samples for wo ##d . thus , it is unreliable to directly apply sal ##ien ##cy detection to wo ##d . to alleviate this problem , we propose to bridge sal ##ien ##cy detection to wo ##d via a self - paced curriculum learning ( sp ##cl ) regime . sp ##cl was proposed in as a general learning framework including both the curriculum learning ( cl ) and self - paced learning ( sp ##l ) components . to the best of our knowledge , both of these two learning components are critical in successfully br ##id ##ging sal ##ien ##cy detection to wo ##d , whereas none of the existing literature has explored them before . specifically , cl was proposed by [ reference ] , which is usually learned based on the learning priorities derived by pre ##de ##ter ##mined he ##uri ##stic ##s for particular problems . sp ##l was proposed by [ reference ] , where the learning pace is dynamic ##ally generated by the learn ##er itself , according to which the learn ##er has already learned from the data . thus , the cl and sp ##l components in sp ##cl can be corresponding ##ly used to solve the training sample initial ##ization and object detector up ##dating problems in the proposed sal ##ien ##cy - guided wo ##d . to implement sp ##cl for our task , we first design a task - specific curriculum to assign the \" easy \" images with larger priority than the \" hard \" images during the learning procedure , which indicates that only the sal ##ient object h ##yp ##oth ##eses in the \" easy \" images are selected as the initial training samples , while the object h ##yp ##oth ##eses in the \" hard \" images will be gradually involved in the subsequent learning iteration ##s . to guide the learn ##er to gradually learn faithful knowledge of multi - class objects from the \" easy \" ( high - confidence ) images to the \" hard \" ( high - ambiguity ) ones , a novel self - paced learning regular ##izer is proposed to enforce the learn ##er to select confident and diverse training h ##yp ##oth ##eses in each iteration and learn the object detectors of multiple categories simultaneously . finally , the proposed sp ##cl regime can fit well to solve the problems in this paper . compared with the sp ##cl model in , the learning regime proposed in this paper mainly has three differences : 1 ) we design a task - specific learning curriculum for br ##id ##ging sal ##ien ##cy detection and wo ##d effectively . 2 ) we introduce an additional term , the sample diversity term , in the self - paced regular ##izer to prevent the selected training h ##yp ##oth ##eses from drifting to a small collection of training images . 3 ) considering the late ##nt relationship among the multiple categories of co - occurring objects , we further general ##ize the sp ##l regime into multi - class formulation , which facilitates the learning system to penal ##ize ind ##is ##cr ##imi ##nat ##ive object h ##yp ##oth ##eses predicted as belonging to multiple object categories at the same time . to sum up , there are three - fold contributions in this paper : we comprehensive ##ly analyze the prospect and challenges in the idea of br ##id ##ging sal ##ien ##cy detection to wo ##d and propose an effective way to alleviate the problem , which achieve ##s the state - of - the - art detection performance under the weak supervision . we establish a novel sp ##cl regime containing both the task - specific learning curriculum and the data - driven self - learning pace . the regime is well formulated as a con ##cise optimization model . we incorporate sp ##cl with a sample diversity term and further general ##ize it to work in multi - class scenario . section : related works sal ##ien ##cy detection : most sal ##ien ##cy detection methods highlight the attractive image regions by exploring some bottom - up cues . as one frequently explored cue , local contrast [ reference ] [ reference ] is usually used in the sal ##ien ##cy detection models to highlight the image regions appearing differently with their spatial ##ly neighbor regions . another widely used bottom - up cue is the global contrast . being different from local contrast , global contrast [ reference ] [ reference ] is used to discover image regions which are unique in the entire image context . more recently , background prior becomes another important cue for sal ##ien ##cy detection . this kind of methods , e . g . , , assume that regions near image boundaries are probably backgrounds and detect sal ##ient regions as figure 1 : this figure illustrates the main idea of this paper . as can be seen , in the training images with weak labels , some of them ( in blue frame ) are labelled only containing one object category , which are considered as \" easy \" images for the sal ##ien ##cy detection methods . while others ( in pink frame ) labelled as containing multiple object categories are considered as \" hard \" images . due to the category free property of sal ##ien ##cy detection , the objects in \" easy \" images have larger confidence to be extracted correctly by the sal ##ien ##cy detection methods , whereas the objects in \" hard \" images can not be extracted successfully . to this end , we develop a novel self - paced curriculum learning paradigm to guide the learn ##er to gradually achieve the faithful knowledge of the multiple object categories from easy ( confident ) to hard ( ambiguous ) . calculating the contrast to these image boundary regions . as can be seen , the bottom - up cues explored by sal ##ien ##cy detection models are highly potential to provide helpful prior ##s to the object local ##izations in each image . weakly - supervised object detection : two key issues in wo ##d are 1 ) predict the potential object local ##izations and 2 ) learn the object detectors . some early wo ##d methods [ reference ] held the view that a better initial estimation of the object local ##izations is critical to this task as they can largely impact the subsequent learning process . thus , they explored different ways , e . g . intra - class similarity [ reference ] , inter - class variance [ reference ] , and distance mapping relation [ reference ] , to initial ##ize the training object h ##yp ##oth ##eses . later on , some recent wo ##d methods started to pay more attention to the optimization procedure designed for better training object detectors under the weak supervision . for example , [ reference ] [ reference ] proposed to smooth the object formulation to better obtain the optimal solutions . [ reference ] proposed to incorporate convex cluster ##ing in the learning procedure , which enforce ##s the local similarity of the selected h ##yp ##oth ##eses during optimization . essentially , both of the above mentioned problems are critical in wo ##d task . to this end , this paper proposes a novel sp ##cl model which explicitly en ##code both the former problem ( with the designed curriculum ) and the later ( with the self - paced regular ##izer ) into a unified formulation and handle both problems in a theoretically sound manner . section : self - paced ( curriculum ) learning : inspired by the learning process of humans / animals , the theory of self - paced ( or curriculum ) learning [ reference ] [ reference ] is proposed lately . the idea is to learn the model it ##erative ##ly from easy to complex samples in a self - paced fashion . by virtue of its general ##ity , the sp ##l theory has been widely applied to various tasks , such as multi - view cluster ##ing [ reference ] , multi - label propagation [ reference ] , multimedia event detection , and co - sal ##ien ##cy detection . more recently , introduced the pre - defined learning curriculum to the conventional self - paced learning regime which can take into account both the helpful prior knowledge known before training and the self - learning progress during training . inspired by this work , we design a task - specific learning curriculum and construct a unified sp ##cl model specifically for both the sal ##ien ##cy detection and wo ##d tasks , through which both can be naturally related . section : the proposed approach section : algorithm overview given k training images with weak labels consisting of c categories , we first extract the bound ##ing box object h ##yp ##oth ##eses and their corresponding feature representations from each image . denote the features of each hypothesis in the k th image as { , where , ( ) \u2208 [ 0 , 1 ] , indicating the labels and the real - valued importance weights of each hypothesis , respectively , which are unknown at the beginning and will be opt ##imi ##zed during the proposed learning regime . the aim of the proposed approach is to learn the object detectors { w , b } , where = { } , = { } , of c object categories from the weakly - labeled training images , and then use them to detect objects in the test images . specifically , we first design a simple yet effective curriculum to select the sal ##ient h ##yp ##oth ##eses in \" easy \" images as initial ##ization . then , the object detectors are trained and updated gradually under the guidance of the proposed self - paced learning strategy . finally , the obtained object detectors are used to detect the corresponding objects in the test images . the overall algorithm is shown in algorithm 1 . section : problem formulation given object h ##yp ##oth ##eses from the training images , we propose a simple yet effective curriculum to initial ##ize the learning procedure . specifically , we first obtain the \" easy \" images based on the number of weak labels of each image , i . e . , images weakly labelled as only containing one object category are considered as \" easy \" . then , for each \" easy \" image , we adopt an un ##su ##per ##vis ##ed sal ##ien ##cy detection method , i . e . , rc [ reference ] in this paper due to its efficiency , to generate the corresponding sal ##ien ##cy estimation . finally , the important weights v are initial ##ized as the intersection - over - union ( io ##u ) score between each hypothesis and the sal ##ient region . the h ##yp ##oth ##eses with weights larger than 0 are selected as the initial training h ##yp ##oth ##eses and their labels in y are set according to the label of the images containing them . afterwards , in order to gradually adapt the learn ##er from the \" easy \" domain to the \" hard \" domain and finally capture the faithful knowledge of the objects of interest , a novel self - paced learning regular ##izer is proposed as follows : + 1 ##\u2264 2 , enforce ##s that each hypothesis should belong to only one object category , or no class , i . e . , the background category . this constraint inherently penal ##izes the ind ##is ##cr ##imi ##nat ##ive object h ##yp ##oth ##eses , i . e . the h ##yp ##oth ##eses predicted to belong to multiple object categories , when calculating their importance weight in ( 2 ) . the third one , i . e . [UNK] , * ( ) + 1 ##\u2265 2 , means that for all object h ##yp ##oth ##eses located in the k th image , at least one should belong to the class which the image has been weakly ann ##ota ##ted . this will make the learned result finely comply with the prior knowledge . in the proposed sp ##cl regime , the self - paced capability is followed by the involvement of the sp ##l regular ##izer ( ) ; , with the following form : where , are the class - specific parameters imposed on the ea ##sin ##ess term and the diversity term , respectively . the negative l 1 - norm term is inherited from the conventional sp ##l [ reference ] , which favors selecting easy over complex h ##yp ##oth ##eses . if we om ##it the diversity term , i . e . let = 0 , the regular ##izer de ##gen ##erate ##s to the traditional hard sp ##l function proposed in [ reference ] , which conducts either 1 or 0 ( i . e . selected in training or not ) for the weight , ( ) imposed on hypothesis ( ) , by judging whether its loss value is smaller than the pace parameter or not . that is , a sample with smaller loss is taken as an easy sample and thus should be learned prefer ##ential ##ly and vice versa . another regular ##ization term favors selecting diverse h ##yp ##oth ##eses residing in more images . this can be easily understood by seeing that its negative leads to the group - wise sparse representation of v . contra ##ri ##wise , this diversity term should have a counter - effect to group - wise spa ##rs ##ity . that is , mini ##mi ##zing this diversity term tends to disperse non - zero elements of v over more images , and thus favors selecting more diverse h ##yp ##oth ##eses . consequently , this anti - group - spa ##rs ##ity representation is expected to realize the desired diversity . different from the commonly utilized l 2 , 1 norm , our utilized group - spa ##rs ##ity term is con ##cave , leading to the convex ##ity of its negative . this on one side sim ##pl ##ifies the designation of the solving strategy , and on the other hand well fits the previous ax ##iom ##atic definition for the sp ##l regular ##izer . section : optimization method the solution of ( 1 ) can be approximately attained by alternatively opt ##imi ##zing the involved parameters { w , b } , y and v as described in algorithm 1 . the optimization mainly contains following steps : object detectors up ##dating : opt ##imi ##ze object detector parameters { w , b } via one - vs - all sv ##m under fixed y and v . in this case , ( 1 ) de ##gen ##erate ##s to the following form : ; , , which can be equivalent ##ly reform ##ulated as solving the following sub - optimization problems for each c = 1 , 2 , \u2026 , c : , . this is a standard one - vs - all ( weighted ) sv ##m model [ reference ] . section : h ##yp ##oth ##eses label ##ling : opt ##imi ##ze under fixed { w , b } and v : the goal of this step is to learn the pseudo - labels of training h ##yp ##oth ##eses from the current object detectors . the model in this case can be reform ##ulated as : this problem can be equivalent ##ly deco ##mp ##osed into sub - problems with respect to each = 1 , [UNK] , , i . e . for each image , where c * is the weak labels of the k th image : [ reference ] indicates the labels of the h ##yp ##oth ##eses in the k th image . its global opt ##imum can be attained by algorithm 2 , which can be derived from the theorem in . h ##yp ##oth ##eses re - weight ##ing : opt ##imi ##ze v under fixed { w , b } and y : after up ##dating the pseudo - labels , we aim to renew the weights on all h ##yp ##oth ##eses to reflect their different importance to learning of the current decision surface . in this case , ( 1 ) de ##gen ##erate ##s to the following form : which is equivalent to independently solving the following sub - optimization problem for each = 1 , [UNK] , and = 1 , [UNK] , via : we can easily sim ##plify the above optimization problem as : this model is convex and according to , we can ap ##do ##pt an effective algorithm , i . e . , the algorithm 3 , for extract ##ing the global opt ##imum to it . section : experimental results section : experimental settings we evaluate our method on the pascal vo ##c 2007 data ##set [ reference ] which is widely used by the previous works . in our experiments , we follow the previous works [ reference ] [ reference ] [ reference ] [ reference ] to disc ##ard any images that only contain object instances marked as \" difficult \" or \" truncated \" during the training phase , while all the images in the vo ##c ##0 ##7 - test are used during the test phase . for fair comparison , we follow the standard vo ##c procedure [ reference ] and report average precision ( ap ) on the pascal vo ##c 2007 test split . being consistent with the recently proposed wo ##d methods [ reference ] [ reference ] [ reference ] , we apply selective search [ reference ] to generate around 1500 bound ##ing box object h ##yp ##oth ##eses in each image and adopt the cnn features [ reference ] pre - trained on the image ##net 2012 to represent each of the extracted object h ##yp ##oth ##eses . before training , and in ( 2 ) need to be set in advance . as suggested in , we set according to the number of the selected training h ##yp ##oth ##eses which is set to be 2 % of the total bound ##ing box windows extracted from the images weakly labelled as containing the c th object category . then , is set to be equal to empirical ##ly . section : comparison to the state - of - the - arts in this section , we evaluate the object detection performance of our framework by comparing it with 6 state - of - the - art wo ##d approaches which are pr [ reference ] , cc [ reference ] , md ##d [ reference ] , ll ##o [ reference ] , vp ##c [ reference ] , and m ##fm ##il [ reference ] . for quantitative comparison , we report the evaluation results in terms of the ap score in fig . 2 . as can be seen , the proposed approach obtain ##s the highest score of 29 . 96 on average . according to our analysis , the proposed approach can obtain significantly better results than md ##d and m ##fm ##il mainly due to the better feature representation , stronger sal ##ien ##cy prior , and more powerful learning scheme . compared with pr , cc , ll ##o , and vp ##c , the performance gain of the proposed approach mainly comes from the core insight of this paper , i . e . , developing property way to bridge sal ##ien ##cy detection to wo ##d , as we used the same feature representation with these methods . more specifically , compared with pr and cc , the performance gain of the proposed approach comes mainly from the idea to bridge sal ##ien ##cy detection to wo ##d because they only adopted weak prior ##s in their initial ##ization . compared with ll ##o and vp ##c , the performance gain of the proposed approach mainly comes from the proposed sp ##cl regime as these two methods also explored strong prior information for initial ##izing the training h ##yp ##oth ##eses in their framework ##s . some examples of the detection results are also shown in fig . 2 , which includes some successful cases , i . e . , the examples in the bus and cat categories , as well as some failure cases , i . e . , examples in the plant and chair categories . the successful cases subjective ##ly demonstrate the effectiveness of the proposed approach . for the failure cases , the main problem is that very limited number of images only contains the objects like plant and chair , leading to the insufficient training h ##yp ##oth ##eses in the initial ##ization stage . this problem can be solved by designing more proper learning curriculum for wo ##d in future works . section : model analysis to further analyze the proposed framework in this paper , we make more comprehensive evaluation ##s in this section by comparing with five baseline models as described in table 1 . the experimental results are shown in fig . 3 , from which we can see : 1 ) the performance gap between sal + sv ##m and ours demonstrates the importance to develop proper ways to bridge sal ##ien ##cy detection and wo ##d . 2 ) the experimental results of sal + sp ##l , sal + sp ##cl , and ours demonstrate the better performance of the proposed learning regime as compared with some existing self - paced ( curriculum ) learning regimes . 3 ) the performance gap between ours and ll ##o + sp ##cl * demonstrates the sal ##ien ##cy prior can provide more helpful information than the prior designed in ll ##o . 4 ) the performance gap between ll ##o + sp ##cl * and ll ##o indicates the better capability of the proposed learning regime as compared with the learning model in one state - of - the - art wo ##d framework . according to the above analysis , the key insight of this paper , i . e . , developing powerful learning regime , i . e . , the proposed sp ##cl , can better bridge sal ##ien ##cy detection to wo ##d and help the learn ##er to capture the faithful knowledge of the object categories under weak supervision , has been demonstrated comprehensive ##ly . section : conclusion in this paper , in order to address the challenging wo ##d problem , we proposed an effective framework to bridge sal ##ien ##cy detection to the investigated task based on a novel sp ##cl regime . the insight of this paper is that by developing powerful learning regime which contains both the task - specific learning curriculum and the data - driven self - learning pace , sal ##ien ##cy detection technique can be better leverage ##d to provide beneficial information for helping the learn ##er to capture the faithful knowledge of the object categories under weak supervision . experiments including comparisons to other state - of - the - arts and comprehensive analysis of the proposed framework on the bench ##mark data ##set have demonstrated the effectiveness of our approach . for the future work , inspired by [ reference ] , we plane to enable the proposed method to transfer the knowledge that has be captured to new concepts via novel regular ##izer ##s . . ll ##o baseline wo ##d framework [ reference ] . ll ##o + sp ##cl * replace the learning model , i . e . , sl ##s ##v ##m , in ll ##o with the proposed sp ##cl regime . section : section : ac ##k ##now ##led ##gm ##ents : this work was supported in part by the national science foundation of china under grants 61 ##52 ##22 ##0 ##7 and 61 ##47 ##32 ##31 , the doctorate foundation , and the excellent doctorate foundation of northwestern polytechnic ##al university . section :",
        "pred_seq": "[SEP] [SEP] [SEP] weakly detection [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "weakly supervised object detection"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "pascal voc 2007 dataset",
                        "pascal voc 2007 test split"
                    ]
                ],
                "Method": [
                    [
                        "selfpaced curriculum learning",
                        "spcl",
                        "spcl regime",
                        "spcl model",
                        "curriculum",
                        "selfpaced curriculum learning",
                        "selfpaced or curriculum learning",
                        "unified spcl model",
                        "rc",
                        "selfpaced curriculum learning regimes"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "weakly supervised object detection",
                        "weaklysupervised object detection",
                        "wod",
                        "wod tasks",
                        "object detection",
                        "wod systems",
                        "saliencyguided wod",
                        "wod task",
                        "detection",
                        "wod problem"
                    ]
                ]
            }
        ]
    },
    "3": {
        "doctext": "document : the ibm 2016 english conversation ##al telephone speech recognition system we describe a collection of acoustic and language modeling techniques that lowered the word error rate of our english conversation ##al telephone l ##vc ##sr system to a record 6 . 6 % on the switch ##board subset of the hub ##5 2000 evaluation tests ##et . on the acoustic side , we use a score fusion of three strong models : rec ##urrent nets with max ##out activation ##s , very deep con ##vo ##lu ##tion ##al nets with 3 ##x ##3 kernel ##s , and bid ##ire ##ction ##al long short - term memory nets which operate on fm ##ll ##r and i - vector features . on the language modeling side , we use an updated model \" m \" and hierarchical neural network l ##ms . georges ##ao ##n , tom ##ser ##cu , steven ##ren ##nie ##and ##hong - kwan ##g ##j . ku ##o , york ##town ##hei ##ght ##s , ny , 105 ##9 ##8 gs ##ao ##n @ us . ibm . com ##\u00a9 ieee ##200 ##4 , pleased ##ono ##tre ##dis ##tri ##bu ##te index terms : rec ##urrent neural networks , con ##vo ##lu ##tion ##al neural networks , conversation ##al speech recognition section : introduction the landscape of neural network acoustic modeling is rapidly evolving . spurred by the success of deep feed - forward neural nets for l ##vc ##sr in and inspired by other research areas like image classification and natural language processing , many speech groups have looked at more sophisticated architecture ##s such as deep con ##vo ##lu ##tion ##al nets , deep rec ##urrent nets , time - delay neural nets , and long - short term memory nets . the trend is to remove a lot of the complexity and human knowledge that was necessary in the past to build good as ##r systems ( e . g . speaker adaptation , phonetic context modeling , disc ##rim ##ina ##tive feature processing , etc . ) and to replace them with a powerful neural network architecture that can be trained ag ##nostic ##ally on a lot of data . with the advent of numerous neural network tool ##kit ##s which can implement these sophisticated models out - of - the - box and powerful hardware based on gp ##us , the barrier of entry for building high performing as ##r systems has been lowered considerably . first case in point : front - end processing has been simplified considerably with the use of cnn ##s which treat the log - mel spectral representation as an image and do n ' t require extra processing steps such as pl ##p ce ##ps ##tra , ld ##a , fm ##ll ##r , fm ##pe transforms , etc . second case in point : end - to - end as ##r systems such as bypass the need of having phonetic context decision trees and hmm ##s altogether and directly map the sequence of acoustic features to a sequence of characters or context independent phones . third case in point : training algorithms such as connection ##ist temporal classification do n ' t require an initial alignment of the training data which is typically done with a gm ##m - based baseline model . the above points beg the question whether , in this age of readily available n ##n tool ##kit ##s , speech recognition expertise is still necessary or whether one can simply point a neural net to the audio and transcript ##s , let it train , and obtain a good acoustic model . while it is true that , as the amount of training data increases , the need for human as ##r expertise is less ##ened , at the moment the performance of end - to - end systems ultimately remains inferior to that of more traditional , i . e . hmm and decision tree - based , approaches . since the goal of this work is to obtain the lowest possible we ##r on the switch ##board data ##set regardless of other practical considerations such as speed and / or simplicity , we have focused on the latter approaches . the paper is organized as follows : in section [ reference ] we discuss acoustic and language modeling improvements and in section [ reference ] we sum ##mar ##ize our findings . section : system improvements in this section we describe three different acoustic models that were trained on 2000 hours of english conversation ##al telephone speech : rec ##urrent nets with max ##out activation ##s and anne ##ale ##d drop ##out , very deep con ##vo ##lu ##tion ##al nets with 3 3 kernel ##s , and bid ##ire ##ction ##al long short - term memory nets operating on fm ##ll ##r and i - vector features . all models are used in a hybrid hmm deco ##ding scenario by sub ##tra ##cting the log ##ari ##th ##m of the hmm state prior ##s from the log of the soft ##max output scores . the training and test data , front ##end processing , speaker adaptation are identical to and their description will be omitted . at the end of the section , we also provide an update on our vocabulary and language modeling experiments . sub ##section : rec ##urrent nets with max ##out activation ##s we remind the reader that max ##out nets general ##ize re ##lu units by employing non - linear ##ities of the form where the subset ##s of neurons are typically di ##s ##jo ##int . in we have shown that max ##out d ##nn ##s and cnn ##s trained with anne ##ale ##d drop ##out out ##per ##form their si ##gm ##oid - based counterparts on both 300 hours and 2000 hours training regimes . what was missing there was a comparison between max ##out and si ##gm ##oid for unfolded rn ##ns . the architecture of the max ##out rn ##ns comprises one rec ##urrent layer with 282 ##8 units projected to 141 ##4 units via non - overlapping max ##out operations . this layer is followed by 4 non - rec ##urrent layers with 282 ##8 units ( also projected to 141 ##4 ) followed by a bottle ##neck with 102 ##4 512 units and an output layer with 320 ##00 neurons corresponding to as many context - dependent hmm states . the number of neurons for the max ##out layers have been chosen such that the weight matrices have roughly the same number of parameters as the baseline si ##gm ##oid network which has 204 ##8 units per hidden layer . the rec ##urrent layer is unfolded backwards in time for 6 time steps and has 340 - dimensional inputs consisting of 6 sp ##lice ##d right context 40 - dimensional fm ##ll ##r frames ( ) to which we app ##end a 100 - dimensional speaker - based iv ##ect ##or . the unfolded max ##out rn ##n architecture is depicted in figure [ reference ] . the network is trained one hidden layer at a time with disc ##rim ##ina ##tive pre ##train ##ing followed by 12 epoch ##s of sg ##d ce training on random ##ized mini ##bat ##ches of 250 samples . the model is refined with hess ##ian - free sequence disc ##rim ##ina ##tive training using the state - based mb ##r criterion for 10 iteration ##s . in table [ reference ] we report the error rates for si ##gm ##oid and max ##out rn ##ns on the switch ##board and call ##hom ##e subset ##s of hub ##5 ' 00 . the deco ##ding ##s are done with a small vocabulary of 30 k words and a small 4 - gram language model with 4 m n - grams . note that the si ##gm ##oid rn ##ns have better error rates than what was reported in because they have been re ##train ##ed after the data has been real ##ign ##ed with the best joint rn ##n / cnn model . we observe that the max ##out rn ##ns are consistently better and that , by themselves , they achieve a similar we ##r as our previous best model which was the joint rn ##n / cnn with si ##gm ##oid activation ##s . sub ##section : very deep con ##vo ##lu ##tion ##al networks very deep cnn ##s with small kernel ##s have recently been shown to achieve very strong performance as acoustic models in hybrid n ##n - hmm speech recognition systems . results were provided after cross - entropy training on the 300 hours switch ##board - 1 data ##set in , and results from sequence training on both switch ##board - 1 and the 2000 hours switch ##board + fisher data ##set are in . the very deep con ##vo ##lu ##tion ##al networks are inspired by the \" v ##gg net \" architecture introduced in for the 2014 image ##net classification challenge , with the central idea to replace large con ##vo ##lu ##tion ##al kernel ##s by small kernel ##s . by stack ##ing many of these con ##vo ##lu ##tion ##al layers with re ##lu nonlinear ##ities before pool ##ing layers , the same rec ##eptive field is created with less parameters and more nonlinear ##ity . figure [ reference ] shows the design of the networks . note that as we go deeper in the network , the time and frequency resolution is reduced through pool ##ing only , while the con ##vo ##lu ##tions are zero - padded as to not reduce the size of the feature maps . we increase the number of feature maps gradually from 64 to 512 ( indicated by the different colors ) . we pool right before the layer that increases the number of feature maps . note that the indication of feature map size on the right only applies to the right ##most 2 designs . in contrast , the classical cnn architecture has only two layers , goes to 512 feature maps directly , and uses a large kernel on the first layer . our 10 - layer cnn has about the same number of parameters as the classical cnn , converge ##s in 5 times fewer epoch ##s , but is computational ##ly more expensive . results for 3 variations of the 10 - layer cnn are in table [ reference ] . for model combination , we use the version with pool ##ing , which is the exact same model without modifications from the original paper . our implementation was done in torch . we adopt the balanced sampling from , by sampling from context dependent state with probability . we keep throughout the experiments during cross - entropy training . during ce training , we opt ##imi ##ze with simple sg ##d or na ##g , during st we found na ##g to be superior to sg ##d . we regular ##ize the st ##och ##astic sequence training by adding the gradient of cross - entropy loss , as proposed in . sub ##section : bid ##ire ##ction ##al l ##st ##ms given the recent popularity of l ##st ##ms for acoustic modeling , we have experimented with such models on the switch ##board task using the torch tool ##kit . we have looked at the effect of the input features on l ##st ##m performance , the number of layers and whether start states for the rec ##urrent layers should be reset or carried over . we use bid ##ire ##ction ##al l ##st ##ms that are trained on non - overlapping sub ##se ##que ##nce ##s of 20 frames . the sub ##se ##que ##nce ##s coming from the same utter ##ance are contiguous so that the left - to - right final states for the current sub ##se ##que ##nce can be copied to the left - to - right start states for the next sub ##se ##que ##nce ( i . e . carried over ) . for processing speed and in order to get good gradient estimates , we group sub ##se ##que ##nce ##s from multiple utter ##ances into mini ##bat ##ches of size 256 . regardless of the number of l ##st ##m layers , all models use a linear bottle ##neck of size 256 before the soft ##max output layer ( of size 320 ##00 ) . in one experiment , we compare the effect of input features on model performance . the baseline models are trained on 40 - dimensional fm ##ll ##r + 100 - dimensional iv ##ect ##or frames and have 102 ##4 ( or 512 ) l ##st ##m units per layer and per direction ( left - to - right and right - to - left ) . the forward and backward activation ##s from the previous l ##st ##m layer are con ##cate ##nated and fed into the next l ##st ##m layer . the contrast model is a single layer bid ##ire ##ction ##al l ##st ##m trained on 128 - dim features obtained by performing pc ##a on 512 - dimensional bottle ##neck features . the features are obtained from a 6 - layer d ##nn cross entropy trained on blocks of 11 consecutive fm ##ll ##r frames and 100 - dimensional i - vectors . in table [ reference ] , we report recognition results on hub ##5 ' 00 for these four models trained with 15 passes of cross - entropy sg ##d on the 300 hour ( sw ##b - 1 ) subset . due to a bug that affected our earlier multi - layer l ##st ##m results , we decided to go ahead with single layer bid ##ire ##ction ##al l ##st ##ms on bottle ##neck features on the full 2000 hour training set . we also experimented with how to deal with the start states at the beginning of the left - to - right pass . one option is to carry them over from the previous sub ##se ##que ##nce and the other one is to reset the start states at the beginning of each sub ##se ##que ##nce . in figure [ reference ] we compare the cross - entropy loss on held - out data between these two models . as can be seen , the l ##st ##m model with carried over start states is much better at predicting the correct hmm state . however , when comparing word error rates in table [ reference ] , the l ##st ##m with start states that are reset has a better performance . we sur ##mise that this is because the increased memory of the l ##st ##m with carried over start states is in conflict with the state sequence constraints imposed by the hmm topology and the language model . additionally , we show the we ##rs of the d ##nn used for the bottle ##neck features and of a 4 - layer 512 unit l ##st ##m . we observe that the 4 layer l ##st ##m is significantly better than the d ##nn and the two single layer l ##st ##ms trained on bottle ##neck features . sub ##section : model combination in table [ reference ] we report the performance of the individual models ( rn ##n , v ##gg and 4 - layer l ##st ##m ) described in the previous sub ##section ##s as well as the results after frame - level score fusion . all deco ##ding ##s are done with a 30 k word vocabulary and a small 4 - gram language model with 4 m n - grams . we note that rn ##ns and v ##gg nets exhibit similar performance and have a strong complement ##ari ##ty which improves the we ##r by 0 . 6 % and 0 . 9 % on sw ##b and ch , respectively . sub ##section : language modeling experiments our language modeling strategy largely parallels that described in . for complete ##ness , we will repeat some of the details here . the main difference is an increase in the vocabulary size from 30 k words to 85 k words . when comparing acoustic models in previous sections , we used a relatively small legacy language model used in previous publications : a 4 m n - gram ( n = 4 ) language model with a vocabulary of 30 . 5 k words . we wanted to increase the language model coverage in a manner that others can replicate . to this end , we increased the vocabulary size from 30 . 5 k words to 85 k words by adding the vocabulary of the publicly available broadcast news task . we also added to the l ##m publicly available text data from ld ##c , including switch ##board , fisher , gig ##aw ##ord , and broadcast news and conversations . the most relevant data are the transcript ##s of the 1975 hour audio data used to train the acoustic model , consisting of about 24 m words . for each corpus we trained a 4 - gram model with modified kn ##ese ##r - ne ##y smoothing . the component l ##ms are linear ##ly inter ##pol ##ated with weights chosen to opt ##imi ##ze per ##plex ##ity on a held - out set . entropy pr ##uni ##ng was applied , resulting in a single 4 - gram l ##m consisting of 36 m n - grams . this new n - gram l ##m was used together with our best acoustic model to deco ##de and generate word lattice ##s for l ##m res ##cor ##ing experiments . the first two lines of table [ reference ] show the improvement using this larger n - gram l ##m with larger vocabulary trained on more data . the we ##r improved by 1 . 0 % for sw ##b . part of this improvement ( 0 . 1 - 0 . 2 % ) was due to also using a larger beam for deco ##ding and a change in vocabulary token ##ization . we used two types of l ##ms for l ##m res ##cor ##ing : model m , a class - based exponential model and feed - forward neural network l ##m ( n ##nl ##m ) . we built a model m l ##m on each corpus and inter ##pol ##ated the models , together with the 36 m n - gram l ##m . as shown in table [ reference ] , using model m results in an improvement of 0 . 6 % on sw ##b . we built two n ##nl ##ms for inter ##pol ##ation . one was trained on just the most relevant data : the 24 m word corpus ( switch ##board / fisher / call ##hom ##e acoustic transcript ##s ) . another was trained on a 560 m word subset of the l ##m training data : in order to speed up training for this larger set , we employed a hierarchical n ##nl ##m approximation . table [ reference ] shows that the n ##nl ##ms provided an additional 0 . 4 % improvement over the model m result on sw ##b . compared with the n - gram l ##m baseline , l ##m res ##cor ##ing yielded a total improvement of 1 . 0 % on sw ##b ( 7 . 6 % to 6 . 6 % ) and 1 . 5 % on ch ( 13 . 7 % to 12 . 2 % ) . section : conclusion in our previous switch ##board system paper we have observed a good complement ##ari ##ty between rec ##urrent nets and con ##vo ##lu ##tion ##al nets and their combination led to significant accuracy gains . in this paper we have presented an improved unfolded rn ##n ( with max ##out instead of si ##gm ##oid activation ##s ) and a stronger cnn obtained by adding more con ##vo ##lu ##tion ##al layers with smaller kernel ##s and re ##lu nonlinear ##ities . these improved models still have good complement ##ari ##ty and their frame - level score combination in conjunction with a multi - layer l ##st ##m leads to a 0 . 4 % - 0 . 7 % decrease in we ##r over the l ##st ##m . multi - layer l ##st ##ms were the strongest performing model followed closely by the rn ##n and v ##gg nets . we also believe that l ##st ##ms have more potential for direct sequence - to - sequence modeling and we are actively exploring this area of research . on the language modeling side , we have increased our vocabulary from 30 k to 85 k words and updated our component l ##ms . at the moment , we are less than 3 % away from achieving human performance on the switch ##board data ( estimated to be around 4 % ) . unfortunately , it looks like future improvements on this task will be considerably harder to get and will probably require a breakthrough in direct sequence - to - sequence modeling and a significant increase in training data . section : ac ##k ##now ##led ##gm ##ent the authors wish to thank e . marche ##ret , j . cu ##i and m . nu ##ss ##baum - thom for useful suggestions about l ##st ##ms . bibliography : references",
        "pred_seq": "hub ##et [SEP] word rate [SEP] word rate [SEP] [unused0] switch subset ##et [SEP] word rate [SEP] word rate [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "hub5 2000 evaluation testset"
                    ]
                ],
                "Method": [
                    [
                        "word error rate"
                    ]
                ],
                "Metric": [
                    [
                        "word error rate"
                    ]
                ],
                "Task": []
            },
            {
                "Material": [
                    [
                        "switchboard subset"
                    ]
                ],
                "Method": [
                    [
                        "word error rate"
                    ]
                ],
                "Metric": [
                    [
                        "word error rate"
                    ]
                ],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "switchboard subset",
                        "switchboard dataset",
                        "switchboard",
                        "hub500",
                        "switchboard task",
                        "switchboard data"
                    ]
                ],
                "Method": [
                    [
                        "m ngram lm"
                    ]
                ],
                "Metric": [
                    [
                        "word error rate",
                        "error rates",
                        "word error rates"
                    ]
                ],
                "Task": [
                    [
                        "ibm 2016 english conversational telephone speech recognition system",
                        "conversational speech recognition",
                        "speech recognition expertise",
                        "hybrid nnhmm speech recognition systems",
                        "recognition",
                        "ch"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "swb"
                    ]
                ],
                "Method": [
                    [
                        "m ngram lm"
                    ]
                ],
                "Metric": [
                    [
                        "word error rate",
                        "error rates",
                        "word error rates"
                    ]
                ],
                "Task": [
                    [
                        "ibm 2016 english conversational telephone speech recognition system",
                        "conversational speech recognition",
                        "speech recognition expertise",
                        "hybrid nnhmm speech recognition systems",
                        "recognition",
                        "ch"
                    ]
                ]
            }
        ]
    },
    "4": {
        "doctext": "document : re ##thi ##nk ##ing the inception architecture for computer vision con ##vo ##lu ##tion ##al networks are at the core of most state - of - the - art computer vision solutions for a wide variety of tasks . since 2014 very deep con ##vo ##lu ##tion ##al networks started to become mainstream , yielding substantial gains in various bench ##marks . although increased model size and computational cost tend to translate to immediate quality gains for most tasks ( as long as enough labeled data is provided for training ) , computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big - data scenarios . here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suit ##ably factor ##ized con ##vo ##lu ##tions and aggressive regular ##ization . we bench ##mark our methods on the il ##s ##vr ##c 2012 classification challenge validation set demonstrate substantial gains over the state of the art : top - and top - error for single frame evaluation using a network with a computational cost of billion multi ##ply - adds per inference and with using less than 25 million parameters . with an ensemble of models and multi - crop evaluation , we report top - error and top - error . section : introduction since the 2012 image ##net competition winning entry by k ##riz ##he ##vsky et al , their network \" alex ##net \" has been successfully applied to a larger variety of computer vision tasks , for example to object - detection , segment ##ation , human pose estimation , video classification , object tracking , and super ##res ##ol ##ution . these successes spurred a new line of research that focused on finding higher performing con ##vo ##lu ##tion ##al neural networks . starting in 2014 , the quality of network architecture ##s significantly improved by utilizing deeper and wider networks . v ##gg ##net and google ##net yielded similarly high performance in the 2014 il ##s ##vr ##c classification challenge . one interesting observation was that gains in the classification performance tend to transfer to significant quality gains in a wide variety of application domains . this means that architectural improvements in deep con ##vo ##lu ##tion ##al architecture can be utilized for improving performance for most other computer vision tasks that are increasingly re ##lian ##t on high quality , learned visual features . also , improvements in the network quality resulted in new application domains for con ##vo ##lu ##tion ##al networks in cases where alex ##net features could not compete with hand engineered , crafted solutions , e . g . proposal generation in detection . although v ##gg ##net has the compelling feature of architectural simplicity , this comes at a high cost : evaluating the network requires a lot of computation . on the other hand , the inception architecture of google ##net was also designed to perform well even under strict constraints on memory and computational budget . for example , google ##net employed only 5 million parameters , which represented a reduction with respect to its predecessor alex ##net , which used million parameters . furthermore , v ##gg ##net employed about 3 ##x more parameters than alex ##net . the computational cost of inception is also much lower than v ##gg ##net or its higher performing successors . this has made it feasible to utilize inception networks in big - data scenarios , , where huge amount of data needed to be processed at reasonable cost or scenarios where memory or computational capacity is inherently limited , for example in mobile vision settings . it is certainly possible to mit ##igate parts of these issues by applying specialized solutions to target memory use , or by opt ##imi ##zing the execution of certain operations via computational tricks . however , these methods add extra complexity . furthermore , these methods could be applied to opt ##imi ##ze the inception architecture as well , widening the efficiency gap again . still , the complexity of the inception architecture makes it more difficult to make changes to the network . if the architecture is scaled up naive ##ly , large parts of the computational gains can be immediately lost . also , does not provide a clear description about the contributing factors that lead to the various design decisions of the google ##net architecture . this makes it much harder to adapt it to new use - cases while maintaining its efficiency . for example , if it is deemed necessary to increase the capacity of some inception - style model , the simple transformation of just doubling the number of all filter bank sizes will lead to a 4 ##x increase in both computational cost and number of parameters . this might prove prohibit ##ive or unreasonable in a lot of practical scenarios , especially if the associated gains are modest . in this paper , we start with describing a few general principles and optimization ideas that that proved to be useful for scaling up con ##vo ##lu ##tion networks in efficient ways . although our principles are not limited to inception - type networks , they are easier to observe in that context as the generic structure of the inception style building blocks is flexible enough to incorporate those constraints naturally . this is enabled by the generous use of dimensional reduction and parallel structures of the inception modules which allows for mit ##iga ##ting the impact of structural changes on nearby components . still , one needs to be cautious about doing so , as some guiding principles should be observed to maintain high quality of the models . section : general design principles here we will describe a few design principles based on large - scale experimentation with various architectural choices with con ##vo ##lu ##tion ##al networks . at this point , the utility of the principles below are speculative and additional future experimental evidence will be necessary to assess their accuracy and domain of validity . still , grave deviation ##s from these principles tended to result in deterioration in the quality of the networks and fixing situations where those deviation ##s were detected resulted in improved architecture ##s in general . avoid representation ##al bottle ##neck ##s , especially early in the network . feed - forward networks can be represented by an ac ##y ##cl ##ic graph from the input layer ( s ) to the class ##ifier or reg ##ress ##or . this defines a clear direction for the information flow . for any cut separating the inputs from the outputs , one can access the amount of information passing though the cut . one should avoid bottle ##neck ##s with extreme compression . in general the representation size should gently decrease from the inputs to the outputs before reaching the final representation used for the task at hand . theoretically , information content can not be assessed merely by the dimensional ##ity of the representation as it disc ##ards important factors like correlation structure ; the dimensional ##ity merely provides a rough estimate of information content . higher dimensional representations are easier to process locally within a network . increasing the activation ##s per tile in a con ##vo ##lu ##tion ##al network allows for more di ##sen ##tangled features . the resulting networks will train faster . spatial aggregation can be done over lower dimensional em ##bed ##ding ##s without much or any loss in representation ##al power . for example , before performing a more spread out ( e . g . ) con ##vo ##lu ##tion , one can reduce the dimension of the input representation before the spatial aggregation without expecting serious adverse effects . we h ##yp ##oth ##es ##ize that the reason for that is the strong correlation between adjacent unit results in much less loss of information during dimension reduction , if the outputs are used in a spatial aggregation context . given that these signals should be easily com ##press ##ible , the dimension reduction even promotes faster learning . balance the width and depth of the network . optimal performance of the network can be reached by balancing the number of filters per stage and the depth of the network . increasing both the width and the depth of the network can contribute to higher quality networks . however , the optimal improvement for a constant amount of computation can be reached if both are increased in parallel . the computational budget should therefore be distributed in a balanced way between the depth and width of the network . although these principles might make sense , it is not straightforward to use them to improve the quality of networks out of box . the idea is to use them ju ##dic ##iously in ambiguous situations only . section : factor ##izing con ##vo ##lu ##tions with large filter size much of the original gains of the google ##net network arise from a very generous use of dimension reduction . this can be viewed as a special case of factor ##izing con ##vo ##lu ##tions in a computational ##ly efficient manner . consider for example the case of a con ##vo ##lu ##tion ##al layer followed by a con ##vo ##lu ##tion ##al layer . in a vision network , it is expected that the outputs of near - by activation ##s are highly correlated . therefore , we can expect that their activation ##s can be reduced before aggregation and that this should result in similarly expressive local representations . here we explore other ways of factor ##izing con ##vo ##lu ##tions in various settings , especially in order to increase the computational efficiency of the solution . since inception networks are fully con ##vo ##lu ##tion ##al , each weight corresponds to one multiplication per activation . therefore , any reduction in computational cost results in reduced number of parameters . this means that with suitable factor ##ization , we can end up with more di ##sen ##tangled parameters and therefore with faster training . also , we can use the computational and memory savings to increase the filter - bank sizes of our network while maintaining our ability to train each model replica on a single computer . sub ##section : factor ##ization into smaller con ##vo ##lu ##tions con ##vo ##lu ##tions with larger spatial filters ( e . g . or ) tend to be di ##sp ##rop ##ort ##ional ##ly expensive in terms of computation . for example , a con ##vo ##lu ##tion with filters over a grid with filters is 25 / 9 = 2 . 78 times more computational ##ly expensive than a con ##vo ##lu ##tion with the same number of filters . of course , a filter can capture depend ##encies between signals between activation ##s of units further away in the earlier layers , so a reduction of the geometric size of the filters comes at a large cost of expressive ##ness . however , we can ask whether a con ##vo ##lu ##tion could be replaced by a multi - layer network with less parameters with the same input size and output depth . if we zoom into the computation graph of the con ##vo ##lu ##tion , we see that each output looks like a small fully - connected network sliding over tiles over its input ( see figure [ reference ] ) . since we are constructing a vision network , it seems natural to exploit translation in ##var ##iance again and replace the fully connected component by a two layer con ##vo ##lu ##tion ##al architecture : the first layer is a con ##vo ##lu ##tion , the second is a fully connected layer on top of the output grid of the first layer ( see figure [ reference ] ) . sliding this small network over the input activation grid boil ##s down to replacing the con ##vo ##lu ##tion with two layers of con ##vo ##lu ##tion ( compare figure [ reference ] with [ reference ] ) . this setup clearly reduces the parameter count by sharing the weights between adjacent tiles . to analyze the expected computational cost savings , we will make a few sim ##plify ##ing assumptions that apply for the typical situations : we can assume that , that is that we want to change the number of activation ##s / unit by a constant alpha factor . since the con ##vo ##lu ##tion is ag ##gre ##gating , is typically slightly larger than one ( around 1 . 5 in the case of google ##net ) . having a two layer replacement for the layer , it seems reasonable to reach this expansion in two steps : increasing the number of filters by in both steps . in order to sim ##plify our estimate by choosing ( no expansion ) , if we would na ##iv ##ly slide a network without re ##using the computation between neighboring grid tiles , we would increase the computational cost . sliding this network can be represented by two con ##vo ##lu ##tion ##al layers which re ##uses the activation ##s between adjacent tiles . this way , we end up with a net reduction of computation , resulting in a relative gain of by this factor ##ization . the exact same saving holds for the parameter count as each parameter is used exactly once in the computation of the activation of each unit . still , this setup raises two general questions : does this replacement result in any loss of expressive ##ness ? if our main goal is to factor ##ize the linear part of the computation , would it not suggest to keep linear activation ##s in the first layer ? we have ran several control experiments ( for example see figure [ reference ] ) and using linear activation was always inferior to using rec ##ti ##fied linear units in all stages of the factor ##ization . we attribute this gain to the enhanced space of variations that the network can learn especially if we batch - normal ##ize the output activation ##s . one can see similar effects when using linear activation ##s for the dimension reduction components . sub ##section : spatial factor ##ization into as ##ym ##metric con ##vo ##lu ##tions the above results suggest that con ##vo ##lu ##tions with filters larger a might not be generally useful as they can always be reduced into a sequence of con ##vo ##lu ##tion ##al layers . still we can ask the question whether one should factor ##ize them into smaller , for example con ##vo ##lu ##tions . however , it turns out that one can do even better than by using as ##ym ##metric con ##vo ##lu ##tions , e . g . . for example using a con ##vo ##lu ##tion followed by a con ##vo ##lu ##tion is equivalent to sliding a two layer network with the same rec ##eptive field as in a con ##vo ##lu ##tion ( see figure [ reference ] ) . still the two - layer solution is cheaper for the same number of output filters , if the number of input and output filters is equal . by comparison , factor ##izing a con ##vo ##lu ##tion into a two con ##vo ##lu ##tion represents only a saving of computation . . in theory , we could go even further and argue that one can replace any con ##vo ##lu ##tion by a con ##vo ##lu ##tion followed by a con ##vo ##lu ##tion and the computational cost saving increases dramatically as grows ( see figure 6 ) . in practice , we have found that employing this factor ##ization does not work well on early layers , but it gives very good results on medium grid - sizes ( on feature maps , where ranges between and ) . on that level , very good results can be achieved by using con ##vo ##lu ##tions followed by con ##vo ##lu ##tions . section : utility of auxiliary class ##ifiers has introduced the notion of auxiliary class ##ifiers to improve the convergence of very deep networks . the original motivation was to push useful gradient ##s to the lower layers to make them immediately useful and improve the convergence during training by combat ##ing the vanishing gradient problem in very deep networks . also lee et al argues that auxiliary class ##ifiers promote more stable learning and better convergence . interesting ##ly , we found that auxiliary class ##ifiers did not result in improved convergence early in the training : the training progression of network with and without side head looks virtually identical before both models reach high accuracy . near the end of training , the network with the auxiliary branches starts to over ##take the accuracy of the network without any auxiliary branch and reaches a slightly higher plateau . also used two side - heads at different stages in the network . the removal of the lower auxiliary branch did not have any adverse effect on the final quality of the network . together with the earlier observation in the previous paragraph , this means that original the hypothesis of that these branches help evolving the low - level features is most likely mis ##placed . instead , we argue that the auxiliary class ##ifiers act as regular ##izer . this is supported by the fact that the main class ##ifier of the network performs better if the side branch is batch - normal ##ized or has a drop ##out layer . this also gives a weak supporting evidence for the conjecture that batch normal ##ization acts as a regular ##izer . section : efficient grid size reduction traditionally , con ##vo ##lu ##tion ##al networks used some pool ##ing operation to decrease the grid size of the feature maps . in order to avoid a representation ##al bottle ##neck , before applying maximum or average pool ##ing the activation dimension of the network filters is expanded . for example , starting a grid with filters , if we would like to arrive at a grid with filters , we first need to compute a stride - 1 con ##vo ##lu ##tion with filters and then apply an additional pool ##ing step . this means that the overall computational cost is dominated by the expensive con ##vo ##lu ##tion on the larger grid using operations . one possibility would be to switch to pool ##ing with con ##vo ##lu ##tion and therefore resulting in reducing the computational cost by a quarter . however , this creates a representation ##al bottle ##neck ##s as the overall dimensional ##ity of the representation drops to resulting in less expressive networks ( see figure [ reference ] ) . instead of doing so , we suggest another variant the reduces the computational cost even further while removing the representation ##al bottle ##neck . ( see figure [ reference ] ) . we can use two parallel stride 2 blocks : and . is a pool ##ing layer ( either average or maximum pool ##ing ) the activation , both of them are stride the filter banks of which are con ##cate ##nated as in figure [ reference ] . section : inception - v ##2 here we are connecting the dots from above and propose a new architecture with improved performance on the il ##s ##vr ##c 2012 classification bench ##mark . the layout of our network is given in table [ reference ] . note that we have factor ##ized the traditional con ##vo ##lu ##tion into three con ##vo ##lu ##tions based on the same ideas as described in section [ reference ] . for the inception part of the network , we have traditional inception modules at the with filters each . this is reduced to a grid with filters using the grid reduction technique described in section [ reference ] . this is is followed by instances of the factor ##ized inception modules as depicted in figure [ reference ] . this is reduced to a grid with the grid reduction technique depicted in figure [ reference ] . at the coarse ##st level , we have two inception modules as depicted in figure [ reference ] , with a con ##cate ##nated output filter bank size of 204 ##8 for each tile . the detailed structure of the network , including the sizes of filter banks inside the inception modules , is given in the supplementary material , given in the model . tx ##t that is in the tar - file of this submission . however , we have observed that the quality of the network is relatively stable to variations as long as the principles from section [ reference ] are observed . although our network is layers deep , our computation cost is only about higher than that of google ##net and it is still much more efficient than v ##gg ##net . section : model regular ##ization via label smoothing here we propose a mechanism to regular ##ize the class ##ifier layer by est ##imating the marginal ##ized effect of label - drop ##out during training . for each training example , our model compute ##s the probability of each label : . here , are the log ##its or un ##nor ##mal ##ized log - pro ##ba ##bilities . consider the ground - truth distribution over labels for this training example , normal ##ized so that . for br ##ev ##ity , let us om ##it the dependence of and on example . we define the loss for the example as the cross entropy : . mini ##mi ##zing this is equivalent to maxim ##izing the expected log - likelihood of a label , where the label is selected according to its ground - truth distribution . cross - entropy loss is different ##iable with respect to the log ##its and thus can be used for gradient training of deep models . the gradient has a rather simple form : , which is bounded between and . consider the case of a single ground - truth label , so that and for all . in this case , mini ##mi ##zing the cross entropy is equivalent to maxim ##izing the log - likelihood of the correct label . for a particular example with label , the log - likelihood is maximize ##d for , where is dir ##ac delta , which equals for and otherwise . this maximum is not ac ##hi ##eva ##ble for finite but is approached if for all - that is , if the log ##it corresponding to the ground - truth label is much great than all other log ##its . this , however , can cause two problems . first , it may result in over - fitting : if the model learns to assign full probability to the ground - truth label for each training example , it is not guaranteed to general ##ize . second , it encourages the differences between the largest log ##it and all others to become large , and this , combined with the bounded gradient , reduces the ability of the model to adapt . intuitive ##ly , this happens because the model becomes too confident about its predictions . we propose a mechanism for encouraging the model to be less confident . while this may not be desired if the goal is to maximize the log - likelihood of training labels , it does regular ##ize the model and makes it more adapt ##able . the method is very simple . consider a distribution over labels , independent of the training example x , and a smoothing parameter . for a training example with ground - truth label , we replace the label distribution with which is a mixture of the original ground - truth distribution and the fixed distribution , with weights and , respectively . this can be seen as the distribution of the label obtained as follows : first , set it to the ground - truth label ; then , with probability , replace with a sample drawn from the distribution . we propose to use the prior distribution over labels as . in our experiments , we used the uniform distribution , so that we refer to this change in ground - truth label distribution as label - smoothing regular ##ization , or l ##sr . note that l ##sr achieve ##s the desired goal of preventing the largest log ##it from becoming much larger than all others . indeed , if this were to happen , then a single would approach while all others would approach . this would result in a large cross - entropy with because , unlike , all have a positive lower bound . another interpretation of l ##sr can be obtained by considering the cross entropy : thus , l ##sr is equivalent to replacing a single cross - entropy loss with a pair of such losses and . the second loss penal ##izes the deviation of predicted label distribution from the prior , with the relative weight . note that this deviation could be equivalent ##ly captured by the k ##l diver ##gence , since and is fixed . when is the uniform distribution , is a measure of how di ##ssi ##mi ##lar the predicted distribution is to uniform , which could also be measured ( but not equivalent ##ly ) by negative entropy ; we have not experimented with this approach . in our image ##net experiments with classes , we used and . for il ##s ##vr ##c 2012 , we have found a consistent improvement of about absolute both for top - error and the top - error ( cf . table [ reference ] ) . section : training methodology we have trained our networks with st ##och ##astic gradient utilizing the tensor ##flow distributed machine learning system using replica ##s running each on a n ##vid ##ia kepler gp ##u with batch size for epoch ##s . our earlier experiments used momentum with a decay of , while our best models were achieved using rms ##pro ##p with decay of and . we used a learning rate of , decay ##ed every two epoch using an exponential rate of . in addition , gradient clip ##ping with threshold was found to be useful to stabilize the training . model evaluation ##s are performed using a running average of the parameters computed over time . section : performance on lower resolution input a typical use - case of vision networks is for the the post - classification of detection , for example in the multi ##box context . this includes the analysis of a relative small patch of the image containing a single object with some context . the tasks is to decide whether the center part of the patch corresponds to some object and determine the class of the object if it does . the challenge is that objects tend to be relatively small and low - resolution . this raises the question of how to properly deal with lower resolution input . the common wisdom is that models employing higher resolution rec ##eptive fields tend to result in significantly improved recognition performance . however it is important to distinguish between the effect of the increased resolution of the first layer rec ##eptive field and the effects of larger model cap ##ac ##itan ##ce and computation . if we just change the resolution of the input without further adjustment to the model , then we end up using computational ##ly much cheaper models to solve more difficult tasks . of course , it is natural , that these solutions loose out already because of the reduced computational effort . in order to make an accurate assessment , the model needs to analyze vague hints in order to be able to \" hall ##uc ##inate \" the fine details . this is computational ##ly costly . the question remains therefore : how much does higher input resolution helps if the computational effort is kept constant . one simple way to ensure constant effort is to reduce the strides of the first two layer in the case of lower resolution input , or by simply removing the first pool ##ing layer of the network . for this purpose we have performed the following three experiments : rec ##eptive field with stride and maximum pool ##ing after the first layer . rec ##eptive field with stride and maximum pool ##ing after the first layer . rec ##eptive field with stride and without pool ##ing after the first layer . all three networks have almost identical computational cost . although the third network is slightly cheaper , the cost of the pool ##ing layer is marginal and ( within of the total cost of the ) network . in each case , the networks were trained until convergence and their quality was measured on the validation set of the image ##net il ##s ##vr ##c 2012 classification bench ##mark . the results can be seen in table [ reference ] . although the lower - resolution networks take longer to train , the quality of the final result is quite close to that of their higher resolution counterparts . however , if one would just naive ##ly reduce the network size according to the input resolution , then network would perform much more poorly . however this would an unfair comparison as we would are comparing a 16 times cheaper model on a more difficult task . also these results of table [ reference ] suggest , one might consider using dedicated high - cost low resolution networks for smaller objects in the r - cnn context . section : experimental results and comparisons inception - v ##2 rms ##pro ##p inception - v ##2 label smoothing inception - v ##2 factor ##ized table [ reference ] shows the experimental results about the recognition performance of our proposed architecture ( inception - v ##2 ) as described in section [ reference ] . each inception - v ##2 line shows the result of the cumulative changes including the highlighted new modification plus all the earlier ones . label smoothing refers to method described in section [ reference ] . factor ##ized includes a change that factor ##izes the first con ##vo ##lu ##tion ##al layer into a sequence of con ##vo ##lu ##tion ##al layers . bn - auxiliary refers to the version in which the fully connected layer of the auxiliary class ##ifier is also batch - normal ##ized , not just the con ##vo ##lu ##tions . we are referring to the model in last row of table [ reference ] as inception - v ##3 and evaluate its performance in the multi - crop and ensemble settings . all our evaluation ##s are done on the 48 ##23 ##8 non - black ##list ##ed examples on the il ##s ##vr ##c - 2012 validation set , as suggested by . we have evaluated all the 5000 ##0 examples as well and the results were roughly 0 . 1 % worse in top - 5 error and around 0 . 2 % in top - 1 error . in the upcoming version of this paper , we will verify our ensemble result on the test set , but at the time of our last evaluation of bn - inception in spring indicates that the test and validation set error tends to co ##rre ##late very well . section : conclusions we have provided several design principles to scale up con ##vo ##lu ##tion ##al networks and studied them in the context of the inception architecture . this guidance can lead to high performance vision networks that have a relatively modest computation cost compared to simpler , more mono ##lithic architecture ##s . our highest quality version of inception - v ##3 reaches , top - and top - 5 error for single crop evaluation on the il ##s ##vr 2012 classification , setting a new state of the art . this is achieved with relatively modest ( ) increase in computational cost compared to the network described in io ##ffe et al . still our solution uses much less computation than the best published results based on dense ##r networks : our model out ##per ##forms the results of he et al - cutting the top - ( top - ) error by ( ) relative , respectively - while being six times cheaper computational ##ly and using at least five times less parameters ( estimated ) . our ensemble of four inception - v ##3 models reaches with multi - crop evaluation reaches top - error which represents an over reduction to the best published results and is almost half of the error of il ##s ##vr ##c 2014 win ##ining google ##net ensemble . we have also demonstrated that high quality results can be reached with rec ##eptive field resolution as low as . this might prove to be helpful in systems for detecting relatively small objects . we have studied how factor ##izing con ##vo ##lu ##tions and aggressive dimension reductions inside neural network can result in networks with relatively low computational cost while maintaining high quality . the combination of lower parameter count and additional regular ##ization with batch - normal ##ized auxiliary class ##ifiers and label - smoothing allows for training high quality networks on relatively modest sized training sets . bibliography : references",
        "pred_seq": "[SEP] [SEP] [SEP] top error [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "topand toperror"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "ilsvrc 2012 classification challenge validation set",
                        "2012 imagenet",
                        "imagenet",
                        "ilsvrc 2012",
                        "imagenet ilsvrc 2012 classification benchmark",
                        "ilsvrc2012 validation set",
                        "ilsvr 2012 classification"
                    ]
                ],
                "Method": [
                    [
                        "inception architecture",
                        "inception",
                        "inception networks",
                        "inceptionstyle model",
                        "inceptiontype networks",
                        "inception style building blocks",
                        "inception modules",
                        "factorized inception modules",
                        "rmsprop inception",
                        "smoothing inception",
                        "inceptionv3",
                        "ensemble of four inceptionv3 models"
                    ]
                ],
                "Metric": [
                    [
                        "top",
                        "toperror",
                        "accuracy",
                        "top1 error"
                    ]
                ],
                "Task": [
                    [
                        "ilsvrc classification challenge",
                        "postclassification of detection"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "ilsvrc 2012 classification challenge validation set",
                        "2012 imagenet",
                        "imagenet",
                        "ilsvrc 2012",
                        "imagenet ilsvrc 2012 classification benchmark",
                        "ilsvrc2012 validation set",
                        "ilsvr 2012 classification"
                    ]
                ],
                "Method": [
                    [
                        "inception architecture",
                        "inception",
                        "inception networks",
                        "inceptionstyle model",
                        "inceptiontype networks",
                        "inception style building blocks",
                        "inception modules",
                        "factorized inception modules",
                        "rmsprop inception",
                        "smoothing inception",
                        "inceptionv3",
                        "ensemble of four inceptionv3 models"
                    ]
                ],
                "Metric": [
                    [
                        "top",
                        "toperror",
                        "accuracy",
                        "top5 error"
                    ]
                ],
                "Task": [
                    [
                        "ilsvrc classification challenge",
                        "postclassification of detection"
                    ]
                ]
            }
        ]
    },
    "5": {
        "doctext": "document : prone ##t : learning to propose object - specific boxes for cascade ##d neural networks this paper aims to classify and locate objects accurately and efficiently , without using bound ##ing box ann ##ota ##tions . it is challenging as objects in the wild could appear at arbitrary locations and in different scales . in this paper , we propose a novel classification architecture prone ##t based on con ##vo ##lu ##tion ##al neural networks . it uses computational ##ly efficient neural networks to propose image regions that are likely to contain objects , and applies more powerful but slower networks on the proposed regions . the basic building block is a multi - scale fully - con ##vo ##lu ##tion ##al network which assigns object confidence scores to boxes at different locations and scales . we show that such networks can be trained effectively using image - level ann ##ota ##tions , and can be connected into cascade ##s or trees for efficient object classification . prone ##t out ##per ##forms previous state - of - the - art significantly on pascal vo ##c 2012 and ms coco data ##set ##s for object classification and point - based local ##ization . section : introduction we address the problem of object classification and local ##ization in natural images . as objects could be small and appear at arbitrary locations , several framework ##s rely on bound ##ing boxes to train object - cent ##ric class ##ifiers , and apply the class ##ifiers by searching over different locations of the images . however , the ann ##ota ##tion process for object bound ##ing boxes is usually resource intensive and difficult to scale up . in light of this , we aim to simultaneously classify and locate objects given only image - level ann ##ota ##tions for training . to cope with the lack of object - level ann ##ota ##tions , several methods extract feature activation ##s from con ##vo ##lu ##tion ##al neural networks ( cnn ) by scanning over different image regions . they then aggregate the extracted features into image - level representations for classification purpose . under this scheme , regions that belong to the background are considered as important as regions that contain objects . such global approaches tend to be sensitive to background , and can not be used directly for local ##ization . we choose to use the fully - con ##vo ##lu ##tion ##al network ( fc ##n ) architecture for simultaneous object classification and local ##ization . it replaces the fully - connected layers of a standard cnn ( e . g . alex ##net ) with con ##vo ##lu ##tion ##al layers . this enables an fc ##n to take images of arbitrary sizes , and generate classification score maps efficiently . each element in a score map corresponds to a rectangular box ( rec ##eptive field ) in the original image . the score maps can then be used for classification and local ##ization . the sampling strides and box sizes are determined by the fc ##n ' s network architecture . as box sizes are fixed , fc ##n might face difficulty dealing with objects of different scales . we address this problem by using a multi - stream multi - scale architecture . all streams share the same parameters , but take input images of different scales . to train the multi - scale fc ##n without object - level ann ##ota ##tions , we generate image - level scores by pool ##ing the score maps over multiple - scales , and compute the losses with image - level labels for back - propagation . once a multi - scale fc ##n is trained , it can be used for classification and local ##ization directly . from another perspective , it also proposes a set of promising boxes that are likely to contain objects . we can then build a cascade architecture by zoom ##ing onto those promising boxes , and train new class ##ifiers to verify them . the cascade allows the system to balance accuracy and speed : each stage filters out parts of image regions that are unlikely to contain objects . we name this propose and zoom pipeline as prone ##t . figure [ reference ] provides the high - level intuition behind prone ##t : three boxes are proposed for bird , pot ##ted plant and cat categories . the boxes are crop ##ped out and verified further , until a certain decision is made . to train the later class ##ifiers in prone ##t , we sample hard negative ##s based on image - level labels . for positive ##s , as no object - level ann ##ota ##tions are available , it is impossible to tell objects from background . to avoid over - fitting , we randomly sample positive boxes above a relative low threshold . different positive boxes from the same image can be sampled at different iteration ##s of the st ##och ##astic gradient descent training process . at test time , only a small subset of boxes ( 10 to 20 per image ) with highest object confidence scores are fed to the later class ##ifiers . this allows us to utilize cnn ##s that have stronger representation power with little computational overhead . prone ##t is highly con ##fi ##gur ##able : for example , one could set a list of important object categories , and only verify the proposed boxes for those categories . moreover , apart from a traditional chain - structured cascade , we show that it is also possible to build tree - structured cascade ##s , where each branch handles categories from a particular domain ( set of vehicles or animals ) . in summary , our paper makes the following contributions : we propose prone ##t , a cascade ##d neural network framework that zoom ##s onto promising object - specific boxes for efficient object classification and local ##ization . we introduce strategies to train prone ##t with image - level ann ##ota ##tions effectively ; and demonstrate the implementations of chain - and tree - structured cascade ##s . we show that prone ##t out ##per ##forms previous state - of - the - art significantly on the object classification and point - based local ##ization tasks of the pascal vo ##c 2012 data ##set and the recently released ms coco data ##set . section : related work object classification is a fundamental problem in computer vision . earlier work focused on classification from object - cent ##ric images . they usually extract hand - crafted low - level features and aggregate the features into image - level feature vectors . more challenging data ##set ##s have since been collected . they are of larger scale , and contain smaller objects which could be partially o ##cc ##lu ##ded . recently , deep con ##vo ##lu ##tion ##al neural networks ( cnn ) have achieved state - of - the - art performance on a wide range of visual recognition tasks , including object classification and detection . although cnn ##s require large amount of data for training , it has been shown that they are able to learn representations that general ##ize to other tasks . such representations can be adapted to image classification by fine - tuning , or extracted as ho ##listic features for classification with linear sv ##ms . when used as generic feature extract ##ors , feature aggregation techniques designed for hand - crafted features can also work with cnn em ##bed ##ding ##s and achieve competitive performance . an alternative approach for object classification is via detection . among those utilizing bound ##ing box ann ##ota ##tions , rc ##nn achieve ##s competitive performance by directly representing image boxes with cnn features and learning class ##ifiers on top of the features . object proposal techniques are used to sample the image patches for classification . a recent framework , fast rc ##nn , uses fully - con ##vo ##lu ##tion ##al networks ( fc ##n ) to generate box - level features in batch , and is thus more computational efficient . object local ##ization with image - level ann ##ota ##tions is a weakly - supervised problem . it can be formulated as a multiple instance learning problem , and has been addressed to learn concept detectors from internet data . it has also been studied for object detection and segment ##ation . for object classification , wei et al . treat images as bags of patches , where the patches are selected using object ##ness criteria . they then use max pool ##ing to fine - tune cnn ##s based on image - level ann ##ota ##tions . o ##qua ##b et al . follow a similar approach , but make the training process end - to - end by converting cnn ##s into fc ##ns . the proposal generation network in prone ##t is also based on fc ##n , but uses a multi - stream architecture and cross - scale l ##se pool ##ing to achieve scale - awareness . cascade ##d class ##ifiers are a well - studied technique in computer vision . cascade ##s with cnn ##s have been explored for facial point detection , face detection and pose estimation . however , such methods require fully ann ##ota ##ted training examples . prone ##t adopt ##s the cascade philosophy to balance speed and accuracy , but does not require object bound ##ing boxes for training . since prone ##t is a general object class ##ifier , it can also be extended to have tree structure , where each leaf is a domain expert . section : prone ##t framework prone ##t has two basic components : an object - specific box proposal unit , and a verification unit . for each image , for each object category , the box proposal unit generates a list of confidence scores of the presence of the object instances , and the coordinates indicating the locations of the objects . prone ##t then zoom ##s onto image boxes with higher scores to further verify if they are positive or hard negative ##s . the verification units can either take all boxes , which forms a chain structure ; or a subset of boxes corresponding to certain domains ( animal ) , which forms a tree structure . we implement these two units with con ##vo ##lu ##tion ##al neural networks . figure [ reference ] illustrates the overall prone ##t framework . sub ##section : proposal generation the first stage in our framework is to generate object - specific box proposals with cnn ##s . for an input image and object category , we want to learn a proposal scoring function where corresponds to the location of a rectangular image region denoted by its top left and bottom right corners . a typical cnn architecture for image classification task ( e . g . alex ##net ) involves a hierarchy of con ##vo ##lu ##tion ##al layers and fully connected layers . the con ##vo ##lu ##tion ##al layers operate on local image patches to extract feature representations . for a color image with 3 channels , the con ##vo ##lu ##tion ##al layers generate a feature map of elements , where is the output feature dimension . and correspond to the width and height of the feature map , they are controlled by input image size , as well as the kernel size , sampling step and pad ##ding size of the con ##vo ##lu ##tion ##al layers . the fully connected layers serve as class ##ifiers which take fixed - size inputs , thus require the width and height of input images to be fixed . therefore , one possible way to compute is to en ##ume ##rate locations and scales in a sliding window fashion or with bound ##ing box proposals , and feed such image regions to cnn ##s . we take an alternative approach based on fully con ##vo ##lu ##tion ##al networks ( e . g . over ##fe ##at ) . fully con ##vo ##lu ##tion ##al networks ( fc ##n ) do not contain fully - connected layers . rather , they use only the con ##vo ##lu ##tion ##al layers , which allows them to process images of arbitrary sizes . the outputs of fc ##ns are in the form of feature maps , where is the number of categories . each element in a feature map corresponds to the activation response for a particular category over a certain region . such regions are called rec ##eptive fields for the activation ##s . compared with region sampling with sliding windows or bound ##ing box proposals , fc ##ns offer a seam ##less solution for end - to - end training under the cnn framework , and also naturally allow the sharing of intermediate features over overlapping image regions . scale adaptation with multi - stream fc ##ns . one issue in use of fc ##ns is that the sizes of rec ##eptive fields are typically fixed , while the object scales may vary a lot . we address this problem by using a multi - stream architecture . assume an fc ##n has been trained with inputs where objects have been res ##ized to the same scale . we expand the network into streams , where every stream shares the same parameters as the pre - trained one . given an image , we scale it to different sizes and feed to the - stream fc ##n . the output feature map of each stream corresponds to a different scale in the original image . training with image - level ann ##ota ##tions . when object bound ##ing boxes are available , training fc ##ns is straight - forward : one could either crop images with the bound ##ing boxes , or use a loss function which operates directly on feature maps and takes the object locations into account . as such supervision is absent , we need to aggregate local responses into global ones so that image - level labels can be used for training . we use the log - sum - ex ##p ( l ##se ) pool ##ing function applied by for semantic segment ##ation : where is the category , corresponds to the - th stream of fc ##n , correspond to location in the feature map , is the total number of such elements and is a hyper parameter . the function ' s output is close to average when is small and maximum when is large . setting larger makes the aggregation focus on a smaller subset of image boxes , and has the potential to handle smaller objects better . l ##se pool ##ing function can be implemented as a layer in a neural network . as illustrated in figure [ reference ] , it is connected to the final layers of all - stream fc ##ns and produces a dimensional vector for each image . we then compute the loss for each category and back - prop ##aga ##te the error gradient ##s to the earlier layers . computing proposal scores . once the fc ##ns have been trained , we compute proposal scores from the feature maps . specifically , for every ne ##uron in the final layer of single - stream fc ##n , we compute its rec ##eptive field and use it as the location ; the corresponding activation of the ne ##uron is used as proposal score . although the exact rec ##eptive field may vary due to different pad ##ding strategies , we use a simple estimation which has been reported to work well in practice . denote the sampling stride of a spatial con ##vo ##lu ##tion ##al layer as and the kernel size of a max pool ##ing layer as , the overall sampling stride is given by where is the collection of all con ##vo ##lu ##tion ##al layers and is the collection of all max pool ##ing layers . implementation . our - stream fc ##ns are implemented with torch . for each stream , we use the cnn - m 204 ##8 architecture proposed in . it has 5 con ##vo ##lu ##tion ##al layers and 3 fully - connected layers . it achieve ##s higher accuracy on image ##net than alex ##net , while being faster and less memory consuming than very deep cnn ##s . we use the model parameters released by the authors , which were pre - trained from image ##net data ##set with 1 , 000 categories . we convert the model into an fc ##n by replacing the three fully - connected layers with con ##vo ##lu ##tion ##al layers . the first con ##vo ##lu ##tion ##al layer has 512 input planes , 40 ##9 ##6 output planes and kernel size of 6 . the second has 40 ##9 ##6 input planes , 204 ##8 output planes and kernel size of 1 . since the final layer is task - specific , it is initial ##ized from scratch with 204 ##8 input planes , output planes and kernel size of 1 . to adapt the model parameters for object classification on different data ##set ##s , we only fine - tune the final two layers and freeze the model parameters from previous layers . the sampling stride of feature maps is 32 pixels , and the window size is 223 pixels . we set the number of streams to be 3 . during training , all three streams share the same set of parameters . to facilitate training with mini - batch ##es , every image is res ##cal ##ed to , and pixels . as the aspect ratios of images could be different , we res ##cal ##e the longer edge to 300 , 500 and 700 respectively , and fill the empty pixels by mirror ##ing the images . traditional cross entropy loss for multi - class classification introduces competition between different classes , thus it is not suitable for images with multiple labels . we compute the loss with binary cross entropy criteria for each class separately , and sum up the error gradient ##s from losses of all classes for back - propagation . sub ##section : cascade - style proposal verification by setting threshold ##s on proposal scores , a small subset of image boxes which might contain objects are selected . similar to object detection framework ##s , we run cnn class ##ifiers on the selected boxes . the proposal step also serves as a filter whose goal is to preserve the object boxes with high recall rate , while removing the easy negative ##s . the verification class ##ifiers then address a more focused problem on a smaller set of instances . connecting the two steps is essentially the same as training a cascade of class ##ifiers . verification network architecture . as a later class ##ifier in the cascade , accuracy is more important than speed . we choose the v ##gg - 16 network architecture . compared with alex ##net variants , it offers better accuracy for most visual recognition tasks , but is also slower and more memory demanding . we use the v ##gg - 16 model parameters released by the authors , which was trained on 1 , 000 image ##net categories . we use the same binary cross entropy criterion to compute losses . to make the training process faster , we only fine - tune the final two fully - connected layers and freeze all previous layers . input ##in ##put output ##out ##put training images with proposal scores , batch size , threshold stopping criteria not met randomly select images from ; initial ##ize mini - batch ; has proposal with score randomly sample a proposal where ; set the sample ' s active class to ; add proposed region to ; res ##ize and add full image to ; set all classes as active ; forward pass with ; compute loss for the active class of each sample ; update model parameters . mini - batch sampling algorithm for training cascade class ##ifier with st ##och ##astic gradient descent . training strategy for the cascade . ideally , we want the verification network to handle hard examples from both positive and negative data . when a proposed region from an image not containing a given label has a high score of that class , we know it is a hard negative . however , it is impossible to tell a hard positive from background without using bound ##ing box ann ##ota ##tions . we attempt to avoid using background by selecting only the top scoring image region for each positive class . this results in significant over - fitting and poor general ##iza ##bility for the trained verification net . the main problem with the above sampling strategy is that for positive instances , only easy examples which have been learned well are preserved . to fix this , we use a random sampling strategy as described in algorithm [ reference ] . for each image , we randomly select an image box whose proposal score is higher than threshold for class . in practice , the threshold is set to a relative low value ( ) . if is labeled as positive for the image , we treat the box as a positive instance ( though it might belong to background ) , and otherwise negative . note that the sampled box could be easy negative ##s for classes beyond . to avoid overs ##amp ##ling the easy negative ##s , we set as the active class during back - propagation and only compute the loss for the active class . inference with cascade . during inference , an image is passed to the proposal generation fc ##n to compute proposal scores . a small subset of proposed boxes with high scores are then passed to the verification network . for each class , we select the top scoring proposals if the scores are higher than threshold . we then use the following equation to combine the outputs from both networks : where is the set of selected proposals for class , is the score of class from the proposal network after l ##se pool ##ing , and is the verification network ' s output for class on region . when no proposal is selected , we preserve scores from the proposal network without cal ##ib ##ration as they are typically low . discussion . deco ##mp ##osing classification into cascade of proposal and verification networks allows the system to achieve high accuracy while maintaining a reasonable computational cost . it is also a flexible framework for different design choices . for example , one could decide to verify a subset of object classes which require higher accuracy . with the cascade training algorithm , we can build tree - structured cascade ##d neural networks , where each branch focuses on a subset of categories . we can also extend the cascade to have more stages , and train the new stages with newly ann ##ota ##ted training data . figure [ reference ] illustrates these structures . section : experiments experimental setup . we work with the pascal vo ##c 2012 data ##set and the ms coco data ##set . vo ##c 2012 has 5 , 000 images for training , 5 , 000 for validation and 10 , 000 for testing . there are 20 object classes in total . coco has 80 , 000 images for training and 40 , 000 images for validation . it has 80 object classes in 12 super - categories . we evaluated prone ##t on object classification and point - based object local ##ization tasks . for object classification , we use the average precision metric . we used vo ##c ' s result server to compute average precision ##s on the vo ##c 2012 data ##set . for point - based object local ##ization , we use the criteria introduced in . for every image and every class , we output a location with maximum response for that class . the location is deemed correct if it falls into any bound ##ing box associated with that class , with a tolerance of 18 pixels as used in . this information is then used to compute average precision . although object extent is not evaluated , the metric remains challenging as shown by . to generate local ##ization coordinates for evaluation , we kept track of the image boxes which give highest responses at each stage , and used the center point of the selected boxes . we tried different values of hyper - parameter for l ##se pool ##ing , and found that generally gave good performance . we fixed in all the following experiments . we used the st ##och ##astic gradient descent algorithm for training . to train proposal network , the learning rate was set to 0 . 01 ; to train verification network , the learning rate was set to 0 . 001 . we set the filtering threshold for cascade to 0 . 1 . which pool ##ing method is better ? we compare maximum pool ##ing , average pool ##ing and l ##se pool ##ing methods to train proposal network with image - level supervision . table [ reference ] lists the classification and local ##ization performance of the three different pool ##ing methods . we can see that l ##se achieve ##s the best classification map . average pool ##ing is 3 . 7 % worse than l ##se , which we believe is because it assigns equal importance to fore ##ground and background . max pool ##ing is 1 . 4 % worse ; compared with l ##se pool ##ing , it only uses a single patch to generate image - level score , thus is more sensitive to noise and model initial ##ization during training . we also generated visual ##izations to study the impact of pool ##ing method on trained models . figure [ reference ] shows heat maps of the class train when different models are applied to the same image . we can see that the model trained by average pool ##ing has high activation ##s not only on the train but also on part of the background . for max pool ##ing , only the wheel of the train has high response , presumably because it is the most disc ##rim ##ina ##tive for the train . model trained by l ##se pool ##ing has high response on the train , but not on the background . does cascade help ? we study the impact of adding cascade ##d class ##ifiers on classification and local ##ization performance . we first use a single level of cascade with one multi - scale fc ##n and one verification network . for each image and each class , we selected the top 3 regions per scale if their scores are higher than 0 . 1 . the average number of regions to be verified is 24 per image . in table [ reference ] , we can see that on pascal vo ##c 2012 , using a cascade helps improve classification map by 3 . 3 % and local ##ization map by 2 . 9 % . is a longer cascade better ? we are interested in observing how the performance changes with more levels of cascade . for this purpose , we first trained another set of proposal and verification networks using pascal vo ##c data alone , but found that the network over ##fi ##tted easily . since the training set of vo ##c 2012 has only 5 , 000 images , we found that the first set of proposal and verification networks \" perfectly solved \" this training set , leaving little room to improve its general ##iza ##bility . in light of this , we used the 80 , 000 images from coco training set as complementary data source . it covers the 20 categories used in vo ##c but also has 60 other categories . rather than re - training all the networks by combining vo ##c and coco data , we take that the previous cnn ##s in the cascade have already been trained and fixed , and only train new cnn ##s with the extra data . note that our cascade architecture offers a natural way to select the challenging instances from such incoming images . the final row in table [ reference ] shows the maps after adding a new set of cascade ##s trained from coco images . we can see that it offers another 1 % improvement over the previous cascade , which indicates that it is desirable to train a longer cascade when more training data becomes available . expanding cascade ##s into trees . we also investigated the effect of building tree - structured cascade ##s . coco data ##set is used for evaluation as it has 3 times more categories than vo ##c . we trained 12 verification networks corresponding to the 12 super - categories of coco . each network focuses on a single super - category , and processes the sampled boxes whose active classes belong to that super - category . at test time , each proposed box only goes through a single root to leaf path in the tree . the final row of table [ reference ] shows its classification and local ##ization performance . we can see that compared with the chain structured cascade , tree - structured cascade achieve ##s better performance , probably because it trains the neural networks to be focused on a small subset of similar categories . comparison with detection based approaches . we compare our proposed framework with two recent state - of - the - art object detection methods : rc ##nn and fast rc ##nn . unlike our framework , they require bound ##ing box ann ##ota ##tions for training . both methods use selective search to generate object proposals and cnn ##s for classification . rc ##nn uses alex ##net pre - trained from image ##net , while fast rc ##nn uses v ##gg - 16 pre - trained from image ##net . to generate classification and local ##ization results , for each class we select the detection output with maximum confidence score , and use the center of the detected bound ##ing box for local ##ization evaluation . we first fix the number of window proposals to 1000 for rc ##nn and fast rc ##nn . table [ reference ] shows the performance comparison . we can see that for classification , our proposed framework out ##per ##forms both rc ##nn and fast rc ##nn . for local ##ization , our proposed framework out ##per ##forms rc ##nn , but is 4 % worse than fast rc ##nn . we also study the impact of number of proposed boxes on our system ' s performance . for this purpose , we let the proposal network select top regions per scale for each class , and compute the average number of proposed boxes per image . for comparison , we ask fast rc ##nn to use up to 10 , 50 , 500 and 1000 selective search proposals per image . table [ reference ] shows the classification and local ##ization performances respectively . we can see that prone ##t is quite robust to the number of proposed boxes , and achieve ##s reasonably good performance with only 9 boxes on average . this confirms that prone ##t offers better accuracy with relatively small computational overhead . meanwhile , fast rc ##nn requires many more proposals to reach peak performance , presumably because the selective search proposals are for general object ##ness and not opt ##imi ##zed for object classification in cascade fashion . comparison with other weakly - supervised methods . we compare prone ##t with several state - of - the - art object classification framework ##s . classification and local ##ization performance on pascal vo ##c 2012 are shown in table [ reference ] and table [ reference ] respectively . table [ reference ] and figure [ reference ] show results and local ##ization examples on coco data ##set . among the compared systems , o ##qua ##b et al . and nu ##s - hc ##p use cnn ##s pre - trained on the expanded image ##net data with more than 1500 categories , which has been shown to be useful for classification . since prone ##t uses cascade ##s or trees of cnn ##s , it can apply a more powerful cnn model v ##gg - 16 with small computational overhead . this helps our system out ##per ##form most of the previous state - of - the - art systems significantly on both data ##set ##s . prone ##t is also slightly better than simon ##yan et al . which extracts v ##gg - 16 features at three different scales over full images . their system is 3 ##x to 6 ##x slower than our cascade at test time . limitation . we evaluate prone ##t using the standard io ##u metric , which considers object extent as well as location . since the boxes generated by our proposal cnn have fixed aspect ratios , we follow to aggregate the heat maps over 1000 bound ##ing box proposals generated by selective search per image . no bound ##ing box regression is conducted . cascade cnn is then used to verify the high - scoring proposals . on pascal vo ##c 2012 validation set , our proposal cnn has an map of 13 . 0 % when overlap threshold is 0 . 5 . the cascade cnn improves the map to 15 . 5 % . although both results are higher than 11 . 7 % as reported by , there is still a huge gap between the state - of - the - art object detection pipeline ##s . our proposal network tends to select the most disc ##rim ##ina ##tive / confusing parts of objects , which is good for cascade classification but bad for getting full object extent ##s . separating and counting multiple objects are also challenging issues . section : conclusion we proposed prone ##t , a cascade ##d neural network for object classification and local ##ization . prone ##t learns to propose object - specific boxes by multi - scale fc ##ns trained from image - level ann ##ota ##tions . it then sends a small subset of promising boxes to latter cnn ##s for verification . detailed experimental evaluation ##s have shown the effectiveness of prone ##t on the challenging pascal vo ##c 2012 data ##set and ms coco data ##set . acknowledge ##ment : we would like to thank sergey za ##gor ##uy ##ko for help with fast rc ##nn experiments ; pedro o . pin ##hei ##ro , bo ##lei zhou , maxim ##e o ##qua ##b , [UNK] leg ##rand , yuan ##dong tian , [UNK] bot ##tou and fl ##ore ##nt per ##ron ##nin for valuable discussions . bibliography : references",
        "pred_seq": "pascal 2012 [SEP] prone ##t [SEP] [SEP] [SEP] [unused0] pascal 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ##s ##s ##s ##s ##s ##s ##s ##s ##s ##s ##s [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "pascal voc 2012"
                    ]
                ],
                "Method": [
                    [
                        "pronet"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "ms coco datasets",
                        "ms coco dataset",
                        "coco",
                        "coco training set",
                        "coco data",
                        "coco images",
                        "coco dataset"
                    ]
                ],
                "Method": [
                    [
                        "pronet",
                        "learning to propose objectspecific boxes for cascaded neural networks",
                        "classification architecture pronet",
                        "pronet framework"
                    ]
                ],
                "Metric": [
                    [
                        "map"
                    ]
                ],
                "Task": [
                    [
                        "detection",
                        "weaklysupervised problem",
                        "object detection",
                        "supervision"
                    ]
                ]
            }
        ]
    },
    "6": {
        "doctext": "document : real - time video super - resolution with spat ##io - temporal networks and motion compensation con ##vo ##lu ##tion ##al neural networks have enabled accurate image super - resolution in real - time . however , recent attempts to benefit from temporal correlation ##s in video super - resolution have been limited to naive or in ##ef ##fi ##cie ##nt architecture ##s . in this paper , we introduce spat ##io - temporal sub - pixel con ##vo ##lu ##tion networks that effectively exploit temporal red ##unda ##ncies and improve reconstruction accuracy while maintaining real - time speed . specifically , we discuss the use of early fusion , slow fusion and 3d con ##vo ##lu ##tions for the joint processing of multiple consecutive video frames . we also propose a novel joint motion compensation and video super - resolution algorithm that is orders of magnitude more efficient than competing methods , relying on a fast multi - resolution spatial transform ##er module that is end - to - end train ##able . these contributions provide both higher accuracy and temporal ##ly more consistent videos , which we confirm qu ##ali ##tative ##ly and quantitative ##ly . relative to single - frame models , spat ##io - temporal networks can either reduce the computational cost by 30 % whilst maintaining the same quality or provide a 0 . 2d ##b gain for a similar computational cost . results on publicly available data ##set ##s demonstrate that the proposed algorithms sur ##pass current state - of - the - art performance in both accuracy and efficiency . sr ##sr ##su ##per - resolution l ##rl ##rl ##ow - resolution hr ##hr ##hi ##gh - resolution tv ##tv ##to ##tal ##var ##iation hd ##hd ##hi ##gh ##de ##fin ##ition ms ##em ##se ##me ##ans ##qua ##red ##er ##ror ps ##nr ##ps ##nr ##pe ##aks ##ign ##al - to - noise ##rat ##io section : introduction image and video sr are long - standing challenges of signal processing . sr aims at recovering a hr image or video from its l ##r version , and finds direct applications ranging from medical imaging to satellite imaging , as well as facilitating tasks such as face recognition . the reconstruction of hr data from a l ##r input is however a highly ill - posed problem that requires additional constraints to be solved . while those constraints are often application - dependent , they usually rely on data red ##unda ##ncy . in single image sr , where only one l ##r image is provided , methods exploit inherent image red ##unda ##ncy in the form of local correlation ##s to recover lost high - frequency details by imposing spa ##rs ##ity constraints or assuming other types of image statistics such as multi - scale patch rec ##ur ##rence . in multi - image sr it is assumed that different observations of the same scene are available , hence the shared explicit red ##unda ##ncy can be used to con ##stra ##in the problem and attempt to in ##vert the downs ##cal ##ing process directly . transition ##ing from images to videos implies an additional data dimension ( time ) with a high degree of correlation that can also be exploited to improve performance in terms of accuracy as well as efficiency . sub ##section : related work video sr methods have mainly emerged as adaptations of image sr techniques . kernel regression methods have been shown to be applicable to videos using 3d kernel ##s instead of 2d ones . dictionary learning approaches , which define l ##r images as a sparse linear combination of dictionary atoms coupled to a hr dictionary , have also been adapted from images to videos . another approach is example - based patch rec ##ur ##rence , which assumes patches in a single image or video obey multi - scale relationships , and therefore missing high - frequency content at a given scale can be in ##fer ##red from coarse ##r scale patches . this was successfully presented by g ##las ##ner et al . for image sr and has later been extended to videos . when adapting a method from images to videos it is usually beneficial to incorporate the prior knowledge that frames of the same scene of a video can be approximate ##d by a single image and a motion pattern . est ##imating and com ##pen ##sat ##ing motion is a powerful mechanism to further con ##stra ##in the problem and expose temporal correlation ##s . it is therefore very common to find video sr methods that explicitly model motion through frames . a natural choice has been to prep ##ro ##ces ##s input frames by com ##pen ##sat ##ing inter - frame motion using displacement fields obtained from off - the - shelf optical flow algorithms . this nevertheless requires frame prep ##ro ##ces ##sing and is usually expensive . alternatively , motion compensation can also be performed jointly with the sr task , as done in the bay ##esian approach of liu et al . by it ##erative ##ly est ##imating motion as part of its wider modeling of the downs ##cal ##ing process . the advent of neural network techniques that can be trained from data to approximate complex nonlinear functions has set new performance standards in many applications including sr . dong et al . proposed to use a cnn architecture for single image sr that was later extended by ka ##ppel ##er et al . in a video sr network ( vs ##rnet ) which jointly processes multiple input frames . additionally , com ##pen ##sat ##ing the motion of input images with a tv - based optical flow algorithm showed an improved accuracy . joint motion compensation for sr with neural networks has also been studied through rec ##urrent bid ##ire ##ction ##al networks . the common paradigm for cnn based approaches has been to upscale the l ##r image with bi ##cu ##bic inter ##pol ##ation before attempting to solve the sr problem . however , increasing input image size through inter ##pol ##ation considerably impacts the computational burden for cnn processing . a solution was proposed by shi et al . with an efficient sub - pixel con ##vo ##lu ##tion network ( es ##pc ##n ) , where an ups ##cal ##ing operation directly mapping from l ##r to hr space is learnt by the network . this technique reduces run ##time by an order of magnitude and enables real - time video sr by independently processing frames with a single frame model . similar solutions to improve efficiency have also been proposed based on trans ##posed con ##vo ##lu ##tions . sub ##section : motivation and contributions [ b ] 0 . 32 [ b ] 0 . 32 [ b ] 0 . 32 existing solutions for hd video sr have not been able to effectively exploit temporal correlation ##s while performing in real - time . on the one hand , es ##pc ##n leverage ##s sub - pixel con ##vo ##lu ##tion for a very efficient operation , but its naive extension to videos treating frames independently fails to exploit inter - frame red ##unda ##ncies and does not enforce a temporal ##ly consistent result . vs ##rnet , on the other hand , can improve reconstruction quality by jointly processing multiple input frames . however , the prep ##ro ##ces ##sing of l ##r images with bi ##cu ##bic ups ##cal ##ing and the use of an in ##ef ##fi ##cie ##nt motion compensation mechanism slow ##s run ##time to about frames per second even on videos smaller than standard definition resolution . spatial transform ##er networks provide a means to in ##fer parameters for a spatial mapping between two images . these are different ##iable networks that can be seam ##lessly combined and jointly trained with networks targeting other objectives to enhance their performance . for instance , spatial transform ##er networks were initially shown to facilitate image classification by transforming images onto the same frame of reference . recently , it has been shown how spatial transformers can en ##code optical flow features with un ##su ##per ##vis ##ed training , but they have nevertheless not yet been investigated for video motion compensation . related approaches have emerged for view synthesis assuming rigid transformations . in this paper , we combine the efficiency of sub - pixel con ##vo ##lu ##tion with the performance of spat ##io - temporal networks and motion compensation to obtain a fast and accurate video sr algorithm . we study different treatments of the temporal dimension with early fusion , slow fusion and 3d con ##vo ##lu ##tions , which have been previously suggested to extend classification from images to videos . additionally , we build a motion compensation scheme based on spatial transformers , which is combined with spat ##io - temporal models to lead to a very efficient solution for video sr with motion compensation that is end - to - end train ##able . a high - level diagram of the proposed approach is show in fig : network . the main contributions of this paper are : presenting a real - time approach for video sr based on sub - pixel con ##vo ##lu ##tion and spat ##io - temporal networks that improves accuracy and temporal consistency . comparing early fusion , slow fusion and 3d con ##vo ##lu ##tions as alternative architecture ##s for discovering spat ##io - temporal correlation ##s . proposing an efficient method for dense inter - frame motion compensation based on a multi - scale spatial transform ##er network . combining the proposed motion compensation technique with spat ##io - temporal models to provide an efficient , end - to - end train ##able motion compensated video sr algorithm . section : methods our starting point is the real - time image sr method es ##pc ##n . we restrict our analysis to standard architectural choices and do not further investigate potentially beneficial extensions such as rec ##ur ##rence , residual connections or training networks based on per ##ce ##pt ##ual loss functions . throughout the paper we assume all image processing is performed on the y - channel in colour space , and thus we represent all images as 2d matrices . sub ##section : sub - pixel con ##vo ##lu ##tion sr for a given l ##r image which is assumed to be the result of low - pass filtering and downs ##cal ##ing by a factor the hr image , the cnn super - resolved solution can be expressed as here , are model parameters and represents the mapping function from l ##r to hr . a con ##vo ##lu ##tion ##al network models this function as a con ##cate ##nation of layers defined by sets of weights and bias ##es , each followed by non - linear ##ities , with . formally , the output of each layer is written as with . we assume the shape of filtering weights to be , where and represent the number and size of filters in layer , with the single frame input meaning . model parameters are opt ##imi ##sed mini ##mis ##ing a loss given a set of l ##r and hr example image pairs , commonly ms ##e : methods prep ##ro ##ces ##sing with bi ##cu ##bic ups ##amp ##ling before mapping from l ##r to hr impose that the output number of filters is . using sub - pixel con ##vo ##lu ##tion allows to process directly in the l ##r space and then use output filters to obtain an hr output tensor with shape that can be re ##ord ##ered to obtain . this implies that if there exists an ups ##cal ##ing operation that is better suited for the problem than bi ##cu ##bic ups ##amp ##ling , the network can learn it . moreover , and most importantly , all con ##vo ##lu ##tion ##al processing is performed in l ##r space , making this approach very efficient . sub ##section : spat ##io - temporal networks spat ##io - temporal networks assume input data to be a block of spat ##io - temporal information , such that instead of a single input frame , a sequence of consecutive frames is considered . this can be represented in the network by introducing an additional dimension for temporal depth , with the input depth representing an odd number of consecutive input frames . if we denote the temporal radius of a spat ##io - temporal block to be , we define the group of input frames centered at time as , and the problem in e ##q : image - sr becomes the shape of weight ##ing filters is also extended by their temporal size , and their tensor shape becomes . we note that it is possible to consider solutions that aim to jointly rec ##ons ##truct more than a single output frame , which could have advantages at least in terms of computational efficiency . however , in this work we focus on the reconstruction of only a single output frame . sub ##su ##bs ##ection : early fusion one of the most straightforward approaches for a cnn to process videos is to match the temporal depth of the input layer to the number of frames . this will collapse all temporal information in the first layer and the remaining operations are identical to those in a single image sr network , meaning . an illustration of early fusion is shown in fig : early - fusion for , where the temporal dimension has been colour coded and the output mapping to 2d space is omitted . this design has been studied for video classification and action recognition , and was also one of the architecture ##s proposed in vs ##rnet . however , vs ##rnet requires bi ##cu ##bic ups ##amp ##ling as opposed to sub - pixel con ##vo ##lu ##tion , making the framework computational ##ly much less efficient in comparison . sub ##su ##bs ##ection : slow fusion another option is to partially merge temporal information in a hierarchical structure , so it is slowly fused as information progresses through the network . in this case , the temporal depth of network layers is configured to be , and therefore some layers also have a temporal extent until all information has been merged and the depth of the network reduces to . this architecture , termed slow fusion , has shown better performance than early fusion for video classification . in fig : slow - fusion we show a slow fusion network where and the rate of fusion is defined by for or otherwise , meaning that at each layer only two consecutive frames or filter activation ##s are merged until the network ' s temporal depth shrink ##s to . note that early fusion is an special case of slow fusion . sub ##su ##bs ##ection : 3d con ##vo ##lu ##tions another variation of slow fusion is to force layer weights to be shared across the temporal dimension , which has computational advantages . assuming an online processing of frames , when a new frame becomes available the result of some layers for the previous frame can be reused . for instance , refer ##ing to the diagram in fig : slow - fusion and assuming the bottom frame to be the latest frame received , all activation ##s above the dashed line are readily available because they were required for processing the previous frame . this architecture is equivalent to using 3d con ##vo ##lu ##tions , initially proposed as an effective tool to learn spat ##io - temporal features that can help for video action recognition . an illustration of this design from a 3d con ##vo ##lu ##tion perspective is shown in fig : 3d ##con ##v , where the arrangement of the temporal and filter features is swapped relative to fig : slow - fusion . sub ##section : spatial transform ##er motion compensation we propose the use of an efficient spatial transform ##er network to compensate the motion between frames fed to the sr network . it has been shown how spatial transformers can effectively en ##code optical flow to describe motion , and are therefore suitable for motion compensation . we will compensate blocks of three consecutive frames to combine the compensation module with the sr network as shown in fig : network , but for simplicity we first introduce motion compensation between two frames . notice that the data used contains inherent motion blur and ( di ##s ) o ##cc ##lusion ##s , and even though an explicit modelling for these effects is not used it could potentially improve results . the task is to find the best optical flow representation relating a new frame with a reference current frame . the flow is assumed pixel - wise dense , allowing to di ##sp ##lace each pixel to a new position , and the resulting pixel arrangement requires inter ##pol ##ation back onto a regular grid . we use bi ##line ##ar inter ##pol ##ation as it is much more efficient than the thin - plate sp ##line inter ##pol ##ation originally proposed in . optical flow is a function of parameters and is represented with two feature maps corresponding to displacement ##s for the and dimensions , thus a compensated image can be expressed as , or more con ##cise ##ly we adopt a multi - scale design to represent the flow , which has been shown to be effective in classical methods and also in more recently proposed spatial transform ##er techniques . a sc ##hema ##tic of the design is shown in fig : transform ##er and flow estimation modules are detailed in tab : transform ##er . first , a coarse estimate of the flow is obtained by early fu ##sing the two input frames and downs ##cal ##ing spatial dimensions with stride ##d con ##vo ##lu ##tions . the estimated flow is upscale ##d with sub - pixel con ##vo ##lu ##tion and the result is applied to warp the target frame producing . the warped image is then processed together with the coarse flow and the original images through a fine flow estimation module . this uses a single stride ##d con ##vo ##lu ##tion with stride and a final ups ##cal ##ing stage to obtain a finer flow map . the final motion compensated frame is obtained by warp ##ing the target frame with the total flow . output activation ##s use tan ##h to represent pixel displacement in normal ##ised space , such that a displacement of means maximum displacement from the center to the border of the image . to train the spatial transform ##er to perform motion compensation we opt ##imi ##se its parameters to mini ##mise the ms ##e between the transformed frame and the reference frame . similar ##y to classical optical flow methods , we found that it is generally helpful to con ##stra ##in the flow to behave smoothly in space , and so we penal ##ise the hub ##er loss of the flow map gradient ##s , namely in practice we approximate the hub ##er loss with , where . this function has a smooth behaviour near the origin and is spa ##rs ##ity promoting far from it . the spatial transform ##er module is advantage ##ous relative to other motion compensation mechanisms as it is straightforward to combine with a sr network to perform joint motion compensation and video sr . referring to fig : network , the same parameters can be used to model motion of the outer two frames relative to the central frame . the spatial transform ##er and sr modules are both different ##iable and therefore end - to - end train ##able . as a result , they can be jointly opt ##imi ##sed to mini ##mise a composite loss combining the accuracy of the reconstruction in e ##q : image - sr - objective with the fidelity of motion compensation in e ##q : motion - compensation - objective , namely section : experiments and results in this section , we first anal ##yse spat ##io - temporal networks for video sr in isolation and later evaluate the benefits of introducing motion compensation . we restrict our experiments to tackle and ups ##cal ##ing of full hd video resolution ( ) , and no compression is applied . to ensure a fair comparison of methods , the number of network parameters need to be comparable so that gains in performance can be attributed to specific choices of network resource allocation and not to a trivial increase in capacity . for a layer , the number of floating - point operations to rec ##ons ##truct a frame is approximate ##d by in measuring the complexity of slow fusion networks with weight sharing we look at steady - state operation where the output of some layers is reused from one frame to the following . we note that the analysis of vs ##rnet variants in does not take into account model complexity . sub ##section : experimental setup sub ##su ##bs ##ection : data we use the cd ##v ##l database , which contains un ##com ##pressed full hd videos excluding repeated videos , and choose a subset of videos for training . the videos are downs ##cal ##ed and random samples are extracted from each hr - l ##r video pair to obtain training samples , of which are used for validation . depending on the network architecture , we refer to a sample as a single input - output frame pair for single frame networks , or as a block of consecutive l ##r input frames and the corresponding central hr frame for spat ##io - temporal networks . the remaining videos are used for testing . although the total number of training frames is large , we fore ##see that the methods presented could benefit from a richer , more diverse set of videos . additionally , we present a bench ##mark against various sr methods on publicly available videos that are rec ##urrent ##ly used in the literature and we refer to as vi ##d ##4 . sub ##su ##bs ##ection : network training and parameters all sr models are trained following the same protocol and share similar hyper ##para ##meter ##s . filter sizes are set to , and all non - linear ##ities are rec ##ti ##fied linear units except for the output layer , which uses a linear activation . bias ##es are initial ##ised to and weights use orthogonal initial ##isation with gain following recommendations in . all hidden layers are set to have the same number of features . video samples are broken into non - overlapping sub - samples of spatial dimensions , which are randomly grouped in batch ##es for st ##och ##astic opt ##imi ##sation . we employ adam with a learning rate and an initial batch size . every epoch ##s the batch size is doubled until it reaches a maximum size of . we choose for layers where the network temporal depth is ( layers in gray in fig : early - fusion , fig : slow - fusion , fig : 3d ##con ##v ) , and to maintain comparable network sizes we choose . this ensures that the number of features per hidden layer in early and slow fusion networks is always the same . for instance , the network shown in fig : slow - fusion , for which and for , the number of features in a layer network for sr would be 6 , 8 , 12 , 24 , 24 , . sub ##section : spat ##io - temporal video sr sub ##su ##bs ##ection : single vs multi frame early fusion first , we investigate the impact of the number of input frames on complexity and accuracy without motion compensation . we compare single frame models ( sf ) against early fusion spat ##io - temporal models using 3 , 5 and 7 input frames ( e ##3 , e ##5 and e ##7 ) . ps ##nr results on the cd ##v ##l data ##set for networks of 6 to 11 layers are plotted in fig : single - vs - multi - frame . exploit ##ing spat ##io - temporal correlation ##s provides a more accurate result relative to an independent processing of frames . the increase in complexity from early fusion is marginal because only the first layer contributes to an increase of operations . although the accuracy of spat ##io - temporal models is relatively similar , we find that e ##7 slightly under ##per ##forms . it is likely that temporal depend ##encies beyond 5 frames become too complex for networks to learn useful information and act as noise de ##grad ##ing their performance . notice also that , whereas the performance increase from network depth is minimal after 8 layers for single frame networks , this increase is more consistent for spat ##io - temporal models . sub ##su ##bs ##ection : early vs slow fusion here we compare the different treatments of the temporal dimension discussed in ss ##ec : st - networks . we assume networks with an input of frames and slow fusion models with filter temporal depths as in fig : st - networks . using sf , e ##5 , s ##5 , and s ##5 - sw to refer to single frame networks and 5 frame input networks using early fusion , slow fusion , and slow fusion with shared weights , we show in tab : early - vs - slow - fusion results for 7 and 9 layer networks . as seen previously , early fusion networks attain a higher accuracy at a marginal 3 % increase in operations relative to the single frame models , and as expected , slow fusion architecture ##s provide efficiency advantages . slow fusion is faster than early fusion because it uses fewer features in the initial layers . referring to e ##q : operations , slow fusion uses in the first layers and , which results in fewer operations than , as used in early fusion . while the 7 layer network sees a considerable decrease in accuracy using slow fusion relative to early fusion , the 9 layer network can benefit from the same accuracy while reducing its complexity with slow fusion by about 30 % . this suggests that in shallow networks the best use of network resources is to ut ##ilis ##e the full network capacity to jointly process all temporal information as done by early fusion , but that in deeper networks slowly fu ##sing the temporal dimension is beneficial , which is in line with the results presented by for video classification . additionally , weight sharing decreases accuracy because of the reduction in network parameters , but the re ##usa ##bility of network features means fewer operations are needed per frame . for instance , the 7 layer s ##5 - sw network shows a reduction of almost 30 % of operations with a minimal decrease in accuracy relative to sf . using 7 layers with e ##5 nevertheless shows better performance and faster operation than s ##5 - sw with 9 layers , and in all cases we found that early or slow fusion consistently out ##per ##formed slow fusion with shared weights in this performance and efficiency trade - off . con ##vo ##lu ##tions in spat ##io - temporal domain were shown in to work well for video action recognition , but with larger capacity and many more frames processed jointly . we spec ##ulate this could be the reason why the conclusions drawn from this high - level vision task do not extra ##pol ##ate to the sr problem . sub ##section : motion compensated video sr [ b ] 0 . 32 [ b ] 0 . 32 [ b ] 0 . 32 in this section , the proposed frame motion compensation is combined with an early fusion network of temporal depth . first , the motion compensation module is trained independently using e ##q : video - sr - me ##mc , where the first term is ignored and , . this results in a network that will compensate the motion of three consecutive frames by est ##imating the flow maps of outer frames relative to the middle frame . an example of a flow map obtained for one frame is shown in fig : me ##mc , where we also show the effect the motion compensation module has on three consecutive frames . the early fusion motion compensated sr network ( e ##3 - mc ) is initial ##ised with a compensation and a sr network pre ##train ##ed separately , and the full model is then jointly opt ##imi ##sed with e ##q : video - sr - me ##mc ( , ) . results for sr on cd ##v ##l are compared in tab : motion - compensated - video - sr against a single frame ( sf ) model and early fusion without motion compensation ( e ##3 ) . e ##3 - mc results in a ps ##nr that is sometimes almost twice the improvement of e ##3 relative to sf , which we attribute to the fact that the network adapt ##s the sr input to maxim ##ise temporal red ##unda ##ncy . in fig : mc ##sr - x ##3 we show how this improvement is reflected in better structure preservation . sub ##section : comparison to state - of - the - art we show in tab : set ##4 the performance on vi ##d ##4 for sr ##c ##nn , es ##pc ##n , vs ##rnet and the proposed method , which we refer to as video es ##pc ##n ( ve ##sp ##c ##n ) . to demonstrate its benefits in efficiency and quality we evaluate two early fusion models : a 5 layer 3 frame network ( 5 ##l - e ##3 ) and a 9 layer 3 frame network with motion compensation ( 9 ##l - e ##3 - mc ) . the metric ##s compared are ps ##nr , ss ##im and movie indices . the movie index was designed as a metric measuring video quality that co ##rre ##lates with human perception and incorporates a notion of temporal consistency . we also directly compare the number of operations per frame of all cnn - based approaches for ups ##cal ##ing a generic p frame . reconstruction ##s for sr ##c ##nn , es ##pc ##n and vs ##rnet use models provided by the authors . sr ##c ##nn , es ##pc ##n and ve ##sp ##c ##n were tested on the ##ano and las ##agne , and for vs ##rnet we used available caf ##fe mat ##lab code . we crop spatial borders as well as initial and final frames on all reconstruction ##s for fair comparison against vs ##rnet . sub ##su ##bs ##ection : quality comparison an example of visual differences is shown in fig : set ##4 - visual ##isation against the motion compensated network . from the close - up images , we see how the structural detail of the original video is better recovered by the proposed ve ##sp ##c ##n method . this is reflected in tab : set ##4 , where it sur ##pass ##es any other method in ps ##nr and ss ##im by a large margin . fig : set ##4 - visual ##isation also shows temporal profiles on the row highlighted by a dashed line through 25 consecutive frames , demonstrating a better temporal co ##her ##ence of the reconstruction proposed . the great temporal co ##her ##ence of ve ##sp ##c ##n also explains the significant reduction in the movie index . sub ##su ##bs ##ection : efficiency comparison the complexity of methods in tab : set ##4 is determined by network and input image sizes . sr ##c ##nn and vs ##rnet ups ##amp ##le l ##r images before attempting to super - resolve them , which considerably increases the required number of operations . vs ##rnet is particularly expensive because it processes input frames in and feature layers , whereas sub - pixel con ##vo ##lu ##tion greatly reduces the number of operations required in es ##pc ##n and ve ##sp ##c ##n . as a reference , es ##pc ##n runs at ms per frame on a k ##2 gp ##u . the enhanced capabilities of spat ##io - temporal networks allow to reduce the network operations of ve ##sp ##c ##n relative to es ##pc ##n while still matching its accuracy . as an example we show ve ##sp ##c ##n with 5 ##l - e ##3 , which reduces the number of operations by about 20 % relative to es ##pc ##n while maintaining a similar performance in all evaluated quality metric ##s . the operations for motion compensation in ve ##sp ##c ##n with 9 ##l - e ##3 - mc , included in tab : set ##4 results , amount to and go ##ps for and ups ##cal ##ing , applied twice for each input frame requiring motion compensation . this makes the proposed motion compensated video sr very efficient relative to other approaches . for example , motion compensation in vs ##rnet is said to require 55 seconds per frame and is the computational bottle ##neck . this is not accounted for in tab : set ##4 but is slower than ve ##sp ##c ##n with 9 ##l - e ##3 - mc , which can run in the order of seconds . the optical flow method in vs ##rnet was originally shown to run at ms on gp ##u for each frame of dimensions , but this is still considerably slower than the proposed solution considering motion compensation is required for more than a single frame of hd dimensions . section : conclusion in this paper we combine the efficiency advantages of sub - pixel con ##vo ##lu ##tions with temporal fusion strategies to present real - time spat ##io - temporal models for video sr . the spat ##io - temporal models used are shown to facilitate an improvement in reconstruction accuracy and temporal consistency or reduce computational complexity relative to independent single frame processing . the models investigated are extended with a motion compensation mechanism based on spatial transform ##er networks that is efficient and jointly train ##able for video sr . results obtained with approaches that incorporate explicit motion compensation are demonstrated to be superior in terms of ps ##nr and temporal consistency compared to spat ##io - temporal models alone , and out ##per ##form the current state of the art in video sr . bibliography : references",
        "pred_seq": "[SEP] [SEP] [SEP] real resolution [SEP] [unused0] [SEP] [SEP] real resolution [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "realtime video superresolution"
                    ]
                ]
            },
            {
                "Material": [],
                "Method": [],
                "Metric": [
                    [
                        "realtime video superresolution"
                    ]
                ],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "vid4"
                    ]
                ],
                "Method": [
                    [
                        "video espcn",
                        "vespcn",
                        "vespcn method"
                    ]
                ],
                "Metric": [
                    [
                        "movie",
                        "movie index"
                    ]
                ],
                "Task": [
                    [
                        "realtime video superresolution",
                        "video superresolution",
                        "video",
                        "sr",
                        "realtime video sr",
                        "hd video sr",
                        "spatiotemporal video sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "vid4"
                    ]
                ],
                "Method": [
                    [
                        "bicubic interpolation",
                        "bicubic upsampling"
                    ]
                ],
                "Metric": [
                    [
                        "movie",
                        "movie index"
                    ]
                ],
                "Task": [
                    [
                        "realtime video superresolution",
                        "video superresolution",
                        "video",
                        "sr",
                        "realtime video sr",
                        "hd video sr",
                        "spatiotemporal video sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "vid4"
                    ]
                ],
                "Method": [
                    [
                        "video espcn",
                        "vespcn",
                        "vespcn method"
                    ]
                ],
                "Metric": [
                    [
                        "psnr"
                    ]
                ],
                "Task": [
                    [
                        "realtime video superresolution",
                        "video superresolution",
                        "video",
                        "sr",
                        "realtime video sr",
                        "hd video sr",
                        "spatiotemporal video sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "vid4"
                    ]
                ],
                "Method": [
                    [
                        "bicubic interpolation",
                        "bicubic upsampling"
                    ]
                ],
                "Metric": [
                    [
                        "psnr"
                    ]
                ],
                "Task": [
                    [
                        "realtime video superresolution",
                        "video superresolution",
                        "video",
                        "sr",
                        "realtime video sr",
                        "hd video sr",
                        "spatiotemporal video sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "vid4"
                    ]
                ],
                "Method": [
                    [
                        "video espcn",
                        "vespcn",
                        "vespcn method"
                    ]
                ],
                "Metric": [
                    [
                        "ssim"
                    ]
                ],
                "Task": [
                    [
                        "realtime video superresolution",
                        "video superresolution",
                        "video",
                        "sr",
                        "realtime video sr",
                        "hd video sr",
                        "spatiotemporal video sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "vid4"
                    ]
                ],
                "Method": [
                    [
                        "bicubic interpolation",
                        "bicubic upsampling"
                    ]
                ],
                "Metric": [
                    [
                        "ssim"
                    ]
                ],
                "Task": [
                    [
                        "realtime video superresolution",
                        "video superresolution",
                        "video",
                        "sr",
                        "realtime video sr",
                        "hd video sr",
                        "spatiotemporal video sr"
                    ]
                ]
            }
        ]
    },
    "7": {
        "doctext": "document : line : large - scale information network em ##bed ##ding this paper studies the problem of em ##bed ##ding very large information networks into low - dimensional vector spaces , which is useful in many tasks such as visual ##ization , node classification , and link prediction . most existing graph em ##bed ##ding methods do not scale for real world information networks which usually contain millions of nodes . in this paper , we propose a novel network em ##bed ##ding method called the \" line , \" which is suitable for arbitrary types of information networks : und ##ire ##cted , directed , and / or weighted . the method opt ##imi ##zes a carefully designed objective function that preserves both the local and global network structures . an edge - sampling algorithm is proposed that addresses the limitation of the classical st ##och ##astic gradient descent and improves both the effectiveness and the efficiency of the inference . empirical experiments prove the effectiveness of the line on a variety of real - world information networks , including language networks , social networks , and citation networks . the algorithm is very efficient , which is able to learn the em ##bed ##ding of a network with millions of vertices and billions of edges in a few hours on a typical single machine . the source code of the line is available online . ( i ##w ##3 ##c ##2 ) . www ##20 ##15 , may ##18 - 22 , 2015 , florence , italy . ac ##m 978 - 1 - 450 ##3 - 34 ##6 ##9 - 3 / 15 / 05 . http : / / d ##x . doi . org / 10 . 114 ##5 / 273 ##6 ##27 ##7 . 274 ##10 ##9 ##3 i . 2 . 6 ##art ##ific ##ial intelligence ##lea ##rn ##ing algorithms , experimentation section : introduction information networks are ubiquitous in the real world with examples such as airline networks , publication networks , social and communication networks , and the world wide web . the size of these information networks ranges from hundreds of nodes to millions and billions of nodes . analyzing large information networks has been attracting increasing attention in both academia and industry . this paper studies the problem of em ##bed ##ding information networks into low - dimensional spaces , in which every vertex is represented as a low - dimensional vector . such a low - dimensional em ##bed ##ding is very useful in a variety of applications such as visual ##ization , node classification , link prediction , and recommendation . various methods of graph em ##bed ##ding have been proposed in the machine learning literature ( e . g . , ) . they generally perform well on smaller networks . the problem becomes much more challenging when a real world information network is concerned , which typically contains millions of nodes and billions of edges . for example , the twitter follow ##ee - follower network contains 175 million active users and around twenty billion edges in 2012 . most existing graph em ##bed ##ding algorithms do not scale for networks of this size . for example , the time complexity of classical graph em ##bed ##ding algorithms such as md ##s , iso ##ma ##p , lap ##la ##cian e ##igen ##ma ##p are at least quad ##ratic to the number of vertices , which is too expensive for networks with millions of nodes . although a few very recent studies approach the em ##bed ##ding of large - scale networks , these methods either use an indirect approach that is not designed for networks ( e . g . , ) or lack a clear objective function tailored for network em ##bed ##ding ( e . g . , ) . we anti ##ci ##pate that a new model with a carefully designed objective function that preserves properties of the graph and an efficient optimization technique should effectively find the em ##bed ##ding of millions of nodes . in this paper , we propose such a network em ##bed ##ding model called the \" line , \" which is able to scale to very large , arbitrary types of networks : und ##ire ##cted , directed and / or weighted . the model opt ##imi ##zes an objective which preserves both the local and global network structures . naturally , the local structures are represented by the observed links in the networks , which capture the first - order proximity between the vertices . most existing graph em ##bed ##ding algorithms are designed to preserve this first - order proximity , e . g . , iso ##ma ##p and lap ##la ##cian e ##igen ##ma ##p , even if they do not scale . we observe that in a real - world network many ( if not the majority of ) legitimate links are actually not observed . in other words , the observed first - order proximity in the real world data is not sufficient for preserving the global network structures . as a complement , we explore the second - order proximity between the vertices , which is not determined through the observed tie strength but through the shared neighborhood structures of the vertices . the general notion of the second - order proximity can be interpreted as nodes with shared neighbors being likely to be similar . such an intuition can be found in the theories of sociology and linguistics . for example , \" the degree of overlap of two people ' s friendship networks co ##rre ##lates with the strength of ties between them , \" in a social network ; and \" you shall know a word by the company it keeps \" ( firth , j . r . 1957 : 11 ) in text corp ##ora . indeed , people who share many common friends are likely to share the same interest and become friends , and words that are used together with many similar words are likely to have similar meanings . fig . [ reference ] presents an ill ##ust ##rative example . as the weight of the edge between vertex 6 and 7 is large , i . e . , 6 and 7 have a high first - order proximity , they should be represented closely to each other in the embedded space . on the other hand , though there is no link between vertex 5 and 6 , they share many common neighbors , i . e . , they have a high second - order proximity and therefore should also be represented closely to each other . we expect that the consideration of the second - order proximity effectively complement ##s the spa ##rs ##ity of the first - order proximity and better preserves the global structure of the network . in this paper , we will present carefully designed objectives that preserve the first - order and the second - order pro ##xi ##mit ##ies . even if a sound objective is found , opt ##imi ##zing it for a very large network is challenging . one approach that attracts attention in recent years is using the st ##och ##astic gradient descent for the optimization . however , we show that directly deploy ##ing the st ##och ##astic gradient descent is problematic for real world information networks . this is because in many networks , edges are weighted and the weights usually present a high variance . consider a word co - occurrence network , in which the weights ( co - occurrences ) of word pairs may range from one to hundreds of thousands . these weights of the edges will be multiplied into the gradient ##s , resulting in the explosion of the gradient ##s and thus compromise the performance . to address this , we propose a novel edge - sampling method , which improves both the effectiveness and efficiency of the inference . we sample the edges with the pro ##ba ##bilities proportional to their weights , and then treat the sampled edges as binary edges for model up ##dating . with this sampling process , the objective function remains the same and the weights of the edges no longer affect the gradient ##s . the line is very general , which works well for directed or und ##ire ##cted , weighted or un ##weight ##ed graphs . we evaluate the performance of the line with various real - world information networks , including language networks , social networks , and citation networks . the effectiveness of the learned em ##bed ##ding ##s is evaluated within multiple data mining tasks , including word analogy , text classification , and node classification . the results suggest that the line model out ##per ##forms other competitive baseline ##s in terms of both effectiveness and efficiency . it is able to learn the em ##bed ##ding of a network with millions of nodes and billions of edges in a few hours on a single machine . to sum ##mar ##ize , we make the following contributions : we propose a novel network em ##bed ##ding model called the \" line , \" which suits arbitrary types of information networks and easily scales to millions of nodes . it has a carefully designed objective function that preserves both the first - order and second - order pro ##xi ##mit ##ies . we propose an edge - sampling algorithm for opt ##imi ##zing the objective . the algorithm tackles the limitation of the classical st ##och ##astic gradient decent and improves the effectiveness and efficiency of the inference . we conduct extensive experiments on real - world information networks . experimental results prove the effectiveness and efficiency of the proposed line model . organization . the rest of this paper is organized as follows . section [ reference ] sum ##mar ##izes the related work . section [ reference ] formally defines the problem of large - scale information network em ##bed ##ding . section [ reference ] introduces the line model in details . section [ reference ] presents the experimental results . finally we conclude in section [ reference ] . section : related work our work is related to classical methods of graph em ##bed ##ding or dimension reduction in general , such as multi ##dim ##ens ##ional scaling ( md ##s ) , iso ##ma ##p , ll ##e and lap ##la ##cian e ##igen ##ma ##p . these approaches typically first construct the affinity graph using the feature vectors of the data points , e . g . , the k - nearest neighbor graph of data , and then em ##bed the affinity graph into a low dimensional space . however , these algorithms usually rely on solving the leading e ##igen ##ve ##ctors of the affinity matrices , the complexity of which is at least quad ##ratic to the number of nodes , making them in ##ef ##fi ##cie ##nt to handle large - scale networks . among the most recent literature is a technique called graph factor ##ization . it finds the low - dimensional em ##bed ##ding of a large graph through matrix factor ##ization , which is opt ##imi ##zed using st ##och ##astic gradient descent . this is possible because a graph can be represented as an affinity matrix . however , the objective of matrix factor ##ization is not designed for networks , therefore does not necessarily preserve the global network structure . intuitive ##ly , graph factor ##ization expects nodes with higher first - order proximity are represented closely . instead , the line model uses an objective that is particularly designed for networks , which preserves both the first - order and the second - order pro ##xi ##mit ##ies . practically , the graph factor ##ization method only applies to und ##ire ##cted graphs while the proposed model is applicable for both und ##ire ##cted and directed graphs . the most recent work related with ours is deep ##walk , which deploy ##s a truncated random walk for social network em ##bed ##ding . although empirical ##ly effective , the deep ##walk does not provide a clear objective that art ##iculate ##s what network properties are preserved . intuitive ##ly , deep ##walk expects nodes with higher second - order proximity yield similar low - dimensional representations , while the line preserves both first - order and second - order pro ##xi ##mit ##ies . deep ##walk uses random walks to expand the neighborhood of a vertex , which is analog ##ical to a depth - first search . we use a breadth - first search strategy , which is a more reasonable approach to the second - order proximity . practically , deep ##walk only applies to un ##weight ##ed networks , while our model is applicable for networks with both weighted and un ##weight ##ed edges . in section [ reference ] , we empirical ##ly compare the proposed model with these methods using various real world networks . section : problem definition we formally define the problem of large - scale information network em ##bed ##ding using first - order and second - order pro ##xi ##mit ##ies . we first define an information network as follows : theorem : ( information network ) an information network is defined as = g ( v , e ) , where v is the set of vertices , each representing a data object and e is the set of edges between the vertices , each representing a relationship between two data objects . each edge ##\u2208 ##ee is an ordered pair = e ( u , v ) and is associated with a weight > wu ##v ##0 , which indicates the strength of the relation . if g is und ##ire ##cted , we have ##\u2261 ( u , v ) ( v , u ) and ##\u2261 ##wu ##v ##w ##vu ; if g is directed , we [UNK] ( u , v ) ( v , u ) [UNK] . in practice , information networks can be either directed ( e . g . , citation networks ) or und ##ire ##cted ( e . g . , social network of users in facebook ) . the weights of the edges can be either binary or take any real value . note that while negative edge weights are possible , in this study we only consider non - negative weights . for example , in citation networks and social networks , takes binary values ; in co - occurrence networks between different objects , can take any non - negative value . the weights of the edges in some networks may diver ##ge as some objects co - occur many times while others may just co - occur a few times . em ##bed ##ding an information network into a low - dimensional space is useful in a variety of applications . to conduct the em ##bed ##ding , the network structures must be preserved . the first intuition is that the local network structure , i . e . , the local pair ##wise proximity between the vertices , must be preserved . we define the local network structures as the first - order proximity between the vertices : theorem : ( first - order proximity ) the first - order proximity in a network is the local pair ##wise proximity between two vertices . for each pair of vertices linked by an edge ( u , v ) , the weight on that edge , wu ##v , indicates the first - order proximity between u and v . if no edge is observed between u and v , their first - order proximity is 0 . the first - order proximity usually implies the similarity of two nodes in a real - world network . for example , people who are friends with each other in a social network tend to share similar interests ; pages linking to each other in world wide web tend to talk about similar topics . because of this importance , many existing graph em ##bed ##ding algorithms such as iso ##ma ##p , ll ##e , lap ##la ##cian e ##igen ##ma ##p , and graph factor ##ization have the objective to preserve the first - order proximity . however , in a real world information network , the links observed are only a small proportion , with many others missing . a pair of nodes on a missing link has a zero first - order proximity , even though they are intrinsic ##ally very similar to each other . therefore , first - order proximity alone is not sufficient for preserving the network structures , and it is important to seek an alternative notion of proximity that addresses the problem of spa ##rs ##ity . a natural intuition is that vertices that share similar neighbors tend to be similar to each other . for example , in social networks , people who share similar friends tend to have similar interests and thus become friends ; in word co - occurrence networks , words that always co - occur with the same set of words tend to have similar meanings . we therefore define the second - order proximity , which complement ##s the first - order proximity and preserves the network structure . theorem : ( second - order proximity ) the second - order proximity between a pair of vertices ( u , v ) in a network is the similarity between their neighborhood network structures . mathematical ##ly , let = pu ( wu , 1 , \u2026 , wu , | v | ) denote the first - order proximity of u with all the other vertices , then the second - order proximity between u and v is determined by the similarity between pu and pv . if no vertex is linked from / to both u and v , the second - order proximity between u and v is 0 . we investigate both first - order and second - order proximity for network em ##bed ##ding , which is defined as follows . theorem : ( large - scale information network em ##bed ##ding ) given a large network = g ( v , e ) , the problem of large - scale information network em ##bed ##ding aims to represent each vertex ##\u2208 ##v ##v into a low - dimensional space rd , i . e . , learning a function : f ##g ##\u2192 ##vr ##d , [UNK] | v | . in the space rd , both the first - order proximity and the second - order proximity between the vertices are preserved . next , we introduce a large - scale network em ##bed ##ding model that preserves both first - and second - order pro ##xi ##mit ##ies . section : line : large - scale information network em ##bed ##ding a desirable em ##bed ##ding model for real world information networks must satisfy several requirements : first , it must be able to preserve both the first - order proximity and the second - order proximity between the vertices ; second , it must scale for very large networks , say millions of vertices and billions of edges ; third , it can deal with networks with arbitrary types of edges : directed , und ##ire ##cted and / or weighted . in this section , we present a novel network em ##bed ##ding model called the \" line , \" which sat ##is ##fies all the three requirements . sub ##section : model description we describe the line model to preserve the first - order proximity and second - order proximity separately , and then introduce a simple way to combine the two proximity . sub ##su ##bs ##ection : line with first - order proximity the first - order proximity refers to the local pair ##wise proximity between the vertices in the network . to model the first - order proximity , for each und ##ire ##cted edge , we define the joint probability between vertex and as follows : where is the low - dimensional vector representation of vertex . e ##q ##n . ( [ reference ] ) defines a distribution over the space , and its empirical probability can be defined as , where . to preserve the first - order proximity , a straightforward way is to minimize the following objective function : where is the distance between two distributions . we choose to minimize the k ##l - diver ##gence of two probability distributions . replacing with k ##l - diver ##gence and om ##itt ##ing some constant ##s , we have : note that the first - order proximity is only applicable for und ##ire ##cted graphs , not for directed graphs . by finding the that minimize the objective in e ##q ##n . ( [ reference ] ) , we can represent every vertex in the d - dimensional space . sub ##su ##bs ##ection : line with second - order proximity the second - order proximity is applicable for both directed and und ##ire ##cted graphs . given a network , without loss of general ##ity , we assume it is directed ( an und ##ire ##cted edge can be considered as two directed edges with opposite directions and equal weights ) . the second - order proximity assumes that vertices sharing many connections to other vertices are similar to each other . in this case , each vertex is also treated as a specific \" context \" and vertices with similar distributions over the \" contexts \" are assumed to be similar . therefore , each vertex plays two roles : the vertex itself and a specific \" context \" of other vertices . we introduce two vectors and , where is the representation of when it is treated as a vertex while is the representation of when it is treated as a specific \" context \" . for each directed edge , we first define the probability of \" context \" generated by vertex as : where is the number of vertices or \" contexts . \" for each vertex , e ##q ##n . ( [ reference ] ) actually defines a conditional distribution over the contexts , i . e . , the entire set of vertices in the network . as mentioned above , the second - order proximity assumes that vertices with similar distributions over the contexts are similar to each other . to preserve the second - order proximity , we should make the conditional distribution of the contexts specified by the low - dimensional representation be close to the empirical distribution . therefore , we minimize the following objective function : where is the distance between two distributions . as the importance of the vertices in the network may be different , we introduce in the objective function to represent the prestige of vertex in the network , which can be measured by the degree or estimated through algorithms such as page ##rank . the empirical distribution is defined as , where is the weight of the edge and is the out - degree of vertex , i . e . , where is the set of out - neighbors of . in this paper , for simplicity we set as the degree of vertex , i . e . , , and here we also adopt k ##l - diver ##gence as the distance function . replacing with k ##l - diver ##gence , setting and om ##itt ##ing some constant ##s , we have : by learning and that minimize this objective , we are able to represent every vertex with a d - dimensional vector . sub ##su ##bs ##ection : combining first - order and second - order pro ##xi ##mit ##ies to em ##bed the networks by preserving both the first - order and second - order proximity , a simple and effective way we find in practice is to train the line model which preserves the first - order proximity and second - order proximity separately and then con ##cate ##nate the em ##bed ##ding ##s trained by the two methods for each vertex . a more principle ##d way to combine the two proximity is to jointly train the objective function ( [ reference ] ) and ( [ reference ] ) , which we leave as future work . sub ##section : model optimization opt ##imi ##zing objective ( [ reference ] ) is computational ##ly expensive , which requires the sum ##mation over the entire set of vertices when calculating the conditional probability . to address this problem , we adopt the approach of negative sampling proposed in , which samples multiple negative edges according to some noisy distribution for each edge . more specifically , it specifies the following objective function for each edge : where is the si ##gm ##oid function . the first term models the observed edges , the second term models the negative edges drawn from the noise distribution and is the number of negative edges . we set as proposed in , where is the out - degree of vertex . for the objective function ( [ reference ] ) , there exists a trivial solution : , for i = and . to avoid the trivial solution , we can still utilize the negative sampling approach ( [ reference ] ) by just changing to . we adopt the as ##yn ##ch ##ron ##ous st ##och ##astic gradient algorithm ( as ##g ##d ) for opt ##imi ##zing e ##q ##n . ( [ reference ] ) . in each step , the as ##g ##d algorithm samples a mini - batch of edges and then updates the model parameters . if an edge is sampled , the gradient w . r . t . the em ##bed ##ding vector of vertex will be calculated as : note that the gradient will be multiplied by the weight of the edge . this will become problematic when the weights of edges have a high variance . for example , in a word co - occurrence network , some words co - occur many times ( e . g . , tens of thousands ) while some words co - occur only a few times . in such networks , the scales of the gradient ##s diver ##ge and it is very hard to find a good learning rate . if we select a large learning rate according to the edges with small weights , the gradient ##s on edges with large weights will explode while the gradient ##s will become too small if we select the learning rate according to the edges with large weights . sub ##su ##bs ##ection : optimization via edge sampling the intuition in solving the above problem is that if the weights of all the edges are equal ( e . g . , network with binary edges ) , then there will be no problem of choosing an appropriate learning rate . a simple treatment is thus to un ##fold a weighted edge into multiple binary edges , e . g . , an edge with weight is unfolded into binary edges . this will solve the problem but will significantly increase the memory requirement , especially when the weights of the edges are very large . to resolve this , one can sample from the original edges and treat the sampled edges as binary edges , with the sampling pro ##ba ##bilities proportional to the original edge weights . with this edge - sampling treatment , the overall objective function remains the same . the problem boil ##s down to how to sample the edges according to their weights . let denote the sequence of the weights of the edges . one can simply calculate the sum of the weights first , and then to sample a random value within the range of to see which interval [ the random value falls into . this approach takes time to draw a sample , which is costly when the number of edges is large . we use the alias table method to draw a sample according to the weights of the edges , which takes only time when repeatedly drawing samples from the same discrete distribution . sampling an edge from the alias table takes constant time , , and optimization with negative sampling takes time , where is the number of negative samples . therefore , overall each step takes time . in practice , we find that the number of steps used for optimization is usually proportional to the number of edges . therefore , the overall time complexity of the line is , which is linear to the number of edges , and does not depend on the number of vertices . the edge sampling treatment improves the effectiveness of the st ##och ##astic gradient descent without com ##promising the efficiency . sub ##section : discussion we discuss several practical issues of the line model . low degree vertices . one practical issue is how to accurately em ##bed vertices with small degrees . as the number of neighbors of such a node is very small , it is very hard to accurately in ##fer its representation , especially with the second - order proximity based methods which heavily rely on the number of \" contexts . \" an intuitive solution to this is expanding the neighbors of those vertices by adding higher order neighbors , such as neighbors of neighbors . in this paper , we only consider adding second - order neighbors , i . e . , neighbors of neighbors , to each vertex . the weight between vertex and its second - order neighbor is measured as in practice , one can only add a subset of vertices which have the largest proximity with the low degree vertex . new vertices . another practical issue is how to find the representation of newly arrived vertices . for a new vertex , if its connections to the existing vertices are known , we can obtain the empirical distribution and over existing vertices . to obtain the em ##bed ##ding of the new vertex , according to the objective function e ##q ##n . ( [ reference ] ) or e ##q ##n . ( [ reference ] ) , a straightforward way is to minimize either one of the following objective functions by up ##dating the em ##bed ##ding of the new vertex and keeping the em ##bed ##ding ##s of existing vertices . if no connections between the new vertex and existing vertices are observed , we must resort to other information , such as the textual information of the vertices , and we leave it as our future work . section : experiments we empirical ##ly evaluated the effectiveness and efficiency of the line . we applied the method to several large - scale real - world networks of different types , including a language network , two social networks , and two citation networks . sub ##section : experiment setup paragraph : data sets ( 1 ) language network . we constructed a word co - occurrence network from the entire set of english wikipedia pages . words within every 5 - word sliding window are considered to be co - occurring with each other . words with frequency smaller than 5 are filtered out . ( 2 ) social networks . we use two social networks : flick ##r and youtube ##ava ##ila ##ble at http : / / social ##net ##works . mp ##i - sw ##s . org / data - im ##c ##200 ##7 . html . the flick ##r network is dense ##r than the youtube network ( the same network as used in deep ##walk ) . ( 3 ) citation networks . two types of citation networks are used : an author citation network and a paper citation network . we use the db ##lp data set to construct the citation networks between authors and between papers . the author citation network records the number of papers written by one author and cited by another author . the detailed statistics of these networks are summarized into table [ reference ] . they represent a variety of information networks : directed and und ##ire ##cted , binary and weighted . each network contains at least half a million nodes and millions of edges , with the largest network containing around two million nodes and a billion edges . paragraph : compared algorithms we compare the line model with several existing graph em ##bed ##ding methods that are able to scale up to very large networks . we do not compare with some classical graph em ##bed ##ding algorithms such as md ##s , iso ##ma ##p , and lap ##la ##cian e ##igen ##ma ##p , as they can not handle networks of this scale . graph factor ##ization ( g ##f ) . we compare with the matrix factor ##ization techniques for graph factor ##ization . an information network can be represented as an affinity matrix , and is able to represent each vertex with a low - dimensional vector through matrix factor ##ization . graph factor ##ization is opt ##imi ##zed through st ##och ##astic gradient descent and is able to handle large networks . it only applies to und ##ire ##cted networks . deep ##walk . deep ##walk is an approach recently proposed for social network em ##bed ##ding , which is only applicable for networks with binary edges . for each vertex , truncated random walks starting from the vertex are used to obtain the context ##ual information , and therefore only second - order proximity is utilized . line - sg ##d . this is the line model introduced in section [ reference ] that opt ##imi ##zes the objective e ##q ##n . ( [ reference ] ) or e ##q ##n . ( [ reference ] ) directly with st ##och ##astic gradient descent . with this approach , the weights of the edges are directly multiplied into the gradient ##s when the edges are sampled for model up ##dating . there are two variants of this approach : line - sg ##d ( 1st ) and line - sg ##d ( 2nd ) , which use first - and second - order proximity respectively . line . this is the line model opt ##imi ##zed through the edge - sampling treatment introduced in section [ reference ] . in each st ##och ##astic gradient step , an edge is sampled with the probability proportional to its weight and then treated as binary for model up ##dating . there are also two variants : line ( 1st ) and line ( 2nd ) . like the graph factor ##ization , both line ( 1st ) and line - sg ##d ( 1st ) only apply to und ##ire ##cted graphs . line ( 2nd ) and line - sg ##d ( 2nd ) apply to both und ##ire ##cted and directed graphs . line ( 1st + 2nd ) : to utilize both first - order and second - order proximity , a simple and effective way is to con ##cate ##nate the vector representations learned by line ( 1st ) and line ( 2nd ) into a longer vector . after con ##cate ##nation , the dimensions should be re - weighted to balance the two representations . in a supervised learning task , the weight ##ing of dimensions can be automatically found based on the training data . in an un ##su ##per ##vis ##ed task , however , it is more difficult to set the weights . therefore we only apply line ( 1st + 2nd ) to the scenario of supervised tasks . paragraph : parameter settings the mini - batch size of the st ##och ##astic gradient descent is set as 1 for all the methods . similar to , the learning rate is set with the starting value and , where is the total number of mini - batch ##es or edge samples . for fair comparisons , the dimensional ##ity of the em ##bed ##ding ##s of the language network is set to 200 , as used in word em ##bed ##ding . for other networks , the dimension is set as 128 by default , as used in . other default settings include : the number of negative samples for line and line - sg ##d ; the total number of samples billion for line ( 1st ) and line ( 2nd ) , billion for g ##f ; window size , walk length , walks per vertex for deep ##walk . all the em ##bed ##ding vectors are finally normal ##ized by setting . sub ##section : quantitative results sub ##su ##bs ##ection : language network we start with the results on the language network , which contains two million nodes and a billion edges . two applications are used to evaluate the effectiveness of the learned em ##bed ##ding ##s : word analogy and document classification . word analogy . this task is introduced by mi ##ko ##lov et al . . given a word pair and a word , the task aims to find a word , such that the relation between and is similar to the relation between and , or denoted as : . for instance , given a word pair ( \" china \" , \" beijing \" ) and a word \" france , \" the right answer should be \" paris \" because \" beijing \" is the capital of \" china \" just as \" paris \" is the capital of \" france . \" given the word em ##bed ##ding ##s , this task is solved by finding the word whose em ##bed ##ding is closest to the vector in terms of co ##sin ##e proximity , i . e . , . two categories of word analogy are used in this task : semantic and syn ##ta ##ctic . significantly out ##per ##forms g ##f at the : * * 0 . 01 and * 0 . 05 level , paired t - test . table [ reference ] reports the results of word analogy using the em ##bed ##ding ##s of words learned on the wikipedia corp ##ora ( skip ##gram ) or the wikipedia word network ( all other methods ) . for graph factor ##ization , the weight between each pair of words is defined as the log ##ari ##th ##m of the number of co - occurrences , which leads to better performance than the original value of co - occurrences . for deep ##walk , different cut ##off threshold ##s are tried to convert the language network into a binary network , and the best performance is achieved when all the edges are kept in the network . we also compare with the state - of - the - art word em ##bed ##ding model skip ##gram , which learns the word em ##bed ##ding ##s directly from the original wikipedia pages and is also implicit ##ly a matrix factor ##ization approach . the window size is set as 5 , the same as used for constructing the language network . we can see that line ( 2nd ) out ##per ##forms all other methods , including the graph em ##bed ##ding methods and the skip ##gram . this indicates that the second - order proximity better captures the word semantics compared to the first - order proximity . this is not surprising , as a high second - order proximity implies that two words can be replaced in the same context , which is a stronger indicator of similar semantics than first - order co - occurrences . it is intriguing that the line ( 2nd ) out ##per ##forms the state - of - the - art word em ##bed ##ding model trained on the original corpus . the reason may be that a language network better captures the global structure of word co - occurrences than the original word sequences . among other methods , both graph factor ##ization and line ( 1st ) significantly out ##per ##form deep ##walk even if deep ##walk explores second - order proximity . this is because deep ##walk has to ignore the weights ( i . e . , co - occurrences ) of the edges , which is very important in a language network . the performance by the line models directly opt ##imi ##zed with sg ##d is much worse , because the weights of the edges in the language network diver ##ge , which range from a single digit to tens of thousands , making the learning process suffer . the line opt ##imi ##zed by the edge - sampling treatment effectively addresses this problem , and performs very well using either first - order or second - order proximity . all the models are run on a single machine with 1 t memory , 40 cpu cores at 2 . 0 ##gh ##z using 16 threads . both the line ( 1st ) and line ( 2nd ) are quite efficient , which take less than 3 hours to process such a network with 2 million nodes and a billion edges . both are at least 10 % faster than graph factor ##ization , and much more efficient than deep ##walk ( five times slower ) . the reason that line - sg ##ds are slightly slower is that a threshold - cutting technique has to be applied to prevent the gradient ##s from exploding . document classification . another way to evaluate the quality of the word em ##bed ##ding ##s is to use the word vectors to compute document representation , which can be evaluated with document classification tasks . to obtain document vectors , we choose a very simple approach , taking the average of the word vector representations in that document . this is because we aim to compare the word em ##bed ##ding ##s with different approaches instead of finding the best method for document em ##bed ##ding ##s . the readers can find advanced document em ##bed ##ding approaches in . we download the abstracts of wikipedia pages from and the categories of these pages from . we choose 7 diverse categories for classification including \" arts , \" \" history , \" \" human , \" \" mathematics , \" \" nature , \" \" technology , \" and \" sports . \" for each category , we randomly select 10 , 000 articles , and articles belonging to multiple categories are discarded . we randomly sample different percentage ##s of the labeled documents for training and use the rest for evaluation . all document vectors are used to train a one - vs - rest log ##istic regression class ##ifier using the li ##bl ##ine ##ar package . we report the classification metric ##s micro - f1 and macro - f1 . the results are averaged over 10 different runs by sampling different training data . table [ reference ] reports the results of wikipedia page classification . similar conclusion can be made as in the word analogy task . the graph factor ##ization out ##per ##forms deep ##walk as deep ##walk ignores the weights of the edges . the line - sg ##ds perform worse due to the diver ##gence of the weights of the edges . the line opt ##imi ##zed by the edge - sampling treatment performs much better than directly deploy ##ing sg ##d . the line ( 2nd ) out ##per ##forms line ( 1st ) and is slightly better than the graph factor ##ization . note that with the supervised task , it is feasible to con ##cate ##nate the em ##bed ##ding ##s learned with line ( 1st ) and line ( 2nd ) . as a result , the line ( 1st + 2nd ) method performs significantly better than all other methods . this indicates that the first - order and second - order pro ##xi ##mit ##ies are complementary to each other . to provide the readers more insight about the first - order and second - order pro ##xi ##mit ##ies , table [ reference ] compares the most similar words to a given word using first - order and second - order proximity . we can see that by using the context ##ual proximity , the most similar words returned by the second - order proximity are all semantic ##ally related words . the most similar words returned by the first - order proximity are a mixture of syn ##ta ##ctic ##ally and semantic ##ally related words . sub ##su ##bs ##ection : social network significantly out ##per ##forms deep ##walk at the : * * 0 . 01 and * 0 . 05 level , paired t - test . compared with the language networks , the social networks are much sparse ##r , especially the youtube network . we evaluate the vertex em ##bed ##ding ##s through a multi - label classification task that assigns every node into one or more communities . different percentage ##s of the vertices are randomly sampled for training and the rest are used for evaluation . the results are averaged over 10 different runs . flick ##r network . let us first take a look at the results on the flick ##r network . we choose the most popular 5 communities as the categories of the vertices for multi - label classification . table [ reference ] reports the results . again , line ( 1st + 2nd ) significantly out ##per ##forms all other methods . line ( 1st ) is slightly better than line ( 2nd ) , which is opposite to the results on the language network . the reasons are two fold : ( 1 ) first - order proximity is still more important than second - order proximity in social network , which indicates strong ties ; ( 2 ) when the network is too sparse and the average number of neighbors of a node is too small , the second - order proximity may become inaccurate . we will further investigate this issue in section [ reference ] . line ( 1st ) out ##per ##forms graph factor ##ization , indicating a better capability of modeling the first - order proximity . line ( 2nd ) out ##per ##forms deep ##walk , indicating a better capability of modeling the second - order proximity . by con ##cate ##nat ##ing the representations learned by line ( 1st ) and line ( 2nd ) , the performance further improves , confirming that the two pro ##xi ##mit ##ies are complementary to each other . significantly out ##per ##forms deep ##walk at the : * * 0 . 01 and * 0 . 05 level , paired t - test . significantly out ##per ##forms deep ##walk at the : * * 0 . 01 and * 0 . 05 level , paired t - test . significantly out ##per ##forms deep ##walk at the : * * 0 . 01 and * 0 . 05 level , paired t - test . youtube network . table [ reference ] reports the results on youtube network , which is extremely sparse and the average degree is as low as 5 . in most cases with different percentage ##s of training data , line ( 1st ) out ##per ##forms line ( 2nd ) , consistent with the results on the flick ##r network . due to the extreme spa ##rs ##ity , the performance of line ( 2nd ) is even inferior to deep ##walk . by combining the representations learned by the line with both the first - and second - order proximity , the performance of line out ##per ##forms deep ##walk with either 128 or 256 dimension , showing that the two pro ##xi ##mit ##ies are complementary to each other and able to address the problem of network spa ##rs ##ity . it is interesting to observe how deep ##walk tackles the network spa ##rs ##ity through truncated random walks , which en ##rich the neighbors or contexts of each vertex . the random walk approach acts like a depth - first search . such an approach may quickly alleviate the spa ##rs ##ity of the neighborhood of nodes by bringing in indirect neighbors , but it may also introduce nodes that are long range away . a more reasonable way is to expand the neighborhood of each vertex using a breadth - first search strategy , i . e . , rec ##urs ##ively adding neighbors of neighbors . to verify this , we expand the neighborhood of the vertices whose degree are less than 1 , 000 by adding the neighbors of neighbors until the size of the extended neighborhood reaches 1 , 000 nodes . we find that adding more than 1 , 000 vertices does not further increase the performance . the results in the brackets in table [ reference ] are obtained on this reconstructed network . the performance of g ##f , line ( 1st ) and line ( 2nd ) all improves , especially line ( 2nd ) . in the reconstructed network , the line ( 2nd ) out ##per ##forms deep ##walk in most cases . we can also see that the performance of line ( 1st + 2nd ) on the reconstructed network does not improve too much compared with those on the original network . this implies that the combination of first - order and second - order proximity on the original network has already captured most information and line ( 1st + 2nd ) approach is a quite effective and efficient way for network em ##bed ##ding , suitable for both dense and sparse networks . sub ##su ##bs ##ection : citation network we present the results on two citation networks , both of which are directed networks . both the g ##f and line methods , which use first - order proximity , are not applicable for directed networks , and hence we only compare deep ##walk and line ( 2nd ) . we also evaluate the vertex em ##bed ##ding ##s through a multi - label classification task . we choose 7 popular conferences including aaa ##i , ci ##km , ic ##ml , k ##dd , ni ##ps , si ##gi ##r , and www as the classification categories . authors publishing in the conferences or papers published in the conferences are assumed to belong to the categories corresponding to the conferences . db ##lp ( author ##cit ##ation ) network . table [ reference ] reports the results on the db ##lp ( author ##cit ##ation ) network . as this network is also very sparse , deep ##walk out ##per ##forms line ( 2nd ) . however , by rec ##ons ##tructing the network through rec ##urs ##ively adding neighbors of neighbors for vertices with small degrees ( smaller than 500 ) , the performance of line ( 2nd ) significantly increases and out ##per ##forms deep ##walk . the line model directly opt ##imi ##zed by st ##och ##astic gradient descent , line ( 2nd ) , does not perform well as expected . db ##lp ( paper ##cit ##ation ) network . table [ reference ] reports the results on the db ##lp ( paper ##cit ##ation ) network . the line ( 2nd ) significantly out ##per ##forms deep ##walk . this is because the random walk on the paper citation network can only reach papers along the citing path ( i . e . , older papers ) and can not reach other references . instead , the line ( 2nd ) represents each paper with its references , which is obviously more reasonable . the performance of line ( 2nd ) is further improved when the network is reconstructed by en ##rich ##ing the neighbors of vertices with small degrees ( smaller than 200 ) . sub ##section : network layout ##s an important application of network em ##bed ##ding is to create meaningful visual ##izations that layout a network on a two dimensional space . we visual ##ize a co - author network extracted from the db ##lp data . we select some conferences from three different research fields : www , k ##dd from \" data mining , \" ni ##ps , ic ##ml from \" machine learning , \" and cv ##pr , icc ##v from \" computer vision . \" the co - author network is built from the papers published in these conferences . authors with degree less than 3 are filtered out , and finally the network contains 18 , 56 ##1 authors and 207 , 07 ##4 edges . laying out this co - author network is very challenging as the three research fields are very close to each other . we first map the co - author network into a low - dimensional space with different em ##bed ##ding approaches and then further map the low - dimensional vectors of the vertices to a 2 - d space with the t - s ##ne package . fig . [ reference ] compares the visual ##ization results with different em ##bed ##ding approaches . the visual ##ization using graph factor ##ization is not very meaningful , in which the authors belonging to the same communities are not clustered together . the result of deep ##walk is much better . however , many authors belonging to different communities are clustered tightly into the center area , most of which are high degree vertices . this is because deep ##walk uses a random walk based approach to en ##rich the neighbors of the vertices , which brings in a lot of noise due to the random ##ness , especially for vertices with higher degrees . the line ( 2nd ) performs quite well and generates meaningful layout of the network ( nodes with same colors are distributed closer ) . sub ##section : performance w . r . t . network spa ##rs ##ity in this sub ##section , we formally analyze the performance of the above models w . r . t . the spa ##rs ##ity of networks . we use the social networks as examples . we first investigate how the spa ##rs ##ity of the networks affects the line ( 1st ) and line ( 2nd ) . fig . [ reference ] shows the results w . r . t . the percentage of links on the flick ##r network . we choose flick ##r network as it is much dense ##r than the youtube network . we randomly select different percentage ##s of links from the original network to construct networks with different levels of spa ##rs ##ity . we can see that in the beginning , when the network is very sparse , the line ( 1st ) out ##per ##forms line ( 2nd ) . as we gradually increase the percentage of links , the line ( 2nd ) begins to out ##per ##form the line ( 1st ) . this shows that the second - order proximity suffers when the network is extremely sparse , and it out ##per ##forms first - order proximity when there are sufficient nodes in the neighborhood of a node . fig . [ reference ] shows the performance w . r . t . the degrees of the vertices on both the original and reconstructed youtube networks . we cat ##ego ##rize the vertices into different groups according to their degrees including , and then evaluate the performance of vertices in different groups . overall , the performance of different models increases when the degrees of the vertices increase . in the original network , the line ( 2nd ) out ##per ##forms line ( 1st ) except for the first group , which confirms that the second - order proximity does not work well for nodes with a low degree . in the reconstructed dense network , the performance of the line ( 1st ) or line ( 2nd ) improves , especially the line ( 2nd ) that preserves the second - order proximity . we can also see that the line ( 2nd ) model on the reconstructed network out ##per ##forms deep ##walk in all the groups . sub ##section : parameter sensitivity next , we investigate the performance w . r . t . the parameter dimension and the con ##ver ##ging performance of different models w . r . t the number of samples on the reconstructed youtube network . fig . [ reference ] reports the performance of the line model w . r . t . the dimension . we can see that the performance of the line ( 1st ) or line ( 2nd ) drops when the dimension becomes too large . fig . [ reference ] shows the results of the line and deep ##walk w . r . t . the number of samples during the optimization . the line ( 2nd ) consistently out ##per ##forms line ( 1st ) and deep ##walk , and both the line ( 1st ) and line ( 2nd ) converge much faster than deep ##walk . sub ##section : scala ##bility finally , we investigate the scala ##bility of the line model opt ##imi ##zed by the edge - sampling treatment and as ##yn ##ch ##ron ##ous st ##och ##astic gradient descent , which deploy ##s multiple threads for optimization . fig . [ reference ] shows the speed up w . r . t . the number of threads on the youtube data set . the speed up is quite close to linear . fig . [ reference ] shows that the classification performance remains stable when using multiple threads for model up ##dating . the two figures together show that the inference algorithm of the line model is quite scala ##ble . section : conclusion this paper presented a novel network em ##bed ##ding model called the \" line , \" which can easily scale up to networks with millions of vertices and billions of edges . it has carefully designed objective functions that preserve both the first - order and second - order pro ##xi ##mit ##ies , which are complementary to each other . an efficient and effective edge - sampling method is proposed for model inference , which solved the limitation of st ##och ##astic gradient descent on weighted edges without com ##promising the efficiency . experimental results on various real - world networks prove the efficiency and effectiveness of line . in the future , we plan to investigate higher - order proximity beyond the first - order and second - order pro ##xi ##mit ##ies in the network . besides , we also plan to investigate the em ##bed ##ding of het ##ero ##gen ##eous information networks , e . g . , vertices with multiple types . section : ac ##k ##now ##led ##gm ##ents the authors thank the three anonymous reviewers for the helpful comments . the co - author ming zhang is supported by the national natural science foundation of china ( ns ##fc grant no . 61 ##47 ##200 ##6 ) ; qi ##ao ##zh ##u mei is supported by the national science foundation under grant numbers ii ##s - 105 ##41 ##9 ##9 and cc ##f - 104 ##8 ##16 ##8 . bibliography : references",
        "pred_seq": "[SEP] line line line line line line [SEP] [SEP] node classification [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "line"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "node classification"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "english wikipedia pages",
                        "wikipedia corpora",
                        "skipgram",
                        "wikipedia word network",
                        "wikipedia pages"
                    ]
                ],
                "Method": [
                    [
                        "line",
                        "largescale information network embedding",
                        "lowdimensional embedding",
                        "sampling process",
                        "lowdimensional representation",
                        "line 1st",
                        "line 2nd",
                        "1st 2nd"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "node classification",
                        "document classification",
                        "document classification tasks",
                        "classification",
                        "wikipedia page classification",
                        "multilabel classification task",
                        "multilabel classification",
                        "classification categories"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "english wikipedia pages",
                        "wikipedia corpora",
                        "skipgram",
                        "wikipedia word network",
                        "wikipedia pages"
                    ]
                ],
                "Method": [
                    [
                        "line",
                        "largescale information network embedding",
                        "lowdimensional embedding",
                        "sampling process",
                        "lowdimensional representation",
                        "line 1st",
                        "line 2nd",
                        "1st 2nd"
                    ]
                ],
                "Metric": [
                    [
                        "macrof1"
                    ]
                ],
                "Task": [
                    [
                        "node classification",
                        "document classification",
                        "document classification tasks",
                        "classification",
                        "wikipedia page classification",
                        "multilabel classification task",
                        "multilabel classification",
                        "classification categories"
                    ]
                ]
            }
        ]
    },
    "8": {
        "doctext": "document : composition ##al sequence labeling models for error detection in learn ##er writing in this paper , we present the first experiments using neural network models for the task of error detection in learn ##er writing . we perform a systematic comparison of alternative composition ##al architecture ##s and propose a framework for error detection based on bid ##ire ##ction ##al l ##st ##ms . experiments on the con ##ll - 14 shared task data ##set show the model is able to out ##per ##form other participants on detecting errors in learn ##er writing . finally , the model is integrated with a publicly deployed self - assessment system , leading to performance comparable to human ann ##ota ##tors . section : introduction automated systems for detecting errors in learn ##er writing are valuable tools for second language learning and assessment . most work in recent years has focus ##sed on error correction , with error detection performance measured as a by ##pro ##du ##ct of the correction output . however , this assumes that systems are able to propose a correction for every detected error , and accurate systems for correction might not be optimal for detection . while closed - class errors such as incorrect prep ##osition ##s and determine ##rs can be modeled with a supervised classification approach , content - content word errors are the 3rd most frequent error type and pose a serious challenge to error correction framework ##s . evaluation of error correction is also highly subjective and human ann ##ota ##tors have rather low agreement on gold - standard corrections . therefore , we treat error detection in learn ##er writing as an independent task and propose a system for labeling each token as being correct or incorrect in context . common approaches to similar sequence labeling tasks involve learning weights or pro ##ba ##bilities for context n - grams of varying sizes , or relying on previously extracted high - confidence context patterns . both of these methods can suffer from data spa ##rs ##ity , as they treat words as independent units and miss out on potentially related patterns . in addition , they need to specify a fixed context size and are therefore often limited to using a small window near the target . neural network models aim to address these weaknesses and have achieved success in various nl ##p tasks such as language modeling and speech recognition . recent developments in machine translation have also shown that text of varying length can be represented as a fixed - size vector using con ##vo ##lu ##tion ##al networks or rec ##urrent neural networks . in this paper , we present the first experiments using neural network models for the task of error detection in learn ##er writing . we perform a systematic comparison of alternative composition ##al structures for constructing inform ##ative context representations . based on the findings , we propose a novel framework for performing error detection in learn ##er writing , which achieve ##s state - of - the - art results on two data ##set ##s of error - ann ##ota ##ted learn ##er essays . the sequence labeling model creates a single variable - size network over the whole sentence , conditions each label on all the words , and predict ##s all labels together . the effects of different data ##set ##s on the overall performance are investigated by inc ##rem ##ental ##ly providing additional training data to the model . finally , we integrate the error detection framework with a publicly deployed self - assessment system , leading to performance comparable to human ann ##ota ##tors . section : background and related work the field of automatically detecting errors in learn ##er text has a long and rich history . most work has focus ##sed on tack ##ling specific types of errors , such as usage of incorrect prep ##osition ##s , articles , verb forms , and adjective - noun pairs . however , there has been limited work on more general error detection systems that could handle all types of errors in learn ##er text . cho ##dor ##ow ##19 ##9 ##8 proposed a method based on mutual information and the chi - square stat ##istic to detect sequences of part - of - speech tags and function words that are likely to be un ##gram ##matic ##al in english . ga ##mon ##20 ##11 used maximum entropy marko ##v models with a range of features , such as po ##s tags , string features , and outputs from a constituency par ##ser . the pilot helping our own shared task also evaluated grammatical error detection of a number of different error types , though most systems were error - type specific and the best approach was heavily sk ##ew ##ed towards article and prep ##osition errors . we extend this line of research , working towards general error detection systems , and investigate the use of neural composition ##al models on this task . the related area of grammatical error correction has also gained considerable momentum in the past years , with four recent shared tasks highlighting several emerging directions . the current state - of - the - art approaches can broadly be separated into two categories : phrase - based statistical machine translation techniques , essentially translating the incorrect source text into the corrected version averaged per ##ce ##pt ##rons and naive bay ##es class ##ifiers making use of native - language error correction prior ##s . error correction systems require very specialised models , as they need to generate an improved version of the input text , whereas a wider range of tag ##ging and classification models can be deployed on error detection . in addition , automated writing feedback systems that indicate the presence and location of errors may be better from a pe ##da ##go ##gic point of view , rather than providing a pan ##ace ##a and correct ##ing all errors in learn ##er text . in section [ reference ] we evaluate a neural sequence tag ##ging model on the latest shared task test data , and compare it to the top participating systems on the task of error detection . section : sequence labeling architecture ##s we construct a neural network sequence labeling framework for the task of error detection in learn ##er writing . the model receives only a series of token ##s as input , and outputs the probability of each token in the sentence being correct or incorrect in a given context . the architecture ##s start with the vector representations of individual words , , where is the length of the sentence . different composition functions are then used to calculate a hidden vector representation of each token in context , . these representations are passed through a soft ##max layer , producing a probability distribution over the possible labels for every token in context : where is the weight matrix between the hidden vector and the output layer . we investigate six alternative neural network architecture ##s for the task of error detection : con ##vo ##lu ##tion ##al , bid ##ire ##ction ##al rec ##urrent , bid ##ire ##ction ##al l ##st ##m , and multi - layer variants of each of them . in the con ##vo ##lu ##tion ##al neural network ( cnn , figure [ reference ] a ) for token labeling , the hidden vector is calculated based on a fixed - size context window . the con ##vo ##lu ##tion acts as a feed ##for ##ward network , using surrounding context words as input , and therefore it will learn to detect the presence of different types of n - grams . the assumption behind the con ##vo ##lu ##tion ##al architecture is that memo ##ris ##ing er ##rone ##ous token sequences from the training data is sufficient for performing error detection . the con ##vo ##lu ##tion uses token ##s on either side of the target token , and the vectors for these token ##s are con ##cate ##nated , preserving the ordering : where is used as notation for vector con ##cate ##nation of and . the combined vector is then passed through a non - linear layer to produce the hidden representation : the deep con ##vo ##lu ##tion ##al network ( figure [ reference ] b ) adds an extra con ##vo ##lu ##tion ##al layer to the architecture , using the first layer as input . it creates con ##vo ##lu ##tions of con ##vo ##lu ##tions , thereby capturing more complex higher - order features from the data ##set . in a rec ##urrent neural network ( rn ##n ) , each hidden representation is calculated based on the current token em ##bed ##ding and the hidden vector at the previous time step : where is a nonlinear function , such as the si ##gm ##oid function . instead of a fixed context window , information is passed through the sentence using a rec ##urs ##ive function and the network is able to learn which patterns to disregard or pass forward . this rec ##urrent network structure is referred to as an elm ##an - type network , after elm ##an ##19 ##90 . the bid ##ire ##ction ##al rn ##n ( figure [ reference ] c ) consists of two rec ##urrent components , moving in opposite directions through the sentence . while the un ##idi ##re ##ction ##al version takes into account only context on the left of the target token , the bid ##ire ##ction ##al version rec ##urs ##ively builds separate context representations from either side of the target token . the left and right context are then con ##cate ##nated and used as the hidden representation : rec ##urrent networks have been shown to perform well on the task of language modeling , where they learn an inc ##rem ##ental composition function for predicting the next token in the sequence . however , while language models can estimate the probability of each token , they are unable to differentiate between in ##fr ##e ##quent and incorrect token sequences . for error detection , the composition function needs to learn to identify semantic an ##oma ##lies or un ##gram ##matic ##al combinations , independent of their frequency . the bid ##ire ##ction ##al model provides extra information , as it allows the network to use context on both sides of the target token . irs ##oy ##20 ##14 ##a created an extension of this architecture by connecting together multiple layers of bid ##ire ##ction ##al elm ##an - type rec ##urrent network modules . this deep bid ##ire ##ction ##al rn ##n ( figure [ reference ] d ) calculate ##s a context - dependent representation for each token using a bid ##ire ##ction ##al rn ##n , and then uses this as input to another bid ##ire ##ction ##al rn ##n . the multi - layer structure allows the model to learn more complex higher - level features and effectively perform multiple rec ##urrent passes through the sentence . the long - short term memory ( l ##st ##m ) is an advanced alternative to the elm ##an - type networks that has recently become increasingly popular . it uses two separate hidden vectors to pass information between different time steps , and includes ga ##ting mechanisms for mod ##ulating its own output . l ##st ##ms have been successfully applied to various tasks , such as speech recognition , machine translation , and natural language generation . two sets of ga ##ting values ( referred to as the input and forget gates ) are first calculated based on the previous states of the network : where is the current input , is the previous hidden state , and are bias ##es , is the previous internal state ( referred to as the cell ) , and is the log ##istic function . the new internal state is calculated based on the current input and the previous hidden state , and then inter ##pol ##ated with the previous internal state using and as weights : where is element - wise multiplication . finally , the hidden state is calculated by passing the internal state through a nonlinear ##ity , and weight ##ing it with . the values of are conditioned on the new internal state ( ) , as opposed to the previous one ( ) : because of the linear combination in equation ( [ reference ] ) , the l ##st ##m is less susceptible to vanishing gradient ##s over time , thereby being able to make use of longer context when making predictions . in addition , the network learns to mod ##ulate itself , effectively using the gates to predict which operation is required at each time step , thereby incorporating higher - level features . in order to use this architecture for error detection , we create a bid ##ire ##ction ##al l ##st ##m , making use of the advanced features of l ##st ##m and incorporating context on both sides of the target token . in addition , we experiment with a deep bid ##ire ##ction ##al l ##st ##m , which includes two consecutive layers of bid ##ire ##ction ##al l ##st ##ms , modeling even more complex features and performing multiple passes through the sentence . for comparison with non - neural models , we also report results using cr ##fs , which are a popular choice for sequence labeling tasks . we trained the cr ##f + + implementation on the same data ##set , using as features un ##ig ##ram ##s , big ##ram ##s and tri ##gram ##s in a 7 - word window sur ##roud ##ing the target word ( 3 words before and after ) . the predicted label is also conditioned on the previous label in the sequence . section : experiments we evaluate the alternative network structures on the publicly released first certificate in english data ##set ( fc ##e - public , yan ##na ##kou ##dak ##is ##20 ##11 ) . the data ##set contains short texts , written by learners of english as an additional language in response to exam prompt ##s eli ##cit ##ing free - text answers and assessing mastery of the upper - intermediate proficiency level . the texts have been manually error - ann ##ota ##ted using a taxonomy of 77 error types . we use the released test set for evaluation , containing 2 , 720 sentences , leaving 30 , 95 ##3 sentences for training . we further separate 2 , 222 sentences from the training set for development and hyper - parameter tuning . the data ##set contains manually ann ##ota ##ted error spans of various types of errors , together with their suggested corrections . we convert this to a token - level error detection task by labeling each token inside the error span as being incorrect . in order to capture errors involving missing words , the error label is assigned to the token immediately after the incorrect gap - this is motivated by the intuition that while this token is correct when considered in isolation , it is incorrect in the current context , as another token should have pre ##ce ##eded it . as the main evaluation measure for error detection we use , which was also the measure adopted in the con ##ll - 14 shared task on error correction . it combines both precision and recall , while assign ##ing twice as much weight to precision , since accurate feedback is often more important than coverage in error detection applications . following cho ##dor ##ow ##20 ##12 , we also report raw counts for predicted and correct token ##s . related evaluation measures , such as the - scorer and the i - measure , require the system to propose a correction and are therefore not directly applicable on the task of error detection . during the experiments , the input text was lower ##cased and all token ##s that occurred less than twice in the training data were represented as a single un ##k token . word em ##bed ##ding ##s were set to size and initial ##ised using the publicly released pre ##train ##ed word ##2 ##ve ##c vectors . the con ##vo ##lu ##tion ##al networks use window size on either side of the target token and produce a 300 - dimensional context - dependent vector . the rec ##urrent networks use hidden layers of size 200 in either direction . we also added an extra hidden layer of size between each of the composition functions and the output layer - this allows the network to learn a separate non - linear transformation and reduces the dimensional ##ity of the composition ##al vectors . the parameters were opt ##imi ##sed using gradient descent with initial learning rate , the adam algorithm for dynamic ##ally adapting the learning rate , and batch size of 64 sentences . on the development set was evaluated at each epoch , and the best model was used for final evaluation ##s . section : results table [ reference ] contains results for experiments comparing different composition architecture ##s on the task of error detection . the cr ##f has the lowest score compared to any of the neural models . it memo ##rise ##s frequent error sequences with high precision , but does not general ##ise sufficiently , resulting in low recall . the ability to condition on the previous label also does not provide much help on this task - there are only two possible labels and the errors are relatively sparse . the architecture using con ##vo ##lu ##tion ##al networks performs well and achieve ##s the second - highest result on the test set . it is designed to detect error patterns from a fixed window of 7 words , which is large enough to not require the use of more advanced composition functions . in contrast , the performance of the bid ##ire ##ction ##al rec ##urrent network ( bi - rn ##n ) is somewhat lower , especially on the test set . in elm ##an - type rec ##urrent networks , the context signal from distant words decreases fairly rapidly due to the si ##gm ##oid activation function and dim ##ini ##shing gradient ##s . this is likely why the bi - rn ##n achieve ##s the highest precision of all systems - the predicted label is mostly influenced by the target token and its immediate neighbours , allowing the network to only detect short high - confidence error patterns . the con ##vo ##lu ##tion ##al network , which uses 7 context words with equal attention , is able to out ##per ##form the bi - rn ##n despite the fixed - size context window . the best overall result and highest is achieved by the bid ##ire ##ction ##al l ##st ##m composition model ( bi - l ##st ##m ) . this architecture makes use of the full sentence for building context vectors on both sides of the target token , but improves on bi - rn ##n by ut ##ilis ##ing a more advanced composition function . through the application of a linear update for the internal cell representation , the l ##st ##m is able to capture depend ##encies over longer distances . in addition , the ga ##ting functions allow it to adaptive ##ly decide which information to include in the hidden representations or output for error detection . we found that using multiple layers of composition ##al functions in a deeper network gave comparable or slightly lower results for all the composition architecture ##s . this is in contrast to irs ##oy ##20 ##14 ##a , who experimented with elm ##an - type networks and found some improvements using multiple layers of bi - rn ##ns . the differences can be explained by their task benefit ##ing from alternative features : the evaluation was performed on opinion mining where most target sequences are longer phrases that need to be identified based on their semantics , whereas many errors in learn ##er writing are short and can only be identified by a context ##ual mis ##mat ##ch . in addition , our networks contain an extra hidden layer before the output , which allows the models to learn higher - level representations without adding complexity through an extra composition ##al layer . section : additional training data there are essentially infinitely many ways of committing errors in text and introducing additional training data should alleviate some of the problems with data spa ##rs ##ity . we experimented with inc ##rem ##ental ##ly adding different error - tagged corp ##ora into the training set and measured the resulting performance . this allows us to provide some context to the results obtained by using each of the data ##set ##s , and gives us an estimate of how much ann ##ota ##ted data is required for optimal performance on error detection . the data ##set ##s we consider are as follows : fc ##e - public - the publicly released subset of fc ##e , as described in section [ reference ] . nu ##cle - the nu ##s corpus of learn ##er english , used as the main training set for con ##ll shared tasks on error correction . ie ##lts - a subset of the ie ##lts examination data ##set extracted from the cambridge learn ##er corpus ( cl ##c , nicholls ##200 ##3 ) , containing 68 , 505 sentences from all proficiency levels , also used by fe ##lice ##20 ##14 . fc ##e - a larger selection of fc ##e texts from the cl ##c , containing 323 , 192 sentences . cp ##e - essays from the proficient examination level in the cl ##c , containing 210 , 67 ##8 sentences . ca ##e - essays from the advanced examination level in the cl ##c , containing 219 , 95 ##3 sentences . table [ reference ] contains results obtained by inc ##rem ##ental ##ly adding training data to the bi - l ##st ##m model . we found that incorporating the nu ##cle data ##set does not improve performance over using only the fc ##e - public data ##set , which is likely due to the two corp ##ora containing texts with different domains and writing styles . the texts in fc ##e are written by young intermediate students , in response to prompt ##s eli ##cit ##ing letters , emails and reviews , whereas nu ##cle contains mostly argument ##ative essays written by advanced adult learners . the differences in the data ##set ##s offset the benefits from additional training data , and the performance remains roughly the same . in contrast , substantial improvements are obtained when introducing the ie ##lts and fc ##e data ##set ##s , with each of them increasing the score by roughly . the ie ##lts data ##set contains essays from all proficiency levels , and fc ##e from mid - level english learners , which provides the model with a distribution of ' average ' errors to learn from . adding even more training data from high - proficiency essays in cp ##e and ca ##e only provides minor further improvements . figure [ reference ] also shows on the fc ##e - public test set as a function of the total number of token ##s in the training data . the optimal trade - off between performance and data size is obtained at around 8 million token ##s , after introducing the fc ##e data ##set . section : con ##ll - 14 shared task the con ##ll - 14 shared task focus ##sed on automatically correct ##ing errors in learn ##er writing . the nu ##cle data ##set was provided as the main training data ##set , but participants were allowed to include other ann ##ota ##ted corp ##ora and external resources . for evaluation , 25 students were recruited to each write two new essays , which were then ann ##ota ##ted by two experts . we used the same methods from section [ reference ] for converting the shared task ann ##ota ##tion to a token - level labeling task in order to evaluate the models on error detection . in addition , the correction outputs of all the participating systems were made available online , therefore we are able to report their performance on this task . in order to convert their output to error detection labels , the corrected sentences were aligned with the original input using lev ##ens ##ht ##ein distance , and any changes proposed by the system resulted in the corresponding source words being labeled as errors . the results on the two ann ##ota ##tions of the shared task test data can be seen in table [ reference ] . we first evaluated each of the human ann ##ota ##tors with respect to the other , in order to estimate the upper bound on this task . the average of roughly 50 % shows that the task is difficult and even human experts have a rather low agreement . it has been shown before that correct ##ing grammatical errors is highly subjective , but these results indicate that trained ann ##ota ##tors can disagree even on the number and location of errors . in the same table , we provide error detection results for the top 3 participants in the shared task : cam ##b , cu ##ui , and am ##u . they each preserve their relative ranking also in the error detection evaluation . the cam ##b system has a lower precision but the highest recall , also resulting in the highest . cu ##ui and am ##u are close in performance , with am ##u having slightly higher precision . after the official shared task , susan ##to ##20 ##14 published a system which combines several alternative models and out ##per ##forms the shared task participants when evaluated on error correction . however , on error detection it receives lower results , ranking 3rd and 4th when evaluated on ( p ##1 + p ##2 + s ##1 + s ##2 in table [ reference ] ) . the system has detected a small number of errors with high precision , and does not reach the highest . finally , we present results for the bi - l ##st ##m sequence labeling system for error detection . using only fc ##e - public for training , the overall performance is rather low as the training set is very small and contains texts from a different domain . however , these results show that the model behave ##s as expected - since it has not seen similar language during training , it labels a very large portion of token ##s as errors . this indicates that the network is trying to learn correct language constructions from the limited data and class ##ifies unseen structures as errors , as opposed to simply memo ##ris ##ing error sequences from the training data . when trained on all the data ##set ##s from section [ reference ] , the model achieve ##s the highest of all systems on both of the con ##ll - 14 shared task test ann ##ota ##tions , with an absolute improvement of over the previous best result . it is worth noting that the full bi - l ##st ##m has been trained on more data than the other con ##ll contestants . however , as the shared task systems were not restricted to the nu ##cle training set , all the submissions also used differing amounts of training data from various sources . in addition , the con ##ll systems are mostly combinations of many alternative models : the cam ##b system is a hybrid of machine translation , a rule - based system , and a language model re - rank ##er ; cu ##ui consists of different class ##ifiers for each individual error type ; and p ##1 + p ##2 + s ##1 + s ##2 is a combination of four different error correction systems . in contrast , the bi - l ##st ##m is a single model for detecting all error types , and therefore represents a more scala ##ble data - driven approach . section : essay scoring in this section , we perform an ex ##tri ##ns ##ic evaluation of the efficacy of the error detection system and examine the extent to which it general ##ises at higher levels of gran ##ular ##ity on the task of automated essay scoring . more specifically , we replicate experiments using the text - level model described by andersen ##20 ##13 , which is currently deployed in a self - assessment and tutor ##ing system ( sat ) , an online automated writing feedback tool actively used by language learners . the sat system predict ##s an overall score for a given text , which provides a ho ##listic assessment of linguistic competence and language proficiency . the authors trained a supervised ranking per ##ce ##pt ##ron model on the fc ##e - public data ##set , using features such as error - rate estimates from a language model and various lexi ##cal and grammatical properties of text ( e . g . , word n - grams , part - of - speech n - grams and phrase - structure rules ) . we replicate this experiment and add the average probability of each token in the essay being correct , according to the error detection model , as an additional feature for the scoring framework . the system was then re ##train ##ed on fc ##e - public and evaluated on correctly predicting the assigned essay score . table [ reference ] presents the experimental results . the human performance on the test set is calculated as the average inter - ann ##ota ##tor correlation on the same data , and the existing sat system has demonstrated levels of performance that are very close to that of human assess ##ors . nevertheless , the bi - l ##st ##m model trained only on fc ##e - public complement ##s the existing features , and the combined model achieve ##s an absolute improvement of around 1 % percent , corresponding to 20 - 31 % relative error reduction with respect to the human performance . even though the bi - l ##st ##m is trained on the same data ##set and the sat system already includes various linguistic features for capturing errors , our error detection model manages to further improve its performance . when the bi - l ##st ##m is trained on all the available data from section [ reference ] , the combination achieve ##s further substantial improvements . the relative error reduction on pearson ' s correlation is 64 % , and the system actually out ##per ##forms human ann ##ota ##tors on spear ##man ' s correlation . section : conclusions in this paper , we presented the first experiments using neural network models for the task of error detection in learn ##er writing . six alternative composition ##al network architecture ##s for modeling context were evaluated . based on the findings , we propose a novel error detection framework using token - level em ##bed ##ding ##s , bid ##ire ##ction ##al l ##st ##ms for context representation , and a multi - layer architecture for learning more complex features . this structure allows the model to classify each token as being correct or incorrect , using the full sentence as context . the self - modulation architecture of l ##st ##ms was also shown to be beneficial , as it allows the network to learn more advanced composition rules and remember depend ##encies over longer distances . substantial performance improvements were achieved by training the best model on additional data ##set ##s . we found that the largest benefit was obtained from training on 8 million token ##s of text from learners with varying levels of language proficiency . in contrast , including even more data from higher - proficiency learners gave marginal further improvements . as part of future work , it would be beneficial to investigate the effect of automatically generated training data for error detection ( e . g . , ro ##zo ##vs ##kaya ##20 ##10 ) . we evaluated the performance of existing error correction systems from con ##ll - 14 on the task of error detection . the experiments showed that success on error correction does not necessarily mean success on error detection , as the current best correction system ( p ##1 + p ##2 + s ##1 + s ##2 ) is not the same as the best shared task detection system ( cam ##b ) . in addition , the neural sequence tag ##ging model , specialised for error detection , was able to out ##per ##form all other participating systems . finally , we performed an ex ##tri ##ns ##ic evaluation by incorporating pro ##ba ##bilities from the error detection system as features in an essay scoring model . even without any additional data , the combination further improved performance which is already close to the results from human ann ##ota ##tors . in addition , when the error detection model was trained on a larger training set , the essay scorer was able to exceed human - level performance . section : ac ##k ##now ##led ##gm ##ents we would like to thank prof ted br ##is ##coe and the reviewers for providing useful feedback . bibliography : references",
        "pred_seq": "con ##set [SEP] composition models [SEP] [SEP] error detection [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "conll14 shared task dataset"
                    ]
                ],
                "Method": [
                    [
                        "compositional sequence labeling models"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "error detection"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "conll14 shared task dataset",
                        "conll14 shared task",
                        "conll contestants",
                        "conll14"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "error detection",
                        "detection",
                        "automatically detecting errors",
                        "grammatical error detection",
                        "error detection systems",
                        "grammatical error correction",
                        "error detection applications",
                        "error detection evaluation"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "first certificate in english dataset",
                        "fcepublic",
                        "fce",
                        "fce texts",
                        "fce datasets",
                        "fce dataset"
                    ]
                ],
                "Method": [
                    [
                        "deep bidirectional lstm",
                        "bidirectional lstm composition model"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "error detection",
                        "detection",
                        "automatically detecting errors",
                        "grammatical error detection",
                        "error detection systems",
                        "grammatical error correction",
                        "error detection applications",
                        "error detection evaluation"
                    ]
                ]
            }
        ]
    },
    "9": {
        "doctext": "document : filtered channel features for pedestrian detection this paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low - level features in combination with a boosted decision forest . based on this observation we propose a un ##ifying framework and experimental ##ly explore different filter families . we report extensive results enabling a systematic analysis . using filtered channel features we obtain top performance on the challenging cal ##tech and kit ##ti data ##set ##s , while using only hog + lu ##v as low - level features . when adding optical flow features we further improve detection quality and report the best known results on the cal ##tech data ##set , reaching 93 % recall at 1 f ##pp ##i . = - 1 section : introduction pedestrian detection is an active research area , with 1000 + papers published in the last decade , and well established bench ##mark data ##set ##s . it is considered a canonical case of object detection , and has served as playground to explore ideas that might be effective for generic object detection . although many different ideas have been explored , and detection quality has been steadily improving , arguably it is still unclear what are the key ingredients for good pedestrian detection ; e . g . it remains unclear how effective parts , components , and features learning are for this task . current top performing pedestrian detection methods all point to an intermediate layer ( such as max - pool ##ing or filtering ) between the low - level feature maps and the classification layer . in this paper we explore the simplest of such inter ##media ##ry : a linear transformation implemented as con ##vo ##lu ##tion with a filter bank . we propose a framework for filtered channel features ( see figure [ reference ] ) that un ##ifies multiple top performing methods , and that enables a systematic exploration of different filter banks . with our experiments we show that , with the proper filter bank , filtered channel features reach top detection quality . it has been shown that using extra information at test time ( such as context , stereo images , optical flow , etc . ) can boost detection quality . in this paper we focus on the ' ' core ' ' sliding window algorithm using solely hog + lu ##v features ( i . e . oriented gradient magnitude and colour features ) . we consider context information and optical flow as add - on ##s , included in the experiments section for the sake of complete ##ness and comparison with existing methods . using only hog + lu ##v features we already reach top performance on the challenging cal ##tech and kit ##ti data ##set ##s , matching results using optical flow and significantly more features ( such as lb ##p and co ##var ##iance ) . sub ##section : related work recent survey papers discuss the diverse set of ideas explored for pedestrian detection . the most recent survey indicates that the class ##ifier choice ( e . g . linear / non - linear sv ##m versus decision forest ) is not a clear different ##ia ##tor regarding quality ; rather the features used seem more important . creativity regarding different types of features has not been lacking . hog ) the classic hog des ##cript ##or is based on local image differences ( plus pool ##ing and normal ##ization steps ) , and has been used directly , as input for a def ##or ##mable parts model , or as features to be boosted . the integral channel features detector uses a simpler hog variant with sum pool ##ing and no normal ##izations . many extensions of hog have been proposed ( e . g . ) . lb ##p ) instead of using the magnitude of local pixel differences , lb ##p uses the difference sign only as signal . colour ) although the appearance of pedestrians is diverse , the background and skin areas do exhibit a colour bias . colour has shown to be an effective feature for pedestrian detection and hence multiple colour spaces have been explored ( both hand - crafted and learned ) . local structure ) instead of simple pixel values , some approaches try to en ##code a larger local structure based on colour similarities ( soft - cue ) , segment ##ation methods ( hard - decision ) , or by est ##imating local boundaries . co ##var ##iance ) another popular way to en ##code richer information is to compute the co ##var ##iance amongst features ( commonly colour , gradient , and oriented gradient ) . etc . ) other features include bag - of - words over colour , hog , or lb ##p features ; learning sparse dictionary en ##code ##rs ; and training features via a con ##vo ##lu ##tion ##al neural network . additional features specific for stereo depth or optical flow have been proposed , however we consider these beyond the focus of this paper . for our flow experiments we will use difference of frames from weakly stabilized videos ( sd ##t ) . all the feature types listed above can be used in the integral channel features detector framework . this family of detectors is an extension of the old ideas from viola & jones . sums of rectangular regions are used as input to decision trees trained via ada ##bo ##ost . both the regions to pool from and the threshold ##s in the decision trees are selected during training . the crucial difference from the pioneer work is that the sums are done over feature channels other than simple image lu ##mina ##nce . current top performing pedestrian detection methods ( dominating in ##ria , cal ##tech and kit ##ti data ##set ##s ) are all extensions of the basic integral channel features detector ( named ch ##n ##ft ##rs in , which uses only hog + lu ##v features ) . squares ##ch ##n ##ft ##rs , informed ##ha ##ar , and ld ##c ##f , are discussed in detail in section [ reference ] . kata ##mar ##i exploits context and optical flow for improved performance . spatial ##pool ##ing ( + ) adds max - pool ##ing on top of sum - pool ##ing , and uses additional features such as co ##var ##iance , lb ##p , and optical flow . similarly , region ##lets also uses extended features and max - pool ##ing , together with stronger weak class ##ifiers and training a cascade of class ##ifiers . out of these , region ##lets is the only method that has also shown good performance on general classes data ##set ##s such as pascal vo ##c and image ##net . in this paper we will show that vanilla hog + lu ##v features have not yet saturated , and that , when properly used , they can reach top performance for pedestrian detection . sub ##section : contributions we point out the link between ac ##f , ( squares ) ch ##n ##ft ##rs , informed ##ha ##ar , and ld ##c ##f . see section [ reference ] . we provide extensive experiments to enable a systematic analysis of the filtered integral channels , covering aspects not explored by related work . we report the summary of trained models ( corresponding days of single machine computation ) . see sections [ reference ] , [ reference ] and [ reference ] . we show that top detection performance can be reached on cal ##tech and kit ##ti using hog + lu ##v features only . we additionally report the best known results on cal ##tech . see section [ reference ] . section : filtered channel features before entering the experimental section , let us describe our general architecture . methods such as ch ##n ##ft ##rs , squares ##ch ##n ##ft ##rs and ac ##f all use the basic architecture depicted in figure [ reference ] top part ( best viewed in colours ) . the input image is transformed into a set of feature channels ( also called feature maps ) , the feature vector is constructed by sum - pool ##ing over a ( large ) set of rectangular regions . this feature vector is fed into a decision forest learned via ada ##bo ##ost . the split nodes in the trees are a simple comparison between a feature value and a learned threshold . commonly only a subset of the feature vector is used by the learned decision forest . ada ##bo ##ost serves both for feature selection and for learning the threshold ##s in the split nodes . a key observation , illustrated in figure [ reference ] ( bottom ) , is that such sum - pool ##ing can be re - written as con ##vo ##lu ##tion with a filter bank ( one filter per rectangular shape ) followed by reading a single value of the con ##vo ##lu ##tion ' s response map . this ' ' filter + pick ' ' view general ##izes the integral channel features detectors by allowing to use any filter bank ( instead of only rectangular shapes ) . we name this general ##ization ' ' filtered channel features detectors ' ' . in our framework , ac ##f has a single filter in its bank , corresponding to a uniform pool ##ing region . ch ##n ##ft ##rs was a very large ( tens of thousands ) filter bank comprised of random rectangular shapes . squares ##ch ##n ##ft ##rs , on the other hand , was only filters , each with a square - shaped uniform pool ##ing region of different sizes . see figure [ reference ] for an illustration of the squares ##ch ##n ##ft ##rs filters , the upper - left filter corresponds to ac ##f ' s one . the informed ##ha ##ar method can also be seen as a filtered channel features detector , where the filter bank ( and read locations ) are based on a human shape template ( thus the ' ' informed ' ' naming ) . ld ##c ##f is also a particular instance of this framework , where the filter bank consists of pc ##a bases of patches from the training data ##set . in sections [ reference ] and [ reference ] we provide experiments rev ##isi ##ting some of the design decisions of these methods . note that all the methods mentioned above ( and in the majority of experiments below ) use only hog + lu ##v feature channels ( 10 channels total ) . using linear filters and decision trees on top of these does not allow to rec ##ons ##truct the decision functions obtained when using lb ##p or co ##var ##iance features ( used by spatial ##pool ##ing and region ##lets ) . we thus consider the approach considered here orthogonal to adding such types of features . sub ##section : evaluation protocol for our experiments we use the cal ##tech and kit ##ti data ##set ##s . the popular in ##ria data ##set is considered too small and too close to sat ##uration to provide interesting results . all cal ##tech results are evaluated using the provided tool ##box , and sum ##mar ##ised by log - average miss - rate ( mr , lower is better ) in the range for the ' ' reasonable ' ' setup . kit ##ti results are evaluated via the online evaluation portal , and sum ##mar ##ised as average precision ( ap , higher is better ) for the ' ' moderate ' ' setup . paragraph : cal ##tech ##10 ##x the raw cal ##tech data ##set consists of videos ( acquired at ) with every frame ann ##ota ##ted . the standard training and evaluation considers one out of each frames ( pedestrians over frames in training , pedestrians over frames in testing ) . in our experiments of section [ reference ] we will also consider a increased training set where every rd frame is used ( linear growth in pedestrians and images ) . we name this extended training set ' ' cal ##tech ##10 ##x ' ' . ld ##c ##f uses a similar extended set for training its model ( every th frame ) . paragraph : flow methods using optical flow do not only use additional neighbour frames during training ( depending on the method ) , but they also do so at test time . because they have access to additional information at test time , we consider them as a separate group in our results section . paragraph : validation set in order to explore the design space of our pedestrian detector we setup a cal ##tech validation set by splitting the six training videos into five for training and one for testing ( one of the splits suggested in ) . most of our experiments use this validation setup . we also report ( a posterior ##i ) our key results on the standard test set for comparison to the state of the art . for the kit ##ti experiments we also valid ##ate some design choices ( such as search range and number of scales ) before submission on the evaluation server . there we use a validation setup . sub ##section : baseline ##s paragraph : ac ##f our experiments are based on the open source release of ac ##f . our first baseline is vanilla ac ##f re - trained on the standard cal ##tech set ( not cal ##tech ##10 ##x ) . on the cal ##tech test set it obtain ##s ( on validation set ) . note that this baseline already improves over more than previously published methods on this data ##set . there is also a large gap between ac ##f - ours ( ) and the original number from ac ##f - cal ##tech ( ) . the improvement is mainly due to the change towards a larger model size ( from to ) . all parameter details are described in section [ reference ] , and kept identical across experiments unless explicitly stated . paragraph : informed ##ha ##ar our second baseline is a re - implementation of informed ##ha ##ar . here again we observe an important gain from using a larger model size ( same change as for ac ##f ) . while the original informed ##ha ##ar paper reports , informed ##ha ##ar - ours reaches on the cal ##tech test set ( on validation set ) . for both our baseline ##s we use exactly the same training set as the original papers . note that the informed ##ha ##ar - ours baseline ( ) is right away the best known result for a method trained on the standard cal ##tech training set . in section [ reference ] we will discuss our re - implementation of ld ##c ##f . sub ##section : model parameters unless otherwise specified we train all our models using the following parameters . feature channels are hog + lu ##v only . the final class ##ifier includes level - 2 decision trees ( l ##2 , 3 stump ##s per tree ) , trained via vanilla discrete ada ##bo ##ost . each tree is built by doing exhaust ##ive greedy search for each node ( no random ##ization ) . the model has size , and is built via four rounds of hard negative mining ( starting from a model with trees , and then , , , trees ) . each round adds additional negative ##s to the training set . the sliding window stride is ( both during hard negative mining and at test time ) . compared to the default ac ##f parameters , we use a bigger model , more trees , more negative samples , and more boost ##ing rounds . but we do use the same code - base and the same training set . starting from section [ reference ] we will also consider results with the cal ##tech ##10 ##x data , there we use level - 4 decision trees ( l ##4 ) , and real ##bo ##ost instead of discrete ada ##bo ##ost . all other parameters are left unchanged . section : filter bank families given the general architecture and the baseline ##s described in section [ reference ] , we now proceed to explore different types of filter banks . some of them are designed using prior knowledge and they do not change when applied across data ##set ##s , others exploit data - driven techniques for learning their filters . sections [ reference ] and [ reference ] will compare their detection quality . paragraph : informed ##fi ##lter ##s starting from the informed ##ha ##ar baseline we use the same ' ' informed ' ' filters but let free the positions where they are applied ( instead of fixed in informed ##ha ##ar ) ; these are selected during the boost ##ing learning . our initial experiments show that removing the position constraint has a small ( positive ) effect . additionally we observe that the original informed ##ha ##ar filters do not include simple square pool ##ing regions ( [UNK] la squares ##ch ##n ##ft ##rs ) , we thus add these too . we end up with filters in total , to be applied over each of the feature channels . this is equivalent to training decision trees over ( non filtered ) channel features . as illustrated in figure [ reference ] the informed ##fi ##lter ##s have different sizes , from to cells ( ) , and each cell takes a value in . these filters are applied with a step size of . for a model of this results in features per channel , features in total . in practice considering border effects ( large filters are not applied on the border of the model to avoid reading outside it ) we end up with features . when training level - 2 decision trees , at most features will be used , that is of the total . in this scenario ( and all others considered in this paper ) ada ##bo ##ost has a strong role of feature selection . paragraph : check ##er ##boards as seen in section [ reference ] informed ##ha ##ar is a strong baseline . it is however unclear how much the ' ' informed ' ' design of the filters is effective compared to other possible choices . check ##er ##boards is a [UNK] set of filters that covers the same sizes ( in number of cells ) as informed ##ha ##ar / informed ##fi ##lter ##s and for each size defines ( see figure [ reference ] ) : a uniform square , all horizontal and vertical gradient detectors ( values ) , and all possible check ##er ##board patterns . these configurations are comparable to informed ##fi ##lter ##s but do not use the human shape as prior . the total number of filters is a direct function of the maximum size selected . for up to cells we end up with filters , up to cells filters , up to cells filters , and up to cells filters . paragraph : random ##fi ##lter ##s our next step towards removing a hand - crafted design is simply using random filters ( see figure [ reference ] ) . given a desired number of filters and a maximum filter size ( in cells ) , we sample the filter size with uniform distribution , and set its cell values to with uniform probability . we also experimented with values and observed a ( small ) quality decrease compared to the binary option ) . the design of the filters considered above completely ignores the available training data . in the following , we consider additional filters learned from data . paragraph : ld ##c ##f the work on pc ##ane ##t showed that applying arbitrary non - linear ##ities on top of pc ##a projections of image patches can be surprisingly effective for image classification . following this intuition ld ##c ##f uses learned pc ##a e ##igen ##ve ##ctors as filters ( see figure [ reference ] ) . we present a re - implementation of based on ac ##f ' s source code . we try to follow the original description as closely as possible . we use the same top filters of , selected per feature channel based on their e ##igen ##val ##ues ( filters total ) . we do change some parameters to be consistent amongst all experiments , see sections [ reference ] and [ reference ] . the main changes are the training set ( we use cal ##tech ##10 ##x , sampled every 3 frames , instead of every 4 frames in ) , and the model size ( pixels instead of ) . as will be shown in section [ reference ] , our implementation ( ld ##c ##f - ours ) clearly improves over the previously published numbers , showing the potential of the method . for comparison with pc ##af ##ore ##ground we also consider training ld ##c ##f ##8 where the top filters are selected per channel ( filters total ) . paragraph : pc ##af ##ore ##ground in ld ##c ##f the filters are learned using all of the training data available . in practice this means that the learned filters will be dominated by background information , and will have minimal information about the pedestrians . put differently , learning filters from all the data assumes that the decision boundary is defined by a single distribution ( like in linear disc ##rim ##ina ##nt analysis ) , while we might want to define it based on the relation between the background distribution and the fore ##ground distribution ( like fisher ' s disc ##rim ##ina ##nt analysis ) . in pc ##af ##ore ##ground we train filters per feature channel , learned from background image patches , and learned from patches extracted over pedestrians ( see figure [ reference ] ) . compared to ld ##c ##f ##8 the obtained filters are similar but not identical , all other parameters are kept identical . other than via pc ##af ##ore ##ground / ld ##c ##f ##8 , it is not clear how to further increase the number of filters used in ld ##c ##f . past filters per channel , the e ##igen ##val ##ues decrease to ne ##gli ##gible values and the e ##igen ##ve ##ctors become essentially random ( similar to random ##fi ##lter ##s ) . to keep the filtered channel features setup close to informed ##ha ##ar , the filters are applied with a step of however , to stay close to the original ld ##c ##f , the ld ##c ##f / pc ##af ##ore ##ground filters are evaluated every . although ( for example ) ld ##c ##f ##8 uses only of the number of filters per channel compared to check ##er ##boards ##4 ##x ##4 , due to the step size increase , the obtained feature vector size is . section : how many filters ? given a fixed set of channel features , a larger filter bank provides a richer view over the data compared to a smaller one . with enough training data one would expect larger filter banks to perform best . we want thus to analyze the trade - off between number of filters and detection quality , as well as which filter bank family performs best . figure [ reference ] presents the results of our initial experiments on the cal ##tech validation set . it shows detection quality versus number of filters per channel . this figure densely sum ##mar ##izes trained models . paragraph : informed ##fi ##lter ##s the first aspect to notice is that there is a meaningful gap between informed ##ha ##ar - ours and informed ##fi ##lter ##s despite having a similar number of filters ( versus ) . this valid ##ates the importance of letting ada ##bo ##ost choose the pool ##ing locations instead of hand - craft ##ing them . keep in mind that informed ##ha ##ar - ours is a top performing baseline ( see \u00a7 [ reference ] ) . secondly , we observe that ( for the fixed training data available ) filters is better than . below filters the performance de ##grade ##s for all methods ( as expected ) . to change the number of filters in informed ##fi ##lter ##s we train a full model ( filters ) , pick the most frequently used filters ( selected from node splitting in the decision forest ) , and use these to train the desired reduced model . we can select the most frequent filters across channels or per channel ( marked as in ##f . filters ##per ##channel ) . we observe that per channel selection is slightly worse than across channels , thus we stick to the latter . using the most frequently used filters for selection is clearly a crude strategy since frequent usage does not guarantee disc ##rim ##ina ##tive power , and it ignores relation amongst filters . we find this strategy good enough to convey the main points of this work . paragraph : check ##er ##boards also reaches best results in the filters region . here the number of filters is varied by changing the maximum filter size ( in number of cells ) . regarding the lowest miss - rate there is no large gap between the ' ' informed ' ' filters and this [UNK] baseline . paragraph : random ##fi ##lter ##s the he ##xa ##gonal dots and their deviation bars indicate the mean , maximum and minimum miss - rate obtained out of five random runs . when using a larger number of filters ( ) we observe a lower ( better ) mean but a larger variance compared to when using fewer filters ( ) . here again the gap between the best random run and the best result of other methods is not large . given a set of five models , we select the most frequently used filters and train new reduced models ; these are shown in the random ##fi ##lter ##s line . overall the random filters are surprisingly close to the other filter families . this indicates that expanding the feature channels via filtering is the key step for improving detection quality , while selecting the ' ' perfect ' ' filters is a secondary concern . paragraph : ld ##c ##f / pc ##af ##ore ##ground in contrast to the other filter bank families , ld ##c ##f under - performs when increasing the number of filters ( from to ) while using the standard cal ##tech training set ( consistent with the observations in ) . pc ##af ##ore ##ground improves marginal ##ly over ld ##c ##f ##8 . paragraph : take ##away ##s from figure [ reference ] we observe two overall trends . first , the more filters the mer ##rier , with filters as sweet spot for cal ##tech training data . second , there is no flag ##rant difference between the different filter types . section : additional training data one cave ##at of the previous experiments is that as we increase the number of filters used , so does the number of features ada ##bo ##ost must pick from . since we increased the model capacity ( compared to ac ##f which uses a single filter ) , we consider using the cal ##tech ##10 ##x data ##set ( \u00a7 [ reference ] ) to verify that our models are not starving for data . similar to the experiments in , we also rec ##ons ##ider the decision tree depth , since additional training data enables bigger models . results for two representative methods are collected in table [ reference ] . first we observe that already with the original training data , deeper trees do provide significant improvement over level - 2 ( which was selected when tuning over in ##ria data ) . second , we notice that increasing the training data volume does provide the expected improvement only when the decision trees are deep enough . for our following experiments we choose to use level - 4 decision trees ( ) as a good balance between increased detection quality and reasonable training times . paragraph : real ##bo ##ost although previous papers on ch ##n ##ft ##rs detectors reported that different boost ##ing variants all obtain equal results on this task , the recent indicated that real ##bo ##ost has an edge over discrete ada ##bo ##ost when additional training data is used . we observe the same behaviour in our cal ##tech ##10 ##x setup . as summarized in table [ reference ] using filtered channels , deeper trees , additional training data , and real ##bo ##ost does provide a significant detection quality boost . for the rest of the paper our models trained on cal ##tech ##10 ##x all use level - 4 trees and real ##bo ##ost , instead of level - 2 and discrete ada ##bo ##ost for the cal ##tech ##1 ##x models . paragraph : timing when using cal ##tech data ac ##f takes about one hour for training and one for testing . check ##er ##boards ##4 ##x ##4 takes about and hours respectively . when using cal ##tech ##10 ##x the training times for these methods aug ##ment to 2 and 29 hours , respectively . the training time does not increase proportional ##ly with the training data volume because the hard negative mining reads a variable amount of images to attain the desired quota of negative samples . this amount increases when a detector has less false positive mistakes . sub ##section : validation set experiments based on the results in table [ reference ] we proceed to evaluate on cal ##tech ##10 ##x the most promising configurations ( filter type and number ) from section [ reference ] . the results over the cal ##tech validation set are collected in table [ reference ] . we observe a clear overall gain from increasing the training data . interesting ##ly with enough random ##fi ##lter ##s we can out ##per ##form the strong performance of ld ##c ##f - ours . we also notice that the [UNK] check ##er ##boards out ##per ##forms the manual design of informed ##fi ##lter ##s . section : add - on ##s before presenting the final test set results of our ' ' core ' ' method ( section [ reference ] ) , we also consider some possible ' ' add - on ##s ' ' based on the suggestions from . for the sake of evaluating complement ##ari ##ty , comparison with existing method , and reporting the best possible detection quality , we consider extending our detector with context and optical flow information . paragraph : context context is modelled via the 2 ##ped re - scoring method of . it is a post - processing step that merge ##s our detection scores with the results of a two person d ##pm trained on the in ##ria data ##set ( with extended ann ##ota ##tions ) . in the authors reported an improvement of ( percent points ) on the cal ##tech set , across different methods . in an improvement of is reported over their strong detector ( squares ##ch ##n ##ft ##rs + dc ##t + sd ##t ) . in our experiments however we obtain a gain inferior to . we have also investigated fu ##sing the 2 ##ped detection results via a different , more principle ##d , fusion method . we observe consistent results : as the strength of the starting point increases , the gain from 2 ##ped decreases . when reaching our check ##er ##boards results , all gains have eva ##por ##ated . we believe that the 2 ##ped approach is a promising one , but our experiments indicate that the used d ##pm template is simply too weak in comparison to our filtered channels . paragraph : optical flow optical flow is fed to our detector as an additional set of channels ( not filtered ) . we use the implementation from sd ##t which uses differences of weakly stabilized video frames . on cal ##tech , the authors of reported a gain over ac ##f ( ) , while reported a percent points improvement over their strong baseline ( squares ##ch ##n ##ft ##rs + dc ##t + 2 ##ped % 27 . 4 ##m ##r ) . when using + sd ##t our results are directly comparable to kata ##mar ##i and spatial ##pool ##ing + which both use optical flow too . using our stronger check ##er ##boards results sd ##t provides a gain . here again we observe an erosion as the starting point improves ( for confirmation , reproduced the ac ##f + sd ##t results , ) . we name our check ##er ##boards + sd ##t detector all - in - one . our filtered channel features results are strong enough to er ##ode existing context and flow features . although these remain complementary cues , more sophisticated ways of extract ##ing this information will be required to further progress in detection quality . it should be noted that despite our best efforts we could not reproduce the results from neither 2 ##ped nor sd ##t on the kit ##ti data ##set ( in spite of its apparent similarity to cal ##tech ) . effective methods for context and optical flow across data ##set ##s have yet to be shown . our main contribution remains on the core detector ( only hog + lu ##v features over local sliding window pixels in a single frame ) . section : test set results having done our exploration of the parameters space on the validation set , we now evaluate the most promising methods on the cal ##tech and kit ##ti test sets . paragraph : cal ##tech test set figures [ reference ] and [ reference ] present our key results on the cal ##tech test set . for proper comparison , only methods using the same training set should be compared ( see for a similar table comparing previous methods ) . we include for comparison the baseline ##s mentioned in section [ reference ] , roe ##re ##i [ ] the best known method trained without any cal ##tech images , mt - d ##pm [ ] the best known method based on d ##pm , and sd ##n [ ] the best known method using con ##vo ##lu ##tion ##al neural networks . we also include the top performers kata ##mar ##i and spatial ##pool ##ing + . we mark as ' ' ' ' both the cal ##tech ##10 ##x training set and the one used in ld ##c ##f ( see section [ reference ] ) . paragraph : kit ##ti test set figure [ reference ] presents the results on the kit ##ti test set ( ' ' moderate ' ' setup ) , together with all other reported methods using only mono ##cular image content ( no stereo or lid ##ar data ) . the kit ##ti evaluation server only recently has started receiving submissions ( for this task , in the last year ) , and thus is less prone to data ##set over - fitting . we train our model on the kit ##ti training set using almost identical parameters as for cal ##tech . the only change is a subtle pre - processing step in the hog + lu ##v computation . on kit ##ti the input image is smoothed ( radius ) before the feature channels are computed , while on cal ##tech we do not . this subtle change provided a ( percent points ) improvement on the kit ##ti validation set . sub ##section : analysis with a ( percent points ) gap between ac ##f / informed ##ha ##ar and ac ##f / informed ##ha ##ar - ours ( see figure [ reference ] ) , the results of our baseline ##s show the importance of proper validation of training parameters ( large enough model size and negative samples ) . informed ##ha ##ar - ours is the best reported result when training with cal ##tech ##1 ##x . when considering methods trained on cal ##tech ##10 ##x , we obtain a clear gap with the previous best results ( ld ##c ##f check ##er ##boards ) . using our architecture and the adequate number of filters one can obtain strong results using only hog + lu ##v features . the exact type of filters seems not critical , in our experiments check ##er ##boards ##4 ##x ##3 gets best performance given the available training data . random ##fi ##lter ##s reaches the same result , but requires training and merging multiple models . our results cut by half miss - rate of the best known con ##vn ##et for pedestrian detection ( sd ##n [ ] ) , which in principle could learn similar low - level features and their filtering . when adding optical flow we further push the state of the art and reach , a comfortable improvement over the previous best optical flow method ( spatial ##pool ##ing + ) . this is the best reported result on this challenging data ##set . the results on the kit ##ti data ##set confirm the strength of our approach , reaching , just below the best known result on this data ##set . competing methods ( region ##lets [ ] and spatial ##pool ##ing ) both use hog together with additional lb ##p and co ##var ##iance features . adding these remains a possibility for our system . note that our results also improve over methods using lid ##ar + image , such as fusion - d ##pm ( , not included in figure [ reference ] for clarity ) . section : conclusion through this paper we have shown that the seemingly disconnected methods ac ##f , ( squares ) ch ##n ##ft ##rs , informed ##ha ##ar , and ld ##c ##f can be all put under the filtered channel features detectors umbrella . we have systematically explored different filter banks for such architecture and shown that they provide means for important improvements for pedestrian detection . our results indicate that hog + lu ##v features have not yet saturated , and that competitive results ( over cal ##tech and kit ##ti data ##set ##s ) can be obtained using only them . when optical flow information is added we set the new state of art for the cal ##tech data ##set , reaching ( recall at false positive per image ) . in future work we plan to explore how the insights of this work can be exploited into a more general detection architecture such as con ##vo ##lu ##tion ##al neural networks . paragraph : acknowledge ##ments we thank jan ho ##san ##g for the help provided setting up some of the experiments . we also thank seo ##ng joo ##n oh and sabrina hop ##pe for their useful comments . bibliography : references appendix : learned model in figures [ reference ] and [ reference ] we present some qu ##ali ##tative aspects of the final learned models check ##er ##boards ##4 ##x ##3 and random ##fi ##lter ##s ( see results section of main paper ) , not included in the main submission due to space limitations . in figure [ reference ] we compare the spatial distribution of our models versus a significantly weaker model ( roe ##re ##i , trained on in ##ria , see figure 5 of main paper ) . we observe that our strong models focal ##ize in similar areas than the weak roe ##re ##i model . this indicates that using filtered channels does not change which areas of the pedestrian are inform ##ative , but rather that at the same locations filtered channels are able to extract more disc ##rim ##ina ##tive information . in all three models we observe that diagonal oriented channels focus on left and right shoulders . the u colour channel is mainly used around the face , while l ( lu ##mina ##nce ) and gradient magnitude ( ) channels are used all over the body . overall head , feet , and upper torso areas provide most clues for detection . in figure [ reference ] we observe that the filters usage distribution is similar across different filter bank families . [ filters from roe ##re ##i ( scale ) model . copied from ] [ final check ##er ##boards ##4 ##x ##3 model ] [ final random ##fi ##lter ##s model ] [ filters used in our final check ##er ##boards ##4 ##x ##3 model ] [ filters used in our final random ##fi ##lter ##s model ]",
        "pred_seq": "cal ##set [SEP] filtered features [SEP] [SEP] pedestrian detection [SEP] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "caltech dataset"
                    ]
                ],
                "Method": [
                    [
                        "filtered channel features"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "pedestrian detection"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "caltech",
                        "caltech dataset",
                        "caltech10x",
                        "caltech validation set",
                        "caltech set",
                        "caltech test set",
                        "caltech training set",
                        "caltech training data",
                        "caltech10x setup",
                        "caltech data",
                        "caltech images"
                    ]
                ],
                "Method": [
                    [
                        "checkerboards",
                        "checkerboards4x4",
                        "checkerboards4x3",
                        "checkerboards4x3 model"
                    ]
                ],
                "Metric": [
                    [
                        "logaverage missrate",
                        "mr",
                        "missrate",
                        "minimum missrate"
                    ]
                ],
                "Task": [
                    [
                        "pedestrian detection",
                        "detection",
                        "2ped detection"
                    ]
                ]
            }
        ]
    },
    "10": {
        "doctext": "document : semi - supervised sequence modeling with cross - view training ke ##vc ##lar ##k @ cs . stanford . ed ##u , than ##gl ##uo ##ng @ google . com , manning @ cs . stanford . ed ##u , q ##v ##l @ google . com un ##su ##per ##vis ##ed representation learning algorithms such as word ##2 ##ve ##c and elm ##o improve the accuracy of many supervised nl ##p models , mainly because they can take advantage of large amounts of un ##lab ##ele ##d text . however , the supervised models only learn from task - specific labeled data during the main training phase . we therefore propose cross - view training ( cv ##t ) , a semi - supervised learning algorithm that improves the representations of a bi - l ##st ##m sentence en ##code ##r using a mix of labeled and un ##lab ##ele ##d data . on labeled examples , standard supervised learning is used . on un ##lab ##ele ##d examples , cv ##t teaches auxiliary prediction modules that see restricted views of the input ( e . g . , only part of a sentence ) to match the predictions of the full model seeing the whole input . since the auxiliary modules and the full model share intermediate representations , this in turn improves the full model . moreover , we show that cv ##t is particularly effective when combined with multi - task learning . we evaluate cv ##t on five sequence tag ##ging tasks , machine translation , and dependency par ##sing , achieving state - of - the - art results . section : introduction deep learning models work best when trained on large amounts of labeled data . however , acquiring labels is costly , mo ##tiv ##ating the need for effective semi - supervised learning techniques that leverage un ##lab ##ele ##d examples . a widely successful semi - supervised learning strategy for neural nl ##p is pre - training word vectors mi ##ko ##lov ##20 ##13 ##dis ##tri ##bu ##ted ##ro . more recent work trains a bi - l ##st ##m sentence en ##code ##r to do language modeling and then incorporates its context - sensitive representations into supervised models dai ##20 ##15 ##se ##mi , peters ##20 ##18 ##dee ##p . such pre - training methods perform un ##su ##per ##vis ##ed representation learning on a large corpus of un ##lab ##ele ##d data followed by supervised training . a key disadvantage of pre - training is that the first representation learning phase does not take advantage of labeled data - the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task . older semi - supervised learning algorithms like self - training do not suffer from this problem because they continually learn about a task on a mix of labeled and un ##lab ##ele ##d data . self - training has historically been effective for nl ##p ya ##row ##sky ##19 ##9 ##5 ##un ##su ##per ##vis ##ed , mcc ##los ##ky ##200 ##6 ##ef ##fect ##ive , but is less commonly used with neural models . this paper presents cross - view training ( cv ##t ) , a new self - training algorithm that works well for neural sequence models . in self - training , the model learns as normal on labeled examples . on un ##lab ##ele ##d examples , the model acts as both [UNK] that makes predictions about the examples and [UNK] that is trained on those predictions . although this process has shown value for some tasks , it is somewhat taut ##ological : the model already produces the predictions it is being trained on . recent research on computer vision addresses this by adding noise to the student ' s input , training the model so it is robust to input per ##tur ##bation ##s sa ##j ##jad ##i ##20 ##16 ##re ##gul ##ari ##zation , wei ##20 ##18 ##im ##pro ##ving . however , applying noise is difficult for discrete inputs like text . as a solution , we take inspiration from multi - view learning blu ##m ##19 ##9 ##8 ##comb ##ining , xu ##20 ##13 ##as ##o and train the model to produce consistent predictions across different views of the input . instead of only training the full model as a student , cv ##t adds auxiliary prediction modules - neural networks that transform vector representations into predictions - to the model and also trains them as students . the input to each student prediction module is a subset of the model ' s intermediate representations corresponding to a restricted view of the input example . for example , one auxiliary prediction module for sequence tag ##ging is attached to only the \" forward \" l ##st ##m in the model ' s first bi - l ##st ##m layer , so it makes predictions without seeing any token ##s to the right of the current one . cv ##t works by improving the model ' s representation learning . the auxiliary prediction modules can learn from the full model ' s predictions because the full model has a better , unrest ##ricted view of the input . as the auxiliary modules learn to make accurate predictions despite their restricted views of the input , they improve the quality of the representations they are built on top of . this in turn improves the full model , which uses the same shared representations . in short , our method combines the idea of representation learning on un ##lab ##ele ##d data with classic self - training . cv ##t can be applied to a variety of tasks and neural architecture ##s , but we focus on sequence modeling tasks where the prediction modules are attached to a shared bi - l ##st ##m en ##code ##r . we propose auxiliary prediction modules that work well for sequence tag ##gers , graph - based dependency par ##ser ##s , and sequence - to - sequence models . we evaluate our approach on english dependency par ##sing , comb ##inatory cat ##ego ##rial grammar super ##tag ##ging , named entity recognition , part - of - speech tag ##ging , and text chunk ##ing , as well as english to vietnamese machine translation . cv ##t improves over previously published results on all these tasks . furthermore , cv ##t can easily and effectively be combined with multi - task learning : we just add additional prediction modules for the different tasks on top of the shared bi - l ##st ##m en ##code ##r . training a unified model to jointly perform all of the tasks except machine translation improves results ( out ##per ##form ##ing a multi - task elm ##o model ) while decreasing the total training time . section : cross - view training we first present cross - view training and describe how it can be combined effectively with multi - task learning . see figure [ reference ] for an overview of the training method . sub ##section : method let represent a labeled data ##set and represent an un ##lab ##ele ##d data ##set we use to denote the output distribution over classes produced by the model with parameters on input . during cv ##t , the model alternate ##s learning on a mini ##bat ##ch of labeled examples and learning on a mini ##bat ##ch of un ##lab ##ele ##d examples . for labeled examples , cv ##t uses standard cross - entropy loss : cv ##t adds auxiliary prediction modules to the model , which are used when learning on un ##lab ##ele ##d examples . a prediction module is usually a small neural network ( e . g . , a hidden layer followed by a soft ##max layer ) . each one takes as input an intermediate representation produced by the model ( e . g . , the outputs of one of the l ##st ##ms in a bi - l ##st ##m model ) . it outputs a distribution over labels . each is chosen such that it only uses a part of the input ; the particular choice can depend on the task and model architecture . we propose variants for several tasks in section [ reference ] . the auxiliary prediction modules are only used during training ; the test - time prediction come from the primary prediction module that produces . on an un ##lab ##ele ##d example , the model first produces soft targets by performing inference . cv ##t trains the auxiliary prediction modules to match the primary prediction module on the un ##lab ##ele ##d data by mini ##mi ##zing where is a distance function between probability distributions ( we use k ##l diver ##gence ) . we hold the primary module ' s prediction fixed during training ( i . e . , we do not back - prop ##aga ##te through it ) so the auxiliary modules learn to im ##itate the primary one , but not vice versa . cv ##t works by enhancing the model ' s representation learning . as the auxiliary modules train , the representations they take as input improve so they are useful for making predictions even when some of the model ' s inputs are not available . this in turn improves the primary prediction module , which is built on top of the same shared representations . we combine the supervised and cv ##t losses into the total loss , , and minimize it with st ##och ##astic gradient descent . in particular , we alternate mini ##mi ##zing over a mini ##bat ##ch of labeled examples and mini ##mi ##zing over a mini ##bat ##ch of un ##lab ##ele ##d examples . for most neural networks , adding a few additional prediction modules is computational ##ly cheap compared to the portion of the model building up representations ( such as an rn ##n or cnn ) . therefore our method contributes little overhead to training time over other self - training approaches for most tasks . cv ##t does not change inference time or the number of parameters in the fully - trained model because the auxiliary prediction modules are only used during training . sub ##section : combining cv ##t with multi - task learning cv ##t can easily be combined with multi - task learning by adding additional prediction modules for the other tasks on top of the shared bi - l ##st ##m en ##code ##r . during supervised learning , we randomly select a task and then update using a mini ##bat ##ch of labeled data for that task . when learning on the un ##lab ##ele ##d data , we opt ##imi ##ze jointly across all tasks at once , first running inference with all the primary prediction modules and then learning from the predictions with all the auxiliary prediction modules . as before , the model alternate ##s training on mini ##bat ##ches of labeled and un ##lab ##ele ##d examples . examples labeled across many tasks are useful for multi - task systems to learn from , but most data ##set ##s are only labeled with one task . a benefit of multi - task cv ##t is that the model creates ( artificial ) all - tasks - labeled examples from un ##lab ##ele ##d data . this significantly improves the model ' s data efficiency and training time . since running prediction modules is computational ##ly cheap , computing is not much slower for many tasks than it is for a single one . however , we find the all - tasks - labeled examples substantially speed up model convergence . for example , our model trained on six tasks takes about three times as long to converge as the average model trained on one task , a 50 % decrease in total training time . section : cross - view training models cv ##t relies on auxiliary prediction modules that have restricted views of the input . in this section , we describe specific constructions of the auxiliary prediction modules that are effective for sequence tag ##ging , dependency par ##sing , and sequence - to - sequence learning . sub ##section : bi - l ##st ##m sentence en ##code ##r all of our models use a two - layer cnn - bi ##ls ##tm chi ##u ##20 ##15 ##name ##d , ma ##20 ##16 ##end sentence en ##code ##r . it takes as input a sequence of words . first , each word is represented as the sum of an em ##bed ##ding vector and the output of a character - level con ##vo ##lu ##tion ##al neural network , resulting in a sequence of vectors . the en ##code ##r applies a two - layer bid ##ire ##ction ##al l ##st ##m graves ##200 ##5 ##frame ##wise to these representations . the first layer runs a long short - term memory unit hoc ##hre ##iter ##19 ##9 ##7 ##long in the forward direction ( taking as input at each step ) and the backward direction ( taking at each step ) to produce vector sequences and . the output of the bi - l ##st ##m is the con ##cate ##nation of these vectors : the second bi - l ##st ##m layer works the same , producing outputs , except it takes as input instead of . sub ##section : cv ##t for sequence tag ##ging in sequence tag ##ging , each token has a corresponding label . the primary prediction module for sequence tag ##ging produces a probability distribution over classes for the label using a one - hidden - layer neural network applied to the corresponding en ##code ##r outputs : the auxiliary prediction modules take and , the outputs of the forward and backward l ##st ##ms in the first bi - l ##st ##m layer , as inputs . we add the following four auxiliary prediction modules to the model ( see figure [ reference ] ) : the \" forward \" module makes each prediction without seeing the right context of the current token . the \" future \" module makes each prediction without the right context or the current token itself . therefore it works like a neural language model that , instead of predicting which token comes next in the sequence , predict ##s which class of token comes next . the \" backward \" and \" past \" modules are analogous . sub ##section : cv ##t for dependency par ##sing in a dependency par ##se , words in a sentence are treated as nodes in a graph . typed directed edges connect the words , forming a tree structure describing the syn ##ta ##ctic structure of the sentence . in particular , each word in a sentence receives exactly one in - going edge going from word ( called the \" head \" ) to it ( the \" dependent \" ) of type ( the \" relation \" ) . we use a graph - based dependency par ##ser similar to the one from do ##za ##t ##20 ##17 ##dee ##p . this treats dependency par ##sing as a classification task where the goal is to predict which in - going edge connects to each word . first , the representations produced by the en ##code ##r for the candidate head and dependent are passed through separate hidden layers . a bi ##line ##ar class ##ifier applied to these representations produces a score for each candidate edge . lastly , these scores are passed through a soft ##max layer to produce pro ##ba ##bilities . mathematical ##ly , the probability of an edge is given as : where is the scoring function : the bi ##line ##ar class ##ifier uses a weight matrix specific to the candidate relation as well as a weight matrix shared across all relations . note that unlike in most prior work , our dependency par ##ser only takes words as inputs , not words and part - of - speech tags . we add four auxiliary prediction modules to our model for cross - view training : each one has some missing context ( not seeing either the preceding or following words ) for the candidate head and candidate dependent . sub ##section : cv ##t for sequence - to - sequence learning we use an en ##code ##r - deco ##der sequence - to - sequence model with attention su ##tsk ##ever ##20 ##14 ##se ##que ##nce , ba ##hd ##ana ##u ##20 ##14 ##ne ##ural . each example consists of an input ( source ) sequence and output ( target ) sequence . the en ##code ##r ' s representations are passed into an l ##st ##m deco ##der using a bi ##line ##ar attention mechanism lu ##ong ##20 ##15 ##ef ##fect ##ive ##at . in particular , at each time step the deco ##der compute ##s an attention distribution over source sequence hidden states as where is the deco ##der ' s current hidden state . the source hidden states weighted by the attention distribution form a context vector : . next , the context vector and current hidden state are combined into an attention vector . lastly , a soft ##max layer predict ##s the next token in the output sequence : . we add two auxiliary deco ##ders when applying cv ##t . the auxiliary deco ##ders share em ##bed ##ding and l ##st ##m parameters with the primary deco ##der , but have different parameters for the attention mechanisms and soft ##max layers . for the first one , we restrict its view of the input by applying attention drop ##out , randomly zero ##ing out a fraction of its attention weights . the second one is trained to predict the next word in the target sequence rather than the current one : . since there is no target sequence for un ##lab ##ele ##d examples , we can not apply teacher forcing to get an output distribution over the vocabulary from the primary deco ##der at each time step . instead , we produce hard targets for the auxiliary modules by running the primary deco ##der with beam search on the input sequence . this idea has previously been applied to sequence - level knowledge di ##sti ##llation by kim ##20 ##16 ##se ##que ##nce ##lev ##el ##k ##d and makes the training procedure similar to back - translation sen ##nr ##ich ##20 ##16 ##im ##pro ##ving ##n ##m . section : experiments we compare cross - view training against several strong baseline ##s on seven tasks : comb ##inatory cat ##ego ##rial grammar ( cc ##g ) super ##tag ##ging : we use data from cc ##gb ##an ##k hoc ##ken ##ma ##ier ##200 ##7 ##cc ##gb ##an ##k . text chunk ##ing : we use the con ##ll - 2000 data t ##jong ##200 ##0 ##int ##rod ##uc ##tion . named entity recognition ( ne ##r ) : we use the con ##ll - 2003 data t ##jong ##200 ##3 ##int ##rod ##uc ##tion . fine - grain ##ed ne ##r ( f ##gn ) : we use the onto ##notes ho ##vy ##200 ##6 ##ont ##ono ##tes data ##set . part - of - speech ( po ##s ) tag ##ging : we use the wall street journal portion of the penn tree ##bank marcus ##19 ##9 ##3 ##building . dependency par ##sing : we use the penn tree ##bank converted to stanford depend ##encies version 3 . 3 . 0 . machine translation : we use the english - vietnamese translation data ##set from i ##ws ##lt 2015 i ##ws ##lt ##15 . we report ( token ##ized ) b ##le ##u scores on the ts ##t ##20 ##13 test set . we use the 1 billion word language model bench ##mark che ##lb ##a ##20 ##13 ##one as a pool of un ##lab ##ele ##d sentences for semi - supervised learning . sub ##section : model details and baseline ##s we apply drop ##out during training , but not when running the primary prediction module to produce soft targets on un ##lab ##ele ##d examples . in addition to the auxiliary prediction modules listed in section [ reference ] , we find it slightly improves results to add another one that sees the whole input rather than a subset ( but unlike the primary prediction module , does have drop ##out applied to its representations ) . unless indicated otherwise , our models have l ##st ##ms with 102 ##4 - sized hidden states and 512 - sized projection layers . see the appendix for full training details and hyper ##para ##meter ##s . we compare cv ##t with the following other semi - supervised learning algorithms : word drop ##out . in this method , we only train the primary prediction module . when acting as a teacher it is run as normal , but when acting as a student , we randomly replace some of the input words with a removed token . this is similar to cv ##t in that it expose ##s the model to a restricted view of the input . however , it is less data efficient . by carefully designing the auxiliary prediction modules , it is possible to train the auxiliary prediction modules to match the primary one across many different views of the input a once , rather than just one view at a time . virtual ad ##vers ##aria ##l training ( va ##t ) . va ##t mi ##yat ##o ##20 ##15 ##dis ##tri ##bution ##al works like word drop ##out , but adds noise to the word em ##bed ##ding ##s of the student instead of dropping out words . notably , the noise is chosen ad ##vers ##aria ##lly so it most changes the model ' s prediction . this method was applied successfully to semi - supervised text classification by mi ##yat ##o ##20 ##16 ##ad ##vers ##aria ##l . elm ##o . elm ##o incorporates the representations from a large separately - trained language model into a task - specific model . our implement ##ai ##ton follows peters ##20 ##18 ##dee ##p . when combining elm ##o with multi - task learning , we allow each task to learn its own weights for the elm ##o em ##bed ##ding ##s going into each prediction module . we found applying drop ##out to the elm ##o em ##bed ##ding ##s was crucial for achieving good performance . sub ##section : results results are shown in table [ reference ] . cv ##t on its own out ##per ##forms or is comparable to the best previously published results on all tasks . figure [ reference ] shows an example win for cv ##t over supervised learning . . of the prior results listed in table [ reference ] , only tag ##lm and elm ##o are semi - supervised . these methods first train an enormous language model on un ##lab ##ele ##d data and incorporate the representations produced by the language model into a supervised class ##ifier . our base models use 102 ##4 hidden units in their l ##st ##ms ( compared to 40 ##9 ##6 in elm ##o ) , require fewer training steps ( around one pass over the billion - word bench ##mark rather than many passes ) , and do not require a pipeline ##d training procedure . therefore , although they perform on par with elm ##o , they are faster and simpler to train . increasing the size of our cv ##t + multi - task model so it has 40 ##9 ##6 units in its l ##st ##ms like elm ##o improves results further so they are significantly better than the elm ##o + multi - task ones . we suspect there could be further gains from combining our method with language model pre - training , which we leave for future work . cv ##t + multi - task . we train a single shared - en ##code ##r cv ##t model to perform all of the tasks except machine translation ( as it is quite different and requires more training time than the other ones ) . multi - task learning improves results on all of the tasks except fine - grain ##ed ne ##r , sometimes by large margins . prior work on many - task nl ##p such as hash ##imo ##to ##20 ##16 ##jo ##int uses complicated architecture ##s and training algorithms . our result shows that simple parameter sharing can be enough for effective many - task learning when the model is big and trained on a large amount of data . interesting ##ly , multi - task learning works better in conjunction with cv ##t than with elm ##o . we h ##yp ##oth ##es ##ize that the elm ##o models quickly fit to the data primarily using the elm ##o vectors , which perhaps hind ##ers the model from learning effective representations that transfer across tasks . we also believe cv ##t alleviate ##s the danger of the model \" forgetting \" one task while training on the other ones , a well - known problem in many - task learning . during multi - task cv ##t , the model makes predictions about un ##lab ##ele ##d examples across all tasks , creating ( artificial ) all - tasks - labeled examples , so the model does not only see one task at a time . in fact , multi - task learning plus self training is similar to the learning without forgetting algorithm li ##20 ##16 ##lea ##rn ##ing ##w ##f , which trains the model to keep its predictions on an old task unchanged when learning a new task . to test the value of all - tasks - labeled examples , we trained a multi - task cv ##t model that only compute ##s on one task at a time ( chosen randomly for each un ##lab ##ele ##d mini ##bat ##ch ) instead of for all tasks in parallel . the one - at - a - time model performs substantially worse ( see table [ reference ] ) . model general ##ization . in order to evaluate how our models general ##ize to the dev set from the train set , we plot the dev vs . train accuracy for our different methods as they learn ( see figure [ reference ] ) . both cv ##t and multi - task learning improve model general ##ization : for the same train accuracy , the models get better dev accuracy than purely supervised learning . interesting ##ly , cv ##t continues to improve in dev set accuracy while close to 100 % train accuracy for cc ##g , chunk ##ing , and ne ##r , perhaps because the model is still learning from un ##lab ##ele ##d data even when it has completely fit to the train set . we also show results for a smaller multi - task + cv ##t model . although it general ##izes at least as well as the larger one , it halt ##s making progress on the train set earlier . this suggests it is important to use sufficiently large neural networks for multi - task learning : otherwise the model does not have the capacity to fit to all the training data . auxiliary prediction module ab ##lation . we briefly explore which auxiliary prediction modules are more important for the sequence tag ##ging tasks in table [ reference ] . we find that both kinds of auxiliary prediction modules improve performance , but that the future and past modules improve results more than the forward and backward ones , perhaps because they see a more restricted and challenging view of the input . training models on small data ##set ##s . we explore how cv ##t scales with data ##set size by varying the amount of training data the model has access to . un ##sur ##pr ##ising ##ly , the improvement of cv ##t over purely supervised learning grows larger as the amount of labeled data decreases ( see figure [ reference ] , left ) . using only 25 % of the labeled data , our approach already performs as well or better than a fully supervised model using 100 % of the training data , demonstrating that cv ##t is particularly useful on small data ##set ##s . training larger models . most sequence tag ##gers and dependency par ##ser ##s in prior work use small l ##st ##ms ( hidden state sizes of around 300 ) because larger models yield little to no gains in performance rei ##mers ##20 ##17 ##re ##port ##ing . we found our own supervised approaches also do not benefit greatly from increasing the model size . in contrast , when using cv ##t accuracy scales better with model size ( see figure [ reference ] , right ) . this finding suggests the appropriate semi - supervised learning methods may enable the development of larger , more sophisticated models for nl ##p tasks with limited amounts of labeled data . general ##iza ##ble representations . lastly , we explore training the cv ##t + multi - task model on five tasks , freezing the en ##code ##r , and then only training a prediction module on the sixth task . this tests whether the en ##code ##r ' s representations general ##ize to a new task not seen during its training . only training the prediction module is very fast because ( 1 ) the en ##code ##r ( which is by far the slow ##est part of the model ) has to be run over each example only once and ( 2 ) we do not back - prop ##aga ##te into the en ##code ##r . results are shown in table [ reference ] . training only a prediction module on top of multi - task representations works remarkably well , out ##per ##form ##ing elm ##o em ##bed ##ding ##s and sometimes even a vanilla supervised model , showing the multi - task model is building up effective representations for language . in particular , the representations could be used like skip - thought vectors ki ##ros ##20 ##15 ##ski ##p to quickly train models on new tasks without slow representation learning . section : related work un ##su ##per ##vis ##ed representation learning . early approaches to deep semi - supervised learning pre - train neural models on un ##lab ##ele ##d data , which has been successful for applications in computer vision jarrett ##200 ##9 ##bes ##t , le ##cu ##n ##20 ##10 ##con ##vo ##lu ##tion ##al and nl ##p . particularly noteworthy for nl ##p are algorithms for learning effective word em ##bed ##ding ##s and language model pre ##train ##ing dai ##20 ##15 ##se ##mi , , peters ##20 ##18 ##dee ##p , howard ##20 ##18 ##uni ##vers ##al , ra ##df ##ord ##20 ##18 ##im ##pro ##ving . pre - training on other tasks such as machine translation has also been studied mccann ##20 ##17 ##lea ##rned ##it . other approaches train \" thought vectors \" representing sentences through un ##su ##per ##vis ##ed or supervised learning . self - training . one of the earliest approaches to semi - supervised learning is self - training sc ##ud ##der ##19 ##65 ##pro ##ba ##bility , which has been successfully applied to nl ##p tasks such as word - sense di ##sam ##bi ##gua ##tion ya ##row ##sky ##19 ##9 ##5 ##un ##su ##per ##vis ##ed and par ##sing mcc ##los ##ky ##200 ##6 ##ef ##fect ##ive . in each round of training , the class ##ifier , acting as a \" teacher , \" labels some of the un ##lab ##ele ##d data and adds it to the training set . then , acting as a \" student , \" it is re ##train ##ed on the new training set . many recent approaches ( including the consistent ##ency regular ##ization methods discussed below and our own method ) train the student with soft targets from the teacher ' s output distribution rather than a hard label , making the procedure more akin to knowledge di ##sti ##llation hint ##on ##20 ##15 ##dis ##till ##ing . it is also possible to use multiple models or prediction modules for the teacher , such as in tri - training zhou ##200 ##5 ##tri , rude ##r ##20 ##18 ##st ##rong . consistency regular ##ization . recent works add noise ( e . g . , drawn from a ga ##uss ##ian distribution ) or apply st ##och ##astic transformations ( e . g . , horizontally flipping an image ) to the student ' s inputs . this trains the model to give consistent predictions to nearby data points , encouraging distribution ##al smooth ##ness in the model . consistency regular ##ization has been very successful for computer vision applications bach ##man ##20 ##14 ##lea ##rn ##ing , lai ##ne ##20 ##16 ##tem ##por ##al , tar ##va ##inen ##20 ##17 ##weight . however , st ##och ##astic input alterations are more difficult to apply to discrete data like text , making consistency regular ##ization less used for natural language processing . one solution is to add noise to the model ' s word em ##bed ##ding ##s mi ##yat ##o ##20 ##16 ##ad ##vers ##aria ##l ; we compare against this approach in our experiments . cv ##t is easily applicable to text because it does not require changing the student ' s inputs . multi - view learning . multi - view learning on data where features can be separated into distinct subset ##s has been well studied xu ##20 ##13 ##as ##o . particularly relevant are co - training blu ##m ##19 ##9 ##8 ##comb ##ining and co - regular ##ization sindh ##wani ##200 ##5 ##aca , which trains two models with di ##s ##jo ##int views of the input . on un ##lab ##ele ##d data , each one acts as a \" teacher \" for the other model . in contrast to these methods , our approach trains a single unified model where auxiliary prediction modules see different , but not necessarily independent views of the input . self supervision . self - supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human - provided labels . recent work has jointly trained image class ##ifiers with tasks like relative position and color ##ization doe ##rs ##ch ##20 ##17 ##mu ##lt ##i , sequence tag ##gers with language modeling rei ##20 ##17 ##se ##mi , and reinforcement learning agents with predicting changes in the environment . unlike these approaches , our auxiliary losses are based on self - labeling , not labels deter ##mini ##stic ##ally constructed from the input . multi - task learning . there has been extensive prior work on multi - task learning car ##uan ##a1 ##9 ##9 ##7 ##mu ##lt ##itas ##k ##l , rude ##r ##20 ##17 ##ano ##o . for nl ##p , most work has focused on a small number of closely related tasks lu ##ong ##20 ##15 ##mu ##lt ##itas ##ks ##t , zhang ##20 ##16 ##sta ##ck , sg ##aar ##d ##20 ##16 ##dee ##pm ##l , peng ##20 ##17 ##dee ##pm ##l . many - task systems are less commonly developed . col ##lo ##bert ##200 ##8 ##au ##a propose a many - task system sharing word em ##bed ##ding ##s between the tasks , hash ##imo ##to ##20 ##16 ##jo ##int train a many - task model where the tasks are arranged hierarchical ##ly according to their linguistic level , and sub ##rama ##nian ##20 ##18 ##lea ##rn ##ing train a shared - en ##code ##r many - task model for the purpose of learning better sentence representations for use in downstream tasks , not for improving results on the original tasks . section : conclusion we propose cross - view training , a new method for semi - supervised learning . our approach allows models to effectively leverage their own predictions on un ##lab ##ele ##d data , training them to produce effective representations that yield accurate predictions even when some of the input is not available . we achieve excellent results across seven nl ##p tasks , especially when cv ##t is combined with multi - task learning . section : acknowledge ##ments we thank ab ##i see , christopher clark , he he , peng qi , reid pry ##zan ##t , yu ##ah ##o zhang , and the anonymous reviewers for their thoughtful comments and suggestions . we thank take ##ru mi ##yat ##o for help with his virtual ad ##vers ##aria ##l training code and emma st ##ru ##bell for answering our questions about onto ##notes ne ##r . kevin is supported by a google phd fellowship . bibliography : references appendix : detailed results we provide a more detailed version of the test set results in the paper , adding two decimal ##s of precision , standard deviation ##s of the 5 runs for each model , and more prior work , in table [ reference ] . appendix : model details our models use two layer cnn - bi ##ls ##tm en ##code ##rs chi ##u ##20 ##15 ##name ##d , ma ##20 ##16 ##end , lamp ##le ##20 ##16 ##ne ##ural and task - specific prediction modules . see section [ reference ] of the paper for details . we provide a few minor details not covered there below . sequence tag ##ging . for chunk ##ing and named entity recognition , we use a bio ##es tag ##ging scheme . we apply label smoothing s ##ze ##ged ##y ##20 ##16 ##ret ##hin ##king , pere ##yra ##20 ##17 ##re ##gul ##ari ##zing with a rate of 0 . 1 to the target labels when training on the labeled data . dependency par ##sing . we om ##it pun ##ct ##uation from evaluation , which is standard practice for the pt ##b - sd 3 . 3 . 0 data ##set . root is represented with a fixed vector instead of using a vector from the en ##code ##r , but otherwise depend ##encies coming from root are scored the same way as the other depend ##encies . machine translation . we apply drop ##out to the output of each l ##st ##m layer in the deco ##der . our implementation is heavily based off of the google nm ##t tutor ##ial lu ##ong ##17 . we attribute our significantly better results to using pre - trained word em ##bed ##ding ##s , a character - level cnn , a larger model , stronger regular ##ization , and better hyper ##para ##meter tuning . target words occurring 5 or fewer times in the train set are replaced with a un ##k token ( but not during evaluation ) . we use a beam size of 10 when performing beam search . we found it slightly beneficial to apply label smoothing with a rate of 0 . 1 to the teacher ' s predictions ( unlike our other tasks , the teacher only provides hard targets to the students for translation ) . multi - task learning . several of our data ##set ##s are constructed from the penn tree ##bank . however , we treat them as separate rather than providing examples labeled across multiple tasks to our model during supervised training . furthermore , the penn tree ##bank tasks do not all use the same train / dev / test splits . we ensure the training split of one task never overlap ##s the evaluation split of another by disc ##arding the overlapping examples from the train sets . other details . we apply drop ##out hint ##on ##20 ##12 ##im ##pro ##ving to the word em ##bed ##ding ##s and outputs of each bi - l ##st ##m . we use an exponential - moving - average ( em ##a ) of the model weights from training for the final model ; we found this to slightly improve accuracy and significantly reduce the variance in accuracy between models trained with different random initial ##izations . the model is trained using sg ##d with momentum poly ##ak ##19 ##64 ##some , su ##tsk ##ever ##20 ##13 ##im ##port ##ance . word em ##bed ##ding ##s are initial ##ized with glove vectors penn ##ington ##20 ##14 ##gl ##ove and fine - tuned during training . the full set of model hyper ##para ##meter ##s are listed in table [ reference ] . baseline ##s . baseline ##s were run with the same architecture and hyper ##para ##meter ##s as the cv ##t model . for the \" word drop ##out \" model , we randomly replace words in the input sentence with a removed token with probability 0 . 1 ( this value worked well on the dev sets ) . for virtual ad ##vers ##aria ##l training , we set the norm of the per ##tur ##bation to be 1 . 5 for cc ##g , 1 . 0 for dependency par ##sing , and 0 . 5 for the other tasks ( these values worked best on the dev sets ) . otherwise , the implementation is as described in mi ##yat ##o ##20 ##16 ##ad ##vers ##aria ##l ; we based our implementation off of their code . we were unable to successfully apply va ##t to machine translation , perhaps because the student is provided hard targets for that task . for elm ##o , we applied drop ##out to the elm ##o em ##bed ##ding ##s before they are incorporated into the rest of the model . when training the multi - task elm ##o model , each prediction module has its own set of soft ##max - normal ##ized weights ( in peters ##20 ##18 ##dee ##p ) for the elm ##o em ##edd ##ings going into the task - specific prediction modules . all tasks share the same weights for the elm ##o em ##bed ##ding ##s going into the shared bi - l ##st ##m en ##code ##r . appendix : cv ##t for image recognition although the focus of our work is on nl ##p , we also applied cv ##t to image recognition and found it performs competitive ##ly with existing methods . most of the semi - supervised image recognition approaches we compare against rely on the inputs being continuous , so they would be difficult to apply to text . more specifically , consistency regular ##ization methods sa ##j ##jad ##i ##20 ##16 ##re ##gul ##ari ##zation , lai ##ne ##20 ##16 ##tem ##por ##al , mi ##yat ##o ##20 ##17 ##vir ##tua ##l rely on adding continuous noise and applying image - specific transformations like crop ##ping to inputs , gan ##s salim ##ans ##20 ##16 ##im ##pro ##ved , wei ##20 ##18 ##im ##pro ##ving are very difficult to train on text due to its discrete nature , and mix ##up zhang ##20 ##17 ##mi ##x ##up , ve ##rma ##20 ##18 ##mani ##fold requires a way of smoothly inter ##pol ##ating between different inputs . approach . our image recognition models are based on con ##vo ##lu ##tion ##al neural networks , which produce a set of features from an image . the first two dimensions of index into the spatial coordinates of feature vectors and is the size of the feature vectors . for shallow ##er cnn ##s , a particular feature vector corresponds to a region of the input image . for example , would be a - dimensional vector of features extracted from the upper left corner . for deeper cnn ##s , a particular feature vector would be extracted from the whole image , but still only use a \" region \" of the representations from an earlier layer . the cnn ##s in our experiments are all in the first category . the primary prediction layers of our cnn ##s take as input the mean of over the first two dimensions , which results in a - dimensional vector that is fed into a soft ##max layer : we add auxiliary prediction layers to the top of the cnn . the th layer takes a single feature vector as input : data . we evaluated our models on the ci ##far - 10 k ##riz ##he ##vsky ##200 ##9 ##lea ##rn ##ing data ##set . following previous work , we make the data ##set ##s semi - supervised by only using the provided labels for a subset of the examples in the training set ; the rest are treated as un ##lab ##ele ##d examples . model . we use the con ##vo ##lu ##tion ##al neural network from mi ##yat ##o ##20 ##17 ##vir ##tua ##l , adapting their tensor ##flow implementation . their model contains 9 con ##vo ##lu ##tion ##al layers and 2 max pool ##ing layers . see appendix d of mi ##yat ##o et al . ' s paper for more details . we add 36 auxiliary soft ##max layers to the collection of feature vectors produced by the cnn . each auxiliary layer sees a patch of the image ranging in size from pixels ( the corner ) to pixels ( the center ) of the pixel images . for some experiments , we combine cv ##t with standard consistency regular ##ization by adding a per ##tur ##bation ( e . g . , a small random vector ) to the student ' s inputs when computing . results . the results are shown in table [ reference ] . un ##sur ##pr ##ising ##ly , adding continuous noise to the inputs works much better with images , where the inputs are naturally continuous , than with language . therefore we see much better results from va ##t on semi - supervised ci ##far - 10 compared to on our nl ##p tasks . however , we still find incorporating cv ##t improves over models without cv ##t . our cv ##t + va ##t models are competitive with current start - of - the - art approaches . we found the gains from cv ##t are larger when no data aug ##ment ##ation is applied , perhaps because random translations of the input expose the model to different \" views \" in a similar manner as with cv ##t . appendix : negative results we briefly describe a few ideas we implemented that did not seem to be effective in initial experiments . note these findings are from early one - off experiments . we did not pursue them further after our first attempts did not pan out , so it is possible that some of these approaches could be effective with the proper adjustments and tuning . hard vs soft targets : classic self - training algorithms train the student model with one - hot \" hard \" targets corresponding to the teacher ' s highest probability prediction . in our experiments , this decreased performance compared to using soft targets . this finding is consistent with research on knowledge di ##sti ##llation hint ##on ##20 ##15 ##dis ##till ##ing , fur ##lane ##llo ##20 ##18 ##born where soft targets also work notably better than hard targets . confidence threshold ##ing : classic self - training often only trains the student on a subset of the un ##lab ##ele ##d examples on which the teacher has confident predictions ( i . e . , the output distribution has low entropy ) . we tried both \" hard \" ( where the student ignores low - confidence examples ) and \" soft \" ( where examples are weighted according to the teacher ' s confidence ) versions of this for training our models , but they did not seem to improve performance . mean teacher : the mean teacher method tar ##va ##inen ##20 ##17 ##weight tracks an exponential moving average ( em ##a ) of model weights , which are used to produce targets for the students . the idea is that these targets may be better quality due to a self - en ##se ##mbling effect . however , we found this approach to have little to no benefit in our experiments , although using em ##a model weights at test time did improve results slightly . purely supervised cv ##t : lastly , we explored adding cross - view losses to purely supervised class ##ifiers . we hoped that adding auxiliary soft ##max layers with different views of the input would act as a regular ##izer on the model . however , we found little to no benefit from this approach . this negative result suggests that the gains from cv ##t are from the improved semi - supervised learning mechanism , not the additional prediction layers regular ##izing the model .",
        "pred_seq": "word ##c [SEP] accuracy accuracy [SEP] accuracy [SEP] semi modeling [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "word2vec"
                    ]
                ],
                "Method": [
                    [
                        "accuracy"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "semisupervised sequence modeling"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "ccgbank"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "accuracy",
                        "dev",
                        "train accuracy"
                    ]
                ],
                "Task": [
                    [
                        "combinatory categorial grammar supertagging",
                        "combinatory categorial grammar",
                        "ccg",
                        "supertagging"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "conll2003"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "named entity recognition",
                        "finegrained ner",
                        "ontonotes ner",
                        "image recognition"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "ontonotes"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "named entity recognition",
                        "finegrained ner",
                        "ontonotes ner",
                        "image recognition"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "penn treebank"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "partofspeech",
                        "pos"
                    ]
                ],
                "Task": [
                    [
                        "dependency parsing",
                        "dependency parse",
                        "ner",
                        "parsing"
                    ]
                ]
            }
        ]
    },
    "11": {
        "doctext": "skip - gram language modeling using sparse non - negative matrix probability estimation section : abstract we present a novel family of language model ( l ##m ) estimation techniques named sparse non - negative matrix ( s ##n ##m ) estimation . a first set of experiments empirical ##ly evaluating it on the one billion word bench ##mark [ reference ] shows that s ##n ##m n - gram l ##ms perform almost as well as the well - established kn ##ese ##r - ne ##y ( kn ) models . when using skip - gram features the models are able to match the state - of - the - art rec ##urrent neural network ( rn ##n ) l ##ms ; combining the two modeling techniques yields the best known result on the bench ##mark . the computational advantages of s ##n ##m over both maximum entropy and rn ##n l ##m estimation are probably its main strength , promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n - gram l ##ms do . section : introduction a statistical language model estimates probability values p ( w ) for strings of words w in a vocabulary v whose size is in the tens , hundreds of thousands and sometimes even millions . typically the string w is broken into sentences , or other segments such as utter ##ances in automatic speech recognition , which are often assumed to be conditional ##ly independent ; we will assume that w is such a segment , or sentence . est ##imating full sentence language models is computational ##ly hard if one seeks a properly normal ##ized probability model 1 over strings of words of finite length in [ reference ] we note that in some practical systems the constraint on using a properly normal ##ized language v * . a simple and sufficient way to ensure proper normal ##ization of the model is to deco ##mp ##ose the sentence probability according to the chain rule and make sure that the end - of - sentence symbol < / s > is predicted with non - zero probability in any context . with w = w 1 , w 2 , . . . , w n we get : p ( w i | w 1 , w 2 , . . . , w i ##\u22121 ) since the parameter space of p ( w k | w 1 , w 2 , . . . , w k ##\u22121 ) is too large , the language model is forced to put the context w k ##\u22121 = w 1 , w 2 , . . . , w k ##\u22121 into an equivalence class determined by a function ##\u03c6 ( w k ##\u22121 ) . as a result , research in language modeling consists of finding appropriate equivalence class ##ifiers ##\u03c6 and methods to estimate p ( w k | \u03c6 ( w k ##\u22121 ) ) . the most successful paradigm in language modeling uses the ( n ##\u2212 1 ) - gram equivalence classification , that is , defines ##\u03c6 ( w k ##\u22121 ) . = w k ##\u2212 ##n + 1 , w k ##\u2212 ##n + 2 , . . . , w k ##\u22121 once the form ##\u03c6 ( w k ##\u22121 ) is specified , only the problem of est ##imating p ( w k | \u03c6 ( w k ##\u22121 ) ) from training data remains . section : per ##plex ##ity as a measure of language model quality a statistical language model can be evaluated by how well it predict ##s a string of symbols w t - commonly referred to as test data - generated by the source to be modeled . a commonly used quality measure for a given model m is related to the entropy of the underlying source and was introduced under the name of per ##plex ##ity ( pp ##l ) : for an excellent discussion on the use of per ##plex ##ity in statistical language modeling , as well as various estimates for the entropy of english the reader is referred to [ reference ] , section 8 . 4 , pages 141 - 142 and the additional reading suggested in section 8 . 5 of the same book . model is side - stepped at a gain in modeling power and simplicity . very likely , not all words in the test string w t are part of the language model vocabulary . it is common practice to map all words that are out - of - vocabulary to a distinguished unknown word symbol , and report the out - of - vocabulary ( o ##ov ) rate on test data - the rate at which one encounters o ##ov words in the test string w ta ##s yet another language model performance metric besides per ##plex ##ity . usually the unknown word is assumed to be part of the language model vocabulary - open vocabulary language models - and its occurrences are counted in the language model per ##plex ##ity calculation , e ##q . ( 3 ) . a situation less common in practice is that of closed vocabulary language models where all words in the test data will always be part of the vocabulary v . section : skip - gram language modeling recently , neural network ( n ##n ) smoothing [ reference ] , [ reference ] , [ reference ] , and in particular rec ##urrent neural networks [ reference ] ( rn ##n ) have shown excellent performance in language modeling [ reference ] . their excellent performance is attributed to a combination of lever ##aging long - distance context , and training a vector representation for words . another simple way of lever ##aging long distance context is to use skip - grams . in our approach , a skip - gram feature extracted from the context w k ##\u22121 is characterized by the tu ##ple ( r , s , a ) where : \u2022 r denotes number of remote context words \u2022 s denotes the number of skipped words \u2022 a denotes the number of adjacent context words relative to the target word w k being predicted . for example , in the sentence , < s > the quick brown fox jumps over the lazy dog < / s > a ( 1 , 2 , 3 ) skip - gram feature for the target word dog is : [ brown skip - 2 over the lazy ] for performance reasons , it is recommended to limit s and to limit either ( r + a ) or limit both r and s ; not setting any limits will result in events containing a set of skip - gram features whose total representation size is qui ##nti ##c in the length of the sentence . we con ##fi ##gur ##e the skip - gram feature extract ##or to produce all features f , defined by the equivalence class ##\u03c6 ( w k ##\u22121 ) , that meet constraints on the minimum and maximum values for : \u2022 the number of context words used r + a ; \u2022 the number of remote words r ; \u2022 the number of adjacent words a ; \u2022 the skip length s . we also allow the option of not including the exact value of s in the feature representation ; this may help with smoothing by sharing counts for various skip features . tied skip - gram features will look like : [ curious ##ity skip - * the cat ] in order to build a good probability estimate for the target word w k in a context w k ##\u22121 we need a way of combining an arbitrary number of skip - gram features f k ##\u22121 , which do not fall into a simple hierarchy like regular n - gram features . the following section describes a simple , yet novel approach for combining such predict ##ors in a way that is computational ##ly easy , scales up gracefully to large amounts of data and as it turns out is also very effective from a modeling point of view . section : sparse non - negative matrix modeling section : model definition in the sparse non - negative matrix ( s ##n ##m ) paradigm , we represent the training data as a sequence of events e = e 1 , e 2 , . . . where each event e ##\u2208 e consists of a sparse non - negative feature vector f and a sparse non - negative target word vector t . both vectors are binary - valued , indicating the presence or absence of a feature or target words , respectively . hence , the training data consists of | e | | p os ( f ) | positive and | e | | p os ( f ) | ( | v | \u2212 1 ) negative training examples , where p os ( f ) denotes the number of positive elements in the vector f . a language model is represented by a non - negative matrix m that , when applied to a given feature vector f , produces a dense prediction vector y : upon evaluation , we normal ##ize y such that we end up with a conditional probability distribution p m ( t | f ) for a model m . for each word w ##\u2208 v that corresponds to index j in t , and its feature vector f that is defined by the equivalence class ##\u03c6 applied to the history h ( w ) of that word in a text , the conditional probability p m ( w | \u03c6 ( h ( w ) ) ) then becomes : for convenience , we will write p ( t j | f ) instead of p m ( t j | f ) in the rest of the paper . as required by the den ##omi ##nator in e ##q . ( 5 ) , this computation involves sum ##ming over all of the present features for the entire vocabulary . however , if we pre ##com ##put ##e the row sums | v | u = 1 m i ##u and store them together with the model , the evaluation can be done very efficiently in only | p os ( f ) | time . moreover , only the positive entries in m i need to be considered , making the range of the sum sparse . section : adjustment function and meta ##fe ##at ##ures we let the entries of m be a slightly modified version of the relative frequencies : where c is a feature - target count matrix , computed over the entire training corpus and a ( i , j ) is a real - valued function , dubbed adjustment function . for each feature ##tar ##get pair ( f i , t j ) , the adjustment function extracts k new features ##\u03b1 k , called meta ##fe ##at ##ures , which are hash ##ed as keys to store corresponding weights ##\u03b8 ( hash ( \u03b1 k ) ) in a huge hash table . to limit memory usage , we use a flat hash table and allow collisions , although this has the potentially und ##es ##ira ##ble effect of tying together the weights of different meta ##fe ##at ##ures . computing the adjustment function for any ( f i , t j ) then amounts to sum ##ming the weights that correspond to its meta ##fe ##at ##ures : from the given input features , such as regular n - grams and skip n - grams , we construct our meta ##fe ##at ##ures as conjunction ##s of any or all of the following elementary meta ##fe ##at ##ures : \u2022 feature identity , e . g . [ brown skip - 2 over the lazy ] \u2022 feature type , e . g . ( 1 , 2 , 3 ) skip - grams \u2022 feature count c i * \u2022 target identity , e . g . dog where we reused the example from section 2 . note that the seemingly absent feature - target identity is represented by the conjunction of the feature identity and the target identity . since the meta ##fe ##at ##ures may involve the feature count and feature - target count , in the rest of the paper we will write ##\u03b1 k ( i , j , c i * , c i ##j ) . this will become important later when we discuss leave - one - out training . each elementary meta ##fe ##at ##ure is joined with the others to form more complex meta ##fe ##at ##ures which in turn are joined with all the other elementary and complex meta ##fe ##at ##ures , ultimately ending up with all 2 5 ##\u2212 1 possible combinations of meta ##fe ##at ##ures . before they are joined , count meta ##fe ##at ##ures are bucket ##ed together according to their ( floor ##ed ) log 2 value . as this effectively puts the lowest count values , of which there are many , into a different bucket , we optional ##ly introduce a second ( ce ##iled ) bucket to assure smooth ##er transitions . both bucket ##s are then weighted according to the log 2 fraction lost by the corresponding rounding operation . note that if we apply double bucket ##ing to both the feature and feature - target count , the amount of meta ##fe ##at ##ures per input feature becomes 2 7 ##\u2212 1 . we will come back to these meta ##fe ##at ##ures in section 4 . 4 where we examine their individual effect on the model . section : loss function est ##imating a model m corresponds to finding optimal weights ##\u03b8 k for all the meta ##fe ##at ##ures for all events in such a way that the average loss over all events between the target vector t and the prediction vector y is minimize ##d , according to some loss function l . the most natural choice of loss function is one that is based on the multi ##no ##mia ##l distribution . that is , we consider t to be multi ##no ##mia ##lly distributed with | v | possible outcomes . the loss function l multi then is : another possibility is the loss function based on the po ##isson distribution 2 : we consider each t j in t to be po ##isson distributed with parameter y j . the conditional probability of p p o ##isson ( t | f ) then is : and the corresponding po ##isson loss function is : where we dropped the last term , since t j is binary - valued 3 . although this choice is not obvious in the context of language modeling , it is well suited to gradient - based optimization and , as we will see , the experimental results are in fact excellent . section : model estimation the adjustment function is learned by applying st ##och ##astic gradient descent on the loss function . that is , for each feature - target pair ( f i , t j ) in each event we need to update the parameters of the meta ##fe ##at ##ures by calculating the gradient with respect to the adjustment function . for the multi ##no ##mia ##l loss , this gradient is : the problem with this update rule is that we need to sum over the entire vocabulary v in the den ##omi ##nator . for most features f i , this is not a big deal as c i ##u = 0 , but some features occur with many if not all targets e . g . the empty feature for un ##ig ##ram ##s . although we might be able to get away with this by re - using these sums and applying them to many / all events in a mini batch , we chose to work with the po ##isson loss in our first implementation . if we calculate the gradient of the po ##isson loss , we get the following : if we were to apply this gradient to each ( positive and negative ) training example , it would be computational ##ly too expensive , because even though the second term is zero for all the negative training examples , the first term needs to be computed for all | e | | p os ( f ) | | v | training examples . however , since the first term does not depend on y j , we are able to distribute the updates for the negative examples over the positive ones by adding in gradient ##s for a fraction of the events where f i = 1 , but t j = 0 . in particular , instead of adding the term f i m i ##j , we add f i t j which lets us update the gradient only on positive examples . we note that this update is only strictly correct for batch training , and not for online training since m i ##j changes after each update . nonetheless , we found this to yield good results as well as seriously reducing the computational cost . the online gradient applied to each training example then becomes : which is non - zero only for positive training examples , hence speeding up computation by a factor of | v | . these aggregate ##d gradient ##s however do not allow us to use additional data to train the adjustment function , since they tie the update computation to the relative frequencies . instead , we have to resort to leave - one - out training to prevent the model from over ##fi ##tting the training data . we do this by excluding the event , generating the gradient ##s , from the counts used to compute those gradient ##s . so , for each positive example ( f i , t j ) of each event e = ( f , t ) , we compute the gradient , excluding f i from c i * and f i t j from c i ##j . for the gradient ##s of the negative examples on the other hand we only exclude f i from c i * and we leave c i ##j untouched , since here we did not observe t j . in order to keep the aggregate computation of the gradient ##s for the negative examples , we distribute them uniformly over all the positive examples with the same feature ; each of the c i ##j positive examples will then compute the gradient of negative examples . to sum ##mar ##ize , when we do leave - one - out training we apply the following gradient update rule on all positive training examples : where y \u2032 j is the product of leaving one out for all the relevant features i . e . section : experiments section : corpus : one billion bench ##mark our experimental setup used the one billion word bench ##mark corpus 4 made available by [ reference ] . for complete ##ness , here is a short description of the corpus , containing only mono ##ling ##ual english data : \u2022 total number of training token ##s is about 0 . 8 billion \u2022 the vocabulary provided consists of 79 ##34 ##7 ##1 words including sentence boundary markers < s > , < \\ s > , and was constructed by disc ##arding all words with count below 3 \u2022 words outside of the vocabulary were mapped to < un ##k > token , also part of the vocabulary \u2022 sentence order was random ##ized \u2022 the test data consisted of 159 ##65 ##8 words ( without counting the sentence beginning marker < s > which is never predicted by the language model ) \u2022 the out - of - vocabulary ( o ##ov ) rate on the test set was 0 . 28 % . section : s ##n ##m for n - gram l ##ms when trained using solely n - gram features , s ##n ##m comes very close to the state ##of - the - art kn ##ese ##r - ne ##y [ reference ] ( kn ) models . table 1 shows that katz [ reference ] performs considerably worse than both s ##n ##m and kn which only differ by about 5 % . when we inter ##pol ##ate these two models linear ##ly , the added gain is only about 1 % , suggesting that they are approximately modeling the same things . the difference between kn and s ##n ##m becomes smaller when we increase the size of the context , going from 5 % for 5 - grams to 3 % for 8 - grams , which indicates that s ##n ##m is better suited to a large number of features . section : sparse non - negative modeling for skip n - grams when we incorporate skip - gram features , we can either build a ' pure ' skip - gram s ##n ##m that contains no regular n - gram features , except for un ##ig ##ram ##s , and inter ##pol ##ate this model with kn , or we can build a single s ##n ##m that has both the regular ng ##ram features and the skip - gram features . we compared the two approaches by choosing skip - gram features that can be considered the skip - equivalent of 5 - grams i . e . they contain at most 4 words . in particular , we used skip - gram features where the remote span is limited to at most 3 words for skip ##s of length between 1 and 3 ( r = [ 1 . section : ] ) . we then built a model that uses both these features and regular 5 - grams ( s ##n ##m ##5 - skip ) , as well as one that only uses the skip - gram features ( s ##n ##m ##5 - skip ( no n - grams ) ) . section : model nu ##m . para ##ms pp ##l s ##n ##m ##5 - skip ( no n - grams ) 61 b 69 . 8 s ##n ##m ##5 - skip 62 b 54 . 2 kn ##5 + s ##n ##m ##5 - skip ( no n - grams ) 56 . 5 kn ##5 + s ##n ##m ##5 - skip 53 . 6 table 2 : number of parameters ( in billions ) and per ##plex ##ity results for s ##n ##m ##5 - skip models with and without n - grams , as well as per ##plex ##ity results for the inter ##pol ##ation with kn ##5 . as it turns out and as can be seen from table 2 , it is better to incorporate all the features into one single s ##n ##m model than to inter ##pol ##ate with a kn 5 - gram model ( kn ##5 ) . inter ##pol ##ating the all - in - one s ##n ##m ##5 - skip with kn ##5 yields almost no additional gain . the best s ##n ##m results so far ( s ##n ##m ##10 - skip ) were achieved using 10 - grams , together with un ##tie ##d skip features of at most 5 words with a skip of exactly 1 word ( s = 1 , r + a = [ 1 . section : ] ) . this mixture of rich short - distance and shallow long - distance features enables the model to achieve state - of - the - art results , as can be seen in table 3 . when we compare the per ##plex ##ity of this model with the state - of - the art rn ##n results in [ reference ] , the difference is only 3 % . moreover , although our model has more parameters than the rn ##n ( 33 vs 20 billion ) , training takes about a tenth of the time ( 24 hours vs 240 hours ) . interesting ##ly , when we inter ##pol ##ate the two models , we have an additional gain of 20 % , and as far as we know , the per ##plex ##ity of 41 . 3 is already the best ever reported on this database , beating the previous best by 6 % [ reference ] . finally , when we opt ##imi ##ze inter ##pol ##ation weights over all models in [ reference ] , including s ##n ##m ##5 - skip and s ##n ##m ##10 - skip , the contribution of the other models as well as the per ##plex ##ity reduction is ne ##gli ##gible , as can be seen in table 3 , which also sum ##mar ##izes the per ##plex ##ity results for each of the individual models . section : ab ##lation experiments to find out how much , if anything at all , each meta ##fe ##at ##ure contributes to the adjustment function , we ran a series of ab ##lation experiments in which we ab ##lated one meta ##fe ##at ##ure at a time . when we experimented on s ##n ##m ##5 , we found , un ##sur ##pr ##ising ##ly , that the most important meta ##fe ##at ##ure is the feature - target count . at first glance , it does not seem to matter much whether the counts are stored in 1 or 2 bucket ##s , but the second bucket really starts to pay off for models with a large number of singleton features e . g . s ##n ##m ##10 - skip 5 . this is not the case for the feature counts , where having a single bucket is always better , although in general the feature counts do not contribute much . in any case , feature counts are definitely the least important for the model . the remaining meta ##fe ##at ##ures all contribute more or less equally , all of which can be seen in table 4 . section : related work s ##n ##m estimation is closely related to all n - gram l ##m smoothing techniques that rely on mixing relative frequencies at various orders . unlike most of those , it combines the predict ##ors at various orders without relying on a hierarchical nesting of the contexts , setting it closer to the family of maximum entropy ( me ) [ reference ] , or exponential models . we are not the first ones to highlight the effectiveness of skip n - grams at capturing depend ##encies across longer contexts , similar to rn ##n l ##ms ; previous such results were reported in [ reference ] . [ reference ] attempts to capture long range depend ##encies in language where the skip n - grams are identified using a left - to - right syn ##ta ##ctic par ##ser . approaches such as [ reference ] leverage late ##nt semantic information , whereas [ reference ] integrate ##s both syn ##ta ##ctic and topic - based modeling in a unified approach . the speed - ups to me , and rn ##n l ##m training provided by hierarchical ##ly predicting words at the output layer [ reference ] , and sub ##sam ##pling [ reference ] still require updates that are linear in the vocabulary size times the number of words in the training data , whereas the s ##n ##m updates in e ##q . ( 15 ) for the much smaller adjustment function eliminate the dependency on the vocabulary size . scaling up rn ##n l ##m training is described in [ reference ] and [ reference ] . the computational advantages of s ##n ##m over both maximum entropy and rn ##n l ##m estimation are probably its main strength , promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n - gram l ##ms do . section : section : conclusions and future work we have presented s ##n ##m , a new family of l ##m estimation techniques . a first empirical evaluation on the one billion word bench ##mark [ reference ] shows that s ##n ##m n - gram l ##ms perform almost as well as the well - established kn models . when using skip - gram features the models are able to match the stat - of - the - art rn ##n l ##ms ; combining the two modeling techniques yields the best known result on the bench ##mark . future work items include model pr ##uni ##ng , exploring richer features similar to [ reference ] , as well as richer meta ##fe ##at ##ures in the adjustment model , mixing s ##n ##m models trained on various data sources such that they perform best on a given development set , and estimation techniques that are more flexible in this respect . section :",
        "pred_seq": "[SEP] sparse modeling [SEP] [SEP] skip modeling [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "skipgram language modeling"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "one billion word benchmark",
                        "one billion benchmark"
                    ]
                ],
                "Method": [
                    [
                        "sparse nonnegative matrix probability estimation",
                        "sparse nonnegative matrix",
                        "snm",
                        "snm ngram lms",
                        "nonnegative matrix m",
                        "sparse nonnegative modeling",
                        "pure skipgram snm",
                        "snm5skip",
                        "snm5",
                        "snm model",
                        "snm10skip",
                        "snm estimation",
                        "snm updates",
                        "snm models"
                    ]
                ],
                "Metric": [
                    [
                        "params"
                    ]
                ],
                "Task": [
                    [
                        "skipgram language modeling",
                        "language modeling",
                        "statistical language modeling"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "one billion word benchmark",
                        "one billion benchmark"
                    ]
                ],
                "Method": [
                    [
                        "sparse nonnegative matrix probability estimation",
                        "sparse nonnegative matrix",
                        "snm",
                        "snm ngram lms",
                        "nonnegative matrix m",
                        "sparse nonnegative modeling",
                        "pure skipgram snm",
                        "snm5skip",
                        "snm5",
                        "snm model",
                        "snm10skip",
                        "snm estimation",
                        "snm updates",
                        "snm models"
                    ]
                ],
                "Metric": [
                    [
                        "perplexity",
                        "ppl"
                    ]
                ],
                "Task": [
                    [
                        "skipgram language modeling",
                        "language modeling",
                        "statistical language modeling"
                    ]
                ]
            }
        ]
    },
    "12": {
        "doctext": "domain - ad ##vers ##aria ##l training of neural networks section : abstract we introduce a new representation learning approach for domain adaptation , in which data at training and test time come from similar but different distributions . our approach is directly inspired by the theory on domain adaptation suggesting that , for effective domain transfer to be achieved , predictions must be made based on features that can not disc ##rim ##inate between the training ( source ) and test ( target ) domains . the approach implements this idea in the context of neural network architecture ##s that are trained on labeled data from the source domain and un ##lab ##ele ##d data from the target domain ( no labeled target - domain data is necessary ) . as the training progresses , the approach promotes the emergence of features that are ( i ) disc ##rim ##ina ##tive for the main learning task on the source domain and ( ii ) ind ##is ##cr ##imi ##nate with respect to the shift between the domains . we show that this adaptation behaviour can be achieved in almost any feed - forward model by aug ##ment ##ing it with few standard layers and a new gradient reversal layer . the resulting augmented architecture can be trained using standard back ##pro ##pa ##gation and st ##och ##astic gradient descent , and can thus be implemented with little effort using any of the deep learning packages . we demonstrate the success of our approach for two distinct classification problems ( document sentiment analysis and image classification ) , where state - of - the - art domain adaptation performance on standard bench ##marks is achieved . we also valid ##ate the approach for des ##cript ##or learning task in the context of person re - identification application . section : introduction the cost of generating labeled data for a new machine learning task is often an obstacle for applying machine learning methods . in particular , this is a limiting factor for the further progress of deep neural network architecture ##s , that have already brought impressive advances to the state - of - the - art across a wide variety of machine - learning tasks and applications . for problems lacking labeled data , it may be still possible to obtain training sets that are big enough for training large - scale deep models , but that suffer from the shift in data distribution from the actual data encountered at \" test time \" . one important example is training an image class ##ifier on synthetic or semi - synthetic images , which may come in abundance and be fully labeled , but which inevitably have a distribution that is different from real images [ reference ] [ reference ] [ reference ] [ reference ] . another example is in the context of sentiment analysis in written reviews , where one might have labeled data for reviews of one type of product ( e . g . , movies ) , while having the need to classify reviews of other products ( e . g . , books ) . learning a disc ##rim ##ina ##tive class ##ifier or other predict ##or in the presence of a shift between training and test distributions is known as domain adaptation ( da ) . the proposed approaches build mapping ##s between the source ( training - time ) and the target ( test - time ) domains , so that the class ##ifier learned for the source domain can also be applied to the target domain , when composed with the learned mapping between domains . the appeal of the domain adaptation approaches is the ability to learn a mapping between domains in the situation when the target domain data are either fully un ##lab ##ele ##d ( un ##su ##per ##vis ##ed domain ann ##ota ##tion ) or have few labeled samples ( semi - supervised domain adaptation ) . below , we focus on the harder un ##su ##per ##vis ##ed case , although the proposed approach ( domain - ad ##vers ##aria ##l learning ) can be generalized to the semi - supervised case rather straightforward ##ly . unlike many previous papers on domain adaptation that worked with fixed feature representations , we focus on combining domain adaptation and deep feature learning within one training process . our goal is to em ##bed domain adaptation into the process of learning representation , so that the final classification decisions are made based on features that are both disc ##rim ##ina ##tive and invariant to the change of domains , i . e . , have the same or very similar distributions in the source and the target domains . in this way , the obtained feed - forward network can be applicable to the target domain without being hind ##ered by the shift between the two domains . our approach is motivated by the theory on domain adaptation [ reference ] , that suggests that a good representation for cross - domain transfer is one for which an algorithm can not learn to identify the domain of origin of the input observation . we thus focus on learning features that combine ( i ) disc ##rim ##ina ##tive ##ness and ( ii ) domain ##in ##var ##iance . this is achieved by jointly opt ##imi ##zing the underlying features as well as two disc ##rim ##ina ##tive class ##ifiers operating on these features : ( i ) the label predict ##or that predict ##s class labels and is used both during training and at test time and ( ii ) the domain class ##ifier that disc ##rim ##inates between the source and the target domains during training . while the parameters of the class ##ifiers are opt ##imi ##zed in order to minimize their error on the training set , the parameters of the underlying deep feature mapping are opt ##imi ##zed in order to minimize the loss of the label class ##ifier and to maximize the loss of the domain class ##ifier . the latter update thus works ad ##vers ##aria ##lly to the domain class ##ifier , and it encourages domain - invariant features to emerge in the course of the optimization . crucial ##ly , we show that all three training processes can be embedded into an appropriately composed deep feed - forward network , called domain - ad ##vers ##aria ##l neural network ( dan ##n ) ( illustrated by figure 1 , page 12 ) that uses standard layers and loss functions , and can be trained using standard back ##pro ##pa ##gation algorithms based on st ##och ##astic gradient descent or its modifications ( e . g . , sg ##d with momentum ) . the approach is generic as a dan ##n version can be created for almost any existing feed - forward architecture that is train ##able by back ##pro ##pa ##gation . in practice , the only non - standard component of the proposed architecture is a rather trivial gradient reversal layer that leaves the input unchanged during forward propagation and reverse ##s the gradient by multi ##ply ##ing it by a negative scala ##r during the back ##pro ##pa ##gation . we provide an experimental evaluation of the proposed domain - ad ##vers ##aria ##l learning idea over a range of deep architecture ##s and applications . we first consider the simplest dan ##n architecture where the three parts ( label predict ##or , domain class ##ifier and feature extract ##or ) are linear , and demonstrate the success of domain - ad ##vers ##aria ##l learning for such architecture . the evaluation is performed for synthetic data as well as for the sentiment analysis problem in natural language processing , where dan ##n improves the state - of - the - art marginal ##ized stacked auto ##en ##code ##rs ( ms ##da ) of [ reference ] on the common amazon reviews bench ##mark . we further evaluate the approach extensively for an image classification task , and present results on traditional deep learning image data sets - such as mn ##ist [ reference ] and sv ##hn [ reference ] ) - as well as on office bench ##marks [ reference ] , where domain - ad ##vers ##aria ##l learning allows obtaining a deep architecture that considerably improves over previous state - of - the - art accuracy . finally , we evaluate domain - ad ##vers ##aria ##l des ##cript ##or learning in the context of person re - identification application [ reference ] , where the task is to obtain good pedestrian image des ##cript ##ors that are suitable for retrieval and verification . we apply domain ##ad ##vers ##aria ##l learning , as we consider a des ##cript ##or predict ##or trained with a siam ##ese - like loss instead of the label predict ##or trained with a classification loss . in a series of experiments , we demonstrate that domain - ad ##vers ##aria ##l learning can improve cross - data - set re - identification considerably . section : related work the general approach of achieving domain adaptation explored under many face ##ts . over the years , a large part of the literature has focused mainly on linear hypothesis ( see for instance [ reference ] [ reference ] [ reference ] [ reference ] . more recently , non - linear representations have become increasingly studied , including neural network representations [ reference ] and most notably the state - of - the - art ms ##da [ reference ] . that literature has mostly focused on exploit ##ing the principle of robust representations , based on the den ##ois ##ing auto ##en ##code ##r paradigm [ reference ] . concurrently , multiple methods of matching the feature distributions in the source and the target domains have been proposed for un ##su ##per ##vis ##ed domain adaptation . some ap - pro ##ache ##s perform this by re ##weig ##hing or selecting samples from the source domain [ reference ] , while others seek an explicit feature space transformation that would map source distribution into the target one [ reference ] [ reference ] [ reference ] ) . an important aspect of the distribution matching approach is the way the ( di ##s ) similarity between distributions is measured . here , one popular choice is matching the distribution means in the kernel ##re ##pro ##du ##cing hilbert space [ reference ] , whereas [ reference ] and [ reference ] map the principal axes associated with each of the distributions . our approach also attempts to match feature space distributions , however this is accomplished by modifying the feature representation itself rather than by re ##weig ##hing or geometric transformation . also , our method uses a rather different way to measure the di ##spar ##ity between distributions based on their sep ##ara ##bility by a deep disc ##rim ##ina ##tively - trained class ##ifier . note also that several approaches perform transition from the source to the target domain [ reference ] [ reference ] by changing gradually the training distribution . among these methods , [ reference ] does this in a \" deep \" way by the layer ##wise training of a sequence of deep auto ##en ##code ##rs , while gradually replacing source - domain samples with target - domain samples . this improves over a similar approach of [ reference ] that simply trains a single deep auto ##en ##code ##r for both domains . in both approaches , the actual class ##ifier / predict ##or is learned in a separate step using the feature representation learned by auto ##en ##code ##r ( s ) . in contrast to [ reference ] [ reference ] , our approach performs feature learning , domain adaptation and class ##ifier learning jointly , in a unified architecture , and using a single learning algorithm ( back ##pro ##pa ##gation ) . we therefore argue that our approach is simpler ( both conceptual ##ly and in terms of its implementation ) . our method also achieve ##s considerably better results on the popular office bench ##mark . while the above approaches perform un ##su ##per ##vis ##ed domain adaptation , there are approaches that perform supervised domain adaptation by exploit ##ing labeled data from the target domain . in the context of deep feed - forward architecture ##s , such data can be used to \" fine - tune \" the network trained on the source domain [ reference ] [ reference ] [ reference ] . our approach does not require labeled target - domain data . at the same time , it can easily incorporate such data when they are available . an idea related to ours is described in [ reference ] . while their goal is quite different ( building genera ##tive deep networks that can synth ##es ##ize samples ) , the way they measure and minimize the disc ##re ##pan ##cy between the distribution of the training data and the distribution of the synthesized data is very similar to the way our architecture measures and minimize ##s the disc ##re ##pan ##cy between feature distributions for the two domains . moreover , the authors mention the problem of sat ##ura ##ting si ##gm ##oids which may arise at the early stages of training due to the significant di ##ssi ##mi ##lar ##ity of the domains . the technique they use to ci ##rc ##um ##vent this issue ( the \" ad ##vers ##aria ##l \" part of the gradient is replaced by a gradient computed with respect to a suitable cost ) is directly applicable to our method . also , recent and concurrent reports by [ reference ] focus on domain adaptation in feed - forward networks . their set of techniques measures and minimize ##s the distance between the data distribution means across domains ( potentially , after em ##bed ##ding distributions into r ##kh ##s ) . their approach is thus different from our idea of matching distributions by making them ind ##ist ##ing ##uis ##hab ##le for a disc ##rim ##ina ##tive class ##ifier . below , we compare our approach to ; [ reference ] on the office bench ##mark . another approach to deep domain adaptation , which is arguably more different from ours , has been developed in parallel by [ reference ] . from a theoretical stand ##point , our approach is directly derived from the seminal theoretical works of [ reference ] . indeed , dan ##n directly opt ##imi ##zes the notion of h - diver ##gence . we do note the work of [ reference ] , in which hmm representations are learned for word tag ##ging using a posterior regular ##izer that is also inspired by ben - david et al . ' s work . in addition to the tasks being different - huang and yates ( 2012 ) focus on word tag ##ging problems - , we would argue that dan ##n learning objective more closely opt ##imi ##zes the h - diver ##gence , with huang and yates ( 2012 ) relying on crude ##r approximation ##s for efficiency reasons . a part of this paper has been published as a conference paper [ reference ] . this version extends [ reference ] very considerably by incorporating the report [ reference ] ( presented as part of the second workshop on transfer and multi - task learning ) , which brings in new terminology , in - depth theoretical analysis and justification of the approach , extensive experiments with the shallow dan ##n case on synthetic data as well as on a natural language processing task ( sentiment analysis ) . furthermore , in this version we go beyond classification and evaluate domain - ad ##vers ##aria ##l learning for des ##cript ##or learning setting within the person re - identification application . section : domain adaptation we consider classification tasks where x is the input space and y = { 0 , 1 , . . . , l ##\u22121 } is the set of l possible labels . moreover , we have two different distributions over x ##\u00d7 ##y , called the source domain d s and the target domain d t . an un ##su ##per ##vis ##ed domain adaptation learning algorithm is then provided with a labeled source sample s drawn i . i . d . from d s , and an un ##lab ##ele ##d target sample with n = n + n being the total number of samples . the goal of the learning algorithm is to build a class ##ifier ##\u03b7 : x ##\u2192 y with a low target risk while having no information about the labels of d t . section : domain diver ##gence to tackle the challenging domain adaptation task , many approaches bound the target error by the sum of the source error and a notion of distance between the source and the target distributions . these methods are intuitive ##ly justified by a simple assumption : the source risk is expected to be a good indicator of the target risk when both distributions are similar . several notions of distance have been proposed for domain adaptation [ reference ] [ reference ] [ reference ] . in this paper , we focus on the h - diver ##gence used by [ reference ] , and based on the earlier work of [ reference ] . note that we assume in definition 1 below that the hypothesis class h is a ( discrete or continuous ) set of binary class ##ifiers ##\u03b7 : x ##\u2192 { 0 , 1 } . 1 definition 1 [ reference ] [ reference ] that is , the h - diver ##gence relies on the capacity of the hypothesis class h to distinguish between examples generated by d x s from examples generated by d x t . [ reference ] proved that , for a symmetric hypothesis class h , one can compute the empirical where i [ a ] is the indicator function which is 1 if pre ##dicate a is true , and 0 otherwise . section : proxy distance ben - david et al . [ reference ] suggested that , even if it is generally hard to computed h ( s , t ) exactly ( e . g . , when h is the space of linear class ##ifiers on x ) , we can easily approximate it by running a learning algorithm on the problem of disc ##rim ##inating between source and target examples . to do so , we construct a new data set where the examples of the source sample are labeled 0 and the examples of the target sample are labeled 1 . then , the risk of the class ##ifier trained on the new data set u approximate ##s the \" min \" part of equation ( 1 ) . given a general ##ization error on the problem of disc ##rim ##inating between source and target examples , the h - diver ##gence is then approximate ##d [UNK] in [ reference ] , the valued a is called the , where a is a subset of x . note that , by choosing a = { a ##\u03b7 | \u03b7 ##\u2208 h } , with a ##\u03b7 the set represented by the characteristic function ##\u03b7 , the a - distance and the h - diver ##gence of definition 1 are identical . in the experiments section of this paper , we compute the pad value following the approach of [ reference ] [ reference ] , i . e . , we train either a linear sv ##m or a deeper ml ##p class ##ifier on a subset of u ( equation 2 ) , and we use the obtained class ##ifier error on the other subset as the value of in equation ( 3 ) . more details and illustrations of the linear sv ##m case are provided in section 5 . 1 . 5 . section : general ##ization bound on the target risk the work of [ reference ] is upper bounded by its empirical estimated h ( s , t ) plus a constant complexity term that depends on the vc dimension of h and the size of samples s and t . by combining this result with a similar bound on the source risk , the following theorem is obtained . , and is the empirical source risk . the previous result tells us that r d t ( \u03b7 ) can be low only when the ##\u03b2 term is low , i . e . , only when there exists a class ##ifier that can achieve a low risk on both distributions . it also tells us that , to find a class ##ifier with a small r d t ( \u03b7 ) in a given class of fixed vc dimension , the learning algorithm should minimize ( in that class ) a trade - off between the source risk r s ( \u03b7 ) and the empirical h - diver ##gence ##d h ( s , t ) . as pointed - out by [ reference ] , a strategy to control the h - diver ##gence is to find a representation of the examples where both the source and the target domain are as ind ##ist ##ing ##uis ##hab ##le as possible . under such a representation , a hypothesis with a low source risk will , according to theorem 2 , perform well on the target data . in this paper , we present an algorithm that directly exploits this idea . section : domain - ad ##vers ##aria ##l neural networks ( dan ##n ) an original aspect of our approach is to explicitly implement the idea exhibited by theorem 2 into a neural network class ##ifier . that is , to learn a model that can general ##ize well from one domain to another , we ensure that the internal representation of the neural network contains no disc ##rim ##ina ##tive information about the origin of the input ( source or target ) , while preserving a low risk on the source ( labeled ) examples . in this section , we detail the proposed approach for incorporating a \" domain adaptation component \" to neural networks . in sub ##section 4 . 1 , we start by developing the idea for the simplest possible case , i . e . , a single hidden layer , fully connected neural network . we then describe how to general ##ize the approach to arbitrary ( deep ) network architecture ##s . section : example case with a shallow neural network let us first consider a standard neural network ( n ##n ) architecture with a single hidden layer . for simplicity , we suppose that the input space is formed by m - dimensional real vectors . thus , x = r m . the hidden layer g f learns a function g f : x ##\u2192 r d that maps an example into a new d - dimensional representation 2 , and is parameter ##ized by a matrix - vector pair ( w , b ) \u2208 r d ##\u00d7 ##m ##\u00d7 r d : with si ##gm ( a ) = similarly , the prediction layer g y learns a function here we have l = | y | . by using the soft ##max function , each component of vector g y ( g f ( x ) ) denotes the conditional probability that the neural network assigns x to the class in y represented by that component . given a source example ( x i , y i ) , the natural classification loss to use is the negative log - probability of the correct label : training the neural network then leads to the following optimization problem on the source domain : where , y i is a short ##hand notation for the prediction loss on the i - th example , and r ( w , b ) is an optional regular ##izer that is weighted by hyper - parameter ##\u03bb . the heart of our approach is to design a domain regular ##izer directly derived from the h - diver ##gence of definition 1 . to this end , we view the output of the hidden layer g f ( \u00b7 ) ( equation 4 ) as the internal representation of the neural network . thus , we denote the source sample representations as similarly , given an un ##lab ##ele ##d sample from the target domain we denote the corresponding representations based on equation ( 1 ) , the empirical h - diver ##gence of a symmetric hypothesis class h between samples s ( g f ) and let us consider h as the class of hyper ##planes in the representation space . inspired by the proxy a - distance ( see section 3 . 2 ) , we suggest est ##imating the \" min \" part of equation ( 6 ) by a domain classification layer g d that learns a log ##istic reg ##ress ##or parameter ##ized by a vector - scala ##r pair ( u , z ) \u2208 r d ##\u00d7 r , that models the probability that a given input is from the source domain d hence , the function g d ( \u00b7 ) is a domain reg ##ress ##or . we define its loss by where d i denotes the binary variable ( domain label ) for the i - th example , which indicates whether x i come from the source distribution ( recall that for the examples from the source distribution ( d i = 0 ) , the corresponding labels y i ##\u2208 y are known at training time . for the examples from the target domains , we do not know the labels at training time , and we want to predict such labels at test time . this enables us to add a domain adaptation term to the objective of equation ( 5 ) , giving the following regular ##izer : where this regular ##izer seeks to approximate the h - diver ##gence of equation ( 6 ) , as 2 ( 1 ##\u2212 ##r ( w , b ) ) is a sur ##rogate ford h s ( g f ) , t ( g f ) . in line with theorem 2 , the optimization problem given by equations ( 5 ) and ( 8 ) implements a trade - off between the mini ##mi ##zation of the source risk r s ( \u00b7 ) and the diver ##gence ##d h ( \u00b7 , \u00b7 ) . the hyper - parameter ##\u03bb is then used to tune the trade - off between these two quantities during the learning process . for learning , we first note that we can re ##write the complete optimization objective of equation ( 5 ) as follows : where we are seeking the [UNK] , v , b , [UNK] , [UNK] , [UNK] that deliver a saddle point given by thus , the optimization problem involves a mini ##mi ##zation with respect to some parameters , as well as a maxim ##ization with respect to the others . algorithm 1 shallow dan ##n - st ##och ##astic training update while stopping criterion is not met do 6 : for i from 1 to n do 7 : # forward propagation 8 : 32 : # update neural network parameters 33 : end for 41 : end while note : in this pseudo - code , e ( y ) refers to a \" one - hot \" vector , consisting of all 0 ##s except for a 1 at position y , and is the element - wise product . we propose to tackle this problem with a simple st ##och ##astic gradient procedure , in which updates are made in the opposite direction of the gradient of equation ( 9 ) for the mini ##mi ##zing parameters , and in the direction of the gradient for the maxim ##izing parameters . st ##och ##astic estimates of the gradient are made , using a subset of the training samples to compute the averages . algorithm 1 provides the complete pseudo - code of this learning procedure . 3 in words , during training , the neural network ( parameter ##ized by w , b , v , c ) and the domain reg ##ress ##or ( parameter ##ized by u , z ) are competing against each other , in an ad ##vers ##aria ##l way , over the objective of equation ( 9 ) . for this reason , we refer to networks trained according to this objective as domain - ad ##vers ##aria ##l neural networks ( dan ##n ) . dan ##n will effectively attempt to learn a hidden layer g f ( \u00b7 ) that maps an example ( either source or target ) into a representation allowing the output layer g y ( \u00b7 ) to accurately classify source samples , but cr ##ip ##pling the ability of the domain reg ##ress ##or g d ( \u00b7 ) to detect whether each example belongs to the source or target domains . section : general ##ization to arbitrary architecture ##s for illustration purposes , we ' ve so far focused on the case of a single hidden layer dan ##n . however , it is straightforward to general ##ize to other sophisticated architecture ##s , which might be more appropriate for the data at hand . for example , deep con ##vo ##lu ##tion ##al neural networks are well known for being state - of - the - art models for learning disc ##rim ##ina ##tive features of images [ reference ] . let us now use a more general notation for the different components of dan ##n . namely , let g f ( \u00b7 ; \u03b8 f ) be the d - dimensional neural network feature extract ##or , with parameters ##\u03b8 f . also , let g y ( \u00b7 ; \u03b8 y ) be the part of dan ##n that compute ##s the network ' s label prediction output layer , with parameters ##\u03b8 y , while g d ( \u00b7 ; \u03b8 d ) now corresponds to the computation of the domain prediction output of the network , with parameters ##\u03b8 d . note that for preserving the theoretical guarantees of theorem 2 , the hypothesis class h d generated by the domain prediction component g d should include the hypothesis class h y generated by the label prediction component g y . thus , we will note the prediction loss and the domain loss respectively by training dan ##n then parallels the single layer case and consists in opt ##imi ##zing by finding the saddle point ##\u03b8 f , \u03b8 y , \u03b8 d such that as suggested previously , a saddle point defined by equations ( 11 - 12 ) can be found as a stationary point of the following gradient updates : where ##\u00b5 is the learning rate . we use st ##och ##astic estimates of these gradient ##s , by sampling examples from the data set . the updates of equations ( 13 - 15 ) are very similar to st ##och ##astic gradient descent ( sg ##d ) updates for a feed - forward deep model that comprises feature extract ##or fed into the label figure 1 : the proposed architecture includes a deep feature extract ##or ( green ) and a deep label predict ##or ( blue ) , which together form a standard feed - forward architecture . un ##su ##per ##vis ##ed domain adaptation is achieved by adding a domain class ##ifier ( red ) connected to the feature extract ##or via a gradient reversal layer that multi ##pl ##ies the gradient by a certain negative constant during the back ##pro ##pa ##gation - based training . otherwise , the training proceeds standard ##ly and minimize ##s the label prediction loss ( for source examples ) and the domain classification loss ( for all samples ) . gradient reversal ensures that the feature distributions over the two domains are made similar ( as ind ##ist ##ing ##uis ##hab ##le as possible for the domain class ##ifier ) , thus resulting in the domain - invariant features . predict ##or and into the domain class ##ifier ( with loss weighted by ##\u03bb ) . the only difference is that in ( 13 ) , the gradient ##s from the class and domain predict ##ors are sub ##tracted , instead of being sum ##med ( the difference is important , as otherwise sg ##d would try to make features di ##ssi ##mi ##lar across domains in order to minimize the domain classification loss ) . since sg ##dan ##d its many variants , such as ada ##grad [ reference ] or ada ##del ##ta [ reference ] - is the main learning algorithm implemented in most libraries for deep learning , it would be convenient to frame an implementation of our st ##och ##astic saddle point procedure as sg ##d . fortunately , such a reduction can be accomplished by introducing a special gradient reversal layer ( gr ##l ) , defined as follows . the gradient reversal layer has no parameters associated with it . during the forward propagation , the gr ##l acts as an identity transformation . during the back ##pro ##pa ##gation however , the gr ##l takes the gradient from the subsequent level and changes its sign , i . e . , multi ##pl ##ies it by ##\u22121 , before passing it to the preceding layer . implementing such a layer using existing object - oriented packages for deep learning is simple , requiring only to define procedures for the forward propagation ( identity transformation ) , and back ##pro ##pa ##gation ( multi ##ply ##ing by ##\u22121 ) . the layer requires no parameter update . the gr ##l as defined above is inserted between the feature extract ##or g f and the domain class ##ifier g d , resulting in the architecture depicted in figure 1 . as the back ##pro ##pa ##gation process passes through the gr ##l , the partial derivatives of the loss that is downstream the gr ##l ( i . e . , l d ) w . r . t . the layer parameters that are upstream the gr ##l ( i . e . , \u03b8 f ) get multiplied by ##\u22121 , i . e . , . therefore , running sg ##d in the resulting model implements the updates of equations ( 13 - 15 ) and converge ##s to a saddle point of equation ( 10 ) . mathematical ##ly , we can formally treat the gradient reversal layer as a \" pseudo - function \" r ( x ) defined by two ( incompatible ) equations describing its forward and back ##pro ##pa ##gation behaviour : where i is an identity matrix . we can then define the objective \" pseudo - function \" of ( \u03b8 f , \u03b8 y , \u03b8 d ) that is being opt ##imi ##zed by the st ##och ##astic gradient descent within our method : running updates ( 13 - 15 ) can then be implemented as doing sg ##d for ( 18 ) and leads to the emergence of features that are domain - invariant and disc ##rim ##ina ##tive at the same time . after the learning , the label predict ##or g y ( g f ( x ; \u03b8 f ) ; \u03b8 y ) can be used to predict labels for samples from the target domain ( as well as from the source domain ) . note that we release the source code for the gradient reversal layer along with the usage examples as an extension to caf ##fe . 4 section : experiments in this section , we present a variety of empirical results for both shallow domain ad ##vers ##aria ##l neural networks ( sub ##section 5 . 1 ) and deep ones ( sub ##section ##s 5 . 2 and 5 . 3 ) . section : experiments with shallow neural networks in this first experiment section , we evaluate the behavior of the simple version of dan ##n described by sub ##section 4 . 1 . note that the results reported in the present sub ##section are obtained using algorithm 1 . thus , the st ##och ##astic gradient descent approach here consists of sampling a pair of source and target examples and performing a gradient step update of all parameters of dan ##n . crucial ##ly , while the update of the regular parameters follows as usual the opposite direction of the gradient , for the ad ##vers ##aria ##l parameters the step must follow the gradient ' s direction ( since we maximize with respect to them , instead of mini ##mi ##zing ) . section : experiments on a toy problem as a first experiment , we study the behavior of the proposed algorithm on a variant of the inter - twin ##ning moons 2d problem , where the target distribution is a rotation of the source one . as the source sample s , we generate a lower moon and an upper moon labeled 0 and 1 respectively , each of which containing 150 examples . the target sample t is obtained by the following procedure : ( 1 ) we generate a sample s the same way s has been generated ; ( 2 ) we rotate each example by 35 \u2022 ; and ( 3 ) we remove all the labels . thus , t contains 300 un ##lab ##ele ##d examples . we have represented those examples in figure 2 . we study the adaptation capability of dan ##n by comparing it to the standard neural network ( n ##n ) . in these toy experiments , both algorithms share the same network architecture , with a hidden layer size of 15 neurons . we train the n ##n using the same procedure as the dan ##n . that is , we keep up ##dating the domain reg ##ress ##or component using target sample t ( with a hyper - parameter ##\u03bb = 6 ; the same value is used for dan ##n ) , but we di ##sable the ad ##vers ##aria ##l back - propagation into the hidden layer . to do so , we execute algorithm 1 by om ##itt ##ing the lines numbered 22 and 31 . this allows recovering the n ##n learning algorithm - based on the source risk mini ##mi ##zation of equation ( 5 ) without any regular ##izer - and simultaneously train the domain reg ##ress ##or of equation ( 7 ) to disc ##rim ##inate between source and target domains . with this toy experience , we will first illustrate how dan ##n adapt ##s its decision boundary when compared to n ##n . moreover , we will also illustrate how the representation given by the hidden layer is less adapted to the source domain task with dan ##n than with n ##n ( this is why we need a domain reg ##ress ##or in the n ##n experiment ) . we recall that this is the founding idea behind our proposed algorithm . the analysis of the experiment appears in figure 2 , where upper graphs relate to standard n ##n , and lower graphs relate to dan ##n . by looking at the lower and upper graphs pair ##wise , we compare n ##n and dan ##n from four different perspectives , described in details below . the column \" label classification \" of figure 2 shows the decision boundaries of dan ##n and n ##n on the problem of predicting the labels of both source and the target examples . as expected , n ##n accurately class ##ifies the two classes of the source sample s , but is not fully adapted to the target sample t . on the contrary , the decision boundary of dan ##n perfectly class ##ifies examples from both source and target samples . in the studied task , dan ##n clearly adapt ##s to the target distribution . the column \" representation pc ##a \" studies how the domain adaptation regular ##izer affects the representation g f ( \u00b7 ) provided by the network hidden layer . the graphs are obtained by applying a principal component analysis ( pc ##a ) on the set of all representation of source and target data points , i . e . , s ( g f ) \u222a t ( g f ) . thus , given the trained network ( n ##n or dan ##n ) , every point from s and t is mapped into a 15 - dimensional feature space through the hidden layer , and projected back into a two - dimensional plane by the pc ##a transformation . in the dan ##n - pc ##a representation , we observe that target points are homogeneous ##ly spread out among source points ; in the n ##n - pc ##a representation , a number of target points belong to clusters containing no source points . hence , labeling the target points seems an easier task given the dan ##n - pc ##a representation . to push the analysis further , the pc ##a graphs tag four crucial data points by the letters a , b , c and d , that correspond to the moon ex ##tre ##mit ##ies in the original space ( note that the original point locations are tagged in the first column graphs ) . we observe that points a and b are very close to each other in the n ##n - pc ##a representation , while they clearly belong to different classes . the same happens to points c and d . conversely , these four points are at the opposite four corners in the dan ##n - pc ##a representation . note also that the target point a ( res ##p . d ) - that is difficult to classify in the original space - is located in the \" + \" cluster ( res ##p . \" \u2212 ##\u2212 ##\u2212 \" cluster ) in the dan ##n - pc ##a representation . therefore , the representation promoted by dan ##n is better suited to the adaptation problem . the column \" domain classification \" shows the decision boundary on the domain classification problem , which is given by the domain reg ##ress ##or g d of equation ( 7 ) . more precisely , an example x is classified as a source example when g d ( g f ( x ) ) \u2265 0 . 5 , and is classified as a domain example otherwise . remember that , during the learning process of dan ##n , the g d reg ##ress ##or struggles to disc ##rim ##inate between source and target domains , while the hidden representation g f ( \u00b7 ) is ad ##vers ##aria ##lly updated to prevent it to succeed . as explained above , we trained a domain reg ##ress ##or during the learning process of n ##n , but without allowing it to influence the learned representation g f ( \u00b7 ) . on one hand , the dan ##n domain reg ##ress ##or clearly fails to general ##ize source and target distribution top ##ologies . on the other hand , the n ##n domain reg ##ress ##or shows a better ( although imperfect ) general ##ization capability . inter ali ##a , it seems to roughly capture the rotation angle of the target distribution . this again co ##rro ##bor ##ates that the dan ##n representation does not allow disc ##rim ##inating between domains . the column \" hidden neurons \" shows the configuration of hidden layer neurons ( by equation 4 , we have that each ne ##uron is indeed a linear reg ##ress ##or ) . in other words , each of the fifteen plot line corresponds to the coordinates x ##\u2208 r 2 for which the i - th component of g f ( x ) equals 1 2 , for i ##\u2208 { 1 , . . . , 15 } . we observe that the standard n ##n neurons are grouped in three clusters , each one allowing to generate a straight line of the z ##ig ##za ##g decision boundary for the label classification problem . however , most of these neurons are also able to ( roughly ) capture the rotation angle of the domain classification problem . hence , we observe that the adaptation regular ##izer of dan ##n prevents these kinds of neurons to be produced . it is indeed striking to see that the two predominant patterns in the n ##n neurons ( i . e . , the two parallel lines crossing the plane from lower left to upper right ) are vanishing in the dan ##n neurons . section : un ##su ##per ##vis ##ed hyper - parameter selection to perform un ##su ##per ##vis ##ed domain adapt ##ion , one should provide ways to set hyper - parameters ( such as the domain regular ##ization parameter ##\u03bb , the learning rate , the network architecture for our method ) in an un ##su ##per ##vis ##ed way , i . e . , without referring to labeled data in the target domain . in the following experiments of sections 5 . 1 . 3 and 5 . 1 . 4 , we select the hyper - parameters of each algorithm by using a variant of reverse cross - validation approach proposed by [ reference ] , that we call reverse validation . to evaluate the reverse validation risk associated to a tu ##ple of hyper - parameters , we proceed as follows . given the labeled source sample s and the un ##lab ##ele ##d target sample t , we split each set into training sets ( s and t respectively , containing 90 % of the original examples ) and the validation sets ( s v and t v respectively ) . we use the labeled set s and the un ##lab ##ele ##d target set t to learn a class ##ifier ##\u03b7 . then , using the same algorithm , we learn a reverse class ##ifier ##\u03b7 r using the self - labeled set { ( x , \u03b7 ( x ) ) } x ##\u2208 ##t and the un ##lab ##ele ##d part of s as target sample . finally , the reverse class ##ifier ##\u03b7 r is evaluated on the validation set s v of source sample . we then say that the class ##ifier ##\u03b7 has a reverse validation risk of r s v ( \u03b7 r ) . the process is repeated with multiple values of hyper - parameters and the selected parameters are those corresponding to the class ##ifier with the lowest reverse validation risk . note that when we train neural network architecture ##s , the validation set s v is also used as an early stopping criterion during the learning of ##\u03b7 , and self - labeled validation set { ( x , \u03b7 ( x ) ) } x ##\u2208 ##t v is used as an early stopping criterion during the learning of ##\u03b7 r . we also observed better acc ##ura ##cies when we initial ##ized the learning of the reverse class ##ifier ##\u03b7 r with the configuration learned by the network ##\u03b7 . section : experiments on sentiment analysis data sets we now compare the performance of our proposed dan ##n algorithm to a standard neural network with one hidden layer ( n ##n ) described by equation ( 5 ) , and a support vector machine ( sv ##m ) with a linear kernel . we compare the algorithms on the amazon reviews data set , as pre - processed by [ reference ] . this data set includes four domains , each one composed of reviews of a specific kind of product ( books , dvd disks , electronics , and kitchen appliances ) . reviews are encoded in 5 000 dimensional feature vectors of un ##ig ##ram ##s and big ##ram ##s , and labels are binary : \" 0 \" if the product is ranked up to 3 stars , and \" 1 \" if the product is ranked 4 or 5 stars . we perform twelve domain adaptation tasks . all learning algorithms are given 2 000 labeled source examples and 2 000 un ##lab ##ele ##d target examples . then , we evaluate them on separate target test sets ( between 3 000 and 6 000 examples ) . note that n ##n and sv ##m do not use the un ##lab ##ele ##d target sample for learning . here are more details about the procedure used for each learning algorithms leading to the empirical results of table 1 : classification accuracy on the amazon reviews data set , and pair ##wise po ##isson bin ##omi ##al test . \u2022 for the dan ##n algorithm , the adaptation parameter ##\u03bb is chosen among 9 values between 10 ##\u2212 ##2 and 1 on a log ##ari ##th ##mic scale . the hidden layer size l is either 50 or 100 . finally , the learning rate ##\u00b5 is fixed at 10 ##\u2212 ##3 . \u2022 for the n ##n algorithm , we use exactly the same hyper - parameters grid and training procedure as dan ##n above , except that we do not need an adaptation parameter . note that one can train n ##n by using the dan ##n implementation ( algorithm 1 ) with ##\u03bb = 0 . \u2022 for the sv ##m algorithm , the hyper - parameter c is chosen among 10 values between 10 ##\u2212 ##5 and 1 on a log ##ari ##th ##mic scale . this range of values is the same as used by [ reference ] in their experiments . as presented at section 5 . 1 . 2 , we used reverse cross validation selecting the hyper - parameters for all three learning algorithms , with early stopping as the stopping criterion for dan ##n and n ##n . the \" original data \" part of table 1a shows the target test accuracy of all algorithms , and table 1b reports the probability that one algorithm is significantly better than the others according to the po ##isson bin ##omi ##al test [ reference ] . we note that dan ##n has a significantly better performance than n ##n and sv ##m , with respective pro ##ba ##bilities 0 . 87 and 0 . 83 . as the only difference between dan ##n and n ##n is the domain adaptation regular ##izer , we conclude that our approach successfully helps to find a representation suitable for the target domain . section : combining dan ##n with den ##ois ##ing auto ##en ##code ##rs we now investigate on whether the dan ##n algorithm can improve on the representation learned by the state - of - the - art marginal ##ized stacked den ##ois ##ing auto ##en ##code ##rs ( ms ##da ) proposed by [ reference ] . in brief , ms ##da is an un ##su ##per ##vis ##ed algorithm that learns a new robust feature representation of the training samples . it takes the un ##lab ##ele ##d parts of both source and target samples to learn a feature map from input space x to a new representation space . as a den ##ois ##ing auto ##en ##code ##rs algorithm , it finds a feature representation from which one can ( approximately ) rec ##ons ##truct the original features of an example from its noisy counterpart . [ reference ] showed that using ms ##da with a linear sv ##m class ##ifier reaches state - of - the - art performance on the amazon reviews data sets . as an alternative to the sv ##m , we propose to apply our shallow dan ##n algorithm on the same representations generated by ms ##da ( using representations of both source and target samples ) . note that , even if ms ##da and dan ##n are two representation learning approaches , they opt ##imi ##ze different objectives , which can be complementary . we perform this experiment on the same amazon reviews data set described in the previous sub ##section . for each source - target domain pair , we generate the ms ##da representations using a corruption probability of 50 % and a number of layers of 5 . we then execute the three learning algorithms ( dan ##n , n ##n , and sv ##m ) on these representations . more precisely , following the experimental procedure of [ reference ] , we use the con ##cate ##nation of the output of the 5 layers and the original input as the new representation . thus , each example is now encoded in a vector of 30 000 dimensions . note that we use the same grid search as in the previous sub ##section 5 . 1 . 3 , but use a learning rate ##\u00b5 of 10 ##\u2212 ##4 for both dan ##n and the n ##n . the results of \" ms ##da representation \" columns in table 1a confirm that combining ms ##da and dan ##n is a sound approach . indeed , the po ##isson bin ##omi ##al test shows that dan ##n has a better performance than the n ##n and the sv ##m , with pro ##ba ##bilities 0 . 92 and 0 . 88 respectively , as reported in table 1b . we note however that the standard n ##n and the sv ##m find the best solution on respectively the second and the fourth tasks . this suggests that dan ##n and ms ##da adaptation strategies are not fully complementary . section : proxy distance the theoretical foundation of the dan ##n algorithm is the domain adaptation theory of [ reference ] [ reference ] . we claimed that dan ##n finds a representation in which the source and the target example are hardly distinguish ##able . our toy experiment of section 5 . 1 . 1 already points out some evidence for that and here we provide analysis on real data . to do so , we compare the proxy a - distance ( pad ) on various representations of the amazon reviews data set ; these representations are obtained by running either n ##n , dan ##n , ms ##da , or ms ##da and dan ##n combined . recall that pad , as described in section 3 . 2 , is a metric est ##imating the similarity of the source and the target representations . more precisely , to obtain a pad value , we use the following procedure : ( 1 ) we construct the data set u of equation ( 2 ) using both source and target representations of the training samples ; ( 2 ) we randomly split u in two subset ##s of equal size ; ( 3 ) we train linear sv ##ms on the first subset of u using a large range of c values ; ( 4 ) we compute the error of all obtained class ##ifiers on the second subset of u ; and ( 5 ) we use the lowest error to compute the pad value of equation ( 3 ) . firstly , figure 3a compares the pad of dan ##n representations obtained in the experiments of section 5 . 1 . 3 ( using the hyper - parameters values leading to the results of table 1 ) to the pad computed on raw data . as expected , the pad values are driven down by the dan ##n representations . secondly , figure 3 ##b compares the pad of dan ##n representations to the pad of standard n ##n representations . as the pad is influenced by the hidden layer size ( the disc ##rim ##inating power tends to increase with the representation length ) , we fix here the size to 100 neurons for both algorithms . we also fix the adaptation parameter of dan ##n to ##\u03bb 0 . 31 ; it was the value that has been selected most of the time during our preceding experiments on the amazon reviews data set . again , dan ##n is clearly leading to the lowest pad values . lastly , figure 3 ##c presents two sets of results related to section 5 . 1 . 4 experiments . on one hand , we reproduce the results of [ reference ] , which noticed that the ms ##da representations have greater pad values than original ( raw ) data . although the ms ##da approach clearly helps to adapt to the target task , it seems to contra ##dict the theory of bend ##avi ##d et al . . on the other hand , we observe that , when running dan ##n on top of ms ##da ( using the hyper - parameters values leading to the results of table 1 ) , the obtained representations have much lower pad values . these observations might explain the improvements provided by dan ##n when combined with the ms ##da procedure . section : experiments with deep networks on image classification we now perform extensive evaluation of a deep version of dan ##n ( see sub ##section 4 . 2 ) on a number of popular image data sets and their modifications . these include large - scale data sets of small images popular with deep learning methods , and the office data sets [ reference ] , which are a de facto standard for domain adaptation in computer vision , but have much fewer images . section : baseline ##s the following baseline ##s are evaluated in the experiments of this sub ##section . the source - only model is trained without consideration for target - domain data ( no domain class ##ifier branch included into the network ) . the train - on - target model is trained on the target domain with class labels revealed . this model serves as an upper bound on da methods , assuming that target data are abundant and the shift between the domains is considerable . in addition , we compare our approach against the recently proposed un ##su ##per ##vis ##ed da method based on subsp ##ace alignment ( sa ) [ reference ] , which is simple to setup and test on new data sets , but has also been shown to perform very well in experimental comparisons with other \" shallow \" da methods . to boost the performance of this baseline , we pick its most important free parameter ( the number of principal components ) from the range { 2 , . . . , 60 } , so that the test performance on the target domain is maximize ##d . to apply sa in our setting , we train a source - only model and then consider the activation ##s of the last hidden layer in the label predict ##or ( before the final linear class ##ifier ) as des ##cript ##ors / features , and learn the mapping between the source and the target domains [ reference ] . since the sa baseline requires training a new class ##ifier after adapting the features , and in order to put all the compared settings on an equal footing , we re ##train the last layer of the label predict ##or using a standard linear sv ##m [ reference ] for all four considered methods ( including ours ; the performance on the target domain remains approximately the same after the re ##train ##ing ) . for the office data set [ reference ] , we directly compare the performance of our full network ( feature extract ##or and label predict ##or ) against recent da approaches using previously published results . section : cnn architecture ##s and training procedure in general , we compose feature extract ##or from two or three con ##vo ##lu ##tion ##al layers , picking their exact configurations from previous works . more precisely , four different architecture ##s were used in our experiments . the first three are shown in figure 4 . for the office domains , we use pre - trained alex ##net from the caf ##fe - package . the adaptation architecture is identical to . [ reference ] for the domain adapt ##ion component , we use three ( x ##\u2192 ##10 ##24 ##\u2192 ##10 ##24 ##\u2192 ##2 ) fully connected layers , except for mn ##ist where we used a simpler ( x ##\u2192 ##100 ##\u2192 ##2 ) architecture to speed up the experiments . admitted ##ly these choices for domain class ##ifier are arbitrary , and better adaptation performance might be attained if this part of the architecture is tuned . ( x ##\u2192 ##10 ##24 ##\u2192 ##10 ##24 ##\u2192 ##2 ) for the loss functions , we set l y and l d to be the log ##istic regression loss and the bin ##omi ##al cross - entropy respectively . following [ reference ] we also use drop ##out and 2 - norm restriction when we train the sv ##hn architecture . section : a 2 - layer domain class ##ifier the other hyper - parameters are not selected through a grid search as in the small scale experiments of section 5 . 1 , which would be computational ##ly costly . instead , the learning rate is adjusted during the st ##och ##astic gradient descent using the following formula : where p is the training progress linear ##ly changing from 0 to 1 , \u00b5 0 = 0 . 01 , \u03b1 = 10 and ##\u03b2 = 0 . 75 ( the schedule was opt ##imi ##zed to promote convergence and low error on the source domain ) . a momentum term of 0 . 9 is also used . the domain adaptation parameter ##\u03bb is initiated at 0 and is gradually changed to 1 using the following schedule : where ##\u03b3 was set to 10 in all experiments ( the schedule was not opt ##imi ##zed / t ##we ##ake ##d ) . this strategy allows the domain class ##ifier to be less sensitive to noisy signal at the early stages of the training procedure . note however that these ##\u03bb p were used only for up ##dating the feature extract ##or component g f . for up ##dating the domain classification component , we used a fixed ##\u03bb = 1 , to ensure that the latter trains as fast as the label predict ##or g y . [ reference ] finally , note that the model is trained on 128 - sized batch ##es ( images are prep ##ro ##ces ##sed by the mean sub ##tra ##ction ) . a half of each batch is populated by the samples from the source domain ( with known labels ) , the rest constitutes the target domain ( with labels not revealed to the algorithms except for the train - on - target baseline ) . section : visual ##izations we use t - s ##ne ( van der ma ##ate ##n , 2013 ) projection to visual ##ize feature distributions at different points of the network , while color - coding the domains ( figure 5 ) . as we already observed with the shallow version of dan ##n ( see figure 2 ) , there is a strong correspondence 6 . equivalent ##ly , one can use the same ##\u03bb ##p for both feature extract ##or and domain classification components , but use a learning rate of ##\u00b5 / \u03bb ##p for the latter . between the success of the adaptation in terms of the classification accuracy for the target domain , and the overlap between the domain distributions in such visual ##izations . section : results on image data sets we now discuss the experimental settings and the results . in each case , we train on the source data set and test on a different target domain data set , with considerable shifts between domains ( see figure 6 ) . the results are summarized in table 2 and table 3 . mn ##ist ##\u2192 mn ##ist - m . our first experiment deals with the mn ##ist data set [ reference ] . in order to obtain the target domain ( mn ##ist - m ) we blend digits from the original set over patches randomly extracted from color photos from bs ##ds ##500 [ reference ] . this operation is formally defined for two images i 1 , i 2 as i out i ##jk = | i 1 i ##jk ##\u2212 i 2 i ##jk | , where i , j are the coordinates of a pixel and k is a channel index . in other words , an output sample is produced by taking a patch from a photo and in ##vert ##ing its pixels at positions corresponding to the pixels of a digit . for a human the classification task becomes only slightly harder compared to the original data set ( the digits are still clearly distinguish ##able ) whereas for a cnn trained on mn ##ist this domain is quite distinct , as the background and the strokes are no longer constant . consequently , the source - only model performs poorly . our approach succeeded at align ##ing feature distributions ( figure 5 ) , which led to successful adaptation results ( considering that the adaptation is un ##su ##per ##vis ##ed ) . at the same time , the improvement over source - only model achieved by subsp ##ace alignment ( sa ) [ reference ] ) is quite modest , thus highlighting the difficulty of the adaptation task . synthetic numbers ##\u2192 sv ##hn . to address a common scenario of training on synthetic data and testing on real data , we use street - view house number data set sv ##hn [ reference ] as the target domain and synthetic digits as the source . the latter ( syn numbers ) consists of ##\u2248 500 , 000 images generated by ourselves from windows t ##m font ##s by varying the text ( that includes different one - , two - , and three - digit numbers ) , positioning , orientation , background and stroke colors , and the amount of blur . the degrees of variation were chosen manually to simulate sv ##hn , however the two data sets are still rather distinct , the biggest difference being the structured cl ##utter in the background of sv ##hn images . the proposed back ##pro ##pa ##gation - based technique works well covering almost 80 % of the gap between training with source data only and training on target domain data with known target labels . in contrast , sa [ reference ] results in a slight classification accuracy drop ( probably due to the information loss during the dimensional ##ity reduction ) , indicating that the adaptation task is even more challenging than in the case of the mn ##ist experiment . mn ##ist ##\u2194 sv ##hn . in this experiment , we further increase the gap between distributions , and test on mn ##ist and sv ##hn , which are significantly different in appearance . training on sv ##hn even without adaptation is challenging - classification error stays high during the first 150 epoch ##s . in order to avoid ending up in a poor local minimum we , therefore , do not use learning rate anne ##aling here . obviously , the two directions ( mn ##ist ##\u2192 sv ##hn and sv ##hn ##\u2192 mn ##ist ) are not equally difficult . as sv ##hn is more diverse , a model trained on sv ##hn is expected to be more generic and to perform reasonably on the mn ##ist data set . this , indeed , turns out to be the case and is supported by the appearance of the table 2 : classification acc ##ura ##cies for digit image classifications for different source and target domains . mn ##ist - m corresponds to difference - blended digits over non ##uni ##form background . the first row corresponds to the lower performance bound ( i . e . , if no adaptation is performed ) . the last row corresponds to training on the target domain data with known class labels ( upper bound on the da performance ) . for each of the two da methods [ reference ] we show how much of the gap between the lower and the upper bounds was covered ( in brackets ) . for all five cases , our approach out ##per ##forms [ reference ] considerably , and covers a big portion of the gap . table 3 : accuracy evaluation of different da approaches on the standard office [ reference ] ) data set . all methods ( except sa ) are evaluated in the \" fully ##tra ##ns ##ductive \" protocol ( some results are reproduced from [ reference ] . our method ( last row ) out ##per ##forms competitors setting the new state - of - the - art . syn and real denote available labeled data ( 100 , 000 synthetic and 430 real images respectively ) ; adapted means that ##\u2248 31 , 000 un ##lab ##ele ##d target domain images were used for adaptation . the best performance is achieved by employing both the labeled samples and the large un ##lab ##ele ##d corpus in the target domain . feature distributions . we observe a quite strong separation between the domains when we feed them into the cnn trained solely on mn ##ist , whereas for the sv ##hn - trained network the features are much more inter ##mi ##xed . this difference probably explains why our method succeeded in improving the performance by adaptation in the sv ##hn ##\u2192 mn ##ist scenario ( see table 2 ) but not in the opposite direction ( sa is not able to perform adaptation in this case either ) . un ##su ##per ##vis ##ed adaptation from mn ##ist to sv ##hn gives a failure example for our approach : it does n ' t manage to improve upon the performance of the non - adapted model which achieve ##s ##\u2248 0 . 25 accuracy ( we are unaware of any un ##su ##per ##vis ##ed da methods capable of performing such adaptation ) . synthetic signs ##\u2192 gt ##sr ##b . overall , this setting is similar to the syn numbers ##\u2192 sv ##hn experiment , except the distribution of the features is more complex due to the significantly larger number of classes ( 43 instead of 10 ) . for the source domain we obtained 100 , 000 synthetic images ( which we call syn signs ) sim ##ulating various imaging conditions . in the target domain , we use 31 , 36 ##7 random training samples for un ##su ##per ##vis ##ed adaptation and the rest for evaluation . once again , our method achieve ##s a sensible increase in performance proving its suit ##ability for the synthetic - to - real data adaptation . as an additional experiment , we also evaluate the proposed algorithm for semi - supervised domain adaptation , i . e . , when one is additionally provided with a small amount of labeled target data . here , we reveal 430 labeled examples ( 10 samples per class ) and add them to the training set for the label predict ##or . figure 7 shows the change of the validation error throughout the training . while the graph clearly suggests that our method can be beneficial in the semi - supervised setting , thorough verification of semi - supervised setting is left for future work . office data set . we finally evaluate our method on office data set , which is a collection of three distinct domains : amazon , ds ##lr , and web ##cam . unlike previously discussed data sets , office is rather small - scale with only 281 ##7 labeled images spread across 31 different categories in the largest domain . the amount of available data is crucial for a successful training of a deep model , hence we opted for the fine - tuning of the cnn pre - trained on the image ##net ( alex ##net from the caf ##fe package , see as it is done in some recent da works [ reference ] [ reference ] . we make our approach more comparable with by using exactly the same network architecture replacing domain mean - based regular ##ization with the domain class ##ifier . following previous works , we assess the performance of our method across three transfer tasks most commonly used for evaluation . our training protocol is adopted from ; [ reference ] ; [ reference ] as during adaptation we use all available labeled source examples and un ##lab ##ele ##d target examples ( the premise of our method is the abundance of un ##lab ##ele ##d data in the target domain ) . also , all source domain data are used for training . under this \" fully - trans ##ductive \" setting , our method is able to improve previously - reported state - of - the - art accuracy for un ##su ##per ##vis ##ed adaptation very considerably ( table 3 ) , especially in the most challenging amazon ##\u2192 web ##cam scenario ( the two domains with the largest domain shift ) . interesting ##ly , in all three experiments we observe a slight over - fitting ( performance on the target domain de ##grade ##s while accuracy on the source continues to improve ) as training progresses , however , it does n ' t ruin the validation accuracy . moreover , switching off the domain class ##ifier branch makes this effect far more apparent , from which we conclude that our technique serves as a regular ##izer . section : experiments with deep image des ##cript ##ors for re - identification in this section we discuss the application of the described adaptation method to person re - identification ( re - i d ) problem . the task of person re - identification is to associate people seen from different camera views . more formally , it can be defined as follows : given two sets of images from different cameras ( probe and gallery ) such that each person depicted in the probe set has an image in the gallery set , for each image of a person from the probe set find an image of the same person in the gallery set . di ##s ##jo ##int camera views , different illumination conditions , various poses and low quality of data make this problem difficult even for humans ( e . g . , [ reference ] , reports human performance at rank ##1 = 71 . 08 % ) . unlike classification problems that are discussed above , re - identification problem implies that each image is mapped to a vector des ##cript ##or . the distance between des ##cript ##ors is then used to match images from the probe set and the gallery set . to evaluate results of re - i d methods the cumulative match characteristic ( cm ##c ) curve is commonly used . it is a plot of the identification rate ( recall ) at rank - k , that is the probability of the matching gallery image to be within the closest k images ( in terms of des ##cript ##or distance ) to the probe image . most existing works train des ##cript ##or mapping ##s and evaluate them within the same data set containing images from a certain camera network with similar imaging conditions . several papers , however , observed that the performance of the resulting re - identification systems drops very considerably when des ##cript ##ors trained on one data set and tested on another . it is therefore natural to handle such cross - domain evaluation as a domain - adaptation problem , where each camera network ( data set ) constitutes a domain . recently , several papers with significantly improved re - identification performance [ reference ] [ reference ] [ reference ] have been presented , with [ reference ] reporting good results in cross - data - set evaluation scenario . at the moment , deep learning methods [ reference ] do not achieve state - of - the - art results probably because of the limited size of the training sets . domain adaptation thus represents a viable direction for improving deep re - identification des ##cript ##ors . section : data sets and protocols following [ reference ] , we use pri ##d [ reference ] , viper [ reference ] , cu ##h ##k [ reference ] as target data sets for our experiments . the pri ##d data set exists in two versions , and as in [ reference ] we use a single - shot variant . it contains images of 385 persons viewed from camera a and images of 74 ##9 persons viewed from camera b , 200 persons appear in both cameras . the viper data set also contains images taken with two cameras , and in total 63 ##2 persons are captured , for every person there is one image for each of the two camera views . the cu ##h ##k data set consists of images from five pairs of cameras , two images for each person from each of the two cameras . we refer to the subset of this data set that includes the first pair of cameras only as cu ##h ##k / p ##1 ( as most papers use this subset ) . see figure 8 for samples of these data sets . we perform extensive experiments for various pairs of data sets , where one data set serves as a source domain , i . e . , it is used to train a des ##cript ##or mapping in a supervised way with known correspondence ##s between probe and gallery images . the second data set is used as a target domain , so that images from that data set are used without probe - gallery correspondence . in more detail , cu ##h ##k / p ##1 is used for experiments when cu ##h ##k serves as a target domain and two settings ( \" whole cu ##h ##k \" and cu ##h ##k / p ##1 ) are used for experiments when cu ##h ##k serves as a source domain . given pri ##d as a target data set , we randomly choose 100 persons appearing in both camera views as training set . the images of the other 100 persons from camera a are used as probe , all images from camera b excluding those used in training ( 64 ##9 in total ) are used as gallery at test time . for viper , we use random 316 persons for training and all others for testing . for cu ##h ##k , 97 ##1 persons are split into 48 ##5 for training and 48 ##6 for testing . unlike [ reference ] , we use all images in the first pair of cameras of cu ##h ##k instead of choosing one image of a person from each camera view . we also performed two experiments with all images of the whole cu ##h ##k data set as source domain and viper and pri ##d data sets as target domains as in the original paper [ reference ] . following [ reference ] , we augmented our data with mirror images , and during test time we calculate similarity score between two images as the mean of the four scores corresponding to different flip ##s of the two compared images . in case of cu ##h ##k , where there are 4 images ( including mirror images ) for each of the two camera views for each person , all 16 combinations ' scores are averaged . section : cnn architecture ##s and training procedure in our experiments , we use siam ##ese architecture described in [ reference ] ( deep metric learning or d ##ml ) for learning deep image des ##cript ##ors on the source data set . this architecture incorporates two con ##vo ##lu ##tion layers ( with 7 ##\u00d7 7 and 5 ##\u00d7 5 filter banks ) , followed by re ##lu and max pool ##ing , and one fully - connected layer , which gives 500 - dimensional des ##cript ##ors as an output . there are three parallel flows within the cnn for processing three part of an image : the upper , the middle , and the lower one . the first con ##vo ##lu ##tion layer shares parameters between three parts , and the outputs of the second con ##vo ##lu ##tion layers are con ##cate ##nated . during training , we follow [ reference ] and calculate pair ##wise co ##sin ##e similarities between 500 - dimensional features within each batch and back ##pro ##pa ##gate the loss for all pairs within batch . to perform domain - ad ##vers ##aria ##l training , we construct a dan ##n architecture . the feature extract ##or includes the two con ##vo ##lu ##tion ##al layers ( followed by max - pool ##ing and re ##lu ) discussed above . the label predict ##or in this case is replaced with des ##cript ##or predict ##or that includes one fully - connected layer . the domain class ##ifier includes two fully - connected layers with 500 units in the intermediate representation ( x ##\u2192 ##500 ##\u2192 ##1 ) . for the verification loss function in the des ##cript ##or predict ##or we used bin ##omi ##al devi ##ance loss , defined in [ reference ] with similar parameters : \u03b1 = 2 , \u03b2 = 0 . 5 , c = 2 ( the as ##ym ##metric cost parameter for negative pairs ) . the domain class ##ifier is trained with log ##istic loss as in sub ##section 5 . 2 . 2 . we used learning rate fixed to 0 . 001 and momentum of 0 . 9 . the schedule of adaptation similar to the one described in sub ##section 5 . 2 . 2 was used . we also inserted drop ##out layer with rate 0 . 5 after the con ##cate ##nation of outputs of the second max - pool ##ing layer . 128 - sized batch ##es were used for source data and 128 - sized batch ##es for target data . figure 9 shows results in the form of cm ##c - curves for eight pairs of data sets . depending on the hardness of the ann ##ota ##tion problem we trained either for 50 , 000 iteration ##s ( cu ##h ##k / p ##1 ##\u2192 viper , viper ##\u2192 cu ##h ##k / p ##1 , pri ##d ##\u2192 viper ) or for 20 , 000 iteration ##s ( the other five pairs ) . section : results on re - identification data sets after the sufficient number of iteration ##s , domain - ad ##vers ##aria ##l training consistently improves the performance of re - identification . for the pairs that involve pri ##d data set , which is more di ##ssi ##mi ##lar to the other two data sets , the improvement is considerable . overall , this demonstrates the app ##lica ##bility of the domain - ad ##vers ##aria ##l learning beyond classification problems . section : conclusion the paper proposes a new approach to domain adaptation of feed - forward neural networks , which allows large - scale training based on large amount of ann ##ota ##ted data in the source domain and large amount of una ##nn ##ota ##ted data in the target domain . similarly to many previous shallow and deep da techniques , the adaptation is achieved through align ##ing the distributions of features across the two domains . however , unlike previous approaches , the alignment is accomplished through standard back ##pro ##pa ##gation training . the approach is motivated and supported by the domain adaptation theory of [ reference ] . the main idea behind dan ##n is to en ##jo ##in the network hidden layer to learn a representation which is predict ##ive of the source example labels , but un ##in ##form ##ative about the domain of the input ( source or target ) . we implement this new approach within both shallow and deep feed - forward architecture ##s . the latter allows simple implementation within virtually any deep learning package through the introduction of a simple gradient reversal layer . we have shown that our approach is flexible and achieve ##s state - of - the - art results on a variety of bench ##mark in domain adaptation , namely for sentiment analysis and image classification tasks . a convenient aspect of our approach is that the domain adaptation component can be added to almost any neural network architecture that is train ##able with back ##pro ##pa ##gation . towards this end , we have demonstrated experimental ##ly that the approach is not confined to classification tasks but can be used in other feed - forward architecture ##s , e . g . , for des ##cript ##or learning for person re - identification . section : section : ac ##k ##now ##led ##gm ##ents this work has been supported by national science and engineering research council ( ns ##er ##c ) discovery grants 262 ##0 ##6 ##7 and 01 ##22 ##40 ##5 as well as the russian ministry of science and education grant rf ##me ##fi ##57 ##9 ##14 ##x ##00 ##7 ##1 . computation ##s were performed on the col ##oss ##e super ##com ##put ##er grid at [UNK] laval , under the auspices of cal ##cu ##l [UNK] and compute canada . the operations of col ##oss ##e are funded by the ns ##er ##c , the canada foundation for innovation ( cf ##i ) , [UNK] , and the fond ##s de rec ##her ##che du [UNK] - nature et technologies ( fr ##q ##nt ) . we also thank the graphics & media lab , faculty of computational mathematics and cyber ##net ##ics , lo ##mon ##oso ##v moscow state university for providing the synthetic road signs data set . section :",
        "pred_seq": "[SEP] domain training [SEP] [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "domainadversarial training"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "written reviews",
                        "sentiment analysis data sets",
                        "amazon reviews data set",
                        "books",
                        "dvd disks",
                        "electronics",
                        "kitchen appliances",
                        "reviews",
                        "amazon reviews data sets",
                        "source domain data"
                    ]
                ],
                "Method": [
                    [
                        "domainadversarial training of neural networks",
                        "domainadversarial neural network",
                        "dann",
                        "domainadversarial neural networks",
                        "backpropagationbased training"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "document sentiment analysis",
                        "sentiment analysis",
                        "sentiment analysis problem"
                    ]
                ]
            }
        ]
    },
    "13": {
        "doctext": "document : pixel ##gan auto ##en ##code ##rs in this paper , we describe the ' ' pixel ##gan auto ##en ##code ##r ' ' , a genera ##tive auto ##en ##code ##r in which the genera ##tive path is a con ##vo ##lu ##tion ##al auto ##re ##gre ##ssi ##ve neural network on pixels ( pixel ##c ##nn ) that is conditioned on a late ##nt code , and the recognition path uses a genera ##tive ad ##vers ##aria ##l network ( gan ) to impose a prior distribution on the late ##nt code . we show that different prior ##s result in different decomposition ##s of information between the late ##nt code and the auto ##re ##gre ##ssi ##ve deco ##der . for example , by imposing a ga ##uss ##ian distribution as the prior , we can achieve a global vs . local decomposition , or by imposing a cat ##egorical distribution as the prior , we can di ##sen ##tangle the style and content information of images in an un ##su ##per ##vis ##ed fashion . we further show how the pixel ##gan auto ##en ##code ##r with a cat ##egorical prior can be directly used in semi - supervised settings and achieve competitive semi - supervised classification results on the mn ##ist , sv ##hn and nor ##b data ##set ##s . numbers , com ##press ##nat ##bib section : introduction in recent years , genera ##tive models that can be trained via direct back - propagation have enabled remarkable progress in modeling natural images . one of the most successful models is the genera ##tive ad ##vers ##aria ##l network ( gan ) gan , which employs a two player min - max game . the genera ##tive model , , samples the prior and generates the sample . the disc ##rim ##inator , , is trained to identify whether a point is a sample from the data distribution or a sample from the genera ##tive model . the generator is trained to maximal ##ly confuse the disc ##rim ##inator into believing that generated samples come from the data distribution . the cost function of gan is gan ##s can be considered within the wider framework of implicit genera ##tive models mohamed ##20 ##16 ##lea ##rn ##ing , fe ##rence , dustin . implicit distributions can be sampled through their genera ##tive path , but their likelihood function is not tract ##able . recently , several papers have proposed another application of gan - style algorithms for approximate inference , mohamed ##20 ##16 ##lea ##rn ##ing , fe ##rence , dustin , rang ##ana ##th ##20 ##16 ##oper ##ator , aa ##e , av ##b , ali , big ##an . these algorithms use implicit distributions to learn posterior approximation ##s that are more expressive than the distributions with tract ##able den ##sities that are often used in variation ##al inference . for example , ad ##vers ##aria ##l auto ##en ##code ##rs ( aa ##e ) aa ##e use a universal approx ##ima ##tor posterior as the implicit posterior distribution and use ad ##vers ##aria ##l training to match the aggregate ##d posterior of the late ##nt code to the prior distribution . ad ##vers ##aria ##l variation ##al bay ##es fe ##rence , av ##b uses a more general amor ##tized gan inference framework within a maximum - likelihood learning setting . another type of gan inference technique is used in the ali ali and big ##an big ##an models , which have been shown to approximate maximum likelihood learning fe ##rence . in these models , both the recognition and genera ##tive models are implicit and are jointly learnt by an ad ##vers ##aria ##l training process . variation ##al auto ##en ##code ##rs ( va ##e ) va ##e , re ##zen ##de are another state - of - the - art image modeling technique that use neural networks to para ##met ##rize the posterior distribution and pair it with a top - down genera ##tive network . both networks are jointly trained to maximize a variation ##al lower bound on the data log - likelihood . a different framework for learning density models is auto ##re ##gre ##ssi ##ve neural networks such as nad ##e nad ##e , made made , pixel ##rn ##n pixel ##rn ##n and pixel ##c ##nn pixel ##c ##nn . unlike variation ##al auto ##en ##code ##rs , which capture the statistics of the data in hierarchical late ##nt codes , the auto ##re ##gre ##ssi ##ve models learn the image den ##sities directly at the pixel level without learning a hierarchical late ##nt representation . in this paper , we present the pixel ##gan auto ##en ##code ##r as a genera ##tive auto ##en ##code ##r that combines the benefits of late ##nt variable models with auto ##re ##gre ##ssi ##ve architecture ##s . the pixel ##gan auto ##en ##code ##r is a genera ##tive auto ##en ##code ##r in which the genera ##tive path is a pixel ##c ##nn that is conditioned on a late ##nt variable . the late ##nt variable is in ##fer ##red by matching the aggregate ##d posterior distribution to the prior distribution by an ad ##vers ##aria ##l training technique similar to that of the ad ##vers ##aria ##l auto ##en ##code ##r aa ##e . however , whereas in ad ##vers ##aria ##l auto ##en ##code ##rs the statistics of the data distribution are captured by the late ##nt code , in the pixel ##gan auto ##en ##code ##r they are captured jointly by the late ##nt code and the auto ##re ##gre ##ssi ##ve deco ##der . we show that imposing different distributions as the prior results in different factor ##izations of information between the late ##nt code and the auto ##re ##gre ##ssi ##ve deco ##der . for example , in : pixel ##gan _ ga ##uss ##ian ] section [ reference ] , we show that by imposing a ga ##uss ##ian distribution on the late ##nt code , we can achieve a global vs . local decomposition of information . in this case , the global late ##nt code no longer has to model all the irrelevant and fine details of the image , and can use its capacity to capture more relevant and global statistics of the image . another type of decomposition of information that can be learnt by pixel ##gan auto ##en ##code ##rs is a discrete vs . continuous decomposition . in : pixel ##gan _ cat ] section [ reference ] , we show that we can achieve this decomposition by imposing a cat ##egorical prior on the late ##nt code using ad ##vers ##aria ##l training . in this case , the cat ##egorical late ##nt code captures the discrete underlying factors of variation in the data , such as class label information , and the auto ##re ##gre ##ssi ##ve deco ##der captures the remaining continuous structure , such as style information , in an un ##su ##per ##vis ##ed fashion . we then show how pixel ##gan auto ##en ##code ##rs with cat ##egorical prior ##s can be directly used in cluster ##ing and semi - supervised scenarios and achieve very competitive classification results on several data ##set ##s in : experiments ] section [ reference ] . finally , we present one of the main potential applications of pixel ##gan auto ##en ##code ##rs in learning cross - domain relations between two different domains in : cross - domain ] section [ reference ] . section : pixel ##gan auto ##en ##code ##rs let be a data ##point that comes from the distribution and be the hidden code . the recognition path of the pixel ##gan auto ##en ##code ##r ( : pixel ##gan _ ga ##uss ##ian ] figure [ reference ] ) defines an implicit posterior distribution by using a deter ##mini ##stic neural function that takes the input along with random noise with a fixed distribution and outputs . the aggregate ##d posterior of this model is defined as follows : this para ##met ##rization of the implicit posterior distribution was originally proposed in the ad ##vers ##aria ##l auto ##en ##code ##r work aa ##e as the universal approx ##ima ##tor posterior . we can sample from this implicit distribution , by evaluating at different samples of , but the density function of this posterior distribution is intra ##ctable . end ##ix : input _ noise ] appendix [ reference ] discusses the importance of the input noise in training pixel ##gan auto ##en ##code ##rs . the genera ##tive path is a conditional pixel ##c ##nn pixel ##c ##nn that conditions on the late ##nt vector using an adaptive bias in pixel ##c ##nn layers . the inference is done by an amor ##tized gan inference technique that was originally proposed in the ad ##vers ##aria ##l auto ##en ##code ##r work aa ##e . in this method , an ad ##vers ##aria ##l network is attached on top of the hidden code vector of the auto ##en ##code ##r and matches the aggregate ##d posterior distribution , , to an arbitrary prior , . samples from and are provided to the ad ##vers ##aria ##l network as the negative and positive examples respectively , and the generator of the ad ##vers ##aria ##l network , which is also the en ##code ##r of the auto ##en ##code ##r , tries to match to by the gradient that comes through the disc ##rim ##ina ##tive ad ##vers ##aria ##l network . the ad ##vers ##aria ##l network , the pixel ##c ##nn deco ##der and the en ##code ##r are trained jointly in two phases - the reconstruction phase and the ad ##vers ##aria ##l phase - executed on each mini - batch . in the reconstruction phase , the ground truth input along with the hidden code in ##fer ##red by the en ##code ##r are provided to the pixel ##c ##nn deco ##der . the pixel ##c ##nn deco ##der weights are updated to maximize the log - likelihood of the input . the en ##code ##r weights are also updated at this stage by the gradient that comes through the conditioning vector of the pixel ##c ##nn . in the ad ##vers ##aria ##l phase , the ad ##vers ##aria ##l network updates both its disc ##rim ##ina ##tive network and its genera ##tive network ( the en ##code ##r ) to match to . once the training is done , we can sample from the model by first sampling from the prior distribution , and then sampling from the conditional likelihood para ##met ##rized by the pixel ##c ##nn deco ##der . we now establish a connection between the pixel ##gan auto ##en ##code ##r cost and maximum likelihood learning using a decomposition of the aggregate ##d evidence lower bound ( el ##bo ) proposed in surgery : the first term in el ##bo ] equation [ reference ] is the reconstruction term and the second term is the marginal k ##l diver ##gence between the aggregate ##d posterior and the prior distribution . the third term is the mutual information between the late ##nt code and the input . this is a regular ##ization term that encourages and to be deco ##up ##led by removing the information of the data distribution from the hidden code . if the training set has examples , is bounded as follows ( see surgery ) . in order to maximize the el ##bo , we need to minimize all the three terms of el ##bo ] equation [ reference ] . we consider two cases for the deco ##der : deter ##mini ##stic deco ##der . if the deco ##der is deter ##mini ##stic or has very limited st ##och ##astic ##ity such as the simple factor ##ized deco ##der of the va ##e , the mutual information term acts in the complete opposite direction of the reconstruction term . this is because the only way to minimize the reconstruction error of is to learn a hidden code that is relevant to , which results in maxim ##izing . indeed , it can be shown that mini ##mi ##zing the reconstruction term maximize ##s a variation ##al lower bound on i m , info ##gan . for example , in the case of the va ##e trained on mn ##ist , since the reconstruction is precise , the mutual information term is dominated and is close to its maximum value surgery . st ##och ##astic deco ##der . if we use a powerful deco ##der such as the pixel ##c ##nn , the reconstruction term and the mutual information term will not compete with each other anymore and the network can minimize both independently . in this case , the optimal solution for maxim ##izing the el ##bo would be to model solely by and thereby mini ##mi ##zing the reconstruction term , and at the same time , mini ##mi ##zing the mutual information term by ignoring the late ##nt code . as a result , even though the model achieve ##s a high likelihood , the late ##nt code does not learn any useful representation , which is und ##es ##ira ##ble . this problem has been observed in several previous works bowman , v ##la ##e and different techniques such as anne ##aling the weight of the k ##l term bowman or weakening the deco ##der v ##la ##e have been proposed to make and more dependent . as suggested in fe ##rence _ ml , v ##la ##e , we think that the maximum likelihood objective by itself is not a useful objective for representation learning especially when a powerful deco ##der is used . in pixel ##gan auto ##en ##code ##rs , in order to encourage learning more useful representations , we modify the el ##bo ( el ##bo ] equation [ reference ] ) by removing the mutual information term from it , since this term is explicitly encouraging to become independent of . so our cost function only includes the reconstruction term and the marginal k ##l term . the reconstruction term is opt ##imi ##zed by the reconstruction phase of training and the marginal k ##l term is approximately opt ##imi ##zed by the ad ##vers ##aria ##l phase . note that since the mutual information term is upper bounded by a constant ( ) , we are still maxim ##izing a lower bound on the log - likelihood of data . however , this bound is weaker than the el ##bo , which is the price that is paid for learning more useful late ##nt representations by balancing the decomposition of information between the late ##nt code and the auto ##re ##gre ##ssi ##ve deco ##der . for implementing the conditioning adaptive bias in the pixel ##c ##nn deco ##der , we explore two different architecture ##s pixel ##c ##nn . in the location - invariant bias , for each pixel ##c ##nn layer , we use the late ##nt code to construct a vector that is broadcast ##ed within each feature map of the layer and then added as an adaptive bias to that layer . in the location - dependent bias , we use the late ##nt code to construct a spatial feature map that is broadcast ##ed across different feature maps and then added only to the first layer of the deco ##der as an adaptive bias . we will discuss the effect of these architecture ##s on the learnt representation in : mn ##ist _ code ] figure [ reference ] of : pixel ##gan _ ga ##uss ##ian ] section [ reference ] and their implementation details in end ##ix : conditioning _ of _ pixel ##c ##nn ] appendix [ reference ] . sub ##section : pixel ##gan auto ##en ##code ##rs with ga ##uss ##ian prior ##s here , we show that pixel ##gan auto ##en ##code ##rs with ga ##uss ##ian prior ##s can deco ##mp ##ose the global and local statistics of the images between the late ##nt code and the auto ##re ##gre ##ssi ##ve deco ##der . : mn ##ist ] figure [ reference ] a shows the samples of a pixel ##gan auto ##en ##code ##r model with the location - dependent bias trained on the mn ##ist data ##set . for the purpose of better illustrating the decomposition of information , we have chosen a 2 - d ga ##uss ##ian late ##nt code and a limited the rec ##eptive field of size 9 for the pixel ##gan auto ##en ##code ##r . : mn ##ist ] figure [ reference ] b shows the samples of a pixel ##c ##nn model with the same limited rec ##eptive field size of 9 and : mn ##ist ] figure [ reference ] c shows the samples of an ad ##vers ##aria ##l auto ##en ##code ##r with the 2 - d ga ##uss ##ian late ##nt code . the pixel ##c ##nn can successfully capture the local statistics , but fails to capture the global statistics due to the limited rec ##eptive field size . in contrast , the ad ##vers ##aria ##l auto ##en ##code ##r , whose sample quality is very similar to that of the va ##e , can successfully capture the global statistics , but fails to generate the details of the images . however , the pixel ##gan auto ##en ##code ##r , with the same rec ##eptive field and code size , can combine the best of both and generates sharp images with global statistics . in pixel ##gan auto ##en ##code ##rs , both the pixel ##c ##nn depth and the conditioning architecture affect the decomposition of information between the late ##nt code and the auto ##re ##gre ##ssi ##ve deco ##der . we investigate these effects in : mn ##ist _ code ] figure [ reference ] by training a pixel ##gan auto ##en ##code ##r on mn ##ist where the code size is chosen to be for the visual ##ization purpose . as shown in : mn ##ist _ code ] figure [ reference ] a , b , when a shallow deco ##der is used , most of the information will be encoded in the hidden code and there is a clean separation between the digit clusters . as we make the pixel ##c ##nn more powerful ( : mn ##ist _ code ] figure [ reference ] c , d ) , we can see that the hidden code is still used to capture some relevant information of the input , but the separation of digit clusters is not as sharp when the limited code size of 2 is used . in the next section , we will show that by using a larger code size ( e . g . , 30 ) , we can get a much better separation of digit clusters even when a powerful pixel ##c ##nn is used . the conditioning architecture also affects the decomposition of information . in the case of the location - invariant bias , the hidden code is encouraged to learn the global information that is location - invariant ( the what information and not the where information ) such as the class label information . for example , we can see in : mn ##ist _ code ] figure [ reference ] a , c that the network has learnt to use one of the axes of the 2d ga ##uss ##ian code to explicitly en ##code the digit label even though a continuous prior is imposed . in this case , we can potentially get a much better separation if we impose a discrete prior . this makes this architecture suitable for the discrete vs . continuous decomposition and we use it for our cluster ##ing and semi - supervised learning experiments . in the case of the location - dependent bias ( : mn ##ist _ code ] figure [ reference ] b , d ) , the hidden code is encouraged to learn the global information that has location dependent information such as low - frequency content of the image , similar to what the hidden code of an ad ##vers ##aria ##l or variation ##al auto ##en ##code ##r would learn ( : mn ##ist ] figure [ reference ] c ) . this makes this architecture suitable for the global vs . local decomposition experiments such as : mn ##ist ] figure [ reference ] a . from : mn ##ist _ code ] figure [ reference ] , we can see that the class label information is mostly captured by while the style information of the images is captured by both and . this decomposition of information has also been studied in other works that combine the late ##nt variable models with auto ##re ##gre ##ssi ##ve deco ##ders such as pixel ##va ##e pixel ##va ##e and variation ##al loss ##y auto ##en ##code ##rs ( v ##la ##e ) v ##la ##e . for example , the v ##la ##e model v ##la ##e proposes to use the depth of the pixel ##c ##nn deco ##der to control the decomposition of information . in their model , the pixel ##c ##nn deco ##der is designed to have a shallow depth ( small local rec ##eptive field ) so that the late ##nt code is forced to capture more global information . this approach is very similar to our example of the pixel ##gan auto ##en ##code ##r in : mn ##ist ] figure [ reference ] . however , the question that has remained una ##ns ##wer ##ed is whether it is possible to achieve a complete decomposition of content and style in an un ##su ##per ##vis ##ed fashion , where the class label or discrete structure information is encoded in the late ##nt code , and the remaining continuous structure such as style is captured by a powerful and deep pixel ##c ##nn deco ##der . this kind of decomposition is particularly interesting as it can be directly used for cluster ##ing and semi - supervised classification . in the next section , we show that we can learn this decomposition of content and style by imposing a cat ##egorical distribution on the late ##nt representation using ad ##vers ##aria ##l training . note that this discrete vs . continuous decomposition is very different from the global vs . local decomposition , because a continuous factor of variation such as style can have both global and local effect on the image . indeed , in order to achieve the discrete vs . continuous decomposition , we have to use very deep and powerful pixel ##c ##nn deco ##ders ( up to 20 residual blocks ) to capture both the global and local statistics of the style by the pixel ##c ##nn while the discrete content of the image is captured by the cat ##egorical late ##nt variable . sub ##section : pixel ##gan auto ##en ##code ##rs with cat ##egorical prior ##s in this section , we present an architecture of the pixel ##gan auto ##en ##code ##r that can separate the discrete information ( e . g . , class label ) from the continuous information ( e . g . , style information ) in the images . we then show how our architecture can be naturally adopted for the semi - supervised settings . the architecture that we use is similar to : pixel ##gan _ ga ##uss ##ian ] figure [ reference ] , with the difference that we impose a cat ##egorical distribution as the prior rather the ga ##uss ##ian distribution ( : pixel ##gan _ cat ] figure [ reference ] ) and also use the location - independent bias architecture . another difference is that we use a con ##vo ##lu ##tion ##al network as the inference network to encourage the en ##code ##r to preserve the content and lose the style information of the image . the inference network has a soft ##max output and predict ##s a one - hot vector whose dimension is the number of discrete labels or categories that we wish the data to be clustered into . the ad ##vers ##aria ##l network is trained directly on the continuous probability outputs of the soft ##max layer of the en ##code ##r . imposing a cat ##egorical distribution at the output of the en ##code ##r impose ##s two constraints . the first constraint is that the en ##code ##r has to make confident decisions about the class labels of the inputs . the ad ##vers ##aria ##l training pushes the output of the en ##code ##r to the corners of the soft ##max simple ##x , by which it ensures that the auto ##en ##code ##r can not use the late ##nt vector to carry any continuous style information . the second constraint imposed by ad ##vers ##aria ##l training is that the aggregate ##d posterior distribution of should match the cat ##egorical prior distribution with uniform outcome pro ##ba ##bilities . this constraint enforce ##s the en ##code ##r to evenly distribute the class labels across the corners of the soft ##max simple ##x . because of these constraints , the late ##nt variable will only capture the discrete content of the image and all the continuous style information will be captured by the auto ##re ##gre ##ssi ##ve deco ##der . in order to better understand and visual ##ize the effect of the ad ##vers ##aria ##l training on shaping the hidden code distribution , we train a pixel ##gan auto ##en ##code ##r on the first three digits of mn ##ist ( 1800 ##0 training and 3000 test points ) and choose the number of clusters to be 3 . suppose is the hidden code which in this case is the output pro ##ba ##bilities of the soft ##max layer of the inference network . in : pixel ##gan _ cluster _ toy ] figure [ reference ] a , we project the 3d soft ##max simple ##x of onto a 2d triangle and plot the hidden codes of the training examples when no distribution is imposed on the hidden code . we can see from this figure that the network has learnt to use the surface of the soft ##max simple ##x to en ##code style information of the digits and thus the three corners of the simple ##x do not have any meaningful interpretation . : pixel ##gan _ cluster _ toy ] figure [ reference ] b corresponds to the code space of the same network when a cat ##egorical distribution is imposed using the ad ##vers ##aria ##l training . in this case , we can see the network has successfully learnt to en ##code the label information of the three digits in the three corners of the simple ##x , and all the style information has been separately captured by the auto ##re ##gre ##ssi ##ve deco ##der . this network achieve ##s an almost perfect test error - rate of on the first three digits of mn ##ist , even though it is trained in a purely un ##su ##per ##vis ##ed fashion . once the pixel ##gan auto ##en ##code ##r is trained , its en ##code ##r can be used for cluster ##ing new points and its deco ##der can be used to generate samples from each cluster . figure [ reference ] illustrates the samples of the pixel ##gan auto ##en ##code ##r trained on the full mn ##ist data ##set . the number of clusters is set to be 30 and each row corresponds to the conditional samples of one of the clusters ( only 16 are shown ) . we can see that the discrete late ##nt code of the network has learnt discrete factors of variation such as class label information and some discrete style information . for example digit s are put in different clusters based on how much tilted they are . the network is also assign ##ing different clusters to digit s ( based on whether they have a loop ) and digit s ( based on whether they have a dash in the middle ) . in : experiments : un ##su ##p ] section [ reference ] , we will show that by using the en ##code ##r of this network , we can obtain about 5 % error rate in classify ##ing digits in an un ##su ##per ##vis ##ed fashion , just by matching each cluster to a digit type . semi - supervised pixel ##gan auto ##en ##code ##rs . the pixel ##gan auto ##en ##code ##r can be used in a semi - supervised setting . in order to incorporate the label information , we add a semi - supervised training phase . specifically , we set the number of clusters to be the same as the number of class labels and after executing the reconstruction and the ad ##vers ##aria ##l phases on an un ##lab ##ele ##d mini - batch , the semi - supervised phase is executed on a labeled mini - batch , by up ##dating the weights of the en ##code ##r to minimize the cross - entropy cost . the semi - supervised cost also reduces the mode - missing behavior of the gan training by enforcing the en ##code ##r to learn all the modes of the cat ##egorical distribution . in : experiments : semi ] section [ reference ] , we will evaluate the performance of the pixel ##gan auto ##en ##code ##rs on the semi - supervised classification tasks . section : experiments in this paper , we presented the pixel ##gan auto ##en ##code ##r as a genera ##tive model , but the currently available metric ##s for evaluating the likelihood of gan - based genera ##tive models such as par ##zen window estimate are fundamentally flawed the ##is . so in this section , we only present the performance of the pixel ##gan auto ##en ##code ##r on downstream tasks such as un ##su ##per ##vis ##ed cluster ##ing and semi - supervised classification . the details of all the experiments can be found in end ##ix : experiment ] appendix [ reference ] . sub ##section : un ##su ##per ##vis ##ed cluster ##ing we trained a pixel ##gan auto ##en ##code ##r in an un ##su ##per ##vis ##ed fashion on the mn ##ist data ##set ( : pixel ##gan _ cluster ] figure [ reference ] ) . we chose the number of clusters to be 30 and used the following evaluation protocol : once the training is done , for each cluster , we found the validation example that maximize ##s , and assigned the label of to all the points in the cluster . we then computed the test error based on the assigned class labels to each cluster . as shown in the first column of le : semi ] table [ reference ] , the performance of pixel ##gan auto ##en ##code ##rs is on par with other gan - based cluster ##ing algorithms such as cat ##gan cat ##gan , info ##gan info ##gan and ad ##vers ##aria ##l auto ##en ##code ##rs aa ##e . sub ##section : semi - supervised classification le : semi ] table [ reference ] and : plot ] figure [ reference ] report the results of semi - supervised classification experiments on the mn ##ist , sv ##hn and nor ##b data ##set ##s . on the mn ##ist data ##set with 20 , 50 and 100 labels , our classification results are highly competitive . note that the classification rate of un ##su ##per ##vis ##ed cluster ##ing of mn ##ist is better than semi - supervised mn ##ist with 20 labels . this is because in the un ##su ##per ##vis ##ed case , the number of clusters is 30 , but in the semi - supervised case , there are only 10 class labels which makes it more likely to confuse two digits . on the sv ##hn data ##set with 500 and 1000 labels , the pixel ##gan auto ##en ##code ##r out ##per ##forms all the other methods except the recently proposed temporal en ##se ##mbling work temporal - en ##se ##mbling which is not a genera ##tive model . on the nor ##b data ##set with 1000 labels , the pixel ##gan auto ##en ##code ##r out ##per ##forms all the other reported results . : di ##sen ##tangle ] figure [ reference ] shows the conditional samples of the semi - supervised pixel ##gan auto ##en ##code ##r on the mn ##ist , sv ##hn and nor ##b data ##set ##s . each column of this figure presents sampled images conditioned on a fixed one - hot late ##nt code . we can see from this figure that the pixel ##gan auto ##en ##code ##r can achieve a rather clean separation of style and content on these data ##set ##s with very few labeled data . section : learning cross - domain relations with pixel ##gan auto ##en ##code ##rs in this section , we discuss how the pixel ##gan auto ##en ##code ##r can be viewed in the context of learning cross - domain relations between two different domains . we also describe how the problem of cluster ##ing or semi - supervised learning can be cast as the problem of finding a smooth cross - domain mapping from the data distribution to the cat ##egorical distribution . recently several gan - based methods have been developed to learn a cross - domain mapping between two different domains disco ##gan , cycle ##gan , cross - domain - il ##ya , aa ##e , cross - domain - nl ##p . in cross - domain - il ##ya , an un ##su ##per ##vis ##ed cost function called the output distribution matching ( o ##dm ) is proposed to find a cross - domain mapping between two domains and by imposing the following un ##su ##per ##vis ##ed constraint on the un ##cor ##rel ##ated samples from and : where denotes the distribution of the random variable . the ad ##vers ##aria ##l training is proposed as one of the methods for matching these distributions . if we have access to a few labeled pairs , then can be further trained on them in a supervised fashion to satisfy . for example , in speech recognition , we want to find a cross - domain mapping from a sequence of phone ##mes to a sequence of characters . by opt ##imi ##zing the o ##dm cost function in o ##dm ] equation [ reference ] , we can find a smooth function that takes phone ##mes at its input and outputs a sequence of characters that respects the language model . however , the main problem with this method is that the network can learn to ignore part of the input distribution and still satisfy the o ##dm cost function by its output distribution . this problem has also been observed in other works such as disco ##gan . one way to avoid this problem is to add a reconstruction term to the o ##dm cost function by introducing a reverse mapping from the output of the en ##code ##r to the input domain . the is essentially the idea of the ad ##vers ##aria ##l auto ##en ##code ##r aa ##e which learns a genera ##tive model by finding a cross - domain mapping between a ga ##uss ##ian distribution and the data distribution . using the o ##dm cost function along with a reconstruction term to learn cross - domain relations have been explored in several previous works . for example , info ##gan info ##gan adds a mutual information term to the o ##dm cost function and opt ##imi ##zes a variation ##al lower bound on this term . it can be shown that maxim ##izing this variation ##al bound is indeed mini ##mi ##zing the reconstruction cost of an auto ##en ##code ##r i m . similarly , in cross - domain - nl ##p , zhang ##ad ##vers ##aria ##l , an ad ##vers ##aria ##l auto ##en ##code ##r is used to learn the cross - domain relations of the vector representations of words from two different languages . the architecture of the recent works of disco ##gan disco ##gan and cycle ##gan cycle ##gan are also similar to an ad ##vers ##aria ##l auto ##en ##code ##r in which the late ##nt representation is enforced to have the distribution of the other domain . here we describe how our proposed pixel ##gan auto ##en ##code ##r can be potentially used in all these application areas to learn better cross - domain relations . suppose we want to learn a mapping from domain to . in the architecture of : pixel ##gan _ ga ##uss ##ian ] figure [ reference ] , we can use independent samples of at the input and instead of imposing a ga ##uss ##ian distribution on the late ##nt code , we can impose the distribution of the second domain using its independent samples . unlike ad ##vers ##aria ##l auto ##en ##code ##rs , the en ##code ##r of pixel ##gan auto ##en ##code ##rs does not have to retain all the input information in order to have a loss ##less reconstruction . so the en ##code ##r can use all its capacity to learn the most relevant mapping from to and at the same time , the pixel ##c ##nn deco ##der can capture the remaining information that has been lost by the en ##code ##r . we can adopt the o ##dm idea for semi - supervised learning by assuming is the image domain and is the label domain ( : related ] figure [ reference ] a ) . independent samples of and correspond to samples from the data distribution and the cat ##egorical distribution . the function can be para ##met ##rized by a neural network that is trained to satisfy the o ##dm cost function by matching the aggregate ##d distribution to the cat ##egorical distribution using ad ##vers ##aria ##l training . the few labeled examples are used to further train to satisfy . however , as explained above , the problem with this method is that the network can learn to generate the cat ##egorical distribution by ignoring some part of the input distribution . the ad ##vers ##aria ##l auto ##en ##code ##r ( : related ] figure [ reference ] b ) solve ##s this problem by adding an inverse mapping from the cat ##egorical distribution to the data distribution . however , the main draw ##back of the ad ##vers ##aria ##l auto ##en ##code ##r architecture is that due to the reconstruction term , the late ##nt representation now has to model all the underlying factors of variation in the image . for example , in the architecture of : related ] figure [ reference ] b , while we are only interested in the one - hot label representation to do semi - supervised learning , we also need to in ##fer the style of the image so that we can have a loss ##less reconstruction of the image . the pixel ##gan auto ##en ##code ##r solve ##s this problem by enabling the en ##code ##r to only in ##fer the factor of variation that we are interested in ( i . e . , label information ) , while the remaining structure of the input ( i . e . , style information ) is automatically captured by the auto ##re ##gre ##ssi ##ve deco ##der . section : conclusion in this paper , we proposed the pixel ##gan auto ##en ##code ##r , which is a genera ##tive auto ##en ##code ##r that combines a genera ##tive pixel ##c ##nn with a gan inference network that can impose arbitrary prior ##s on the late ##nt code . we showed that imposing different distributions as the prior enables us to learn a late ##nt representation that captures the type of statistics that we care about , while the remaining structure of the image is captured by the pixel ##c ##nn deco ##der . specifically , by imposing a ga ##uss ##ian prior , we were able to di ##sen ##tangle the low - frequency and high - frequency statistics of the images , and by imposing a cat ##egorical prior we were able to di ##sen ##tangle the style and content of images and learn representations that are specifically useful for cluster ##ing and semi - supervised learning tasks . while the main focus of this paper was to demonstrate the application of pixel ##gan auto ##en ##code ##rs in downstream tasks such as semi - supervised learning , we discussed how these architecture ##s have many other potential ##s such as learning cross - domain relations between two different domains . section : ac ##k ##now ##led ##gm ##ents we would like to thank nathan kill ##oran for helpful discussions . we also thank n ##vid ##ia for gp ##u donations . bibliography : references section : implementation details in this section , we describe two important architecture design choices for training pixel ##gan auto ##en ##code ##rs . sub ##section : input noise in all the semi - supervised experiments , we found it crucial to use the universal approx ##ima ##tor posterior discussed in : pixel ##gan ] section [ reference ] , as opposed to a deter ##mini ##stic posterior . specifically , the input noise that we use is an additive ga ##uss ##ian noise , which results in a posterior distribution that is more expressive than that of a model without the input corruption . this is similar to the den ##ois ##ing criterion idea proposed in den ##ois ##ing - va ##e . we believe this additive noise is also playing an important role in preventing the mode - missing behavior of the gan when imposing a de ##gen ##erate distribution such as the cat ##egorical distribution . similar related ideas have been used to stabilize gan training such as instance noise instance or one - sided label noise improved - gan . sub ##section : conditioning of pixel ##c ##nn there are three methods to implement how the pixel ##c ##nn conditions on the late ##nt vector . location - invariant bias . this is the method that was proposed in the conditional pixel ##c ##nn model pixel ##c ##nn . suppose the size of the con ##vo ##lu ##tion ##al layer of the deco ##der is ( batch , width , height , channels ) . then the pixel ##c ##nn can use a linear mapping to convert the conditioning tensor of size ( batch , condition _ size ) to generate a tensor of size ( batch , channels ) that is then broadcast ##ed and added to the feature maps of all the layers of the pixel ##c ##nn deco ##der as an adaptive bias . in this method , the hidden code is encouraged to learn the global information that is location - invariant ( the what information and not the where information ) such as the class label information . we use this method in all the cluster ##ing and semi - supervised learning experiments . location - dependent bias . suppose the size of the con ##vo ##lu ##tion ##al layer of the pixel ##c ##nn deco ##der is ( batch , width , height , channels ) . then the pixel ##c ##nn can use a one layer neural network to convert the conditioning tensor of size ( batch , condition _ size ) to generate a spatial tensor of size ( batch , width , height , k ) followed by a con ##vo ##lu ##tion ##al layer to construct a tensor of size ( batch , width , height , channels ) that is then added only to the feature maps of the first layer of the deco ##der as an adaptive bias ( similar to the vp ##n model vp ##n ) . when , we can simply broadcast the tensor of size ( batch , width , height , k = 1 ) to get a tensor of size ( batch , width , height , channels ) instead of using the con ##vo ##lu ##tion . in this method , the late ##nt vector has spatial and location - dependent information within the feature map . this is the method that we used in experiments of : mn ##ist ] figure [ reference ] a . input channel . another method for conditioning is proposed in the pixel ##va ##e pixel ##va ##e and the variation ##al loss ##y auto ##en ##code ##r ( v ##la ##e ) v ##la ##e . in this method , first a tensor of size ( batch , width , height , k ) is constructed using the conditioning tensor similar to the location - dependent bias . this tensor is then con ##cate ##nated to the input of the pixel ##c ##nn . the performance and computational complexity of this method is very similar to that of the location - dependent bias method . section : experiment details we used tensor ##flow tensor ##flow ##20 ##15 - white ##paper in all of our experiments . as suggested in gan , in order to improve the stability of gan training , the generator of the gan in all our experiments is trained to maximize rather than mini ##mi ##zing . sub ##section : mn ##ist data ##set the mn ##ist data ##set has 50 k training points , 10 k validation points and 10 k test points . we perform experiments on both the binary mn ##ist and the real - valued mn ##ist . in the real valued mn ##ist experiments , we sub ##tra ##ct 127 . 5 from the data points and then divide them by 127 . 5 and use the disc ##ret ##ized log ##istic mixture likelihood pixel ##c ##nn + + as the cost function for the pixel ##c ##nn . in the case of binary mn ##ist , the data points are bin ##ari ##zed by setting pixel values larger than 0 . 5 to 1 , and values smaller than 0 . 5 to 0 . sub ##su ##bs ##ection : pixel ##gan auto ##en ##code ##rs with ga ##uss ##ian prior on mn ##ist here we describe the model architecture used for training the pixel ##gan auto ##en ##code ##r with a ga ##uss ##ian prior on the binary mn ##ist data ##set in : mn ##ist ] figure [ reference ] a . the pixel ##c ##nn deco ##der uses both the vertical and horizontal stacks similar to pixel ##c ##nn . the cost function of the pixel ##c ##nn is the cross - entropy cost function . the pixel ##c ##nn uses the location - dependent bias as described in end ##ix : conditioning _ of _ pixel ##c ##nn ] appendix [ reference ] . specifically , a tensor of size ( batch , width , height , 1 ) is constructed from the conditioning vector by using a one - layer neural network with 1000 hidden units , re ##lu activation and linear output . this tensor is then broadcast ##ed and added only to the feature maps of the first layer of the pixel ##c ##nn deco ##der . the pixel ##c ##nn is designed to have a local rec ##eptive field by having 3 residual blocks ( filter size of 3 ##x ##5 , 32 feature maps , re ##lu non - linear ##ity as in pixel ##c ##nn ) . the ad ##vers ##aria ##l disc ##rim ##inator has two layers of 2000 hidden units with re ##lu activation function . the en ##code ##r architecture has two fully - connected layers of size 2000 with re ##lu non - linear ##ity . the last layer of the en ##code ##r has a linear activation function . on the late ##nt representation of size , we impose a ga ##uss ##ian distribution with standard deviation of . we used the gradient descent with momentum algorithm for opt ##imi ##zing all the cost functions of the network . for the pixel ##c ##nn reconstruction cost , we used the learning rate of 0 . 001 and the momentum value of 0 . 9 . after 25 epoch ##s we reduce the learning rate to 0 . 000 ##1 . for both of the generator and the disc ##rim ##inator costs , the learning rates and the momentum values were set to 0 . 1 . sub ##su ##bs ##ection : un ##su ##per ##vis ##ed cluster ##ing of mn ##ist here we describe the model architecture used for cluster ##ing the binary mn ##ist data ##set in : pixel ##gan _ cluster ] figure [ reference ] and : experiments : un ##su ##p ] section [ reference ] . the pixel ##c ##nn deco ##der uses both the vertical and horizontal stacks similar to pixel ##c ##nn . the cost function of the pixel ##c ##nn is the cross - entropy cost function . the pixel ##c ##nn uses the location - invariant bias as described in end ##ix : conditioning _ of _ pixel ##c ##nn ] appendix [ reference ] and has 15 residual blocks ( filter size of 3 ##x ##5 , 32 feature maps , re ##lu non - linear ##ity as in pixel ##c ##nn ) . the ad ##vers ##aria ##l disc ##rim ##inator has two layers of 3000 hidden units with re ##lu activation function . the en ##code ##r architecture has a con ##vo ##lu ##tion ##al layer ( filter size of 7 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) , followed by another con ##vo ##lu ##tion ##al layer ( filter size of 7 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) with no fully - connected layer . the last layer of the en ##code ##r has the soft ##max activation function . we found it important to use batch - normal ##ization batch for all the layers of the en ##code ##r including the soft ##max layer . the number of clusters is chosen to be . the clusters are represented by a discrete one - hot variable of size 30 . on the continuous probability output of the soft ##max , we impose a cat ##egorical distribution with uniform pro ##ba ##bilities . we use adam adam opt ##imi ##zer with learning rate of for opt ##imi ##zing the pixel ##c ##nn reconstruction cost function , but we found it important to use the gradient descent with momentum algorithm for opt ##imi ##zing the generator and the disc ##rim ##inator costs of the ad ##vers ##aria ##l network . for both of the generator and the disc ##rim ##inator costs , the momentum values were set to 0 . 1 and the learning rates were set to 0 . 01 . we use an input drop ##out noise with the keep probability of at the input layer and only at the training time . the model architecture used for : pixel ##gan _ cluster _ toy ] figure [ reference ] is the same as this architecture except that the number of clusters is chosen to be . sub ##su ##bs ##ection : semi - supervised mn ##ist we performed semi - supervised learning experiments on both binary and real - valued mn ##ist data ##set . we found that the semi - supervised error - rate of the real - valued mn ##ist is roughly the same as the binary mn ##ist ( about 1 . 10 % with 100 labels ) , but it takes longer to train due to the log ##istic mixture likelihood cost function pixel ##c ##nn + + . so in le : semi ] table [ reference ] , we only report the performance with the binary mn ##ist , but in : di ##sen ##tangle ] figure [ reference ] b we are showing the samples of the real - valued mn ##ist with 100 labels . binary mn ##ist . here we describe the model architecture used for the semi - supervised learning experiments on the binary mn ##ist in : experiments : semi ] section [ reference ] and le : semi ] table [ reference ] . the pixel ##c ##nn deco ##der uses both the vertical and horizontal stacks similar to pixel ##c ##nn and uses the cross - entropy cost function . the pixel ##c ##nn uses the location - invariant bias as described in end ##ix : conditioning _ of _ pixel ##c ##nn ] appendix [ reference ] . the pixel ##c ##nn has 6 residual blocks ( filter size of 3 ##x ##5 , 32 feature maps , re ##lu non - linear ##ity as in pixel ##c ##nn ) . the ad ##vers ##aria ##l disc ##rim ##inator has two layers of 1000 hidden units with re ##lu activation function . the en ##code ##r architecture has three con ##vo ##lu ##tion ##al layers ( filter size of 5 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) , followed by another three con ##vo ##lu ##tion ##al layers ( filter size of 5 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) with no fully - connected layer . the last layer of the en ##code ##r has the soft ##max activation function . all the con ##vo ##lu ##tion ##al layers of the en ##code ##r except the soft ##max layer use batch - normal ##ization batch . on the late ##nt representation , we impose a cat ##egorical distribution with uniform pro ##ba ##bilities . the semi - supervised cost is the cross - entropy cost function at the output of . we use adam adam opt ##imi ##zer with learning rate of for opt ##imi ##zing the pixel ##c ##nn cost and the cross - entropy cost , but we found it important to use the gradient descent with momentum algorithm for opt ##imi ##zing the generator and the disc ##rim ##inator costs of the ad ##vers ##aria ##l network . for both of the generator and the disc ##rim ##inator costs , the momentum values were set to 0 . 1 and the learning rates were set to 0 . 1 . we add a ga ##uss ##ian noise with standard deviation of to the input layer as described in end ##ix : input _ noise ] appendix [ reference ] . the labeled examples were chosen at random but evenly distributed across the classes . real - valued mn ##ist . here we describe the model architecture used for the semi - supervised learning experiments on the real - valued mn ##ist in : di ##sen ##tangle ] figure [ reference ] b . the pixel ##c ##nn deco ##der uses both the vertical and horizontal stacks similar to pixel ##c ##nn and uses a disc ##ret ##ized log ##istic mixture likelihood cost function with 10 log ##istic distribution as proposed in pixel ##c ##nn + + . the pixel ##c ##nn uses the location - invariant bias as described in end ##ix : conditioning _ of _ pixel ##c ##nn ] appendix [ reference ] . the pixel ##c ##nn has 20 residual blocks ( filter size of 2 ##x ##3 , 64 feature maps , gate ##d si ##gm ##oid - tan ##h non - linear ##ity as in pixel ##c ##nn ) . the ad ##vers ##aria ##l disc ##rim ##inator has two layers of 1000 hidden units with re ##lu activation function . the en ##code ##r architecture has three con ##vo ##lu ##tion ##al layers ( filter size of 5 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) , followed by another three con ##vo ##lu ##tion ##al layers ( filter size of 5 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) with no fully - connected layer . the last layer of the en ##code ##r has the soft ##max activation function . all the con ##vo ##lu ##tion ##al layers of the en ##code ##r except the soft ##max layer use batch - normal ##ization batch . on the late ##nt representation , we impose a cat ##egorical distribution with uniform pro ##ba ##bilities . the semi - supervised cost is the cross - entropy cost function at the output of . we use adam adam opt ##imi ##zer with learning rate of for opt ##imi ##zing the pixel ##c ##nn cost and the cross - entropy cost , but we found it important to use the gradient descent with momentum algorithm for opt ##imi ##zing the generator and the disc ##rim ##inator costs of the ad ##vers ##aria ##l network . for both of the generator and the disc ##rim ##inator costs , the momentum values were set to 0 . 1 and the learning rates were set to 0 . 1 . after 150 epoch ##s , we divide all the learning rates by 10 . we add a ga ##uss ##ian noise with standard deviation of to the input layer as described in end ##ix : input _ noise ] appendix [ reference ] . the labeled examples were chosen at random but evenly distributed across the classes . sub ##section : sv ##hn data ##set the sv ##hn data ##set has about 530 k training points and 26 k test points . we use 10 k points for the validation set . similar to va ##t , we downs ##amp ##le the images from to and then sub ##tra ##ct ##e 127 . 5 from the data points and then divide them by 127 . 5 . sub ##su ##bs ##ection : semi - supervised sv ##hn here we describe the model architecture used for the semi - supervised learning experiments on the sv ##hn data ##set in : experiments : semi ] section [ reference ] . the pixel ##c ##nn deco ##der uses both the vertical and horizontal stacks similar to pixel ##c ##nn . the cost function of the pixel ##c ##nn is a disc ##ret ##ized log ##istic mixture likelihood cost function with 10 log ##istic distribution as proposed in pixel ##c ##nn + + . the pixel ##c ##nn uses the location - invariant bias as described in end ##ix : conditioning _ of _ pixel ##c ##nn ] appendix [ reference ] and has 20 residual blocks ( filter size of 3 ##x ##5 , 32 feature maps , gate ##d si ##gm ##oid - tan ##h non - linear ##ity as in pixel ##c ##nn ) . the ad ##vers ##aria ##l disc ##rim ##inator has two layers of 1000 hidden units with re ##lu activation function . the en ##code ##r architecture has two con ##vo ##lu ##tion ##al layers ( filter size of 5 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) , followed by another two con ##vo ##lu ##tion ##al layers ( filter size of 5 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) with no fully - connected layer . the last layer of the en ##code ##r has the soft ##max activation function . all the con ##vo ##lu ##tion ##al layers of the en ##code ##r except the soft ##max layer use batch - normal ##ization batch . on the late ##nt representation , we impose a cat ##egorical distribution with uniform pro ##ba ##bilities . the semi - supervised cost is the cross - entropy cost function at the output of . we use adam adam opt ##imi ##zer for opt ##imi ##zing all the cost function . for the pixel ##c ##nn cost and the cross - entropy cost we use the learning rate of and for the generator and the disc ##rim ##inator costs of the ad ##vers ##aria ##l network we use the learning rate of . we add a ga ##uss ##ian noise with standard deviation of to the input layer as described in end ##ix : input _ noise ] appendix [ reference ] . sub ##section : nor ##b data ##set the nor ##b data ##set has about 24 k training points and 24 k test points . we use 4 k points for the validation set . this data ##set has 5 object categories : animals , human figures , airplanes , trucks and cars . we downs ##amp ##le the images to have the size of , sub ##tra ##ct 127 . 5 from the data points and then divide them by 127 . 5 . sub ##su ##bs ##ection : semi - supervised nor ##b the pixel ##c ##nn deco ##der uses both the vertical and horizontal stacks similar to pixel ##c ##nn . the cost function of the pixel ##c ##nn is a disc ##ret ##ized log ##istic mixture likelihood cost function with 10 log ##istic distribution as proposed in pixel ##c ##nn + + . the pixel ##c ##nn uses the location - invariant bias as described in end ##ix : conditioning _ of _ pixel ##c ##nn ] appendix [ reference ] and has 15 residual blocks ( filter size of 3 ##x ##5 , 32 feature maps , gate ##d si ##gm ##oid - tan ##h non - linear ##ity as in pixel ##c ##nn ) . the ad ##vers ##aria ##l disc ##rim ##inator has two layers of 1000 hidden units with re ##lu activation function . the en ##code ##r architecture has a con ##vo ##lu ##tion ##al layer ( filter size of 7 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) , followed by another con ##vo ##lu ##tion ##al layer ( filter size of 7 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) , followed by another con ##vo ##lu ##tion ##al layer ( filter size of 7 , 32 feature maps , re ##lu activation ) and a max - pool ##ing layer ( pool ##ing size 2 ) with no fully - connected layer . the last layer of the en ##code ##r has the soft ##max activation function . all the con ##vo ##lu ##tion ##al layers of the en ##code ##r except the soft ##max layer use batch - normal ##ization batch . on the late ##nt representation , we impose a cat ##egorical distribution with uniform pro ##ba ##bilities . the semi - supervised cost is the cross - entropy cost function at the output of . we use adam adam opt ##imi ##zer for opt ##imi ##zing all the cost function . for the pixel ##c ##nn cost and the cross - entropy cost we use the learning rate of and for the generator and the disc ##rim ##inator costs of the ad ##vers ##aria ##l network we use the learning rate of . we add a ga ##uss ##ian noise with standard deviation of to the input layer as described in end ##ix : input _ noise ] appendix [ reference ] . the labeled examples were chosen at random but evenly distributed across the classes . in the case of nor ##b with 1000 labels , the test error after 10 epoch ##s is 12 . 97 % , after 100 epoch ##s is 11 . 63 % and after 500 epoch ##s is 8 . 17 % .",
        "pred_seq": "mn ##ist [SEP] pixel ##rs [SEP] [SEP] classification classification [SEP] [unused0] mn ##ist [SEP] pixel ##rs [SEP] [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "mnist"
                    ]
                ],
                "Method": [
                    [
                        "pixelgan autoencoders"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "classification"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "mnist"
                    ]
                ],
                "Method": [
                    [
                        "pixelgan autoencoders"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "mnist",
                        "binary mnist",
                        "realvalued mnist",
                        "realvalued mnist dataset"
                    ]
                ],
                "Method": [
                    [
                        "pixelgan autoencoders"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "semisupervised setting",
                        "unsupervised clustering",
                        "unsupervised case",
                        "unsupervised clustering of mnist"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "mnist",
                        "binary mnist",
                        "realvalued mnist",
                        "realvalued mnist dataset"
                    ]
                ],
                "Method": [
                    [
                        "pixelgan autoencoders"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "classification"
                    ]
                ]
            }
        ]
    },
    "14": {
        "doctext": "document : sub ##mani ##fold sparse con ##vo ##lu ##tion ##al networks con ##vo ##lu ##tion ##al network are the de - facto standard for anal ##ys ##ing spat ##io - temporal data such as images , videos , 3d shapes , etc . whilst some of this data is naturally dense ( for instance , photos ) , many other data sources are inherently sparse . examples include pen - strokes forming on a piece of paper , or ( colored ) 3d point clouds that were obtained using a lid ##ar scanner or r ##gb - d camera . standard ' ' dense ' ' implementations of con ##vo ##lu ##tion ##al networks are very in ##ef ##fi ##cie ##nt when applied on such sparse data . we introduce a sparse con ##vo ##lu ##tion ##al operation tailored to processing sparse data that differs from prior work on sparse con ##vo ##lu ##tion ##al networks in that it operates strictly on sub ##mani ##fold ##s , rather than ' ' dil ##ating ' ' the observation with every layer in the network . our empirical analysis of the resulting sub ##mani ##fold sparse con ##vo ##lu ##tion ##al networks shows that they perform on par with state - of - the - art methods whilst requiring substantially less computation . section : introduction con ##vo ##lu ##tion ##al networks constitute the state - of - the art method for a wide range of tasks that involve the analysis of data with spatial and / or temporal structure , such as photographs , videos , or three - dimensional surface models . while such data frequently comprises a densely filled ( 2d or 3d ) grid , other spat ##io - temporal data ##set ##s are naturally sparse . for instance , handwriting is made up of one - dimensional lines in two - dimensional space , pictures made by r ##gb - d cameras are three - dimensional point clouds , and off models form two - dimensional surfaces in 3d space . the curse of dimensional ##ity applies , in particular , on data that lives on grid ##s that have three or more dimensions : the number of points on the grid grows exponential ##ly with its dimensional ##ity . in such scenarios , it becomes increasingly important to exploit data spa ##rs ##ity whenever possible in order to reduce the computational resources needed for data processing . indeed , exploit ##ing spa ##rs ##ity is paramount when analyzing , for instance , r ##gb - d videos which are sparsely populated 4 ##d structures . traditional con ##vo ##lu ##tion ##al network implementations are opt ##imi ##zed for data that lives on densely populated grid ##s , and can not process sparse data efficiently . more recently , a number of con ##vo ##lu ##tion ##al network implementations have been presented that are tailored to work efficiently on sparse data . mathematical ##ly , some of these implementations are identical to a regular con ##vo ##lu ##tion ##al network , but they require fewer computational resources in terms of flop ##s and / or in terms of memory . oct ##nets slightly modify the con ##vo ##lu ##tion operator to produce ' ' averaged ' ' hidden states in parts of the grid that are away from regions of interest . one of the downs ##ides of prior sparse implementations of con ##vo ##lu ##tion ##al networks is that they ' ' dil ##ate ' ' the sparse data in every layer , because they implement a ' ' full ' ' con ##vo ##lu ##tion . in this work , we show that it is possible to successfully train con ##vo ##lu ##tion ##al networks that keep the same spa ##rs ##ity pattern throughout the layers of the network , without dil ##ating the feature maps . to this end , we explore two novel con ##vo ##lu ##tion operators : sparse con ##vo ##lu ##tion ( sc ) and valid sparse con ##vo ##lu ##tion ( vs ##c ) . in our experiments with recognizing hand ##written digits and 3d shapes , networks using sc and vs ##c achieve state - of - the - art performance whilst reducing the computation and memory requirements by . section : motivation we define a - dimensional con ##vo ##lu ##tion ##al network as a network that takes as input that is a - dimensional tensor : the input tensor contains spat ##iot ##em ##por ##al dimensions ( such as length , width , height , time , etc . ) and one additional feature space dimension ( for instance , r ##gb color channels , surface normal vectors , etc . ) . a sparse input corresponds to a - dimensional grid of sites that is associated with a feature vector . we define a site in the input to be active if any element in the feature vector is not in its ground state , for instance , if it is non - zero . in many practical problems , threshold ##ing may be used to eliminate sites at which the feature vector is within a very small distance from the ground state . note that even though the input tensor is - dimensional , activity is a - dimensional phenomenon : entire planes along the feature dimension are either active or not . the hidden layers of a con ##vo ##lu ##tion ##al network are also represented by - dimensional grid ##s of feature - space vectors . when prop ##aga ##ting the input data through the network , a site in a hidden layer is active if any of the sites in the layer that it takes as input is active . ( note that when using con ##vo ##lu ##tions , each site is connected to sites in the hidden layer below . ) activity in a hidden layer thus follows an ind ##uc ##tive definition in which each layer determines the set of active states in the next . in each hidden layer , inactive sites all have the same feature vector : the one corresponding to the ground state . note that the ground state in a hidden layer is often not equal to zero , in particular , when con ##vo ##lu ##tions with a bias term are used . however , ir ##res ##pe ##ctive of the value of the ground state , the ground - state value only needs to be calculated once per forward pass during training ( and only once for all forward passes at test time ) . this allows for substantial savings in computational and memory requirements ; the exact savings depend on the data spa ##rs ##ity and the network depth . in this paper , we argue that the framework described above is und ##ul ##y restrictive , in particular , because the con ##vo ##lu ##tion operation has not been modified to accommodate the spa ##rs ##ity of the input data . if the input data contains a single active site , then after applying a con ##vo ##lu ##tion , there will be active sites . applying a second con ##vo ##lu ##tion of the same size will yield active sites , and so on . this rapid growth of the number of active sites is a poor prospect when implementing modern con ##vo ##lu ##tion ##al network architecture ##s that comprise tens or even hundreds of con ##vo ##lu ##tions , such as v ##gg networks , res ##nets , and dense ##nets . of course , con ##vo ##lu ##tion ##al networks are not often applied to inputs that only have a single active site , but the aforementioned ' ' dil ##ation ' ' problems are equally problematic when the input data comprises one - dimensional curves in spaces with two or more dimensions , or two - dimensional surfaces in three or more dimensions . to address the problems with dil ##ation of active sites , we propose two slightly different con ##vo ##lu ##tion operations for use in con ##vo ##lu ##tion ##al networks . what the two operations have in common is that they both ignore the ground state : they replace the ground state with a zero vector to sim ##plify the con ##vo ##lu ##tion operations . the difference between both operations is in how they handle active sites : instead of automatically making a site active if any of the inputs to its rec ##eptive field is active ( thereby dil ##ating the set of active sites ) , our most efficient con ##vo ##lu ##tion ##al operation only considers the central input . as a result , the output set of active sites exactly mirrors that of the input set . we empirical ##ly demonstrate that use of our adapted con ##vo ##lu ##tion ##al operators allows us to build much deeper networks that achieve state - of - the - art results whilst requiring much fewer resources by preserving spa ##rs ##ity . sub ##section : sub ##mani ##fold dil ##ation in figure [ reference ] , we show an example of a one - dimensional curve that is embedded on a two - dimensional grid . the figure shows that even when we apply small con ##vo ##lu ##tions on this grid , the spa ##rs ##ity on the grid rapidly disappears . at the same time , if we restrict the output of the con ##vo ##lu ##tion only to the set of active input points , hidden layers in the network can not capture a lot of information that may relevant to the classification of the curve . in particular , two neighboring connected components will be treated completely independently . luckily , nearly all con ##vo ##lu ##tion ##al networks incorporate some form of pool ##ing , or use stride ##d con ##vo ##lu ##tions . these operations are essential in the sparse con ##vo ##lu ##tion ##al networks we investigate , as they allow neighboring components to merge . in particular , the closer the components are , the smaller the number of pool ##ings / stride ##d con ##vo ##lu ##tions is that is necessary for the components to merge in the hidden - layer representations . sub ##section : very deep con ##vo ##lu ##tion ##al networks in image classification , very deep con ##vo ##lu ##tion ##al networks with small filters , often of size pixels and a pad ##ding of pixel ( to preserve the size of the feature maps ) , have proven to be very effective . such small filters were used successfully in v ##gg networks , which have relatively wide layers . the introduction of residual networks ( res ##nets ) showed that deeper but narrow networks with small filters are more efficient . the success of very deep res ##nets , res ##ne ##xt models , and dense ##nets with bottle ##neck connections shows that it can be useful to calculate a relatively small number of features at a time and ama ##lga ##mate these features into a larger state variable , either by vector - addition or feature - vector con ##cate ##nation . unfortunately , these techniques are imp ##rac ##tical using existing sparse con ##vo ##lu ##tion ##al network implementations . one problem is that networks with multiple paths will tend to generate different sets of active paths , which would have to be merged to rec ##onne ##ct the outputs . it seems that this would be difficult to perform this merging efficiently . more importantly , res ##nets and dense ##nets generate such large rec ##eptive fields that spa ##rs ##ity would almost immediately be destroyed by the explosion in the number of active sites . section : ( valid ) sparse con ##vo ##lu ##tions : sc and vs ##c we define a sparse con ##vo ##lu ##tion sc ( with input feature planes , output feature planes , a filter size of , and stride . we assume and to be odd integers , but we can allow general ##ization to non - square filters , e . g . , or , if we want to implement inception - style factor ##ised con ##vo ##lu ##tions . an sc con ##vo ##lu ##tion compute ##s the set of active sites in the same way as a regular con ##vo ##lu ##tion : it looks for the presence of any active sites in its rec ##eptive field of size . if the input has size then the output will have size . an sc con ##vo ##lu ##tion differs from a regular con ##vo ##lu ##tion in that it disc ##ards the ground state for non - active sites by assuming that the input from those sites is exactly zero . whereas this is a seemingly small change to the con ##vo ##lu ##tion operation , it may bring computational benefits in practice . next , we define a second type of sparse con ##vo ##lu ##tion , which forms the main contribution of this paper . again , let denote an odd number , or collection of odd numbers , e . g . , or . we define a valid sparse con ##vo ##lu ##tion vs ##c as a modified sc con ##vo ##lu ##tion . first , we pad the input with on each side , so that the output will have the same size as the input . next , we restrict an output site to be active if and only if the site at the corresponding site in the input is active ( i . e . , if the central site in the rec ##eptive field is active ) . whenever an output site is determined to be active , its output feature vector is calculated by the sc operation . table [ reference ] presents the computational and memory requirements of a regular con ##vo ##lu ##tion ( c ) and of our sc and vs ##c con ##vo ##lu ##tions . to construct con ##vo ##lu ##tion ##al networks using sc and vs ##c , we also need activation functions , batch normal ##ization , and pool ##ing . activation functions are defined as usual , but are restricted to the set of active sites . similarly , we define batch normal ##ization in terms of regular batch - normal ##ization applied over the set of active sites . max - pool ##ing mp and average - pool ##ing ap operations are defined as a variant of sc . mp takes the maximum of the zero vector and the input feature vectors in the rec ##eptive field . ap calculate ##s times the sum of the active input vectors . we also define a deco ##n ##vo ##lu ##tion operation dc as an inverse of the sc con ##vo ##lu ##tion . the set of active output sites from a dc con ##vo ##lu ##tion is exactly the set of input active sites to the matching sc con ##vo ##lu ##tion . the set of connections between input - output sites is simply inverted . sub ##section : sub ##mani ##fold con ##vo ##lu ##tion ##al networks we use a combination of vs ##c con ##vo ##lu ##tions , stride ##d sc con ##vo ##lu ##tions , and sparse pool ##ing operations to build sparse versions of the popular v ##gg , res ##net , and dense ##net con ##vo ##lu ##tion ##al networks . the blocks we use in our networks are presented in figure [ reference ] . we refer to our networks as sub ##mani ##fold con ##vo ##lu ##tion ##al networks , because they are opt ##imi ##sed to process low - dimensional data living in a space of higher dimensional ##ity . we use the name v ##gg to refer to networks that contain a number of vs ##c ( , , 3 , 1 ) con ##vo ##lu ##tions , separated by max - pool ##ing . each con ##vo ##lu ##tion is followed by batch normal ##ization and a re ##lu non - linear ##ity . similarly , we define ' ' pre - activated res ##nets ' ' in which most data processing is performed by pairs of vs ##c ( , , 3 , 1 ) con ##vo ##lu ##tions , and in which the residual connections are identity functions . whenever the number of input / output features is different , we use a vs ##c ( , , 1 , 1 ) instead . whenever there is change of scale , we replace the first con ##vo ##lu ##tion and the residual connection by a sc ( , , 3 , 2 ) con ##vo ##lu ##tion . this ensures that two branches can use the same hash table of active sites , and reduces additions to a simple sum of two equally sized matrices . the increased size of the residual connection ' s rec ##eptive field also prevents excessive information loss . we also experiment with sub ##mani ##fold dense ##nets . here ##in , the word dense does not refer to a lack of spatial spa ##rs ##ity but rather to the pattern of connections between con ##vo ##lu ##tion operations . a simple dense ##net module is a sequence of con ##vo ##lu ##tions in which each con ##vo ##lu ##tion takes as input the con ##cate ##nated output of all the previous con ##vo ##lu ##tion operations . the bottle ##neck layers in our sub ##mani ##fold dense ##nets are implemented in the same way as for res ##nets . section : implementation to implement ( v ) sc con ##vo ##lu ##tions efficiently , we store the state of a input / hidden layer in two parts : a hash table and a matrix . the matrix has size and contains one row for each of the active sites . the hash table contains ( location , row ) pairs for all active sites : the location is a tu ##ple of integer coordinates , and the row number indicates the corresponding row in the feature matrix . given a con ##vo ##lu ##tion with filter size , we define a rule book to be a collection of integer matrices of size . to implement an sc ( con ##vo ##lu ##tion , we : it ##erate once through the the input hash - table . we build the output hash table and rule book on - the - fly by it ##era ##ting over points in the output layer that receive input from a given point in the input layer . when an output site is visited for the first time , a new entry is created in the output hash table . based on the spatial offset between the input and output points , a ( input index , output index ) pair is added to the rule book . initial ##ize the output matrix to all zero ##s . for each , there is a parameter matrix with size . for each , multi ##ply the - th row of the input feature matrix by and add it to the - th row of the the output feature matrix . this can be implemented very efficiently on gp ##us because it is a matrix - matrix multi ##ply - add operation . to implement a vs ##c con ##vo ##lu ##tion , we re - use the input hash table for the output , and construct an appropriate rule book . note that because the spa ##rs ##ity pattern does not change , the same rule book can be re - used in v ##gg / res ##net / dense ##net networks until a pool ##ing or sub ##sam ##pling layer is encountered . if there are active points in the input layer , the cost of building the input hash - table is . for v ##gg / res ##net / dense ##net networks , assuming the number of active sites reduces by a multi ##pl ##icative factor with each pool ##ing operation , the cost of building all the hash - tables and rule - books is also , regardless of the depth of the network . section : experiments we perform experiments on a 2d and a 3d data ##set with sparse images . the cas ##ia data ##set contains samples of 375 ##5 gb ##k level - 1 characters with approximately 240 train and 60 test images per class . c ##j ##v ##k characters are good test cases for our models because they are a worst - case scenario for sparse con ##vo ##lu ##tion ##al networks : when drawn at scale , about 8 % of the pixels are active , but this percentage rapidly decreases after pool ##ing due to the small density of the pen strokes . this makes them a good test case for our models . the model ##net - 40 data ##set ##ht ##tp : / / / contain 246 ##8 cad models that contain shapes corresponding to 40 classes . we follow the prep ##ro ##ces ##sing of before feeding the models into our con ##vo ##lu ##tion ##al networks . all cad models were rendered as surfaces at size . sub ##section : results on cas ##ia we first experiment with two v ##gg architecture ##s on the cas ##ia data ##set . we trained all models for 100 epoch ##s using batch ##es of size 100 , sg ##d with momentum 0 . 9 , a weight decay of , and a learning rate decay of 5 % per epoch . for simplicity , we do not employ any data aug ##ment ##ation . the architecture ##s of our v ##gg networks and their performances are presented in table [ reference ] . we observe that ' ' regular ' ' c con ##vo ##lu ##tions and ' ' sparse ' ' sc con ##vo ##lu ##tions achieve the same error : this result suggests that disc ##arding the ground state has essentially no negative impact on performance . this is an argument for always disc ##arding ground states , as it makes things easier computational ##ly and algorithm ##ically . comparing sc with vs ##c con ##vo ##lu ##tions , we observe a minimal loss in performance by considering only the valid part of the con ##vo ##lu ##tion . this minimal loss in accuracy does facilitate great computational improvements : networks using vs ##c use 2 to 3 less computation and memory . next , we performed experiments on cas ##ia with sub ##mani ##fold res ##nets . the key difference between our implementation of res ##nets and regular res ##nets is that stride - 2 res ##net modules use sc ( , , 3 , 2 ) con ##vo ##lu ##tions for the stride ##d con ##vo ##lu ##tion , rather than sc ( , , 1 , 2 ) . this change is necessary to ensure the two branches produce the same set of active sites , which sim ##pl ##ifies book ##keeping and turns the addition operation into a simple matrix - matrix addition . unlike the vs ##c con ##vo ##lu ##tions that are used in most layers , the sc ( , , 3 , 2 ) we use after downs ##amp ##ling leads sites to be active if any of its inputs are active , which avoids information loss in the transition . the architecture ##s of our res ##net networks and their performances are presented in table [ reference ] . the results with res ##nets are in line with those obtained using v ##gg networks : we obtain reductions in computational and memory requirements by at least a factor of 2 at a minimal loss in accuracy . we also performed experiments with dense ##nets ; please see table [ reference ] . next we experimented with adding extra connections to v ##gg networks to increase the effective rec ##eptive fields of the hidden states ; see table [ reference ] for results . in the table , denotes a vs ##c con ##vo ##lu ##tion performed in parallel with a chain of sc - vs ##c - dc operations ; outputs are con ##cate ##nated to produce output feature planes . to sim ##plify the network design , we switched to size - 3 stride - 2 max - pool ##ing , matching the sc con ##vo ##lu ##tions in the sc - vs ##c - dc branches , and reduce the input size from 64 64 to 63 63 . figure [ reference ] presents an overview of all our results on the cas ##ia data ##set . sub ##section : results on model ##net in a second set of experiments , we compare two sub ##mani ##fold v ##gg networks with a state - of - the - art dense con ##vo ##lu ##tion ##al network on the model ##net - 40 data ##set . the results of these experiments are shown in table [ reference ] : the left part of the table shows the architecture and performance of our sub ##mani ##fold v ##gg networks , whereas the right part of the table shows that of the dense 3d ##nin network . the results clearly demonstrate that sub ##mani ##fold have the potential for designing con ##vo ##lu ##tion ##al networks for sparse data that obtain state - of - the - art performance with limited computational requirements : in particular , our v ##gg - a network makes 2 % more errors at 13 fewer computation ##s , and our v ##gg - b performs roughly on par with the dense 3d ##nin whilst performing fewer computation ##s . section : related work this paper is not the first to study sparse con ##vo ##lu ##tion ##al networks . most prior networks for sparse data implements a standard con ##vo ##lu ##tion ##al operator that increases the number of active sites with each layer . by contrast , our sub ##mani ##fold con ##vo ##lu ##tion ##al networks allows sparse data to be processed whilst re ##train ##ing a much greater degree of spa ##rs ##ity . we have shown that this makes it practical to train deep and efficient v ##gg and res ##net models . sub ##mani ##fold con ##vo ##lu ##tion ##al networks are also much sparse ##r than oct ##nets . oct ##net stores data in oct - trees : a data structure in which the grid cube is progressively subdivided into smaller sub - cube ##s until the sub - cube ##s are either empty or contain a single active site . to compare the efficiency of oct ##nets with that of sub ##mani ##fold con ##vo ##lu ##tion ##al networks , we picked a random sample from the model ##net - 40 data ##set and rendered it in a cube with grid points . the resulting grid had 42 ##3 active sites , which corresponds to 1 . 3 % of the total number of sites . each active site had on average 12 . 4 active neighbors ( the maximum possible number of neighbors is 27 ) . vs ##c con ##vo ##lu ##tions , therefore , require only 0 . 6 % of the work of a dense ( c ) con ##vo ##lu ##tion . however , in the oct ##tree , 80 % , 13 % , 4 % , and 3 % of the volume of the cube is covered by sub - cube ##s of size , , and , respectively . as a result , an oct ##net con ##vo ##lu ##tion , which operates over the surfaces of the smaller cube ##s , requires about 35 % of the computation ##s that a dense ( c ) con ##vo ##lu ##tion requires . in this particular example , an oct ##net con ##vo ##lu ##tion thus has a computational cost that is 60 times higher than that of a vs ##c con ##vo ##lu ##tion . sub ##mani ##fold con ##vo ##lu ##tion ##al networks also have advantages in terms of memory requirements . in particular , a sub ##mani ##fold network stores a single feature vector for each of the active sites . by contrast , oct ##tree ##s have about twice as many empty child nodes as active nodes , which implies they have to store roughly three times as many features as a sub ##mani ##fold con ##vo ##lu ##tion ##al network . having said that , some of the ideas of may be combined with vs ##c con ##vo ##lu ##tions . in particular , it is possible to use oct - trees as a specialized hash function in vs ##c con ##vo ##lu ##tions . such an oct - tree - based hash function has the potential to be faster than a standard universal hash function that operates on integer tu ##ple keys , like in our implementation of vs ##c . section : conclusion we introduced a new sparse con ##vo ##lu ##tion ##al operator , called valid sparse con ##vo ##lu ##tion ( vs ##c ) , that facilitates the design of efficient , deep con ##vo ##lu ##tion ##al networks for sparse data . we have shown that vs ##c con ##vo ##lu ##tions lead to substantial computational savings whilst maintain state - of - the - art acc ##ura ##cies on two data ##set ##s : a data ##set comprising one - dimensional manifold ##s embedded in two - dimensional space , and a data ##set comprising two - dimensional surfaces embedded in three - dimensional space . as part of this paper , we are releasing easy - to - use implementations of vs ##c and the other sparse operations we used in the networks described in this paper . we will also release code to reproduce the results of our experiments . bibliography : references",
        "pred_seq": "[SEP] sub networks [SEP] [SEP] [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "submanifold sparse convolutional networks"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "submanifold sparse convolutional networks"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ]
    },
    "15": {
        "doctext": "document : natural language inference by tree - based con ##vo ##lu ##tion and he ##uri ##stic matching in this paper , we propose the tb ##c ##nn - pair model to recognize en ##tail ##ment and contradiction between two sentences . in our model , a tree - based con ##vo ##lu ##tion ##al neural network ( tb ##c ##nn ) captures sentence - level semantics ; then he ##uri ##stic matching layers like con ##cate ##nation , element - wise product / difference combine the information in individual sentences . experimental results show that our model out ##per ##forms existing sentence encoding - based approaches by a large margin . section : introduction recognizing en ##tail ##ment and contradiction between two sentences ( called a premise and a hypothesis ) is known as natural language inference ( nl ##i ) in inference . provided with a premise sentence , the task is to judge whether the hypothesis can be in ##fer ##red ( en ##tail ##ment ) , or the hypothesis can not be true ( contradiction ) . several examples are illustrated in table [ reference ] . nl ##i is in the core of natural language understanding and has wide applications in nl ##p , e . g . , question answering and automatic sum ##mar ##ization . moreover , nl ##i is also related to other tasks of sentence pair modeling , including para ##ph ##rase detection , relation recognition of discourse units , etc . traditional approaches to nl ##i mainly fall into two groups : feature - rich models and formal reasoning methods . feature - based approaches typically leverage machine learning models , but require intensive human engineering to represent lexi ##cal and syn ##ta ##ctic information in two sentences . formal reasoning , on the other hand , converts a sentence into a formal logical representation and uses interpreter ##s to search for a proof . however , such approaches are limited in terms of scope and accuracy . the renewed prosperity of neural networks has made significant achievements in various nl ##p applications , including individual sentence modeling as well as sentence matching . a typical neural architecture to model sentence pairs is the \" siam ##ese \" structure , which involves an underlying sentence model and a matching layer to determine the relationship between two sentences . prevailing sentence models include con ##vo ##lu ##tion ##al networks and rec ##urrent / rec ##urs ##ive networks . although they have achieved high performance , they may either fail to fully make use of the syn ##ta ##ctic ##al information in sentences or be difficult to train due to the long propagation path . recently , we propose a novel tree - based con ##vo ##lu ##tion ##al neural network ( tb ##c ##nn ) to alleviate the aforementioned problems and have achieved higher performance in two sentence classification tasks . however , it is less clear whether tb ##c ##nn can be harness ##ed to model sentence pairs for implicit logical inference , as is in the nl ##i task . in this paper , we propose the tb ##c ##nn - pair neural model to recognize en ##tail ##ment and contradiction between two sentences . we leverage our newly proposed tb ##c ##nn model to capture structural information in sentences , which is important to nl ##i . for example , the phrase \" riding bicycles on the streets \" in table [ reference ] can be well recognized by tb ##c ##nn via the dependency relations do ##b ##j ( riding , bicycles ) and prep _ on ( riding , street ) . as we can see , tb ##c ##nn is more robust than sequential con ##vo ##lu ##tion in terms of word order distortion , which may be introduced by deter ##mina ##tors , mod ##ifiers , etc . a pool ##ing layer then aggregate ##s information along the tree , serving as a way of semantic com ##po ##sit ##onal ##ity . finally , two sentences ' information is combined by several he ##uri ##stic matching layers , including con ##cate ##nation , element - wise product and difference ; they are effective in capturing relationships between two sentences , but remain low complexity . to sum up , the main contributions of this paper are two - fold : ( 1 ) we are the first to introduce tree - based con ##vo ##lu ##tion to sentence pair modeling tasks like nl ##i ; ( 2 ) lever ##aging additional he ##uri ##stic ##s further improves the accuracy while remaining low complexity , out ##per ##form ##ing existing sentence encoding - based approaches to a large extent , including feature - rich methods and long short term memory ( l ##st ##m ) - based rec ##urrent networks . section : related work en ##tail ##ment recognition can be viewed as a task of sentence pair modeling . most neural networks in this field involve a sentence - level model , followed by one or a few matching layers . they are sometimes called \" siam ##ese \" architecture ##s . cnn : ni ##ps and cnn : na ##ac ##l apply con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) as the individual sentence model , where a set of feature detectors over successive words are designed to extract local features . l ##st ##m : aaa ##i build sentence pair models upon rec ##urrent neural networks ( rn ##ns ) to it ##erative ##ly integrate information along a sentence . rec ##ur ##para ##ph ##rase dynamic ##ally construct tree structures ( analogous to par ##se trees ) by rec ##urs ##ive auto ##en ##code ##rs to detect para ##ph ##rase between two sentences . as shown , inherent structural information in sentences is often ##time ##s important to natural language understanding . the simplest approach to match two sentences , perhaps , is to con ##cate ##nate their vector representations . con ##cate ##nation is also applied in our previous work of matching the subject and object in relation classification . cnn : em ##nl ##p apply additional he ##uri ##stic ##s , namely euclidean distance , co ##sin ##e measure , and element - wise absolute difference . the above methods operate on a fixed - size vector representation of a sentence , categorized as sentence encoding - based approaches . thus the matching complexity is , i . e . , independent of the sentence length . word - by - word similarity matrices are introduced to enhance interaction . to obtain the similarity matrix , cnn : ni ##ps ( arc - ii ) con ##cate ##nate two words ' vectors ( after con ##vo ##lu ##tion ) , rec ##ur ##para ##ph ##rase compute euclidean distance , and l ##st ##m : aaa ##i apply tensor product . in this way , the complexity is of , where is the length of a sentence ; hence similarity matrices are difficult to scale and less efficient for large data ##set ##s . recently , attention introduce several context - aware methods for sentence matching . they report that rn ##ns over a single chain of two sentences are more inform ##ative than separate rn ##ns ; a static attention over the first sentence is also useful when modeling the second one . such context - awareness inter ##we ##aves the sentence modeling and matching steps . in some scenarios like sentence pair re - ranking , it is not feasible to pre - calculate the vector representations of sentences , so the matching complexity is of . attention further develop a word - by - word attention mechanism and obtain a higher accuracy with a complexity order of . section : our approach we follow the \" siam ##ese \" architecture ( like most work in section [ reference ] ) and adopt a two - step strategy to classify the relation between two sentences . concrete ##ly , our model comprises two parts : a tree - based con ##vo ##lu ##tion ##al neural network models each individual sentence ( figure [ reference ] a ) . notice that , the two sentences , premise and hypothesis , share a same tb ##c ##nn model ( with same parameters ) , because this part aims to capture general semantics of sentences . a matching layer combines two sentences ' information by he ##uri ##stic ##s ( figure [ reference ] b ) . after individual sentence models , we design a sentence matching layer to aggregate information . we use simple he ##uri ##stic ##s , including con ##cate ##nation , element - wise product and difference , which are effective and efficient . finally , we add a soft ##max layer for output . the training objective is cross - entropy loss , and we adopt mini - batch st ##och ##astic gradient descent , computed by back - propagation . sub ##section : tree - based con ##vo ##lu ##tion the tree - based con ##vo ##lu ##to ##inal neural network ( tb ##c ##nn ) is first proposed in our previous work to classify program source code ; later , we further propose tb ##c ##nn variants to model sentences . this sub ##section details the tree - based con ##vo ##lu ##tion process . the basic idea of tb ##c ##nn is to design a set of sub ##tree feature detectors sliding over the par ##se tree of a sentence ; either a constituency tree or a dependency tree applies . in this paper , we prefer the dependency tree - based con ##vo ##lu ##tion for its efficiency and compact expressive ##ness . concrete ##ly , a sentence is first converted to a dependency par ##se tree . each node in the dependency tree corresponds to a word in the sentence ; an edge indicates is governed by . edges are labeled with grammatical relations ( e . g . , ns ##ub ##j ) between the parent node and its children . words are represented by pre ##train ##ed vector representations , also known as word em ##bed ##ding ##s . now , we consider a set of two - layer sub ##tree feature detectors sliding over the dependency tree . at a position where the parent node is with child nodes , the output of the feature detector , , is let us assume word em ##bed ##ding ##s ( and ) are of dimensions ; that the con ##vo ##lu ##tion ##al layer is - dimensional . is the weight matrix ; is the bias vector . denotes the dependency relation between and . is the non - linear activation function , and we apply re ##lu in our experiments . after tree - based con ##vo ##lu ##tion , we obtain a set of feature maps , which are one - one corresponding to original words in the sentence . therefore , they may vary in size and length . a dynamic pool ##ing layer is applied to aggregate information along different parts of the tree , serving as a way of semantic composition ##ality . we use the pool ##ing operation , which takes the maximum value in each dimension . then we add a fully - connected hidden layer to further mix the information in a sentence . the obtained vector representation of a sentence is denoted as ( also called a sentence em ##bed ##ding ) . notice that the same tree - based con ##vo ##lu ##tion applies to both the premise and hypothesis . tree - based con ##vo ##lu ##tion along with pool ##ing enables structural features to reach the output layer with short propagation paths , as opposed to the rec ##urs ##ive network , which is also structure - sensitive but may suffer from the problem of long propagation path . by contrast , tb ##c ##nn is effective and efficient in learning such structural information . sub ##section : matching he ##uri ##stic ##s in this part , we introduce how vector representations of individual sentences are combined to capture the relation between the premise and hypothesis . as the data ##set is large , we prefer matching operations because of efficiency concerns . concrete ##ly , we have three matching he ##uri ##stic ##s : con ##cate ##nation of the two sentence vectors , element - wise product , and element - wise difference . the first he ##uri ##stic follows the most standard procedure of the \" siam ##ese \" architecture ##s , while the latter two are certain measures of \" similarity \" or \" closeness . \" these matching layers are further con ##cate ##nated ( figure [ reference ] b ) , given by where and are the sentence vectors of the premise and hypothesis , respectively ; \" \" denotes element - wise product ; semi ##col ##ons refer to column vector con ##cate ##nation . is the output of the matching layer . we would like to point out that , with subsequent linear transformation , element - wise difference is a special case of con ##cate ##nation . if we assume the subsequent transformation takes the form of , where is the weights for con ##cate ##nated sentence representations , then element - wise difference can be viewed as such that . ( is the weights corresponding to element - wise difference . ) thus , our third he ##uri ##stic can be absorbed into the first one in terms of model capacity . however , as will be shown in the experiment , explicitly specify ##ing this he ##uri ##stic significantly improves the performance , indicating that optimization differs , despite the same model capacity . moreover , word em ##bed ##ding studies show that linear offset of vectors can capture relationships between two words , but it has not been exploited in sentence - pair relation recognition . although element - wise distance is used to detect para ##ph ##rase in cnn : em ##nl ##p , it mainly reflects \" similarity \" information . our study ve ##ri ##fies that vector offset is useful in capturing generic sentence relationships , akin to the word analogy task . section : evaluation sub ##section : data ##set to evaluate our tb ##c ##nn - pair model , we used the newly published stanford natural language inference ( s ##nl ##i ) data ##set . the data ##set is constructed by crowds ##our ##ced efforts , each sentence written by humans . moreover , the s ##nl ##i data ##set is magnitude ##s of larger than previous resources , and hence is particularly suitable for comparing neural models . the target labels comprise three classes : en ##tail ##ment , contradiction , and neutral ( two irrelevant sentences ) . we applied the standard train / validation / test split , contra ##ining 550 ##k , 10 ##k , and 10 ##k samples , respectively . figure [ reference ] presents additional data ##set statistics , especially those relevant to dependency par ##se trees . sub ##section : hyper ##para ##meter settings all our neural layers , including em ##bed ##ding ##s , were set to 300 dimensions . the model is mostly robust when the dimension is large , e . g . , several hundred . word em ##bed ##ding ##s were pre ##train ##ed ourselves by word ##2 ##ve ##c on the english wikipedia corpus and fined tuned during training as a part of model parameters . we applied penalty of ; drop ##out was chosen by validation with a gran ##ular ##ity of 0 . 1 ( figure [ reference ] ) . we see that a large drop ##out rate ( 0 . 3 ) hurts the performance ( and also makes training slow ) for such a large data ##set as opposed to small data ##set ##s in other tasks . initial learning rate was set to 1 , and a power decay was applied . we used st ##och ##astic gradient descent with a batch size of 50 . sub ##section : performance table [ reference ] compares our model with previous results . as seen , the tb ##c ##nn sentence pair model , followed by simple con ##cate ##nation alone , out ##per ##forms existing sentence encoding - based approaches ( without pre ##train ##ing ) , including a feature - rich method using 6 groups of human - engineered features , long short term memory ( l ##st ##m ) - based rn ##ns , and traditional cnn ##s . this ve ##ri ##fies the rational ##e for using tree - based con ##vo ##lu ##tion as the sentence - level neural model for nl ##i . table [ reference ] compares different he ##uri ##stic ##s of matching . we first analyze each he ##uri ##stic separately : using element - wise product alone is significantly worse than con ##cate ##nation or element - wise difference ; the latter two are comparable to each other . combining different matching he ##uri ##stic ##s improves the result : the tb ##c ##nn - pair model with con ##cate ##nation , element - wise product and difference yields the highest performance of 82 . 1 % . as analyzed in section [ reference ] , the element - wise difference matching layer does not add to model complexity and can be absorbed as a special case into simple con ##cate ##nation . however , explicitly using such he ##uri ##stic yields an accuracy boost of 1 - 2 % . further applying element - wise product improves the accuracy by another 0 . 5 % . the full tb ##c ##nn - pair model out ##per ##forms all existing sentence encoding - based approaches , including a 102 ##4 ##d gate ##d rec ##urrent unit ( gr ##u ) - based rn ##n with \" skip - thought \" pre ##train ##ing . the results obtained by our model are also comparable to several attention - based l ##st ##ms , which are more computational ##ly intensive than ours in terms of complexity order . sub ##section : complexity concerns for most sentence models including tb ##c ##nn , the overall complexity is at least . however , an efficient matching approach is still important , especially to retrieval - and - re ##rank ##ing systems . for example , in a retrieval - based question - answering or conversation system , we can largely reduce response time by performing sentence matching based on pre ##com ##puted candidates ' em ##bed ##ding ##s . by contrast , context - aware matching approaches as described in section [ reference ] involve processing each candidate given a new user - issued query , which is time - consuming in terms of most industrial products . in our experiments , the matching part ( figure [ reference ] b ) counts 1 . 71 % of the total time during prediction ( single - cpu , c + + implementation ) , showing the potential applications of our approach in efficient retrieval of semantic ##ally related sentences . section : conclusion in this paper , we proposed the tb ##c ##nn - pair model for natural language inference . our model relies on the tree - based con ##vo ##lu ##tion ##al neural network ( tb ##c ##nn ) to capture sentence - level semantics ; then two sentences ' information is combined by several he ##uri ##stic ##s including con ##cate ##nation , element - wise product and difference . experimental results on a large data ##set show a high performance of our tb ##c ##nn - pair model while remaining a low complexity order . section : ac ##k ##now ##led ##gm ##ents we thank all anonymous reviewers for their constructive comments , especially those on complexity issues . we also thank sam bowman , edward gr ##efe ##nst ##ette , and tim [UNK] for their discussion . this research was supported by the national basic research program of china ( the 97 ##3 program ) under grant no . 2015 ##cb ##35 ##22 ##01 and the national natural science foundation of china under grant nos . 61 ##23 ##20 ##15 , 61 ##42 ##10 ##9 ##1 , and 61 ##50 ##20 ##14 . bibliography : references",
        "pred_seq": "[SEP] tb model [SEP] [SEP] natural inference [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "tbcnnpair model"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "natural language inference"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "stanford natural language inference",
                        "snli"
                    ]
                ],
                "Method": [
                    [
                        "tbcnnpair model",
                        "treebased convolutional neural network",
                        "tbcnn",
                        "tbcnnpair neural model",
                        "tbcnn model",
                        "treebased convolutoinal neural network",
                        "tbcnn sentence pair model"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "natural language inference",
                        "nli",
                        "nli task"
                    ]
                ]
            }
        ]
    },
    "16": {
        "doctext": "document : grounded textual en ##tail ##ment capturing semantic relations between sentences , such as en ##tail ##ment , is a long - standing challenge for computational semantics . logic - based models anal ##yse en ##tail ##ment in terms of possible worlds ( interpretations , or situations ) where a premise p en ##tails a hypothesis h if ##f in all worlds where p is true , h is also true . statistical models view this relationship pro ##ba ##bilis ##tically , addressing it in terms of whether a human would likely in ##fer h from p . in this paper , we wish to bridge these two perspectives , by arguing for a visually - grounded version of the textual en ##tail ##ment task . specifically , we ask whether models can perform better if , in addition to p and h , there is also an image ( corresponding to the relevant \" world \" or \" situation \" ) . we use a multi ##mo ##dal version of the s ##nl ##i data ##set and we compare \" blind \" and visually - augmented models of textual en ##tail ##ment . we show that visual information is beneficial , but we also conduct an in - depth error analysis that reveals that current multi ##mo ##dal models are not performing \" ground ##ing \" in an optimal fashion . section : introduction correspondence should be addressed to raf ##fa ##ella bernard ##i ( ) and albert ga ##tt ( albert . ga ##tt @ um . ed ##u . mt ) . this work is licensed under a creative commons at ##tri ##bution 4 . 0 international license . license details : . the data ##set , ann ##ota ##tion and code is available from . evaluating the ability to in ##fer information from a text is a crucial test of the capability of models to grasp meaning . as a result , the computational linguistics community has invested huge efforts into developing textual en ##tail ##ment ( te ) data ##set ##s . after formal semantic ##ists developed fra ##cas in the mid ' 90 , an increase in statistical approaches to computational semantics gave rise to the need for suitable evaluation data ##set ##s . hence , recognizing textual en ##tail ##ment ( rte ) shared tasks have been organized regularly . recent work on composition ##al distribution ##al models has motivated the development of the sick data ##set of sentence pairs in en ##tail ##ment relations for evaluating such models . further advances with neural networks ( n ##ns ) have once more motivated efforts to develop a large natural language inference data ##set , s ##nl ##i , since n ##ns need to be trained on big data . however , meaning is not something we obtain just from text and the ability to reason is not un ##imo ##dal either . the importance of en ##rich ##ing meaning representations with other mod ##ali ##ties has been advocated by cognitive scientists , ( e . g . , ) and computational linguist ##s ( e . g . , ) . while efforts have been put into developing multi ##mo ##dal data ##set ##s for the task of checking semantic text similarity text , we are not aware of any available data ##set ##s to tackle the problem of grounded textual en ##tail ##ment ( gt ##e ) . our paper is a first effort in this direction . [ b ] 0 . 45 [ scale = 0 . 5 ] figures / warm - en ##tail ##ment [ b ] 0 . 45 [ scale = 0 . 5 ] figures / neutral - mis textual en ##tail ##ment is defined in terms of the likelihood of two sentences ( a premise p and an hypothesis h ) to be in a certain relation : p en ##tails , contra ##dict ##s or is unrelated to h . for instance , the premise \" people trying to get warm in front of a chimney \" and the hypothesis \" people trying to get warm at home \" are highly likely to be in an en ##tail ##ment relation . our question is whether having an image that illustrates the event ( e . g . , figure [ reference ] ) can help a model to capture the relation . in order to answer this question , we aug ##ment the largest available te data ##set with images , we enhance a state of the art model of textual en ##tail ##ment to take images into account and we evaluate it against the gt ##e task . the inclusion of images can also alter relations which , based on text alone , would seem likely . for example , to a \" blind \" model the sentences of the sentence pair in figure [ reference ] would seem to be unrelated , but when the two sentences are viewed in the context of the image , they do become related . a suitable gt ##e model therefore has to perform two sub - tasks : ( a ) it needs to ground its linguistic representations of p , h or both in non - linguistic ( visual ) data ; ( b ) it needs to reason about the possible relationship between p and h ( mod ##ulo the visual information ) . section : related work ground ##ing language through vision has recently become the focus of several tasks , including image capt ##ion ##ing ( ic , e . g . ) and visual question answering ( v ##qa , e ##g . ) , and even more recently , visual reasoning and visual dial ##og . our focus is on grounded textual en ##tail ##ment ( gt ##e ) . while the literature on te is rather vast , gt ##e is still rather une ##x ##pl ##ored territory . paragraph : textual en ##tail ##ment throughout the history of computational linguistics various data ##set ##s have been built to evaluate computational semantics models on the te task . usually they contain data divided into en ##tail ##ment , contradiction or unknown classes . the \" unknown \" label has sometimes been replaced with the \" unrelated \" or \" neutral \" label , capturing slightly different types of phenomena . interesting ##ly , the \" en ##tail ##ment \" and \" contradiction \" classes also differ across data ##set ##s . in the mid - ' 90s a group of formal semantic ##ists developed fra ##cas ( framework for computational semantics ) . the data ##set contains logical en ##tail ##ment problems in which a conclusion has to be derived from one or more premises ( but not necessarily all premises are needed to verify the en ##tail ##ment ) . the en ##tail ##ments are driven by logical properties of linguistic expressions , like the mono ##tonic ##ity of quan ##ti ##fi ##ers , or their con ##ser ##vati ##vity property etc . hence , the set of premises en ##tails the conclusion if ##f in all the interpretations ( worlds ) in which the premises are true the conclusion is also true ; otherwise the conclusion contra ##dict ##s the premises . in 2005 , the pascal rte ( recognizing textual en ##tail ##ment ) challenge was launched , to become a task organized annually . in 2008 , the rte - 4 committee made the task more fine - grain ##ed by requiring the classification of the pairs as \" en ##tail ##ment \" , \" contradiction \" and \" unknown \" . the rte data ##set ##s , unlike fra ##cas , contain real - life natural language sentences and the sort of en ##tail ##ment problems which occur in corp ##ora collected from the web . importantly , the sentence pair relations are ann ##ota ##ted as en ##tail ##ment , contradiction or neutral based on a likelihood condition : if a human reading the premise would typically in ##fer that the conclusion ( called the hypothesis ) is most likely true ( en ##tail ##ment ) , its ne ##gation is most likely true ( contradiction ) or the conclusion can be either true or false ( neutral ) . at se ##me ##val 2014 , in order to evaluate composition ##al distribution ##al semantics models focusing on the composition ##ality ability of those models , the sick data ##set ( sentences involving composition ##al knowledge ) was used in a shared en ##tail ##ment task . sentence pairs were obtained through re - writing rules and ann ##ota ##ted with the three rte labels via a crowds ##our ##cing platform . both in rte and sick the label assigned to the sentence pairs captures the relation holding between the two sentences . a different approach has been used to build the much larger s ##nl ##i ( stanford natural language inference ) data ##set : premises are taken from a data ##set of images ann ##ota ##ted with descriptive capt ##ions ; the corresponding h ##yp ##oth ##eses were produced through crowds ##our ##cing , where for a given premise , ann ##ota ##tors provided a sentence which is true or not true with respect to a possible image which the premise could describe . a consequence of this choice is that the contradiction relation can be assigned to pairs which are rather unrelated ( \" a person in a black wet ##suit is surfing a small wave \" and \" a woman is trying to sleep on her bed \" ) , differently from what happens in rte and sick . since the inception of rte shared tasks , there has been an increasing emphasis on data - driven approaches which , given the hypothesis h and premise p , seek to classify the semantic relation ( see for a review ) . more recently , neural approaches have come to dominate the scene , as shown by the recent rep ##eva ##l 2017 task , where all submissions relied on bid ##ire ##ction ##al l ##st ##m models , with or without pre ##train ##ed em ##bed ##ding ##s . rte also intersects with a number of related inference problems , including semantic text similarity and question answering , and some models have been proposed to address several such problems . in one popular approach , both p and h are encoded within the same em ##bed ##ding space , using a single rn ##n , with a decision made based on the encoding ##s of the two sentences . this is the approach we adopt for our baseline l ##st ##m in section [ reference ] , based on the model proposed by s ##nl ##i : em ##nl ##p ##20 ##15 , albeit with some modifications ( see also ) . a second promising approach , based on which we adapt our state of the art model , relies on matching and aggregation . here , the decision concerning the relationship between p and h is based on an aggregate representation achieved after the two sentences are matched . yet another area where neural approaches are being applied to sentence pairs in an en ##tail ##ment relationship is generation , where an rn ##n generates an en ##tail ##ed hypothesis ( or a chain of such h ##yp ##oth ##eses ) given an encoding of the premise . paragraph : vision and textual en ##tail ##ment in recent years , several models have been proposed to integrate the language and vision mod ##ali ##ties ; usually the integration is operational ##ized by element - wise multiplication between linguistic and visual vectors . though the interest in these mod ##ali ##ties has spread in an astonishing way thanks to various multi ##mo ##dal tasks proposed , including the ic , v ##qa , visual reasoning and visual dialogue tasks mentioned above , very little work has been done on ground ##ing en ##tail ##ment . interesting ##ly , you ##n : from ##14 has proposed the idea of considering images as the \" possible worlds \" on which sentences find their den ##ota ##tion . hence , they released a \" visual den ##ota ##tion graph \" which associates sentences with their den ##ota ##tion ( sets of images ) . the idea has been further exploited by lai : lear ##17 and han : vis ##d ##17 . ve ##nd : or ##de ##16 look at hyper ##ny ##my , textual en ##tail ##ment and image capt ##ion ##ing as special cases of a single visual - semantic hierarchy over words , sentences and images , and they claim that modelling the partial order structure of this hierarchy in visual and linguistic semantic spaces improves model performance on those three tasks . we share with this work the idea that the image can be taken as a possible world . however , we do n ' t use sets of images to obtain the visual den ##ota ##tion of text in order to check whether en ##tail ##ment is logical ##ly valid / highly likely . rather , we take the image to be the world / situation in which the text finds its interpretation . the only work that is close to ours is an unpublished student report , which however lacks the in - depth analysis presented here . section : ann ##ota ##ted data ##set of images and sentence pairs we took as our starting point the stanford natural language inference ( s ##nl ##i ) data ##set , the largest natural language inference data ##set available with sentence pairs labelled with en ##tail ##ment , contradiction and neutral relations . we augmented this data ##set with images . it has been shown very recently that s ##nl ##i contains language bias , such that a simple class ##ifier can achieve high accuracy in predicting the three classes just by having as input the hypothesis sentence . a subset of the s ##nl ##i test set with ' hard ' cases , where such a sim ##pl ##istic class ##ifier fails ( here ##af ##ter s ##nl ##i ) has been released . hence , in this paper we will report our results on both the full data ##set and the hard test set , but then zoom in on s ##nl ##i to understand the models ' behaviour . we briefly introduce s ##nl ##i and the new test set and compare them through our ann ##ota ##tion of linguistic phenomena . sub ##section : data ##set construction paragraph : s ##nl ##i and s ##nl ##i test set the s ##nl ##i data ##set was built through amazon mechanical turk . workers were shown capt ##ions of photographs without the photo and were asked to write a new capt ##ion that ( a ) is definitely a true description of the photo ( en ##tail ##ment ) ; ( b ) might be a true description of the photo ( neutral ) ; ( c ) is definitely a false description of the photo ( contradiction ) . examples were provided for each of the three cases . the premises are capt ##ions which come mostly from flick ##r ##30 k ; only 4 k capt ##ions are from visual ##gen ##ome . in total , the data ##set contains 570 , 152 sentence pairs , balanced with respect to the three labels . around 10 % of these data have been valid ##ated ( 4 ann ##ota ##tors for each example plus the label assigned through the previous data collection phase ) . the development and test data ##set ##s contain 10 k examples each . moreover , each image / flick ##r capt ##ion occurs in only one of the three sets , and all the examples in the development and test sets have been valid ##ated . paragraph : v - s ##nl ##i and v - s ##nl ##i test set our grounded version of s ##nl ##i , v - s ##nl ##i , has been built by matching each sentence pair in s ##nl ##i with the corresponding image coming from the flick ##r ##30 k data ##set ; thus the v - s ##nl ##i data ##set is slightly smaller than the original , which also contains capt ##ions from visual ##gen ##ome . v - s ##nl ##i consists of 56 ##5 , 286 pairs ( 187 , 96 ##9 neutral , 188 , 45 ##3 contradiction , and 188 , 86 ##4 en ##tail ##ment ) . training , test , and development splits have been built according to the splits in s ##nl ##i . the main statistics of the splits of the data ##set are reported in table [ reference ] together with statistics for the visual counterpart of hard s ##nl ##i , namely v - s ##nl ##i . by construction , v - s ##nl ##i contains data ##points such that the premise is always true with respect to the image , whereas the hypothesis can be either true ( en ##tail ##ment or neutral cases ) or false ( contradiction or neutral cases . ) sub ##section : data ##set ann ##ota ##tion for deeper analysis and comparison of the contents of s ##nl ##i and s ##nl ##i , we have ann ##ota ##ted the s ##nl ##i data ##set by both automatically detecting some surface linguistic cues and manually label ##ling less trivial phenomena . using an in - house ann ##ota ##tion interface , we collected human judgments aiming to ( a ) filter out those cases for which the gold - standard ann ##ota ##tion was considered to be wrong ; ( b ) connect the three un ##ground ##ed relations to various linguistic phenomena . to achieve this , we ann ##ota ##ted a random sample of the s ##nl ##i test set containing 52 ##7 sentence pairs ( 185 en ##tail ##ment , 171 contradiction , 171 neutral ) , out of which 176 were from the hard test set ( 56 en ##tail ##ment , 62 contradiction , 58 neutral ) . all the pairs were ann ##ota ##ted by at least two ann ##ota ##tors , as follows : ( a ) we filtered out all the pairs which had a wrong gold label ( see table [ reference ] for details ) . when our ann ##ota ##tors did not agree whether a given relation holds for a specific pair , we appealed to the corresponding five judgments coming from the validation stage of the s ##nl ##i data ##set to reach a consensus based on the majority of labels . ( b ) we considered as valid any linguistic tag assigned by at least one ann ##ota ##tor . since the ann ##ota ##tion for ( a ) is binary whereas for ( b ) it is multi - class , we used cohen ' s for the former and also scott ' s and k ##rip ##pen ##dorff ' s for the latter as suggested by pass ##onne ##au pass : int ##e ##0 ##6 . the inter - ann ##ota ##tor agreement for the relation type ( a ) was ; for ( b ) linguistic tags it was , , and . paragraph : linguistic phenomena following the error analysis approach described in recent work , we compiled a new list of linguistic features that can be of interest when contrasting s ##nl ##i and s ##nl ##i , as well as for evaluating rte models . some of these were detected automatically , while others were assigned manually . automatic tags included synonym and antony ##m , which were detected using word ##net . quan ##ti ##fi ##er , pro ##no ##un , di ##ff tense , super ##lative and bare np were identified using penn tree ##bank labels , while labels such as ne ##gation were found with a straightforward key ##word search . the tag long has been assigned to sentence pairs with a premise containing more than 30 token ##s , or a hypothesis with more than 16 token ##s . details about the tags used in the manual ann ##ota ##tion are presented in table [ reference ] . we examined the differences in the tags distributions between the s ##nl ##i and s ##nl ##i test sets ( table [ reference ] ) . interesting ##ly , the hard sentence pairs from our random sample include proportion ##ately more antony ##ms but fewer pronouns , as well as examples with different verb tense ##s in the premise and hypothesis , compared to the full test set . furthermore , s ##nl ##i contains a significantly larger proportion of gold - standard labels which become wrong when the image is factor ##ed in ( - test with ) . section : models in this section , we describe a variety of models that were compared on both v - s ##nl ##i and v - s ##nl ##i , ranging from baseline models based on s ##nl ##i : em ##nl ##p ##20 ##15 to a state of the art model by wang ##20 ##17 . we compare the original ' blind ' version of a model with a visually - augmented counterpart . in what follows , we use p and h to refer to a premise and hypothesis , respectively . paragraph : l ##st ##m baseline ( blind ) this model exploits a rec ##urrent neural network with long short - term memory units to en ##code both p and h in 512 ##d vectors . the two vectors are then con ##cate ##nated in a stack of three 512 ##d layers having a re ##lu activation function , with a final soft ##max layer to classify the relation between the two sentences as en ##tail ##ment , contradiction or neutral . the model is inspired by the l ##st ##m baseline proposed by s ##nl ##i : em ##nl ##p ##20 ##15 . the model exploits the 300 , 000 most frequent pre ##train ##ed glove em ##bed ##ding ##s and improves them during the training process . to regular ##ize the model , drop ##out is applied to the inputs and outputs of the rec ##urrent layers and to the re ##lu fully connected layers with a keeping probability of 0 . 5 . the model is trained using the adam opt ##imi ##zer with a learning rate of 0 . 001 until its accuracy on the development set drops for three successive iteration ##s . paragraph : v - l ##st ##m baseline the l ##st ##m model described above is augmented with a visual component following a standard visual question answering baseline model . following initial representation of p and h in 512 ##d vectors through an l ##st ##m , a fully - connected layer projects the l ##2 - normal ##ized 40 ##9 ##6 ##d image vector coming from the penultimate layer of a v ##gg ##net ##16 con ##vo ##lu ##tion ##al neural network to a reduced 512 ##d vector . a fully - connected layer with a re ##lu activation function is also applied to p and h to obtain two 512 ##d vectors . the multi ##mo ##dal fusion between the text and the image is obtained by performing an element - wise multiplication between the vector of the text representation and the reduced vector of the image . the multi ##mo ##dal fusion is performed between the image and both the premise and the hypothesis , resulting in two multi ##mo ##dal representations . the relation between them is captured as in the model described above . this model uses glove em ##bed ##ding ##s and the same optimization and procedure described above . we have also adapted a state - of - the - art attention - based model for ic and v ##qa to the gt ##e task . it obtain ##s results comparable to the v - l ##st ##m . this lack of improvement might be due to the need of further parameter tuning . we report the details of our implementation and its results in the supplementary material . paragraph : bi ##mp ##m the bilateral multi - perspective matching ( bi ##mp ##m ) model obtain ##s state - of - the - art performance on the s ##nl ##i data ##set , achieving a maximum accuracy of 86 . 9 % , and going up to 88 . 8 % in an ensemble setup . an initial em ##bed ##ding layer vector ##ises words in p and h using pre ##train ##ed glove em ##bed ##ding ##s , and passing them to a context representation layer , which uses bid ##ire ##ction ##al l ##st ##ms ( bi ##ls ##tm ##s ) to en ##code context vectors for each time - step . the core part of the model is the subsequent matching layer , where each context ##ual em ##bed ##ding or time - step of p is matched against all the em ##bed ##ding ##s of h , and vice versa . the output of this layer is composed of two sequences of matching vectors , which constitute the input to another bi ##ls ##tm at the aggregation layer . the vectors from the last time - step of the bi ##ls ##tm are con ##cate ##nated into a fixed - length vector , which is passed to the final prediction tier , a two - layer feed - forward network which class ##ifies the relation between p and h via soft ##max . matching is performed via a co ##sin ##e operation , which yields an - dimensional vector , where is the number of perspectives . wang ##20 ##17 experiment with four different matching strategies . in their results , the best - performing version of the bi ##mp ##m model used all four matching strategies . we adopt this version of the model in what follows . paragraph : v - bi ##mp ##m model we enhanced bi ##mp ##m to account for the image , too . our version of this model is referred to as the v - bi ##mp ##m . here , the feature vector for an image is obtained from the layer before the fully - connected layer of a v ##gg ##net - 16 . this results in a tensor , which we consider as 49 512 - dimensional vectors . the same matching operations are performed , except that matching occurs between p , h , and the image . since the textual and visual vectors have different dimensional ##ity and belong to different spaces , we first map them to a mutual space using an af ##fine transformation . we match textual and image vectors using a co ##sin ##e operation , as before . full details of the model are reported in the supplementary materials for this paper . section : experiments and results the models described in the previous sections were evaluated on both ( v - ) s ##nl ##i and ( v - ) s ##nl ##i . for the visually - augmented models , we experimented with configurations where image vectors were combined with both p and h ( namely p + i and h + i ) , or only with h ( p and h + i ) . the best setting was invariably the one where only h was grounded ; hence , we focus on these results in what follows , comparing them to \" blind \" models . in view of recent results suggesting that bias ##es in s ##nl ##i afford a high accuracy in the prediction task with only the hypothesis sentence as input , we also include results for the blind models without the premise ( denoted with [ h ] in what follows ) . table [ reference ] shows the results of the various models on the full v - s ##nl ##i data ##set . the same models are compared in table [ reference ] on v - s ##nl ##i . first , note that the l ##st ##m [ h ] model ev ##in ##ces a drop in performance compared to l ##st ##m ( from 81 . 49 % to 68 . 49 % ) , though the drop is much greater on the un ##bia ##sed s ##nl ##i subset ( from 60 . 99 to 25 . 57 % ) . this confirms the results reported by guru : ann ##o ##18 and just ##ifies our additional focus on this subset of the data . the effect of ground ##ing in these models is less clear . the l ##st ##m baseline performs worse when it is visually augmented ; this is the case of v - s ##nl ##i and , even more drastically , v - s ##nl ##i . it is also true ir ##res ##pe ##ctive of the relationship type . on the other hand , the v - bi ##mp ##m model improves marginal ##ly across the board , compared to bi ##mp ##m , on the v - s ##nl ##i data . on the hard subset , the images appear to hurt performance somewhat in the case of contradiction ( from 77 . 62 % to 76 . 12 % ) , but improve it by a substantial margin on neutral cases ( from 59 . 36 % to 63 . 67 % ) . the neutral case is the hardest for all models , with the possible exception of l ##st ##m [ h ] on the full data ##set . overall , the results suggest that factor ##ing in images either hind ##ers performance ( as in the case of the v - l ##st ##m baseline ) , or helps only marginal ##ly ( as in the case of v - bi ##mp ##m ) . in the latter case , we also observe instances where factor ##ing in images hurts performance . in an effort to understand the results , we turn to a more detailed error analysis of the v - bi ##mp ##m model , first in relation to the data ##set ann ##ota ##tions , and then by zoom ##ing in somewhat closer on v - s ##nl ##i . sub ##section : error analysis by linguistic ann ##ota ##tion label in table [ reference ] , acc ##ura ##cies for the blind and grounded version of bi ##mp ##m are broken down by the labels given to the sentence pairs in the ann ##ota ##ted subset of s ##nl ##i described in section [ reference ] . we only observe a significant difference in the entity case , that is , where the refer ##ents in p and h are inconsistent . here , the blind model out ##per ##forms the grounded one , an unexpected result , since one would assume a grounded model to be better equipped to identify mis ##mat ##ched refer ##ents . hence , in the following we aim to understand whether the models properly deal with the ground ##ing sub - task . sub ##section : error analysis on ground ##ing in the s ##nl ##i we next turn to the \" hard \" subset of the data , where v - bi ##mp ##m showed some improvement over the blind case , but suffered on contradiction cases ( table [ reference ] ) . we anal ##yse ##d the 207 cases in s ##nl ##i where the v - bi ##mp ##m made incorrect predictions compared to the blind model , that is , where the image hurt performance . these were ann ##ota ##ted independently by two of the authors ( raw inter - ann ##ota ##tor agreement : 96 % ) who ( a ) read the two sentences , p and h ; ( b ) checked whether the relation ann ##ota ##ted in the data ##set actually held or whether it was an ann ##ota ##tion error ; ( c ) in those cases where it held , checked whether including the image actually resulted in a change in the relation . table [ reference ] displays the proportions of image mis ##mat ##ch and incorrect ann ##ota ##tions . as the table suggests , in the cases where images hind ##er performance in the v - bi ##mp ##m , it is usually because the image changes the relation ( thus , these are cases of image mis ##mat ##ch ; see section [ reference ] for an example ) ; this occurs in a large proportion of cases labelled as neutral in the data ##set . inspired by the work in , we further explored the impact of visual ground ##ing in both the v - l ##st ##m and v - bi ##mp ##m by comparing their performance on s ##nl ##i , with the same subset incorporating image \" foil ##s \" . vectors for the images in the v - s ##nl ##i test set were compared pair ##wise using co ##sin ##e , and for each test case in v - s ##nl ##i , the actual image was replaced with the most di ##ssi ##mi ##lar image in the full test set . the rational ##e is that , if visual ground ##ing is really helpful in rec ##og ##nis ##ing the semantic relationship between p and h , we should observe a drop in performance when the images are unrelated to the scenario described by the sentences . the results are displayed in table [ reference ] , which also reproduce ##s the original results on v - s ##nl ##i from table [ reference ] for ease of reference . as the results show , models are not hurt by the foil image , contrary to our expectations . v - bi ##mp ##m overall drops just by 0 . 67 % whereas v - l ##st ##m drop is somewhat higher ( - 2 . 11 % ) showing it might be doing a better job on the ground ##ing sub - task . as a final check , we sought to isolate the ground ##ing from the reasoning sub - task , focusing only on the former . we compared the models when ground ##ing only the hypothesis [ h + i ] , while leaving out the premise . note that this test is different from the evaluation of the model using only the hypothesis [ h ] : whereas in that case the input is not expected to provide any useful information to perform the task , here it is . as we noted in section [ reference ] , by construction the premise is always true with respect to the image while the hypothesis can be either true ( en ##tail ##ment or neutral cases ) or false ( contradiction or neutral cases ) . a model that is ground ##ing the text adequately would be expected to confuse both en ##tail ##ment and contradiction cases with neutral ones ; on the other hand , neutral cases should be confused with en ##tail ##ments or contradiction ##s . confusing contradiction ##s with en ##tail ##ments would be a sign that a model is ground ##ing inadequate ##ly , since it is not rec ##og ##nis ##ing that h is false with respect to the image . as the left panel of table [ reference ] shows , v - bi ##mp ##m out ##per ##forms v - l ##st ##m by a substantial margin , though the performance of both models drops substantially with this setup . the right panel in the table shows that neither model is free of imp ##laus ##ible errors ( confusing en ##tail ##ments and contradiction ##s ) , though v - bi ##mp ##m makes substantially fewer of these . section : conclusion this paper has investigated the potential of ground ##ing the textual en ##tail ##ment task in visual data . we argued that a grounded textual en ##tail ##ment model needs to perform two tasks : ( a ) the ground ##ing itself , and ( b ) reasoning about the relation between the sentences , against the visual information . our results suggest that a model based on matching and aggregation like the bi ##mp ##m model can perform very well at the reasoning task , classify ##ing en ##tail ##ment relations correctly much more frequently than a baseline v - l ##st ##m . on the other hand , it is not clear that ground ##ing is being performed adequately in this model . it is primarily in the case of contradiction ##s that the image seems to play a direct role in bias ##ing the classification towards the right or wrong class , depending on whether the image is correct . in summary , two conclusions can be drawn from these results . first , in those cases where the inclusion of visual information results in a loss of accuracy , this is often due to the image resulting in a change in the original relation ann ##ota ##ted in the data ##set . a related observation is that using foil images results in a greater drop in performance on contradiction cases , possibly because in such cases , ground ##ing serves to identify a mis ##mat ##ch between the hypothesis and the scene described by the premise , a situation which is rendered opaque by the introduction of foil ##s . second , in those cases where improvements are observed in the state of the art v - bi ##mp ##m , the precise role played by the image is not straightforward . indeed , we find that this model still marginal ##ly out ##per ##forms the ' blind ' , text - only model overall , when the images involved are foil ##s rather than actual images . we believe that further research on grounded te is worthy of the nl ##p community ' s attention . while linking language with perception is currently a topical issue , there has been relatively little work on linking ground ##ing directly with inference . by drawing closer to a joint solution to the ground ##ing and inference tasks , models will also be better able to address language understanding in the real world . the present paper presented a first step in this direction using a version of an existing te data ##set which was augmented with images that could be paired directly with the premises , since these were originally capt ##ions for those images . however , it is important to note that in this data ##set premise - h ##yp ##oth ##eses pairs were not generated directly with reference to the images themselves . an important issue to consider in future work on gt ##e , besides the development of better models , is the development of data ##set ##s in which the role of per ##ce ##pt ##ual information is controlled , ensuring that the data on which models are trained represents truly grounded inference ##s . section : acknowledge ##ments we kindly acknowledge the european network on integrating vision and language ( iv & l net ) ict cost action ic ##13 ##0 ##7 . moreover , we thank the erasmus mu ##nd ##us european program in language and communication technology . marc tan ##ti ' s work is partially funded by the endeavour scholarship scheme ( malta ) , part - financed by the european union ' s european social fund ( es ##f ) . finally , we grateful ##ly acknowledge the support of n ##vid ##ia corporation with the donations to the university of trent ##o of the gp ##us used in our research . section : appendix a : bottom - up top - down attention ( v ##qa ) we adapted the visual question answering model proposed in to the grounded textual en ##tail ##ment task . the model presents a more fine - grain ##ed attention mechanism which allows to identify the most important regions discovered in the image and to perform attention over each of them . the model uses a a rec ##urrent neural network with long short - term memory units to en ##code the premise p and hypothesis h in 512 ##d vectors . a bottom - up attention mechanism exploits a fast r - cnn based on a res ##net - 101 con ##vo ##lu ##tion ##al neural network to obtain region proposals corresponding to the 36 most inform ##ative regions of the image . a top - down attention mechanism is used between the premise ( res ##p . hypothesis ) and each of the l ##2 - normal ##ized 204 ##8 ##d image vectors corresponding to the region proposals to obtain an attention score for each of them . then , a 204 ##8 ##d image vector encoding the most interesting visual features for the premise ( hypothesis ) is obtained as a sum of the 36 image vectors weighted by the corresponding attention scores for the premise ( hypothesis ) . a fully - connected layer with a gate ##d tan ##h activation function is applied to the image vector of the most interesting visual features for the premise and for the hypothesis to obtain a reduced 512 ##d vector for each of them . a fully - connected layer with a gate ##d tan ##h activation function is also applied to the premise and to the hypothesis in order to obtain a reduced 512 ##d vector for each of them . the multi ##mo ##dal fusion between the premise ( hypothesis ) and the image vector of the most interesting visual features for the premise ( hypothesis ) is obtained by performing an element - wise multiplication between the reduced vector of the premise ( hypothesis ) and the reduced vector of the most interesting visual features for the premise ( hypothesis ) . after that , the model feeds the con ##cate ##nation of the two resulting multi ##mo ##dal representations to a stack of three 512 ##d layers having a gate ##d tan ##h activation function , with a final soft ##max layer to classify the relation between the two sentences as en ##tail ##ment , contradiction or neutral . this model uses glove em ##bed ##ding ##s and the same optimization tricks and procedure of the l ##st ##m and v - l ##st ##m models . we report the acc ##ura ##cies of the v ##qa models against the various tests reported in the paper . for ease of comparison we reproduce the full table from the main paper , with the addition of the v ##qa results . section : appendix b : v - bi ##mp ##m model details arrows , cal ##c , fit positioning , shapes . multi ##par ##t , shapes . call ##outs decorations . path ##re ##pl ##acing hbo ##x = [ rec ##tangle , minimum width = 7 ##pt , minimum height = 25 ##pt ] v ##box = [ rec ##tangle , fill = black ! 25 , minimum width = 35 ##pt , minimum height = 10 ##pt ] blue hbo ##x = [ hbo ##x , fill = blue ] dots hbo ##x = [ hbo ##x ] green hbo ##x = [ hbo ##x , fill = green ] green v ##box = [ v ##box , fill = green ] tb ##ox = [ rec ##tangle , minimum width = 45 ##pt , minimum height = 20 ##pt , draw = black , text centered , text = black ] red arrow = [ - \u00bf , red ] match arrow = [ - \u00bf , black ] red matching = [ text = red , inner sep = 0 ] black matching = [ text = black , inner sep = 0 ] context container / . style = draw = orange , rec ##tangle call ##out , inner sep = 0 . 6 ##em 0 . 95 ! [ scale = 0 . 2 ] [ ] ( prem _ text ) at ( 10 , - 7 ) premise ; [ ] ( h ##yp ##o _ text ) at ( 35 , - 7 ) hypothesis ; [ blue hbo ##x , label = below : ] ( w - 1 ) at ( 0 , 0 ) ; [ blue hbo ##x , right of = w - 1 , node distance = 0 . 7 cm , label = below : ] ( w - 2 ) ; [ dots hbo ##x , right of = w - 2 , node distance = 0 . 7 cm , label = below : \u2026 ] ( w - 3 ) \u2026 ; [ blue hbo ##x , right of = w - 3 , node distance = 0 . 7 cm , label = below : ] ( w - 4 ) ; [ dots hbo ##x , right of = w - 4 , node distance = 0 . 7 cm , label = below : \u2026 ] ( w - 5 ) \u2026 ; [ blue hbo ##x , right of = w - 5 , node distance = 0 . 7 cm , label = below : ] ( w - 6 ) ; [ blue hbo ##x , right of = w - 6 , node distance = 1 . 5 ##cm * 1 . 2 , label = below : ] ( w - 7 ) ; [ blue hbo ##x , right of = w - 7 , node distance = 0 . 7 cm , label = below : ] ( w - 8 ) ; [ dots hbo ##x , right of = w - 8 , node distance = 0 . 7 cm , label = below : \u2026 ] ( w - 9 ) \u2026 ; [ blue hbo ##x , right of = w - 9 , node distance = 0 . 7 cm , label = below : ] ( w - 10 ) ; [ dots hbo ##x , right of = w - 10 , node distance = 0 . 7 cm , label = below : \u2026 ] ( w - 11 ) \u2026 ; [ blue hbo ##x , right of = w - 11 , node distance = 0 . 7 cm , label = below : ] ( w - 12 ) ; [ inner sep = 0 ##pt , right of = w - 12 , node distance = 1 . 5 ##cm * 2 . 15 ] ( image ) [ width = . 10 ] figures / cat . jp ##g ; [ blue hbo ##x ] ( c - 1 ) at ( 0 , 1 . 5 ##cm * 7 ) ; [ blue hbo ##x , right of = c - 1 , node distance = 0 . 7 cm ] ( c - 2 ) ; [ dots hbo ##x , right of = c - 2 , node distance = 0 . 7 cm ] ( c - 3 ) \u2026 ; [ blue hbo ##x , right of = c - 3 , node distance = 0 . 7 cm ] ( c - 4 ) ; [ dots hbo ##x , right of = c - 4 , node distance = 0 . 7 cm ] ( c - 5 ) \u2026 ; [ blue hbo ##x , right of = c - 5 , node distance = 0 . 7 cm ] ( c - 6 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 1 ) - ( c - 2 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 2 ) - ( c - 3 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 3 ) - ( c - 4 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 4 ) - ( c - 5 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 5 ) - ( c - 6 ) ; [ blue hbo ##x , above of = c - 1 , node distance = 1 . 5 ##cm / 1 . 2 ] ( c - 1 ##u ) ; [ blue hbo ##x , right of = c - 1 ##u , node distance = 0 . 7 cm ] ( c - 2 ##u ) ; [ dots hbo ##x , right of = c - 2 ##u , node distance = 0 . 7 cm ] ( c - 3 ##u ) \u2026 ; [ blue hbo ##x , right of = c - 3 ##u , node distance = 0 . 7 cm ] ( c - 4 ##u ) ; [ dots hbo ##x , right of = c - 4 ##u , node distance = 0 . 7 cm ] ( c - 5 ##u ) \u2026 ; [ blue hbo ##x , right of = c - 5 ##u , node distance = 0 . 7 cm ] ( c - 6 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 2 ##u ) - ( c - 1 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 3 ##u ) - ( c - 2 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 4 ##u ) - ( c - 3 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 5 ##u ) - ( c - 4 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 6 ##u ) - ( c - 5 ##u ) ; [ red arrow , short ##en \u00bf = 10 ##pt , short ##en \u00a1 = - 3 ##pt ] ( w - 3 ) - ( c - 3 ) ; [ inner sep = 0 ] ( p - call ##out - pointer ) at ( ) ; [ context container , call ##out absolute pointer = ( p - call ##out - pointer ) , fit = ( c - 1 ) ( c - 6 ##u ) ] ( premise - call ##out ) ; [ blue hbo ##x , right of = c - 6 , node distance = 1 . 5 ##cm * 1 . 2 ] ( c - 7 ) ; [ blue hbo ##x , right of = c - 7 , node distance = 0 . 7 cm ] ( c - 8 ) ; [ dots hbo ##x , right of = c - 8 , node distance = 0 . 7 cm ] ( c - 9 ) \u2026 ; [ blue hbo ##x , right of = c - 9 , node distance = 0 . 7 cm ] ( c - 10 ) ; [ dots hbo ##x , right of = c - 10 , node distance = 0 . 7 cm ] ( c - 11 ) \u2026 ; [ blue hbo ##x , right of = c - 11 , node distance = 0 . 7 cm ] ( c - 12 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 7 ) - ( c - 8 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 8 ) - ( c - 9 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 9 ) - ( c - 10 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 10 ) - ( c - 11 ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 11 ) - ( c - 12 ) ; [ blue hbo ##x , right of = c - 6 ##u , node distance = 1 . 5 ##cm * 1 . 2 ] ( c - 7 ##u ) ; [ blue hbo ##x , right of = c - 7 ##u , node distance = 0 . 7 cm ] ( c - 8 ##u ) ; [ dots hbo ##x , right of = c - 8 ##u , node distance = 0 . 7 cm ] ( c - 9 ##u ) \u2026 ; [ blue hbo ##x , right of = c - 9 ##u , node distance = 0 . 7 cm ] ( c - 10 ##u ) ; [ dots hbo ##x , right of = c - 10 ##u , node distance = 0 . 7 cm ] ( c - 11 ##u ) \u2026 ; [ blue hbo ##x , right of = c - 11 ##u , node distance = 0 . 7 cm ] ( c - 12 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 8 ##u ) - ( c - 7 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 9 ##u ) - ( c - 8 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 10 ##u ) - ( c - 9 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 11 ##u ) - ( c - 10 ##u ) ; [ red arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( c - 12 ##u ) - ( c - 11 ##u ) ; [ red arrow , short ##en \u00bf = 10 ##pt , short ##en \u00a1 = - 3 ##pt ] ( w - 9 ) - ( c - 9 ) ; [ inner sep = 0 ] ( h - call ##out - pointer ) at ( ) ; [ context container , call ##out absolute pointer = ( h - call ##out - pointer ) , fit = ( c - 7 ) ( c - 12 ##u ) ] ( left - call ##out ) ; [ blue hbo ##x , right of = c - 12 , node distance = 1 . 5 ##cm * 1 . 2 , y ##shi ##ft = 1 . 2 cm , label = below : ] ( ci - 1 ) ; [ blue hbo ##x , right of = ci - 1 , node distance = 0 . 7 cm , label = below : ] ( ci - 2 ) ; [ dots hbo ##x , right of = ci - 2 , node distance = 0 . 7 cm , label = below : \u2026 ] ( ci - 3 ) \u2026 ; [ blue hbo ##x , right of = ci - 3 , node distance = 0 . 7 cm , label = below : ] ( ci - 4 ) ; [ dots hbo ##x , right of = ci - 4 , node distance = 0 . 7 cm , label = below : \u2026 ] ( ci - 5 ) \u2026 ; [ blue hbo ##x , right of = ci - 5 , node distance = 0 . 7 cm , label = below : ] ( ci - 6 ) ; [ red arrow , short ##en \u00bf = 9 ##pt , short ##en \u00a1 = 5 ##pt ] ( image ) - ( ci - 3 ) node [ po ##s = 0 . 35 , right , font = ] v ##gg ##net ; [ inner sep = 0 ##em ] ( i - call ##out - pointer ) at ( - ( - ci ##1 ) ( 3 ##cm , - 1 cm ) ) ; container , call ##out absolute pointer = ( i - call ##out - pointer ) , fit = ( ci - 1 ) ( ci - 6 ) , inner sep = 0 . 26 ##em ] ( left - call ##out ) ; [ blue hbo ##x , above of = c - 1 ##u , node distance = 1 . 5 ##cm * 2 . 9 ] ( m - 1 ) ; hbo ##x , right of = m - 1 , node distance = 0 . 5 cm ] ( m - 2 ) ; hbo ##x , right of = m - 2 , node distance = 0 . 5 cm , ] ( m - 3 ) \u2026 ; hbo ##x , right of = m - 3 , node distance = 0 . 5 cm ] ( m - 4 ) ; hbo ##x , right of = m - 4 , node distance = 0 . 5 cm ] ( m - 5 ) \u2026 ; hbo ##x , right of = m - 5 , node distance = 0 . 5 cm , ] ( m - 6 ) ; ( [ x ##shi ##ft = 0 ##pt , y ##shi ##ft = - 0 . 2 ##cm ] m - 6 . south east ) - ( [ x ##shi ##ft = 0 ##pt , y ##shi ##ft = - 0 . 2 ##cm ] m - 1 . south west ) node [ black , midway , y ##shi ##ft = - 0 . 35 cm ] p vs h ; [ blue hbo ##x , right of = m - 6 , node distance = 0 . 9 ##cm * 1 . 5 ] ( m - 7 ) ; hbo ##x , right of = m - 7 , node distance = 0 . 5 cm ] ( m - 8 ) ; hbo ##x , right of = m - 8 , node distance = 0 . 5 cm ] ( m - 9 ) \u2026 ; hbo ##x , right of = m - 9 , node distance = 0 . 5 cm ] ( m - 10 ) ; hbo ##x , right of = m - 10 , node distance = 0 . 5 cm ] ( m - 11 ) \u2026 ; hbo ##x , right of = m - 11 , node distance = 0 . 5 cm ] ( m - 12 ) ; ( [ x ##shi ##ft = 0 ##pt , y ##shi ##ft = - 0 . 2 ##cm ] m - 12 . south east ) - ( [ x ##shi ##ft = 0 ##pt , y ##shi ##ft = - 0 . 2 ##cm ] m - 7 . south west ) node [ black , midway , y ##shi ##ft = - 0 . 35 cm ] h vs p ; [ blue hbo ##x , right of = m - 12 , node distance = 0 . 9 ##cm * 1 . 5 ] ( mi - 13 ) ; hbo ##x , right of = mi - 13 , node distance = 0 . 5 cm ] ( mi - 14 ) ; hbo ##x , right of = mi - 14 , node distance = 0 . 5 cm ] ( mi - 15 ) \u2026 ; hbo ##x , right of = mi - 15 , node distance = 0 . 5 cm ] ( mi - 16 ) ; hbo ##x , right of = mi - 16 , node distance = 0 . 5 cm ] ( mi - 17 ) \u2026 ; hbo ##x , right of = mi - 17 , node distance = 0 . 5 cm ] ( mi - 18 ) ; ( [ x ##shi ##ft = 0 ##pt , y ##shi ##ft = - 0 . 2 ##cm ] mi - 18 . south east ) - ( [ x ##shi ##ft = 0 ##pt , y ##shi ##ft = - 0 . 2 ##cm ] mi - 13 . south west ) node [ black , midway , y ##shi ##ft = - 0 . 35 cm ] h vs image ; [ blue hbo ##x , right of = mi - 18 , node distance = 0 . 9 ##cm * 1 . 5 ] ( mi - 19 ) ; hbo ##x , right of = mi - 19 , node distance = 0 . 5 cm ] ( mi - 20 ) ; hbo ##x , right of = mi - 20 , node distance = 0 . 5 cm ] ( mi - 21 ) \u2026 ; hbo ##x , right of = mi - 21 , node distance = 0 . 5 cm ] ( mi - 22 ) ; hbo ##x , right of = mi - 22 , node distance = 0 . 5 cm ] ( mi - 23 ) \u2026 ; hbo ##x , right of = mi - 23 , node distance = 0 . 5 cm ] ( mi - 24 ) ; ( [ x ##shi ##ft = 0 ##pt , y ##shi ##ft = - 0 . 2 ##cm ] mi - 24 . south east ) - ( [ x ##shi ##ft = 0 ##pt , y ##shi ##ft = - 0 . 2 ##cm ] mi - 19 . south west ) node [ black , midway , y ##shi ##ft = - 0 . 35 cm ] image vs h ; [ red matching , below of = m - 1 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 1 ) \u2297 ; matching , below of = m - 2 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 2 ) \u2297 ; matching , below of = m - 4 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 3 ) \u2297 ; matching , below of = m - 6 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 4 ) \u2297 ; [ red matching , below of = m - 7 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 5 ) \u2297 ; matching , below of = m - 8 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 6 ) \u2297 ; matching , below of = m - 10 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 7 ) \u2297 ; matching , below of = m - 12 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 8 ) \u2297 ; [ black matching , below of = mi - 13 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 9 ) \u2297 ; matching , below of = mi - 14 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 10 ) \u2297 ; matching , below of = mi - 16 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 11 ) \u2297 ; matching , below of = mi - 18 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 12 ) \u2297 ; [ black matching , below of = mi - 19 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 13 ) \u2297 ; matching , below of = mi - 20 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 14 ) \u2297 ; matching , below of = mi - 22 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 15 ) \u2297 ; matching , below of = mi - 24 , node distance = 1 . 5 ##cm * 1 . 5 ] ( mp - 16 ) \u2297 ; mp - 1 ) ; h - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 165 , [UNK] in = - 70 ] mp - 2 ) ; h - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 160 , [UNK] in = - 75 ] mp - 3 ) ; h - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 155 , [UNK] in = - 80 ] mp - 4 ) ; mp - 5 ) ; p - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 110 , [UNK] in = - 100 ] mp - 6 ) ; p - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 65 , [UNK] in = - 90 ] mp - 7 ) ; p - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 60 , [UNK] in = - 90 ] mp - 8 ) ; mp - 13 ) ; h - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 20 , [UNK] in = - 100 ] mp - 14 ) ; h - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 15 , [UNK] in = - 105 ] mp - 15 ) ; h - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 10 , [UNK] in = - 110 ] mp - 16 ) ; mp - 9 ) ; i - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 145 , [UNK] in = - 60 ] mp - 10 ) ; i - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 140 , [UNK] in = - 105 ] mp - 11 ) ; i - call ##out - pointer ) [UNK] edge [ - > , [UNK] out = 65 , [UNK] in = - 60 ] mp - 12 ) ; [ red arrow , short ##en \u00bf = 18 ##pt ] ( mp - 1 ) - ( m - 1 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 2 ) - ( m - 2 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 3 ) - ( m - 4 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 4 ) - ( m - 6 ) ; [ red arrow , short ##en \u00bf = 18 ##pt ] ( mp - 5 ) - ( m - 7 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 6 ) - ( m - 8 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 7 ) - ( m - 10 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 8 ) - ( m - 12 ) ; [ red arrow , short ##en \u00bf = 18 ##pt ] ( mp - 9 ) - ( mi - 13 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 10 ) - ( mi - 14 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 11 ) - ( mi - 16 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 12 ) - ( mi - 18 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 13 ) - ( mi - 19 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 14 ) - ( mi - 20 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 15 ) - ( mi - 22 ) ; arrow , short ##en \u00bf = 18 ##pt ] ( mp - 16 ) - ( mi - 24 ) ; [ match arrow , short ##en \u00a1 = 6 ##pt ] ( c - 1 ##u ) - ( mp - 1 ) ; arrow , short ##en \u00a1 = 6 ##pt ] ( c - 2 ##u ) - ( mp - 2 ) ; arrow , short ##en \u00a1 = 7 ##pt ] ( c - 4 ##u ) to [ out = 80 , in = - 45 ] ( mp - 3 ) ; arrow , short ##en \u00a1 = 6 ##pt ] ( c - 6 ##u ) to [ out = 100 , in = - 45 ] ( mp - 4 ) ; [ match arrow , short ##en \u00a1 = 9 ##pt ] ( c - 7 ##u ) to [ out = 110 , in = - 40 ] ( mp - 5 ) ; arrow , short ##en \u00a1 = 7 ##pt ] ( c - 8 ##u ) to [ out = 100 , in = - 35 ] ( mp - 6 ) ; arrow , short ##en \u00a1 = 20 ##pt ] ( c - 10 ##u ) - ( mp - 7 ) ; arrow , short ##en \u00a1 = 22 ##pt ] ( c - 12 ##u ) - ( mp - 8 ) ; [ match arrow , short ##en \u00a1 = 22 ##pt ] ( c - 7 ##u ) - ( mp - 9 ) ; arrow , short ##en \u00a1 = 20 ##pt ] ( c - 8 ##u ) - ( mp - 10 ) ; arrow , short ##en \u00a1 = 7 ##pt ] ( c - 10 ##u ) to [ out = 80 , in = 220 ] ( mp - 11 ) ; arrow , short ##en \u00a1 = 7 ##pt ] ( c - 12 ##u ) to [ out = 80 , in = 230 ] ( mp - 12 ) ; [ match arrow , short ##en \u00a1 = 3 ##pt ] ( ci - 1 ) to [ out = 80 , in = 220 ] ( mp - 13 ) ; arrow , short ##en \u00a1 = 3 ##pt ] ( ci - 2 ) to [ out = 90 , in = 220 ] ( mp - 14 ) ; arrow , short ##en \u00a1 = 10 ##pt ] ( ci - 4 ) to [ out = 60 , in = - 70 ] ( mp - 15 ) ; arrow , short ##en \u00a1 = 3 ##pt ] ( ci - 6 ) - ( mp - 16 ) ; [ blue hbo ##x , above of = m - 1 , node distance = 1 . 5 ##cm * 1 . 2 ] ( a - 1 ) ; hbo ##x , right of = a - 1 , node distance = 0 . 5 cm ] ( a - 2 ) ; hbo ##x , right of = a - 2 , node distance = 0 . 5 cm ] ( a - 3 ) \u2026 ; hbo ##x , right of = a - 3 , node distance = 0 . 5 cm ] ( a - 4 ) ; hbo ##x , right of = a - 4 , node distance = 0 . 5 cm ] ( a - 5 ) \u2026 ; hbo ##x , right of = a - 5 , node distance = 0 . 5 cm ] ( a - 6 ) ; arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( a - 1 ) - ( a - 2 ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( a - 2 ) - ( a - 3 ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( a - 3 ) - ( a - 4 ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( a - 4 ) - ( a - 5 ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( a - 5 ) - ( a - 6 ) ; [ green hbo ##x , above of = a - 1 , node distance = 1 . 5 ##cm / 1 . 2 ] ( a - 1 ##u ) ; hbo ##x , right of = a - 1 ##u , node distance = 0 . 5 cm ] ( a - 2 ##u ) ; hbo ##x , right of = a - 2 ##u , node distance = 0 . 5 cm ] ( a - 3 ##u ) \u2026 ; hbo ##x , right of = a - 3 ##u , node distance = 0 . 5 cm ] ( a - 4 ##u ) ; hbo ##x , right of = a - 4 ##u , node distance = 0 . 5 cm ] ( a - 5 ##u ) \u2026 ; hbo ##x , right of = a - 5 ##u , node distance = 0 . 5 cm ] ( a - 6 ##u ) ; arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( a - 2 ##u ) - ( a - 1 ##u ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( a - 3 ##u ) - ( a - 2 ##u ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( a - 4 ##u ) - ( a - 3 ##u ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( a - 5 ##u ) - ( a - 4 ##u ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( a - 6 ##u ) - ( a - 5 ##u ) ; [ blue hbo ##x , right of = a - 6 , node distance = 0 . 9 ##cm * 1 . 5 ] ( a - 7 ) ; hbo ##x , right of = a - 7 , node distance = 0 . 5 cm ] ( a - 8 ) ; hbo ##x , right of = a - 8 , node distance = 0 . 5 cm ] ( a - 9 ) \u2026 ; hbo ##x , right of = a - 9 , node distance = 0 . 5 cm ] ( a - 10 ) ; hbo ##x , right of = a - 10 , node distance = 0 . 5 cm ] ( a - 11 ) \u2026 ; hbo ##x , right of = a - 11 , node distance = 0 . 5 cm ] ( a - 12 ) ; arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( a - 7 ) - ( a - 8 ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( a - 8 ) - ( a - 9 ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( a - 9 ) - ( a - 10 ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( a - 10 ) - ( a - 11 ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( a - 11 ) - ( a - 12 ) ; [ green hbo ##x , right of = a - 6 ##u , node distance = 0 . 9 ##cm * 1 . 5 ] ( a - 7 ##u ) ; hbo ##x , right of = a - 7 ##u , node distance = 0 . 5 cm ] ( a - 8 ##u ) ; hbo ##x , right of = a - 8 ##u , node distance = 0 . 5 cm ] ( a - 9 ##u ) \u2026 ; hbo ##x , right of = a - 9 ##u , node distance = 0 . 5 cm ] ( a - 10 ##u ) ; hbo ##x , right of = a - 10 ##u , node distance = 0 . 5 cm ] ( a - 11 ##u ) \u2026 ; hbo ##x , right of = a - 11 ##u , node distance = 0 . 5 cm ] ( a - 12 ##u ) ; arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( a - 8 ##u ) - ( a - 7 ##u ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( a - 9 ##u ) - ( a - 8 ##u ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( a - 10 ##u ) - ( a - 9 ##u ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( a - 11 ##u ) - ( a - 10 ##u ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( a - 12 ##u ) - ( a - 11 ##u ) ; [ blue hbo ##x , right of = a - 12 , node distance = 0 . 9 ##cm * 1 . 5 ] ( ai - 13 ) ; hbo ##x , right of = ai - 13 , node distance = 0 . 5 cm ] ( ai - 14 ) ; hbo ##x , right of = ai - 14 , node distance = 0 . 5 cm ] ( ai - 15 ) \u2026 ; hbo ##x , right of = ai - 15 , node distance = 0 . 5 cm ] ( ai - 16 ) ; hbo ##x , right of = ai - 16 , node distance = 0 . 5 cm ] ( ai - 17 ) \u2026 ; hbo ##x , right of = ai - 17 , node distance = 0 . 5 cm ] ( ai - 18 ) ; arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( ai - 13 ) - ( ai - 14 ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( ai - 14 ) - ( ai - 15 ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( ai - 15 ) - ( ai - 16 ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( ai - 16 ) - ( ai - 17 ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( ai - 17 ) - ( ai - 18 ) ; [ green hbo ##x , right of = a - 12 ##u , node distance = 0 . 9 ##cm * 1 . 5 ] ( ai - 13 ##u ) ; hbo ##x , right of = ai - 13 ##u , node distance = 0 . 5 cm ] ( ai - 14 ##u ) ; hbo ##x , right of = ai - 14 ##u , node distance = 0 . 5 cm ] ( ai - 15 ##u ) \u2026 ; hbo ##x , right of = ai - 15 ##u , node distance = 0 . 5 cm ] ( ai - 16 ##u ) ; hbo ##x , right of = ai - 16 ##u , node distance = 0 . 5 cm ] ( ai - 17 ##u ) \u2026 ; hbo ##x , right of = ai - 17 ##u , node distance = 0 . 5 cm ] ( ai - 18 ##u ) ; arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( ai - 14 ##u ) - ( ai - 13 ##u ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( ai - 15 ##u ) - ( ai - 14 ##u ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( ai - 16 ##u ) - ( ai - 15 ##u ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( ai - 17 ##u ) - ( ai - 16 ##u ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( ai - 18 ##u ) - ( ai - 17 ##u ) ; [ blue hbo ##x , right of = ai - 18 , node distance = 0 . 9 ##cm * 1 . 5 ] ( ai - 19 ) ; hbo ##x , right of = ai - 19 , node distance = 0 . 5 cm ] ( ai - 20 ) ; hbo ##x , right of = ai - 20 , node distance = 0 . 5 cm ] ( ai - 21 ) \u2026 ; hbo ##x , right of = ai - 21 , node distance = 0 . 5 cm ] ( ai - 22 ) ; hbo ##x , right of = ai - 22 , node distance = 0 . 5 cm ] ( ai - 23 ) \u2026 ; hbo ##x , right of = ai - 23 , node distance = 0 . 5 cm ] ( ai - 24 ) ; arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( ai - 19 ) - ( ai - 20 ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( ai - 20 ) - ( ai - 21 ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( ai - 21 ) - ( ai - 22 ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( ai - 22 ) - ( ai - 23 ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( ai - 23 ) - ( ai - 24 ) ; [ green hbo ##x , right of = ai - 18 ##u , node distance = 0 . 9 ##cm * 1 . 5 ] ( ai - 19 ##u ) ; hbo ##x , right of = ai - 19 ##u , node distance = 0 . 5 cm ] ( ai - 20 ##u ) ; hbo ##x , right of = ai - 20 ##u , node distance = 0 . 5 cm ] ( ai - 21 ##u ) \u2026 ; hbo ##x , right of = ai - 21 ##u , node distance = 0 . 5 cm ] ( ai - 22 ##u ) ; hbo ##x , right of = ai - 22 ##u , node distance = 0 . 5 cm ] ( ai - 23 ##u ) \u2026 ; hbo ##x , right of = ai - 23 ##u , node distance = 0 . 5 cm ] ( ai - 24 ##u ) ; arrow , short ##en \u00bf = 1 ##pt , short ##en \u00a1 = 1 ##pt ] ( ai - 20 ##u ) - ( ai - 19 ##u ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( ai - 21 ##u ) - ( ai - 20 ##u ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( ai - 22 ##u ) - ( ai - 21 ##u ) ; arrow , short ##en \u00bf = 0 ##pt , short ##en \u00a1 = - 2 ##pt ] ( ai - 23 ##u ) - ( ai - 22 ##u ) ; arrow , short ##en \u00bf = - 2 ##pt , short ##en \u00a1 = 0 ##pt ] ( ai - 24 ##u ) - ( ai - 23 ##u ) ; arrow , short ##en \u00bf = 6 ##pt , short ##en \u00a1 = - 5 ##pt ] ( m - 3 ) - ( a - 3 ) ; arrow , short ##en \u00bf = 6 ##pt , short ##en \u00a1 = - 5 ##pt ] ( m - 9 ) - ( a - 9 ) ; arrow , short ##en \u00bf = 6 ##pt , short ##en \u00a1 = - 5 ##pt ] ( mi - 15 ) - ( ai - 15 ) ; arrow , short ##en \u00bf = 6 ##pt , short ##en \u00a1 = - 5 ##pt ] ( mi - 21 ) - ( ai - 21 ) ; [ green v ##box , above of = a - 3 ##u , node distance = 1 . 5 ##cm * 1 . 2 , x ##shi ##ft = - 0 . 3 cm ] ( ap - 1 ) ; v ##box , right of = ap - 1 , node distance = 0 . 9 ##cm * 2 ] ( ap - 2 ) ; v ##box , right of = ap - 2 , node distance = 0 . 9 ##cm * 2 ] ( ap - 3 ) ; v ##box , right of = ap - 3 , node distance = 0 . 9 ##cm * 2 ] ( ap - 4 ) ; v ##box , right of = ap - 4 , node distance = 0 . 9 ##cm * 2 ] ( ap - 5 ) ; v ##box , right of = ap - 5 , node distance = 0 . 9 ##cm * 2 ] ( ap - 6 ) ; v ##box , right of = ap - 6 , node distance = 0 . 9 ##cm * 2 ] ( ap - 7 ) ; v ##box , right of = ap - 7 , node distance = 0 . 9 ##cm * 2 ] ( ap - 8 ) ; [ red arrow , short ##en \u00bf = 3 ##pt , short ##en \u00a1 = 3 ##pt ] ( a - 1 ##u ) - ( ap - 1 ) ; arrow , short ##en \u00bf = 3 ##pt , short ##en \u00a1 = 3 ##pt ] ( a - 6 . east ) - ( [ x ##shi ##ft = - 1 cm ] ap - 2 . south east ) ; [ red arrow , short ##en \u00bf = 3 ##pt , short ##en \u00a1 = 3 ##pt ] ( a - 7 ##u ) - ( ap - 3 ) ; arrow , short ##en \u00bf = 3 ##pt , short ##en \u00a1 = 3 ##pt ] ( a - 12 . west ) - ( [ x ##shi ##ft = - 0 . 5 ##cm ] ap - 4 . south ) ; [ red arrow , short ##en \u00bf = 3 ##pt , short ##en \u00a1 = 3 ##pt ] ( ai - 13 ##u ) - ( ap - 5 ) ; arrow , short ##en \u00bf = 3 ##pt , short ##en \u00a1 = 3 ##pt ] ( ai - 18 . west ) - ( [ x ##shi ##ft = - 1 ##cm ] ap - 6 ) ; [ red arrow , short ##en \u00bf = 3 ##pt , short ##en \u00a1 = 3 ##pt ] ( ai - 19 ##u ) - ( ap - 7 ) ; arrow , short ##en \u00bf = 3 ##pt , short ##en \u00a1 = 3 ##pt ] ( ai - 24 . west ) - ( ap - 8 ) ; [ tb ##ox , above of = ap - 4 , node distance = 0 . 9 ##cm * 1 . 6 , x ##shi ##ft = 1 cm ] ( soft ##max ) soft ##max ; of = soft ##max , node distance = 0 . 9 cm ] ( output ) p ( y | premise , image , hypothesis ) ; arrow , short ##en \u00bf = 3 ##pt , short ##en \u00a1 = 3 ##pt ] ( [ x ##shi ##ft = 1 . 5 ##cm ] ap - 4 . north east ) - ( soft ##max ) ; [ decorate , thick , decoration = brace , amplitude = 6 ##pt , y ##shi ##ft = 0 ##pt ] ( [ y ##shi ##ft = - 65 ##pt , x ##shi ##ft = - 50 ##pt ] w - 1 . south west ) - ( [ x ##shi ##ft = - 50 ##pt , y ##shi ##ft = - 125 ##pt ] c - 1 . south west ) node [ black , midway , x ##shi ##ft = - 1 cm , y ##shi ##ft = - 0 . 2 cm , rotate = 90 , anchor = north ] em ##bed ##ding layer ; [ decorate , thick , decoration = brace , amplitude = 6 ##pt , y ##shi ##ft = 0 ##pt ] ( [ y ##shi ##ft = - 5 ##pt , x ##shi ##ft = - 50 ##pt ] c - 1 . south west ) - ( [ x ##shi ##ft = - 50 ##pt , y ##shi ##ft = 25 ##pt ] c - 1 ##u . north west ) node [ black , midway , x ##shi ##ft = - 1 cm , rotate = 90 , anchor = north ] context layer ; [ decorate , thick , decoration = brace , amplitude = 6 ##pt , y ##shi ##ft = 0 ##pt ] ( [ y ##shi ##ft = - 95 ##pt , x ##shi ##ft = - 50 ##pt ] mp - 1 . south west ) - ( [ x ##shi ##ft = - 50 ##pt , y ##shi ##ft = - 45 ##pt ] m - 1 . north west ) node [ black , midway , x ##shi ##ft = - 1 cm , rotate = 90 , anchor = north ] matching layer ; [ decorate , thick , decoration = brace , amplitude = 6 ##pt , y ##shi ##ft = 0 ##pt ] ( [ x ##shi ##ft = - 50 ##pt ] a - 1 . south west ) - ( [ x ##shi ##ft = - 50 ##pt , y ##shi ##ft = 1 . 5 ##cm * 3 ] a - 1 ##u . north west ) node [ black , midway , x ##shi ##ft = - 1 cm , rotate = 90 , anchor = north ] aggregation layer ; [ decorate , thick , decoration = brace , amplitude = 6 ##pt , y ##shi ##ft = 0 ##pt ] ( [ x ##shi ##ft = - 50 ##pt , y ##shi ##ft = 1 . 5 ##cm * 5 . 5 ] a - 1 ##u . north west ) - ( [ x ##shi ##ft = - 50 ##pt , y ##shi ##ft = 1 . 5 ##cm * 12 . 5 ] a - 1 ##u . north west ) node [ black , midway , x ##shi ##ft = - 1 cm , rotate = 90 , anchor = north ] prediction layer ; here , we report some further details of our implementation of the v - bi ##mp ##m model described in section 4 of the main paper , based on the work of wang ##20 ##17 . our model is displayed in figure [ reference ] . the core part of the original bi ##mp ##m is the matching layer . given two - dimensional vectors and , each replicate ##d times ( is the number of ' perspectives ' ) and a train ##able weight matrix , matching involves a co ##sin ##e similarity computation that yields an - dimensional matching vector , whose elements are defined as follows : the matching operations included are the following : full - matching , where each forward or backward context ##ual em ##bed ##ding of the premise p ( res ##p . the hypothesis h ) is matched to the last time - step of h ( res ##p . p ) ; max - pool ##ing , where each forward / backward context ##ual em ##bed ##ding of one sentence is compared to the em ##bed ##ding ##s of the other , retaining the maximum value for each dimension ; at ##ten ##tive matching , where first , the pair ##wise co ##sin ##e similarity between forward / backward em ##bed ##ding ##s of p and h is estimated , before calculating an at ##ten ##tive vector over the weighted sum of context ##ual em ##bed ##ding ##s for h and matching each forward / backward em ##bed ##ding of p against the at ##ten ##tive vector ; max - at ##ten ##tive matching , a version of at ##ten ##tive matching where the context ##ual em ##bed ##ding with the highest co ##sin ##e is used as the at ##ten ##tive vector , instead of the weighted sum . the visually - augmented version of the original model , v - bi ##mp ##m , is displayed in figure [ reference ] . to perform multi ##mo ##dal matching , the visual and textual vectors are mapped to a mutual space using the following af ##fine transformation : where , , , and are the weight matrix , the bias , the input features and output features , respectively , and is any text ( p or h ) . given weight matrices for text and for images , we compute the matching vector between a textual vector and image vector as : bibliography : references",
        "pred_seq": "s ##set [SEP] [SEP] [SEP] [SEP] [unused0] s ##set [SEP] grounded ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##ment ##tail ##tail [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "snli dataset"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "vsnli",
                        "vsnli test",
                        "vsnli dataset",
                        "vlstm baseline",
                        "vlstm",
                        "v snli"
                    ]
                ],
                "Method": [
                    [
                        "bimpm",
                        "bilateral multiperspective matching"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy",
                        "accuracies"
                    ]
                ],
                "Task": [
                    [
                        "natural language inference",
                        "inference"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "vsnli",
                        "vsnli test",
                        "vsnli dataset",
                        "vlstm baseline",
                        "vlstm",
                        "v snli"
                    ]
                ],
                "Method": [
                    [
                        "vbimpm",
                        "vbimpm model"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy",
                        "accuracies"
                    ]
                ],
                "Task": [
                    [
                        "natural language inference",
                        "inference"
                    ]
                ]
            }
        ]
    },
    "17": {
        "doctext": "document : going deeper with con ##vo ##lu ##tions we propose a deep con ##vo ##lu ##tion ##al neural network architecture [UNK] , which was responsible for setting the new state of the art for classification and detection in the image ##net large - scale visual recognition challenge 2014 ( [UNK] ) . the main hallmark of this architecture is the improved utilization of the computing resources inside the network . this was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant . to opt ##imi ##ze quality , the architectural decisions were based on the he ##bb ##ian principle and the intuition of multi - scale processing . one particular incarnation used in our submission for [UNK] is called google ##net , a 22 layers deep network , the quality of which is assessed in the context of classification and detection . par ##ski ##p [ ] section : introduction in the last three years , mainly due to the advances of deep learning , more concrete ##ly con ##vo ##lu ##tion ##al networks , the quality of image recognition and object detection has been progressing at a dramatic pace . one encouraging news is that most of this progress is not just the result of more powerful hardware , larger data ##set ##s and bigger models , but mainly a consequence of new ideas , algorithms and improved network architecture ##s . no new data sources were used , for example , by the top entries in the il ##s ##vr ##c 2014 competition besides the classification data ##set of the same competition for detection purposes . our google ##net submission to il ##s ##vr ##c 2014 actually uses fewer parameters than the winning architecture of k ##riz ##he ##vsky et al from two years ago , while being significantly more accurate . the biggest gains in object - detection have not come from the utilization of deep networks alone or bigger models , but from the syn ##ergy of deep architecture ##s and classical computer vision , like the r - cnn algorithm by gi ##rs ##hic ##k et al . another notable factor is that with the ongoing traction of mobile and embedded computing , the efficiency of our algorithms - especially their power and memory use - gains importance . it is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fix ##ation on accuracy numbers . for most of the experiments , the models were designed to keep a computational budget of billion multi ##ply - adds at inference time , so that the they do not end up to be a purely academic curiosity , but could be put to real world use , even on large data ##set ##s , at a reasonable cost . in this paper , we will focus on an efficient deep neural network architecture for computer vision , [UNK] , which derives its name from [UNK] in [UNK] paper by lin et al in conjunction with the famous \" we need to go deeper \" internet me ##me . in our case , the word \" deep \" is used in two different meanings : first of all , in the sense that we introduce a new level of organization in the form of the \" inception module \" and also in the more direct sense of increased network depth . in general , one can view the inception model as a logical cu ##lm ##ination of while taking inspiration and guidance from the theoretical work by ar ##ora et al . the benefits of the architecture are experimental ##ly verified on the il ##s ##vr ##c 2014 classification and detection challenges , on which it significantly out ##per ##forms the current state of the art . section : related work starting with len ##et - 5 , con ##vo ##lu ##tion ##al neural networks ( cnn ) have typically had a standard structure - stacked con ##vo ##lu ##tion ##al layers ( optional ##ly followed by contrast normal ##ization and max - pool ##ing ) are followed by one or more fully - connected layers . variants of this basic design are prevalent in the image classification literature and have yielded the best results to - date on mn ##ist , ci ##far and most notably on the image ##net classification challenge . [UNK] for larger data ##set ##s such as image ##net , the recent trend has been to increase the number of layers and layer size , while using drop ##out to address the problem of over ##fi ##tting . despite concerns that max - pool ##ing layers result in loss of accurate spatial information , the same con ##vo ##lu ##tion ##al network architecture as has also been successfully employed for local ##ization , object detection and human pose estimation . inspired by a neuroscience model of the primate visual cortex , ser ##re et al . use a series of fixed ga ##bor filters of different sizes in order to handle multiple scales , similarly to the inception model . however , contrary to the fixed 2 - layer deep model of , all filters in the inception model are learned . furthermore , inception layers are repeated many times , leading to a 22 - layer deep model in the case of the google ##net model . network - in - network is an approach proposed by lin et al . in order to increase the representation ##al power of neural networks . when applied to con ##vo ##lu ##tion ##al layers , the method could be viewed as additional con ##vo ##lu ##tion ##al layers followed typically by the rec ##ti ##fied linear activation . this enables it to be easily integrated in the current cnn pipeline ##s . we use this approach heavily in our architecture . however , in our setting , con ##vo ##lu ##tions have dual purpose : most critically , they are used mainly as dimension reduction modules to remove computational bottle ##neck ##s , that would otherwise limit the size of our networks . [UNK] this allows for not just increasing the depth , but also the width of our networks without significant performance penalty . the current leading approach for object detection is the regions with con ##vo ##lu ##tion ##al neural networks ( r - cnn ) proposed by gi ##rs ##hic ##k et al . . r - cnn deco ##mp ##oses the overall detection problem into two sub ##pro ##ble ##ms : to first utilize low - level cues such as color and super ##pi ##x ##el consistency for potential object proposals in a category - ag ##nostic fashion , and to then use cnn class ##ifiers to identify object categories at those locations . such a two stage approach leverage ##s the accuracy of bound ##ing box segment ##ation with low - level cues , as well as the highly powerful classification power of state - of - the - art cnn ##s . we adopted a similar pipeline in our detection submissions , but have explored enhancement ##s in both stages , such as multi - box prediction for higher object bound ##ing box recall , and ensemble approaches for better cat ##ego ##rization of bound ##ing box proposals . section : motivation and high level considerations the most straightforward way of improving the performance of deep neural networks is by increasing their size . this includes both increasing the depth - the number of levels - of the network and its width : the number of units at each level . this is as an easy and safe way of training higher quality models , especially given the availability of a large amount of labeled training data . however this simple solution comes with two major draw ##backs . bigger size typically means a larger number of parameters , which makes the enlarged network more prone to over ##fi ##tting , especially if the number of labeled examples in the training set is limited . this can become a major bottle ##neck , since the creation of high quality training sets can be tricky and expensive , especially if expert human rate ##rs are necessary to distinguish between fine - grain ##ed visual categories like those in image ##net ( even in the 1000 - class il ##s ##vr ##c subset ) as demonstrated by figure [ reference ] . another draw ##back of uniformly increased network size is the dramatically increased use of computational resources . for example , in a deep vision network , if two con ##vo ##lu ##tion ##al layers are chained , any uniform increase in the number of their filters results in a quad ##ratic increase of computation . if the added capacity is used in ##ef ##fi ##cie ##ntly ( for example , if most weights end up to be close to zero ) , then a lot of computation is wasted . since in practice the computational budget is always finite , an efficient distribution of computing resources is preferred to an ind ##is ##cr ##imi ##nate increase of size , even when the main objective is to increase the quality of results . the fundamental way of solving both issues would be by ultimately moving from fully connected to sparsely connected architecture ##s , even inside the con ##vo ##lu ##tions . besides mimic ##king biological systems , this would also have the advantage of firm ##er theoretical under ##pin ##ning ##s due to the groundbreaking work of ar ##ora et al . . their main result states that if the probability distribution of the data - set is represent ##able by a large , very sparse deep neural network , then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activation ##s of the last layer and cluster ##ing neurons with highly correlated outputs . although the strict mathematical proof requires very strong conditions , the fact that this statement res ##onate ##s with the well known he ##bb ##ian principle - neurons that fire together , wire together - suggests that the underlying idea is applicable even under less strict conditions , in practice . on the downs ##ide , [UNK] computing infrastructure ##s are very in ##ef ##fi ##cie ##nt when it comes to numerical calculation on non - uniform sparse data structures . even if the number of arithmetic operations is reduced by , the overhead of look ##ups and cache misses is so dominant that switching to sparse matrices would not pay off . the gap is widened even further by the use of steadily improving , highly tuned , numerical libraries that allow for extremely fast dense matrix multiplication , exploit ##ing the minute details of the underlying cpu or gp ##u hardware . also , non - uniform sparse models require more sophisticated engineering and computing infrastructure . most current vision oriented machine learning systems utilize spa ##rs ##ity in the spatial domain just by the virtue of employing con ##vo ##lu ##tions . however , con ##vo ##lu ##tions are implemented as collections of dense connections to the patches in the earlier layer . con ##vn ##ets have traditionally used random and sparse connection tables in the feature dimensions since in order to break the symmetry and improve learning , the trend changed back to full connections with in order to better opt ##imi ##ze parallel computing . the uniform ##ity of the structure and a large number of filters and greater batch size allow for utilizing efficient dense computation . this raises the question whether there is any hope for a next , intermediate step : an architecture that makes use of the extra spa ##rs ##ity , even at filter level , as suggested by the theory , but exploits our current hardware by utilizing computation ##s on dense matrices . the vast literature on sparse matrix computation ##s ( e . g . ) suggests that cluster ##ing sparse matrices into relatively dense sub ##mat ##rice ##s tends to give state of the art practical performance for sparse matrix multiplication . it does not seem far - fetch ##ed to think that similar methods would be utilized for the automated construction of non - uniform deep - learning architecture ##s in the near future . the inception architecture started out as a case study of the first author for assessing the hypothetical output of a sophisticated network topology construction algorithm that tries to approximate a sparse structure implied by for vision networks and covering the h ##yp ##oth ##es ##ized outcome by dense , readily available components . despite being a highly speculative undertaking , only after two iteration ##s on the exact choice of topology , we could already see modest gains against the reference architecture based on . after further tuning of learning rate , hyper ##para ##meter ##s and improved training methodology , we established that the resulting inception architecture was especially useful in the context of local ##ization and object detection as the base network for and . interesting ##ly , while most of the original architectural choices have been questioned and tested thoroughly , they turned out to be at least locally optimal . one must be cautious though : although the proposed architecture has become a success for computer vision , it is still questionable whether its quality can be attributed to the guiding principles that have lead to its construction . making sure would require much more thorough analysis and verification : for example , if automated tools based on the principles described below would find similar , but better topology for the vision networks . the most convincing proof would be if an automated system would create network top ##ologies resulting in similar gains in other domains using the same algorithm but with very differently looking global architecture . at very least , the initial success of the inception architecture yields firm motivation for exciting future work in this direction . section : architectural details the main idea of the inception architecture is based on finding out how an optimal local sparse structure in a con ##vo ##lu ##tion ##al vision network can be approximate ##d and covered by readily available dense components . note that assuming translation in ##var ##iance means that our network will be built from con ##vo ##lu ##tion ##al building blocks . all we need is to find the optimal local construction and to repeat it spatial ##ly . ar ##ora et al . suggests a layer - by layer construction in which one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation . these clusters form the units of the next layer and are connected to the units in the previous layer . we assume that each unit from the earlier layer corresponds to some region of the input image and these units are grouped into filter banks . in the lower layers ( the ones close to the input ) correlated units would concentrate in local regions . this means , we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of con ##vo ##lu ##tions in the next layer , as suggested in . however , one can also expect that there will be a smaller number of more spatial ##ly spread out clusters that can be covered by con ##vo ##lu ##tions over larger patches , and there will be a decreasing number of patches over larger and larger regions . in order to avoid patch - alignment issues , current incarnation ##s of the inception architecture are restricted to filter sizes , and , however this decision was based more on convenience rather than necessity . it also means that the suggested architecture is a combination of all those layers with their output filter banks con ##cate ##nated into a single output vector forming the input of the next stage . additionally , since pool ##ing operations have been essential for the success in current state of the art con ##vo ##lu ##tion ##al networks , it suggests that adding an alternative parallel pool ##ing path in each such stage should have additional beneficial effect , too ( see figure [ reference ] ) . as these \" inception modules \" are stacked on top of each other , their output correlation statistics are bound to vary : as features of higher abstraction are captured by higher layers , their spatial concentration is expected to decrease suggesting that the ratio of and con ##vo ##lu ##tions should increase as we move to higher layers . one big problem with the above modules , at least in this [UNK] form , is that even a modest number of con ##vo ##lu ##tions can be prohibit ##ively expensive on top of a con ##vo ##lu ##tion ##al layer with a large number of filters . this problem becomes even more pronounced once pool ##ing units are added to the mix : their number of output filters equals to the number of filters in the previous stage . the merging of the output of the pool ##ing layer with the outputs of con ##vo ##lu ##tion ##al layers would lead to an inevitable increase in the number of outputs from stage to stage . even while this architecture might cover the optimal sparse structure , it would do it very in ##ef ##fi ##cie ##ntly , leading to a computational blow up within a few stages . this leads to the second idea of the proposed architecture : ju ##dic ##iously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise . this is based on the success of em ##bed ##ding ##s : even low dimensional em ##bed ##ding ##s might contain a lot of information about a relatively large image patch . however , em ##bed ##ding ##s represent information in a dense , compressed form and compressed information is harder to model . we would like to keep our representation sparse at most places ( as required by the conditions of ) and com ##press the signals only whenever they have to be aggregate ##d en mass ##e . that is , con ##vo ##lu ##tions are used to compute reductions before the expensive and con ##vo ##lu ##tions . besides being used as reductions , they also include the use of rec ##ti ##fied linear activation which makes them dual - purpose . the final result is depicted in figure [ reference ] . in general , an inception network is a network consisting of modules of the above type stacked upon each other , with occasional max - pool ##ing layers with stride 2 to hal ##ve the resolution of the grid . for technical reasons ( memory efficiency during training ) , it seemed beneficial to start using inception modules only at higher layers while keeping the lower layers in traditional con ##vo ##lu ##tion ##al fashion . this is not strictly necessary , simply reflecting some in ##fra ##st ##ru ##ct ##ural in ##ef ##fi ##cie ##ncies in our current implementation . one of the main beneficial aspects of this architecture is that it allows for increasing the number of units at each stage significantly without an un ##con ##tro ##lled blow - up in computational complexity . the ubiquitous use of dimension reduction allows for shielding the large number of input filters of the last stage to the next layer , first reducing their dimension before con ##vo ##lving over them with a large patch size . another practically useful aspect of this design is that it align ##s with the intuition that visual information should be processed at various scales and then aggregate ##d so that the next stage can abstract features from different scales simultaneously . the improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties . another way to utilize the inception architecture is to create slightly inferior , but computational ##ly cheaper versions of it . we have found that all the included the knob ##s and lever ##s allow for a controlled balancing of computational resources that can result in networks that are faster than similarly performing networks with non - inception architecture , however this requires careful manual design at this point . section : google ##net we chose google ##net as our team - name in the il ##s ##vr ##c ##14 competition . this name is an homage to yan ##n [UNK] pioneering len ##et 5 network . we also use google ##net to refer to the particular incarnation of the inception architecture used in our submission for the competition . we have also used a deeper and wider inception network , the quality of which was slightly inferior , but adding it to the ensemble seemed to improve the results marginal ##ly . we om ##it the details of that network , since our experiments have shown that the influence of the exact architectural parameters is relatively minor . here , the most successful particular instance ( named google ##net ) is described in table [ reference ] for demonstration ##al purposes . the exact same topology ( trained with different sampling methods ) was used for 6 out of the 7 models in our ensemble . all the con ##vo ##lu ##tions , including those inside the inception modules , use rec ##ti ##fied linear activation . the size of the rec ##eptive field in our network is taking r ##gb color channels with mean sub ##tra ##ction . \" reduce \" and \" reduce \" stands for the number of filters in the reduction layer used before the and con ##vo ##lu ##tions . one can see the number of filters in the projection layer after the built - in max - pool ##ing in [UNK] [UNK] column . all these reduction / projection layers use rec ##ti ##fied linear activation as well . the network was designed with computational efficiency and practical ##ity in mind , so that inference can be run on individual devices including even those with limited computational resources , especially with low - memory footprint . the network is 22 layers deep when counting only layers with parameters ( or 27 layers if we also count pool ##ing ) . the overall number [UNK] ( independent building blocks ) used for the construction of the network is about 100 . however this number depends on the machine learning infrastructure system used . the use of average pool ##ing before the class ##ifier is based on , although our implementation differs in that we use an extra linear layer . this enables adapting and fine - tuning our networks for other label sets easily , but it is mostly convenience and we do not expect it to have a major effect . it was found that a move from fully connected layers to average pool ##ing improved the top - 1 accuracy by about 0 . 6 % , however the use of drop ##out remained essential even after removing the fully connected layers . given the relatively large depth of the network , the ability to prop ##aga ##te gradient ##s back through all the layers in an effective manner was a concern . one interesting insight is that the strong performance of relatively shallow ##er networks on this task suggests that the features produced by the layers in the middle of the network should be very disc ##rim ##ina ##tive . by adding auxiliary class ##ifiers connected to these intermediate layers , we would expect to encourage discrimination in the lower stages in the class ##ifier , increase the gradient signal that gets prop ##aga ##ted back , and provide additional regular ##ization . these class ##ifiers take the form of smaller con ##vo ##lu ##tion ##al networks put on top of the output of the inception ( 4a ) and ( 4 ##d ) modules . during training , their loss gets added to the total loss of the network with a discount weight ( the losses of the auxiliary class ##ifiers were weighted by 0 . 3 ) . at inference time , these auxiliary networks are discarded . the exact structure of the extra network on the side , including the auxiliary class ##ifier , is as follows : an average pool ##ing layer with filter size and stride , resulting in an output for the ( 4a ) , and for the ( 4 ##d ) stage . a con ##vo ##lu ##tion with 128 filters for dimension reduction and rec ##ti ##fied linear activation . a fully connected layer with 102 ##4 units and rec ##ti ##fied linear activation . a drop ##out layer with 70 % ratio of dropped outputs . a linear layer with soft ##max loss as the class ##ifier ( predicting the same 1000 classes as the main class ##ifier , but removed at inference time ) . a sc ##hema ##tic view of the resulting network is depicted in figure [ reference ] . section : training methodology our networks were trained using the di ##st ##bel ##ie ##f distributed machine learning system using modest amount of model and data - parallel ##ism . although we used cpu based implementation only , a rough estimate suggests that the google ##net network could be trained to convergence using few high - end gp ##us within a week , the main limitation being the memory usage . our training used as ##yn ##ch ##ron ##ous st ##och ##astic gradient descent with 0 . 9 momentum , fixed learning rate schedule ( decreasing the learning rate by 4 % every 8 epoch ##s ) . poly ##ak averaging was used to create the final model used at inference time . our image sampling methods have changed substantially over the months leading to the competition , and already converge ##d models were trained on with other options , sometimes in conjunction with changed hyper ##para ##meter ##s , like drop ##out and learning rate , so it is hard to give a definitive guidance to the most effective single way to train these networks . to com ##pl ##icate matters further , some of the models were mainly trained on smaller relative crops , others on larger ones , inspired by . still , one prescription that was verified to work very well after the competition includes sampling of various sized patches of the image whose size is distributed evenly between 8 % and 100 % of the image area and whose aspect ratio is chosen randomly between and . also , we found that the photo ##metric distortion ##s by andrew howard were useful to combat over ##fi ##tting to some extent . in addition , we started to use random inter ##pol ##ation methods ( bi ##line ##ar , area , nearest neighbor and cubic , with equal probability ) for res ##izing relatively late and in conjunction with other hyper ##para ##meter changes , so we could not tell definitely whether the final results were affected positively by their use . section : il ##s ##vr ##c 2014 classification challenge setup and results the il ##s ##vr ##c 2014 classification challenge involves the task of classify ##ing the image into one of 1000 leaf - node categories in the image ##net hierarchy . there are about 1 . 2 million images for training , 50 , 000 for validation and 100 , 000 images for testing . each image is associated with one ground truth category , and performance is measured based on the highest scoring class ##ifier predictions . two numbers are usually reported : the top - 1 accuracy rate , which compares the ground truth against the first predicted class , and the top - 5 error rate , which compares the ground truth against the first 5 predicted classes : an image is deemed correctly classified if the ground truth is among the top - 5 , regardless of its rank in them . the challenge uses the top - 5 error rate for ranking purposes . we participated in the challenge with no external data used for training . in addition to the training techniques aforementioned in this paper , we adopted a set of techniques during testing to obtain a higher performance , which we elaborate below . we independently trained 7 versions of the same google ##net model ( including one wider version ) , and performed ensemble prediction with them . these models were trained with the same initial ##ization ( even with the same initial weights , mainly because of an oversight ) and learning rate policies , and they only differ in sampling method ##ologies and the random order in which they see input images . during testing , we adopted a more aggressive crop ##ping approach than that of k ##riz ##he ##vsky et al . . specifically , we res ##ize the image to 4 scales where the shorter dimension ( height or width ) is 256 , 288 , 320 and 352 respectively , take the left , center and right square of these res ##ized images ( in the case of portrait images , we take the top , center and bottom squares ) . for each square , we then take the 4 corners and the center crop as well as the square res ##ized to , and their mirrored versions . this results in crops per image . a similar approach was used by andrew howard in the previous year ' s entry , which we empirical ##ly verified to perform slightly worse than the proposed scheme . we note that such aggressive crop ##ping may not be necessary in real applications , as the benefit of more crops becomes marginal after a reasonable number of crops are present ( as we will show later on ) . the soft ##max pro ##ba ##bilities are averaged over multiple crops and over all the individual class ##ifiers to obtain the final prediction . in our experiments we analyzed alternative approaches on the validation data , such as max pool ##ing over crops and averaging over class ##ifiers , but they lead to inferior performance than the simple averaging . in the remainder of this paper , we analyze the multiple factors that contribute to the overall performance of the final submission . our final submission in the challenge obtain ##s a top - 5 error of 6 . 67 % on both the validation and testing data , ranking the first among other participants . this is a 56 . 5 % relative reduction compared to the supervision approach in 2012 , and about 40 % relative reduction compared to the previous year ' s best approach ( cl ##ari ##fa ##i ) , both of which used external data for training the class ##ifiers . the following table shows the statistics of some of the top - performing approaches . we also analyze and report the performance of multiple testing choices , by varying the number of models and the number of crops used when predicting an image in the following table . when we use one model , we chose the one with the lowest top - 1 error rate on the validation data . all numbers are reported on the validation data ##set in order to not over ##fi ##t to the testing data statistics . section : il ##s ##vr ##c 2014 detection challenge setup and results the il ##s ##vr ##c detection task is to produce bound ##ing boxes around objects in images among 200 possible classes . detected objects count as correct if they match the class of the ground ##tr ##uth and their bound ##ing boxes overlap by at least 50 % ( using the ja ##cca ##rd index ) . extra ##neo ##us detection ##s count as false positive ##s and are penal ##ized . contrary to the classification task , each image may contain many objects or none , and their scale may vary from large to tiny . results are reported using the mean average precision ( map ) . the approach taken by google ##net for detection is similar to the r - cnn by , but is augmented with the inception model as the region class ##ifier . additionally , the region proposal step is improved by combining the selective search approach with multi - box predictions for higher object bound ##ing box recall . in order to cut down the number of false positive ##s , the super ##pi ##x ##el size was increased by . this halves the proposals coming from the selective search algorithm . we added back 200 region proposals coming from multi - box resulting , in total , in about 60 % of the proposals used by , while increasing the coverage from 92 % to 93 % . the overall effect of cutting the number of proposals with increased coverage is a 1 % improvement of the mean average precision for the single model case . finally , we use an ensemble of 6 con ##vn ##ets when classify ##ing each region which improves results from 40 % to 43 . 9 % accuracy . note that contrary to r - cnn , we did not use bound ##ing box regression due to lack of time . we first report the top detection results and show the progress since the first edition of the detection task . compared to the 2013 result , the accuracy has almost doubled . the top performing teams all use con ##vo ##lu ##tion ##al networks . we report the official scores in table [ reference ] and common strategies for each team : the use of external data , ensemble models or context ##ual models . the external data is typically the il ##s ##vr ##c ##12 classification data for pre - training a model that is later refined on the detection data . some teams also mention the use of the local ##ization data . since a good portion of the local ##ization task bound ##ing boxes are not included in the detection data ##set , one can pre - train a general bound ##ing box reg ##ress ##or with this data the same way classification is used for pre - training . the google ##net entry did not use the local ##ization data for pre ##train ##ing . in table [ reference ] , we compare results using a single model only . the top performing model is by deep insight and surprisingly only improves by 0 . 3 points with an ensemble of 3 models while the google ##net obtain ##s significantly stronger results with the ensemble . section : conclusions our results seem to yield a solid evidence that approx ##imating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural networks for computer vision . the main advantage of this method is a significant quality gain at a modest increase of computational requirements compared to shallow ##er and less wide networks . also note that our detection work was competitive despite of neither utilizing context nor performing bound ##ing box regression and this fact provides further evidence of the strength of the inception architecture . although it is expected that similar quality of result can be achieved by much more expensive networks of similar depth and width , our approach yields solid evidence that moving to sparse ##r architecture ##s is feasible and useful idea in general . this suggest promising future work towards creating sparse ##r and more refined structures in automated ways on the basis of . section : acknowledge ##ments we would like to thank san ##jee ##v ar ##ora and adi ##tya b ##has ##kara for fruit ##ful discussions on . also we are ind ##eb ##ted to the di ##st ##bel ##ie ##f team for their support especially to raja ##t mon ##ga , jon sh ##len ##s , alex k ##riz ##he ##vsky , jeff dean , il ##ya su ##tsk ##ever and andrea from ##e . we would also like to thank to tom due ##ri ##g and ni ##ng ye for their help on photo ##metric distortion ##s . also our work would not have been possible without the support of chuck rosenberg and hart ##wig adam . bibliography : references",
        "pred_seq": "[SEP] google ##net [SEP] [SEP] image ##net [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "googlenet"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "imagenet"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "imagenet"
                    ]
                ],
                "Method": [
                    [
                        "inception module",
                        "inception model",
                        "inception",
                        "inception architecture"
                    ]
                ],
                "Metric": [
                    [
                        "top1 accuracy",
                        "top1 error rate"
                    ]
                ],
                "Task": [
                    [
                        "classification",
                        "classification challenge",
                        "ilsvrc12 classification data"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "imagenet"
                    ]
                ],
                "Method": [
                    [
                        "inception module",
                        "inception model",
                        "inception",
                        "inception architecture"
                    ]
                ],
                "Metric": [
                    [
                        "top5 error rate",
                        "top5 error"
                    ]
                ],
                "Task": [
                    [
                        "classification",
                        "classification challenge",
                        "ilsvrc12 classification data"
                    ]
                ]
            },
            {
                "Material": [],
                "Method": [
                    [
                        "inception module",
                        "inception model",
                        "inception",
                        "inception architecture"
                    ]
                ],
                "Metric": [
                    [
                        "mean average precision",
                        "map"
                    ]
                ],
                "Task": [
                    [
                        "detection",
                        "object detection"
                    ]
                ]
            }
        ]
    },
    "18": {
        "doctext": "image super - resolution via feature - augmented random forest section : abstract - recent random - forest ( rf ) - based image super - resolution approaches inherit some properties from dictionary - learning - based algorithms , but the effectiveness of the properties in rf is overlooked in the literature . in this paper , we present a novel feature - augmented random forest ( far ##f ) for image super - resolution , where the conventional gradient - based features are augmented with gradient magnitude ##s and different feature recipes are formulated on different stages in an rf . the advantages of our method are that , firstly , the dictionary - learning - based features are enhanced by adding gradient magnitude ##s , based on the observation that the non - linear gradient magnitude are with highly disc ##rim ##ina ##tive property . secondly , generalized locality - sensitive hash ##ing ( l ##sh ) is used to replace principal component analysis ( pc ##a ) for feature dimensional ##ity reduction and original high - dimensional features are employed , instead of the compressed ones , for the leaf - nodes ' reg ##ress ##ors , since reg ##ress ##ors can benefit from higher dimensional features . this original - compressed coupled feature sets scheme un ##ifies the un ##su ##per ##vis ##ed l ##sh evaluation on both image super - resolution and content - based image retrieval ( cb ##ir ) . finally , we present a generalized weighted ridge regression ( g ##wr ##r ) model for the leaf - nodes ' reg ##ress ##ors . experiment results on several public bench ##mark data ##set ##s show that our far ##f method can achieve an average gain of about 0 . 3 db , compared to traditional rf - based methods . furthermore , a fine - tuned far ##f model can compare to or ( in many cases ) out ##per ##form some recent state ##of - the - art deep - learning - based algorithms . section : introduction in the past few years , random forest ( rf ) [ reference ] [ reference ] as a machine - learning tool , working via an ensemble of multiple decision trees , has been employed for efficient classification or regression problems , and applied to a large variety of computer - vision applications , such as object recognition [ reference ] , face alignment [ reference ] [ reference ] [ reference ] , data cluster ##ing [ reference ] , single image super - resolution ( sis ##r ) [ reference ] [ reference ] , and so on . the rf method , which benefits from its simple implementation of binary trees , has been widely used , and exhibits a number of merits , including ( 1 ) it works with an ensemble of multiple decision trees to express the principle that \" two heads are better than one \" , [ reference ] it is easy to be sped up with parallel processing technology , on both the training and inference stages , ( 3 ) it has sub - linear search complexity , because of the use of the binary tree structure , ( 4 ) the bag ##ging strategy for feature candidates on split ##no ##des enable it to handle high - dimensional features and avoid over - fitting on regression , and ( 5 ) the cluster ##ing - regression scheme employs the \" divide and conquer \" strategy , which can tackle the classification and regression tasks with more stable performance . the rf - based image super - resolution approach can be considered as a cluster ##ing / classification ##base ##d method , as shown in fig . 1 . but the cluster ##ing and regression problems in rf require with different disc ##rim ##ina ##tive features , which have not been systematically studied in existing literature . feature engineering has been a research hot ##sp ##ot for decades . several features have been proposed for learning the mapping functions from low - resolution ( l ##r ) patches to high - resolution ( hr ) patches on image restoration problems . pioneer work in [ reference ] used a simple high - pass filter as simple as sub ##tra ##cting a low - pass filtered values from the input image raw values . meanwhile , most algorithms [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] follow the approach in [ reference ] , which con ##cate ##nate ##s the first - and second - order gradient ##s to form the features , as an inexpensive solution to approx ##imating high - pass filtering . since rf is used as a dictionary ##lea ##rn ##ing - based tool , it inherit ##s many properties from the conventional dictionary - learning - based algorithms on feature extraction . however , the disc ##rim ##ina ##tive ability of those gradient - based features for random forest has been overlooked in the literature . we found , from experiments , that augmented features based on two gradient - magnitude filters can achieve more than 0 . 1 ##db quality improvement in rf based sis ##r , with the same parameter setting . in most dictionary - learning - based algorithms , principal component analysis ( pc ##a ) is used for dimensional ##ity reduction before classification and regression processes . the impact of using pc ##a has also been paid less attention in the literature . pc ##a projection may damage the structure of features , which are originally disc ##rim ##ina ##tive for cluster ##ing at the split - nodes and regression at the leaf - nodes . motivated from content - based image retrieval ( cb ##ir ) [ reference ] [ reference ] , where the coarse - level search uses compressed features , while the fine - level search uses augmented features . therefore , in our method , we use the original features rather than the compressed features generated by pc ##a as worked in [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] , so that more accurate regression and higher image quality improvement can be achieved . moreover , the un ##su ##per ##vis ##ed locality - sensitive hash ##ing ( l ##sh ) model , instead of pc ##a , is employed for feature dimensional ##ity reduction , which can reduce the damage on the feature structure for the compressed features used on cluster ##ing at the split - nodes and thus improve the final image quality . for regression problems at the leaf - nodes , we propose a generalized weighted ridge regression ( g ##wr ##r ) as an extension of the work in [ reference ] . g ##wr ##r models are generated based on the data distributions from the leaf - nodes . the main contribution of our method is on feature aug ##ment ##ation , so we call our method feature ##au ##gm ##ented random forest ( far ##f ) . the pipeline of our far ##f method , which includes feature extraction , the training stage , and inference stages for sis ##r , is shown in fig . 1 . in the far ##f - based image sr scheme , higher disc ##rim ##ina ##tive features are extracted by using the first - and second - order gradient ##s and their magnitude ##s . then , the conventional pc ##a is replaced by the generalized l ##sh for dimensional ##ity reduction , and the compressed features are used for cluster ##ing in the split - nodes on an rf . finally , the respective reg ##ress ##ors at the leaf - nodes are learned by using the original high dimensional features with the g ##wr ##r models . having introduced the main idea of our paper , the remainder of this paper is organized as follows . in section 2 , we review the related works on sis ##r , particularly the rf - based approaches and our insights . in section 3 , we introduce the proposed method far ##f , including the disc ##rim ##ina ##tive feature augmented by the gradient - magnitude filters , the generalized weighted ridge regression ( g ##wr ##r ) model , and the fine - tuned far ##f version . in section 4 , we evaluate our far ##f scheme on public data ##set ##s , and conclusions are given in section 5 . section : image super - resolution via random forest section : image super - resolution image sr attempts to achieve an impressive hr quality image from one or a set of l ##r images via artistic skills , which has been an active research topic for decades in the image restoration area . generalized sr includes inter ##pol ##ation algorithms , such as the classic bi ##cu ##bic inter ##pol ##ation , and other edge - preserving algorithms [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . the traditional super - resolution algorithms are based on pixel operations . intuitive ##ly , operating on a \" big pixel \" , i . e . a patch [ reference ] , is more effective . since patch - based algorithms can preserve the local texture structure of an image , various methods based on image patches , such as non - local means [ reference ] , self - similarity [ reference ] , manifold learning [ reference ] , block - matching and 3d filtering ( b ##m ##3d ) [ reference ] , sparse representation [ reference ] , etc . have been proposed . the neighbor - em ##bed ##ding ( ne ) methods [ reference ] [ reference ] are the milestone for patch - based dictionary learning methods . ne learns the mapping between low - and high - resolution patches , with the use of manifold learning . based on the locally linear em ##bed ##ding ( ll ##e ) theory , an l ##r patch can be represented as a linear combination of its nearest neighbors in a learned dictionary , and its hr counterpart can be approximate ##d as a linear combination of the corresponding hr patches of its l ##r neighbors , with the same coefficients . although the ne method is simple and sounds practical , a problem with the method is how to build a feasible patch dictionary . for example , for a patch size of 5 ##\u00d7 ##5 , with 256 gray levels , it is necessary to have a massive data ##set , which has millions of patches , in order to achieve high - quality reconstructed hr patches , if the patches are collected directly from natural scene images . because of the large dictionary size , it is time consuming to search for a neighbor in such a large data ##set . other method to reduce the dictionary size is to learn a relatively smaller dictionary with discrete co ##sin ##e transform ( dc ##t ) or wave ##let fixed basis , which the adaptive ##ness is sacrificed . in 2010 , yang et al . [ reference ] proposed a sparse prior for dictionary learning . using sparse coding , image representation can work with a relatively smaller dictionary while keep the adaptive ##ness by learning the basis from data directly , which opens the era for sparse coding in the image inverse problems . with the sparse constraint used in the sparse - coding super - resolution ( sc ##sr ) framework , an l ##r patch and its corresponding hr patch can both be reconstructed through two learned coupled di ##ction ##aries , with the same coefficients as following : where and denote an l ##r patch and its hr counterpart , respectively , and d and [UNK] are the low and high - resolution coupled di ##ction ##aries trained jointly from l ##r and hr patch samples . the value of in \u2016 \u2016 [UNK] is the spa ##rs ##ity factor of the coefficients . \u2016 \u2016 0 , called the 0 - norm , is the non - zero count of the coefficients in . the l ##r and hr coupled di ##ction ##aries are trained jointly with a spa ##rs ##ity constraint , as following : an l ##r patch of an input l ##r image y can be formulated in terms of d as following : where is a feature - extraction operator on the l ##r patches , which aims to extract disc ##rim ##ina ##tive features from l ##r patches , rather than using the raw pixel intensity . although the 0 - norm of ##\u03b1 is an ideal regular ##ization term for the sparse constraint , this strong constraint leads to an np - hard problem in solving the coefficients ##\u03b1 . yang et al . [ reference ] relaxed the 0 - norm to 1 - norm , so as to achieve a feasible solution as following : and an equivalent formulation can be achieved by using the la ##gra ##nge multi ##pl ##ier , where the parameter balance ##s the spa ##rs ##ity of the solution and the fidelity of the approximation to . as the sparse constraint in [ reference ] is still a bottle ##neck on training di ##ction ##aries considering the computation , an intuitive way to solve it is to relax the constraint again to 2 - norm . meanwhile , the effectiveness of spa ##rs ##ity is challenged [ reference ] [ reference ] by researchers as to whether spa ##rs ##ity or collaborative representation really helps in image classification and restoration . as a natural solution to that , tim ##oft ##e et al . proposed an anchored neighborhood regression ( an ##r ) [ reference ] framework , where there is no sparse constraint in the model . an ##r replaces the sparse - decomposition optimization ( 1 - norm ) with a ridge regression ( i . e . 2 - norm ) , where the coefficients can be computed off ##line and each coefficient can be stored as an atom ( anchor ) in the dictionary . this off ##line learning can greatly speed - up the prediction stage , and this approach has subsequently led to several variant algorithms . tim ##oft ##e et al . later extended the an ##r approach to the a + [ reference ] . in a + [ reference ] , the coupled di ##ction ##aries are trained from a large pool of training samples ( in the order of millions ) rather than only from the anchor ##ing atoms , which greatly improves the image quality . after that , more extensions based on an ##r and a + have emerged [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . however , in the above - mentioned dictionary - learning methods , the complexity of finding those similar patches by comparing an input patch with all the dictionary items has been overlooked . recently , algorithms using random forest ( rf ) [ reference ] [ reference ] [ reference ] have achieved state - of - the - art performances , in terms of both accuracy and efficiency for classification and regression tasks . this is mainly due to the use of ensemble learning and sub ##line ##ar search based on binary trees . sc ##hul ##ter et al . [ reference ] adopted random forest and the cluster ##ing - regression scheme to learn reg ##ress ##ors from the patches in leaf - nodes for sis ##r . with the same number of reg ##ress ##ors , the rf - based algorithm can out ##per ##form or achieve comparable performance with a + and its variants , in terms of accuracy but with less computational complexity . in recent years , deep learning has achieved promising performances on image super - resolution [ reference ] [ reference ] [ reference ] [ reference ] . in [ reference ] [ reference ] , milestone works on image super - resolution based on deep learning were presented , where a con ##vo ##lu ##tion ##al neural network ( sr ##c ##nn ) was proposed to learn an end - to - end mapping between l ##r and hr images for image super - resolution . later a scheme with very deep networks for sis ##r was proposed in [ reference ] , where the convergence rate of the deep network is improved by using residual learning and extremely high learning rates . in addition , led ##ig et al . [ reference ] introduced a genera ##tive ad ##vers ##aria ##l network ( gan ) based image super - resolution model ( sr ##gan ) , where the image per ##ce ##pt ##ual loss function is reform ##ulated as the combination of content loss and ad ##vers ##aria ##l loss . although deep ##lea ##rn ##ing - based approaches have achieved promising progress on sis ##r , the heavy computational requirement is still a large burden even though the implementation is accelerated by gp ##u . this may limit them from those applications without powerful gp ##u , such as smart mobile terminals . cluster all the feature data assigned to this node . this results in separating the three data samples ( quad ##rangle , pentagon and he ##xa ##gon ) into three leaf nodes . section : image super - resolution via random forest in the inference stage , each decision tree returns a class probability ( | ) for a given test sample ##\u2208 , and the final class label * is then obtained via averaging , as follows : a splitting function ( ; \u03b8 ) is typically parameter ##ized by two values : ( i ) a feature dimensional index : \u03b8 { 1 , . . . , } , and ( ii ) a threshold ##\u03b8 ##\u211d . the splitting function is defined as follows : where the outcome defines to which child node is routed , and 0 and 1 are the two labels belonging to the left and right child node , respectively . each node chooses the best splitting function ##\u03b8 * out of a randomly sampled set { \u03b8 } , and the threshold ##\u03b8 is determined by opt ##imi ##zing the following function : where and are the sets of samples that are routed to the left and right child nodes , respectively , and | | represents the number of samples in the set . during the training of an rf , the decision trees are provided with a random subset of the training data ( i . e . bag ##ging ) , and are trained independently . training a single decision tree involves rec ##urs ##ively splitting each node , such that the training data in the newly created child node is clustered conform ##ing to class labels . each tree is grown until a stopping criterion is reached ( e . g . the number of samples in a node is less than a threshold or the tree depth reaches a maximum value ) and the class probability distributions are estimated in the leaf nodes . after fulfilling one of the stopping criteria , the density model ( ) in each leaf node is estimated by using all the samples falling into the leaf node , which will be used as a prediction of class pro ##ba ##bilities in the inference stage . a simple way to estimate the probability distribution function ( ) is by averaging all the samples in the leaf node , and there are many variants , such as fitting a ga ##uss ##ian distribution , kernel density estimation , etc . in ( 9 ) , ( ) is the local score for a set of samples in s ( s is either l or r ) , which is usually calculated by entropy , as shown in e ##q ##n . [ reference ] , and it can be replaced by variance [ reference ] [ reference ] [ reference ] or by the gin ##i index [ reference ] . where is the number of classes , and ( | ) is the probability for class , which is estimated from the set . for the regression problem , the differential entropy is used , and is defined as , where ( | ) denotes the conditional probability of a target variable given an input sample . assuming that ( . , . ) is of ga ##uss ##ian distribution , and has only a set of finite samples , the differential entropy can be written as , where det ( \u03c3 ) is the deter ##mina ##nt of the estimated co ##var ##iance matrix of the target variables in . rf - based approaches hold some properties , which make them powerful class ##ifiers as sv ##m ( support vector machine ) [ reference ] and ada ##bo ##ost ( short for \" adaptive boost ##ing \" ) [ reference ] . both sv ##m and ada ##bo ##ost work as to approximate the bay ##es decision rule - known to be the optimal class ##ifiers - via mini ##mi ##zing a margin - based global loss function . rf - based image super - resolution ( sr ) , following a recent emerging stream [ reference ] [ reference ] on single - image sr , formula ##tes the sr problem as a cluster ##ing - regression problem . these emerging approaches attempt to rec ##ons ##truct an hr image from patches with the aid of an external database . these methods first deco ##mp ##ose an image into patches , then classify the patches into different clusters , and later reg ##ress ##ors are trained for all the clusters respectively , which generate mapping ##s from an input l ##r patch ' s features to its corresponding hr patch . in the inference stage , an l ##r image follows the same procedures , such that it is divided into patches and features are extracted from each patch . then , the patches are classified into different clusters using k - n ##n [ reference ] [ reference ] or rf [ reference ] [ reference ] [ reference ] , and their super - resolved hr patches are computed through regression in the leaf nodes ( see fig . 1 ) . this kind of cluster ##ing - regression - based random forest [ reference ] [ reference ] [ reference ] methods has achieved state - of - the - art performance in sis ##r , both in terms of accuracy and efficiency . section : feature - augmented random forest classification and regression can be regarded as probability problems from the statistical theory . historical frequent ##ist probability is the probability obtained from the relative frequency in a large number of trials . in contrast , the bay ##esian probability is an interpretation of the concept of probability , in which probability is interpreted as an expectation taking the knowledge and personal belief into account . from the bay ##esian theory , the posterior probability of a random event is a conditional probability , which can be calculated if the relevant evidence or context is considered . therefore , the posterior probability is the probability ( | ) of the parameters given the evidence . we denote the probability distribution function of the prior for parameters as ( ) , and the likelihood as ( | ) , which is the probability of given . then , based on the bay ##esian rule , the posterior probability can be defined as follows : the posterior probability can be denoted in a memorable form as : based on the bay ##esian framework , the likelihood term and the prior term are both required to be determined in order to solve the inverse problems , and the extracted features are normally worked as prior or likelihood , particularly on some image restoration problems . from this point of view , most research works , from classic feature extract ##ors to deep - learning neural networks , are essentially done under the bay ##esian inference framework . since sis ##r is a well - known ill - posed problem , researchers have put their efforts into the prior ##s of the problem with skills from mathematics , computer vision and machine learning . one of the obvious and most studied prior ##s is the edge prior , which can be found in many pioneering works : new edged ##ire ##cted inter ##pol ##ation ( ned ##i ) [ reference ] , soft - decision adaptive inter ##pol ##ation ( sai ) [ reference ] , directional filtering and data - fusion ( d ##f ##df ) [ reference ] , modified edge - directed inter ##pol ##ation ( med ##i ) [ reference ] , and so on . the edge prior is effective on image processing , and the first and second - order gradient ##s are studied and employed by yang et al . [ reference ] in a pioneering dictionary - learning - based algorithm . however , the effect of edge ##base ##d features has not been investigated in depth . for the cluster ##ing and classification problems , feature engineering is a critical research point , and in some cases , the chosen feature may dominate the performance . as shown in e ##q ##n . ( 6 ) , a feature filter , whose coefficients are computed to fit the most relevant parts in the l ##r image patches , is employed , and the generated features can achieve more accurate predictions for rec ##ons ##tructing their counterpart hr image patches , as shown in fig . 3 . section : augmented features via gradient magnitude filters normally it is unstable to directly use pixel int ##ens ##ities as features , which are susceptible to the environmental lighting variations and camera noise . instead , the differences between the neighboring pixels ' intensity values , which are computational ##ly efficient , and are immune to lighting changes and noise , are examined . this type of features can be implemented efficiently through con ##vo ##lu ##tion ##al filters . typically , the feature filter can be chosen as a high - pass filter , while in [ reference ] [ reference ] [ reference ] [ reference ] , the first and second - order gradient operators are used to generate an up - sampled version from a low - resolution image , then four patches are extracted from the gradient maps at each location , and finally the patches are con ##cate ##nated to form feature vectors . the four 1 - d filters used to extract the derivatives are described in e ##q ##n . ( 14 ) , these features can work well on dictionary - learning - based methods , because when searching a matched patch in a dictionary , the distance is calculated based on the whole feature vectors with the euclidean distance . however , when training a split node in a decision tree of an rf , only one or a few of the feature dimensions are chosen as candidate features for comparison . therefore , more disc ##rim ##ina ##tive features are required for rf , when compared with dictionary - learning - based methods . the first and second - order gradient ##s of an image can provide the directions of edges in a per ##ce ##pt ##ual manner as shown in fig . 4 and fig . 5 , which can be calculated as e ##q ##n . ( 15 ) , where / and / are the gradient ##s in the x - axis and y - axis directions , respectively , at a given pixel . meanwhile , the gradient magnitude image can provide the edge strength , as described in e ##q ##n . [ reference ] . fig . 4 shows a toy example of a man - made \" circle \" image , to demonstrate its disc ##rim ##ina ##tive property . with a natural image shown in fig . 5 , it can be observed that the gradient magnitude image has more detailed textures than the gradient images ( / and / ) , as well as the sum of the horizontal gradient and vertical gradient image , i . e . / + / , per ##ce ##pt ##ually . an explanation for this phenomenon is that non - linear features are usually more disc ##rim ##ina ##tive . thus , in our work , all the first and second - order gradient ##s , and gradient magnitude are employed , and are con ##cate ##nated to form more disc ##rim ##ina ##tive , augmented features . on the other hand , the image orientation ( gradient angle ) is defined by the following formulation , where ata ##n ( ) is the gradient orientation , with a value between - 90 and 90 . as shown in e ##q ##n . ( 17 ) , when the value of is equal to 0 or close to 0 , the value [UNK] becomes infinitely large and unstable , i . e . , different will result in approximately the [UNK] value . based on this analysis , we only use the two gradient magnitude filters derived from the four gradient filters [ reference ] to generate the augmented features . experiments valid ##ate that the use of the augmented features can improve the conventional rf algorithm [ reference ] to achieve a performance gain of more than 0 . 1 ##db , which is a remarkable improvement , with the same setting and parameters . section : fine - grain ##ed features for regression the inference stage of the rf - based image super - resolution process is similar to the content - based image retrieval ( cb ##ir ) framework , as shown in fig . 1 . the general approximate ##d nearest neighbor ( ann ) search framework [ reference ] [ reference ] is an efficient strategy for large - scale image retrieval , which mainly consists of 4 parts : ( 1 ) extract ##ing compact features ( e . g . , locality - sensitive hash ##ing ( l ##sh ) [ reference ] feature ) for a query image ; ( 2 ) coarse - level search using ham ##ming distance to measure the similarity between binary compact hash features , then narrow the search scope into a smaller candidate group ; ( 3 ) fine - level search by using euclidean distance to measure the similarity between their corresponding feature vectors ; and ( 4 ) finding the object in the smaller candidate group that is the nearest one to the query image . in the inference stage of conventional rf - based sis ##r , pc ##a projection is worked as a hash - like function to com ##press the feature dimension for decreasing the search range , which can speed up the searching as the coarse - level search in a cb ##ir framework , but the impact of using pc ##a on feature dimensional ##ity reduction has been overlooked in previous works [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . inspired by the fine ##lev ##el search using augmented features in cb ##ir framework ##s , the high dimensional features in the leaf nodes in an rf can further improve the prediction accuracy in the regression step , which has not been studied previously . consequently , we use the original features , rather than pc ##a or the l ##sh compressed features , to perform ridge regression in the leaf nodes . experimental results show that the new rf scheme can greatly improve the quality of super - resolved images , by using this augmented feature . another explanation for this is that the regression problems can benefit more from higher dimensional features than classification problems . based on the observation that the original edge - like features are used for the final reg ##ress ##ors in the leaf nodes and the compressed features ( either produced by pc ##a or l ##sh ) are used for cluster ##ing in the split nodes , a new cluster ##ing - regression - based sis ##r approach can be designed as shown in fig . 6 . in this new scheme , the original - compressed coupled feature sets are worked for different purposes at different stages , i . e . , the original edge features are used for regression in the leaf nodes , and the compressed features derived from the l ##sh - like functions are employed for node splitting ( cluster ##ing ) in the training stage , and node searching in the inference stage in the split nodes . section : fig . 6 : augmented features for reg ##ress ##ors and the l ##sh compressed features for searching in a random forest in the new scheme , we un ##ify the research of l ##sh - based sis ##r and image retrieval ( cb ##ir ) [ reference ] [ reference ] . in brief , the new achievement on un ##su ##per ##vis ##ed l ##sh can be evaluated not only in cb ##ir systems , but also in the cluster ##ing - regression rf - based sis ##r methods . moreover , as evidence from [ reference ] , proper un ##su ##per ##vis ##ed l ##sh models , e . g . , it ##erative quan ##ti ##zation ( it ##q ) [ reference ] used for feature dimension reduction instead of pc ##a , can reduce the damage on the image structure . this can further improve the super ##res ##olved image quality . different from [ reference ] using an it ##q - like algorithm to rotate the original features into a new feature space , with the use of the proposed original - compressed coupled feature sets , any un ##su ##per ##vis ##ed l ##sh generated features can directly be employed . section : generalized weighted ridge regression model in this sub - section , we further analyze the ridge regression employed in the rf leaf nodes . the anchored neighborhood regression ( an ##r ) [ reference ] model relax ##es the 1 - norm in e ##q ##n . ( 6 ) to the 2 - norm constraint , with least - squares mini ##mi ##zation as the following equation , based on the ridge regression [ reference ] theory , this 2 - norm constrained least square regression regular ##ized problem has a closed - form solution , according to the ti ##kh ##ono ##v regular ##ization theory , as follows : with the assumption in [ reference ] , where hr patches and their counterpart l ##r patches share the same reconstructed coefficient ##\u03b1 , i . e . = [UNK] , from e ##q ##n . [ reference ] we have if we define as a pre - calculated projection matrix , as follows , then the hr patches can be reconstructed with = . having studied the model in e ##q ##n . ( 18 ) , the authors in [ reference ] argued that different weights should be given to different atoms when rec ##ons ##tructing an hr patch so as to emphasize the similarity to the anchor atom . based on this idea , [ reference ] proposed a weighted collaborative representation ( wc ##r ) model by general ##izing the normal collaborative representation ( cr ) model in the an ##r , where is a diagonal weight matrix , in which the non - zero entries are proportional to the similarities between the atoms and the anchor atom . same as the an ##r model , a new closed - form solution can be computed off ##line through the following and the new projection matrix can be derived as the wc ##r model further improves the an ##r / a + model in terms of image quality , while keeping the same level of computation . in [ reference ] , the local geometry prior of the data sub - space is used . however , all the weighted ridge regression models [ reference ] [ reference ] are constructed based on an existing dictionary , e . g . , ze ##yde et al . [ reference ] used k - sv ##d to train a sparse - coding - based dictionary with 102 ##4 items . this limits the models to collect samples in a smaller sub - space when constructing linear reg ##ress ##ors based on existing anchor points . section : fig . 7 : ga ##uss ##ian mixture model ( gm ##m ) is used to generate the weights for weighted ridge regression , and the weight of each entry lies on its belonging cluster ' s weight and its weight in the belonging cluster . when training the reg ##ress ##ors in an rf , there is no existing anchor point in the clustered groups of the leaf nodes , similar to the previous models [ reference ] [ reference ] . a solution to mentioned problem is inspired from the work on image classification using locality - constrained linear coding ( llc ) [ reference ] , where ga ##uss ##ian mixture model ( gm ##m ) is used to describe the locality - constrained af ##fine subsp ##ace coding ( las ##c ) [ reference ] . we employ gm ##m to construct the data distribution in the sub - space for each leaf node , which derives the weights of all the entries in the ridge regression models . through the derived weights , we can obtain a generalized weighted ridge regression ( g ##wr ##r ) model for ridge regression . the new projection matrix is given as follows : where is a diagonal weight matrix , and the weight of each diagonal entry is related to its belonging cluster ' s weight and its local weight in its belonging ##w ##hi cluster , as illustrated in the right part of fig . 7 . obviously , a query entry falling into a bigger cluster and closer to the center of the belonging cluster achieve ##s a larger weight . in a rough form , the diagonal weight matrix is given as follows : where is the weight of the th entry , is number of samples in the leaf nodes , is the th cluster ' s weight for the th entry , is the th entry ' s local weight in the th cluster , which is approximate ##d with the inverse value of the distance to the center of the belonging cluster , and is the number of clusters generated by the gm ##m model for a leaf node . experimental results in table - 1 show that the proposed g ##wr ##r model can achieve the same level of performance as wc ##r [ reference ] , and obtain 0 . 2d ##b gain more than the an ##r [ 1 ] model . note that when the number of samples in a leaf node becomes bigger , the performance of the g ##wr ##r model will achieve less advantage than the normal regression model , because the higher weights will be averaged by a large number of other samples . theoretically , the regression of a leaf node can benefit from the g ##wr ##r model , particularly when there are a few samples falling into the leaf node . section : initial estimation with it ##erative back projection generally speaking , sis ##r is a low - level computer vision task , which attempts to restore an hr image from a single input l ##r image . a mathematical model for image degradation can be formulated as follows : [UNK] is a low - pass ( blur ) filter and denotes the down - sampling operator with factor . based on a given l ##r image , how to achieve an approximate ##d hr [UNK] is a classic inverse problem , which requires prior ##s based on the bay ##esian theory . iran ##i and pe ##leg [ reference ] firstly proposed an it ##erative back projection ( ib ##p ) method for sr reconstruction , and ib ##p is the most effective way to obtain an hr image when comparing it with other sr methods . in the ib ##p method , the reconstruction error of an estimated l ##r [UNK] is the difference between the input l ##r and the synthesized [UNK] generated from the estimated hr [UNK] as follows : ib ##p is an efficient approach to obtain the hr image by mini ##mi ##zing the reconstruction error defined by e ##q ##n . [ reference ] . for the ib ##p approach on sis ##r , the up ##dating procedure can be summarized as the following two steps , performed it ##erative ##ly : \u2022 compute the reconstruction error ( [UNK] ) with the following equation : where ##\u2191 is the up - sampling operator and is a constant back - projection kernel to approximate the inverse operation of the low - pass [UNK] . \u2022 update the est ##imating hr [UNK] by back - projecting errors as follows : [UNK] is the estimated hr image at the - th iteration . most learning - based algorithms [ reference ] [ reference ] [ reference ] [ reference ] follow the milestone work in [ reference ] , which uses the coarse estimation firstly obtained via bi ##cu ##bic inter ##pol ##ation . as we know , the classic ib ##p algorithm is an efficient way to obtain high - quality up - scaled images , but it will inevitably produce artifacts ( such as ringing , ja ##ggy effects , and noise ) at the output , because the kernel operator in e ##q ##n . ( 29 ) is hard to estimate accurately . that is the reason why algorithms with ib ##p need an additional den ##ois ##ing process [ reference ] [ reference ] [ reference ] . however , the sparse - constraint - based approach [ reference ] does not have this den ##ois ##ing capability . as the 2 - norm constraint - based ridge regression has the den ##ois ##ing effect , due to its averaging - like process , this means that the ridge regression - based rf scheme has the den ##oise capability intrinsic ##ally . based on this observation , we obtain the coarse estimation of an hr [UNK] by applying ib ##p to the corresponding input l ##r image . experimental results in table - 2 and table - 3 valid ##ate that using ib ##p , instead of bi ##cu ##bic , to obtain the initial coarse estimation can help the rf - based sr method obtain a remarkable improvement . as the number of trees is an important parameter in rf - based approaches , we plot the performance with respect to the number of trees . as shown in fig . 8 , the performance of the rf - based image super ##res ##ol ##ution method increases as expected , but the inc ##rem ##ent becomes relatively smaller after a certain number of trees are used . the experimental results in fig . 8 section : fine - tuning with proper trees in random forest section : ps ##nr section : set ##14 million samples from the data ##set are used for all training stages . it shows that using 45 trees is an optimal number , as a trade - off between performance and computational cost . therefore , we set the number of trees for the proposed far ##f method at 45 , and our method with this number is denoted as far ##f * . the performances of our methods , and other methods , are tab ##ulated in table - 2 and table - 3 . we also compare our methods with a recently proposed deep - learning - based algorithm , sr ##c ##nn algorithm [ reference ] [ reference ] , and our methods out ##per ##form it in some cases . section : algorithm work ##flow the training and inference stages of the proposed far ##f algorithm are described in algorithm 1 and algorithm 2 , respectively . to help the readers understand our paper , the source code of our algorithm will be available at : https : / / gi ##th ##ub . com / harley ##h ##k / far ##f , for reference . section : experiments in this section , we evaluate our algorithm on standard super - resolution bench ##marks set 5 , set ##14 and b1 ##00 [ reference ] , and compare it with some state - of - the - art methods . they are bi ##cu ##bic inter ##pol ##ation , adjusted anchored neighborhood regression ( a + ) [ reference ] , standard rf [ reference ] , alternating regression forests ( ar ##f ) [ reference ] , and the con ##vo ##lu ##tion ##al neural - network - based image super - resolution ( sr ##c ##nn ) [ reference ] [ reference ] , as listed in table - 2 and table - table - 2 : results of the proposed method compared with state - of - the - art works on 3 data ##set ##s in terms of ps ##nr ( db ) using three different mag ##ni ##fication factors ( # ) ( \u00d7 ##2 , \u00d7 ##3 , \u00d7 ##4 ) . table - 2 sum ##mar ##izes the performances of our proposed algorithm on the 3 data ##set ##s , in terms of the average peak signal to noise ratio ( ps ##nr ) scores , with different mag ##ni ##fication factors ( \u00d7 ##2 , \u00d7 ##3 , \u00d7 ##4 ) . the objective quality metric , ps ##nr , in table - 2 also shows that the fine - tuned far ##f , i . e . far ##f * , can further improve the image quality , which is comparable to recently proposed state - of - the - art deep ##lea ##rn ##ing - based algorithms , such as sr ##c ##nn [ reference ] [ reference ] . comparing our proposed far ##f algorithm to other methods , the improved visual quality of our results is obvious , as shown in fig . 9 . this shows that our method can produce more details , particularly on some texture - rich regions . fig . 9 : super - resolution ( \u00d7 ##3 ) images from b1 ##00 , bi ##cu ##bic , a + ( acc ##v - 2014 ) [ reference ] , ar ##f ( cv ##pr - 2015 ) [ reference ] , sr ##c ##nn ( pam ##i - 2016 ) [ reference ] , our proposed algorithm far ##f , and ground truth . the results show that our far ##f algorithm can produce more details and its performance is comparable to a recent state - of - the - art deep - learning method [ reference ] . section : conclusions this paper presents a feature - augmented random forest ( far ##f ) scheme for the single image super ##res ##ol ##ution ( sis ##r ) task by aug ##ment ##ing features and redesign ##ing the inner structure of a random forest ( rf ) , with different feature recipes at different stages , where the compressed features are used for cluster ##ing in the split nodes and the original features are used for regression in the leaf nodes . the contributions of this paper are three ##fold : ( 1 ) the more disc ##rim ##ina ##tive gradient magnitude - based augmented features are proposed for cluster ##ing on split nodes and regression on leaf nodes ; ( 2 ) by extending principal component analysis ( pc ##a ) to a generalized un ##su ##per ##vis ##ed locality - sensitive hash ##ing ( l ##sh ) model for dimensional ##ity reduction , we lay out an original compressed coupled feature set for tack ##ling the cluster ##ing - regression tasks , which un ##ify sis ##r and content - based image retrieval ( cb ##ir ) for l ##sh evaluation ; and ( 3 ) we have extended wc ##r model to a generalized g ##wr ##r model for ridge regression . the proposed fa ##fr scheme can achieve highly competitive quality results , e . g . , obtaining about a 0 . 3d ##b gain in ps ##nr , on average , when compared to conventional rf - based super - resolution approaches . furthermore , a fine - tuned version of our proposed far ##f approach is provided , whose performance is comparable to recent state - of - the - art deep - learning - based algorithms . section :",
        "pred_seq": "[SEP] [SEP] [SEP] image resolution [SEP] [unused0] [SEP] feature forest [SEP] image resolution [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "image superresolution"
                    ]
                ]
            },
            {
                "Material": [],
                "Method": [
                    [
                        "featureaugmented random forest"
                    ]
                ],
                "Metric": [
                    [
                        "image superresolution"
                    ]
                ],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "b100"
                    ]
                ],
                "Method": [
                    [
                        "fafr scheme"
                    ]
                ],
                "Metric": [
                    [
                        "psnr",
                        "average peak signal to noise ratio"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "single image superresolution",
                        "sisr",
                        "image superresolution model",
                        "srgan",
                        "rfbased image superresolution",
                        "sr",
                        "rfbased sisr",
                        "superresolved images",
                        "superresolution",
                        "single image superresolution"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set14"
                    ]
                ],
                "Method": [
                    [
                        "fafr scheme"
                    ]
                ],
                "Metric": [
                    [
                        "psnr",
                        "average peak signal to noise ratio"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "single image superresolution",
                        "sisr",
                        "image superresolution model",
                        "srgan",
                        "rfbased image superresolution",
                        "sr",
                        "rfbased sisr",
                        "superresolved images",
                        "superresolution",
                        "single image superresolution"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set 5"
                    ]
                ],
                "Method": [
                    [
                        "featureaugmented random forest",
                        "farf",
                        "featureaugmented random forest",
                        "farf",
                        "farf algorithm"
                    ]
                ],
                "Metric": [
                    [
                        "psnr",
                        "average peak signal to noise ratio"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "single image superresolution",
                        "sisr",
                        "image superresolution model",
                        "srgan",
                        "rfbased image superresolution",
                        "sr",
                        "rfbased sisr",
                        "superresolved images",
                        "superresolution",
                        "single image superresolution"
                    ]
                ]
            }
        ]
    },
    "19": {
        "doctext": "document : massive ##ly multi ##ling ##ual sentence em ##bed ##ding ##s for zero - shot cross - ling ##ual transfer and beyond we introduce an architecture to learn joint multi ##ling ##ual sentence representations for 93 languages , belonging to more than 30 different language families and written in 28 different scripts . our system uses a single bi ##ls ##tm en ##code ##r with a shared bp ##e vocabulary for all languages , which is coupled with an auxiliary deco ##der and trained on publicly available parallel corp ##ora . this enables us to learn a class ##ifier on top of the resulting sentence em ##bed ##ding ##s using english ann ##ota ##ted data only , and transfer it to any of the 93 languages without any modification . our approach sets a new state - of - the - art on zero - shot cross - ling ##ual natural language inference for all the 14 languages in the x ##nl ##i data ##set but one . we also achieve very competitive results in cross - ling ##ual document classification ( ml ##do ##c data ##set ) . our sentence em ##bed ##ding ##s are also strong at parallel corpus mining , establishing a new state - of - the - art in the bu ##cc shared task for 3 of its 4 language pairs . finally , we introduce a new test set of aligned sentences in 122 languages based on the ta ##to ##eb ##a corpus , and show that our sentence em ##bed ##ding ##s obtain strong results in multi ##ling ##ual similarity search even for low - resource languages . our p ##yt ##or ##ch implementation , pre - trained en ##code ##r and the multi ##ling ##ual test set will be freely available . section : introduction while the recent advent of deep learning has led to impressive progress in natural language processing ( nl ##p ) , these techniques are known to be particularly data hungry , limiting their app ##lica ##bility in many practical scenarios . an increasingly popular approach to alleviate this issue is to first learn general language representations on un ##lab ##ele ##d data , which are then integrated in task - specific downstream systems . this approach was first popularized by word em ##bed ##ding ##s mi ##ko ##lov ##20 ##13 ##dis ##tri ##bu ##ted , penn ##ington ##20 ##14 ##gl ##ove , but has recently been superseded by sentence - level representations alexis : 2017 : em ##nl ##p , peters : 2018 : na ##ac ##l _ elm ##o , devlin : 2018 : ar ##xi ##v _ bert . nevertheless , all these works learn a separate model for each language and are thus unable to leverage information across different languages , greatly limiting their potential performance for low - resource languages . in this work , we are interested in universal language ag ##nostic sentence em ##bed ##ding ##s , that is , vector representations of sentences that are general with respect to two dimensions : the input language and the nl ##p task . the motivation ##s for such a representation are multiple : the hope that languages with limited resources benefit from joint training over many languages , the desire to perform zero - shot transfer of an nl ##p model from one language ( e . g . english ) to another , and the possibility to handle code - switching . we achieve this by using a single en ##code ##r that can handle multiple languages , so that semantic ##ally similar sentences in different languages are close in the resulting em ##bed ##ding space . most research in multi ##ling ##ual nl ##p focuses on high - resource languages like chinese , arabic or major european languages , and is usually limited to a few ( most often only two ) languages . in contrast , we learn joint sentence representations for 93 different languages , including under - resource ##d and minority languages ( see tables [ reference ] and [ reference ] ) . our system is trained on freely available parallel texts only . the contributions of this paper are as follows : we substantially improve on previous work to learn joint multi ##ling ##ual sentence representations . we learn one shared en ##code ##r that can handle 93 different languages . all languages are jointly embedded in a shared space , in contrast to most other works which usually consider separate english / foreign alignment ##s . we cover 34 language families and 28 different scripts . we out ##per ##form the state - of - the - art on zero - shot cross - ling ##ual natural language inference ( x ##nl ##i data ##set ) and classification ( ml ##do ##c data ##set ) , bite ##xt mining ( bu ##cc data ##set ) and multi ##ling ##ual similarity search ( ta ##to ##eb ##a data ##set ) , for almost all considered languages . these results were obtained with a single pre - trained bi ##ls ##tm en ##code ##r for all 93 languages and tasks , without any fine - tuning . we define a new test set based on the freely available ta ##to ##eb ##a corpus and provide baseline results for 122 languages . we report accuracy for multi ##ling ##ual similarity search on this test set , but the corpus could also be used for mt evaluation . the remaining of this paper is organized as follows . in the next section , we first sum ##mar ##ize related work . section [ reference ] then describe our approach in detail . all experimental results are given in sections [ reference ] and [ reference ] , and the paper concludes with a discussion and directions for future research . data ##set details and additional result analysis can be found in the appendix . section : related work following the success of word em ##bed ##ding ##s mi ##ko ##lov ##20 ##13 ##dis ##tri ##bu ##ted , penn ##ington ##20 ##14 ##gl ##ove , there has been an increasing interest in learning continuous vector representations of longer linguistic units like sentences . these sentence em ##bed ##ding ##s are commonly obtained using a rec ##urrent neural network ( rn ##n ) en ##code ##r , which is typically trained in an un ##su ##per ##vis ##ed way over large collections of un ##lab ##elled corp ##ora . for instance , the skip - thought model of couple the en ##code ##r with an auxiliary deco ##der , and train the entire system end - to - end to predict the surrounding sentences over a large collection of books . it was later shown that more competitive results could be obtained by training the en ##code ##r over labeled natural language inference ( nl ##i ) data alexis : 2017 : em ##nl ##p . this was recently extended to multi ##tas ##k learning , combining different training objectives like that of skip - thought , nl ##i and machine translation google : 2018 : ar ##xi ##v _ sr ##ep , ml ##ia - ms ##r : 2018 : ic ##lr _ sr ##ep . while the previous methods consider a single language at a time , multi ##ling ##ual representations have attracted a large attention in recent times . most of this research focuses on cross - ling ##ual word em ##bed ##ding ##s rude ##r ##20 ##17 ##sur ##vey , which are commonly learned jointly from parallel corp ##ora go ##uw ##s ##20 ##15 ##bil ##bow ##a , lu ##ong ##20 ##15 ##bil ##ing ##ual . an alternative approach that is becoming increasingly popular is to train word em ##bed ##ding ##s independently for each language over mono ##ling ##ual corp ##ora , and then map them to a shared space based on a bilingual dictionary mi ##ko ##lov ##20 ##13 ##ex ##pl ##oit ##ing , arte ##t ##x ##e ##20 ##18 ##gen ##eral ##izing or even in a fully un ##su ##per ##vis ##ed manner . cross - ling ##ual word em ##bed ##ding ##s are often used to build bag - of - word representations of longer linguistic units by taking their respective centro ##id . while this approach has the advantage of requiring a weak ( or even no ) cross - ling ##ual signal , it has been shown that the resulting sentence em ##bed ##ding ##s works rather poorly in practical cross - ling ##ual transfer settings con ##nea ##u : 2018 : em ##nl ##p _ x ##nl ##i . a more competitive approach that we follow here is to use a sequence - to - sequence en ##code ##r - deco ##der architecture sc ##h ##wen ##k : 2017 : rep ##l ##4 ##nl ##p , hassan ##20 ##18 ##achi ##eving . the full system is trained end - to - end on parallel corp ##ora akin to neural machine translation : the en ##code ##r maps the source sequence into a fixed - length vector representation , which is used by the deco ##der to create the target sequence . this deco ##der is then discarded , and the en ##code ##r is kept to em ##bed sentences in any of the training languages . while some proposals use a separate en ##code ##r for each language sc ##h ##wen ##k : 2017 : rep ##l ##4 ##nl ##p , sharing a single en ##code ##r for all languages also gives strong results sc ##h ##wen ##k : 2018 : ac ##l _ mine . nevertheless , most existing work is either limited to few , rather close languages or , more commonly , consider pair ##wise joint em ##bed ##ding ##s with english and one foreign language only . to the best of our knowledge , all existing work on learning multi ##ling ##ual representations for a large number of languages is limited to word em ##bed ##ding ##s am ##mar ##20 ##16 ##mas ##sive ##ly , ours being the first paper exploring massive ##ly multi ##ling ##ual sentence representations . finally , while all the previous approaches learn a fixed - length representation for each sentence , a recent research line has obtained very strong results using variable - length representations instead , consisting of context ##ual ##ized em ##bed ##ding ##s of the words in the sentence peters : 2018 : na ##ac ##l _ elm ##o , howard ##20 ##18 ##uni ##vers ##al , devlin : 2018 : ar ##xi ##v _ bert . for that purpose , these methods train either an rn ##n or self - attention ##al en ##code ##r over un ##nan ##ota ##ted corp ##ora using some form of language modeling . a class ##ifier can then be learned on top of the resulting en ##code ##r , which is commonly further fine - tuned during this supervised training . despite the strong performance of these approaches in mono ##ling ##ual settings , we argue that fixed - length approaches provide a more generic , flexible and compatible representation form for our multi ##ling ##ual scenario , and our model indeed out ##per ##forms the multi ##ling ##ual bert model devlin : 2018 : ar ##xi ##v _ bert in zero - shot transfer ( see section [ reference ] ) . section : proposed method we use a single , language ag ##nostic bi ##ls ##tm en ##code ##r to build our sentence em ##bed ##ding ##s , which is coupled with an auxiliary deco ##der and trained over parallel corp ##ora . from section [ reference ] to [ reference ] , we describe its architecture , our training strategy to scale to up to 93 languages , and the training data used for that purpose . sub ##section : architecture figure [ reference ] illustrates the architecture of the proposed system , which is based on sc ##h ##wen ##k : 2018 : ac ##l _ mine . as it can be seen , sentence em ##bed ##ding ##s are obtained by applying a max - pool ##ing operation over the output of a bi ##ls ##tm en ##code ##r . these sentence em ##bed ##ding ##s are used to initial ##ize the deco ##der l ##st ##m through a linear transformation , and are also con ##cate ##nated to its input em ##bed ##ding ##s at every time step . note that there is no other connection between the en ##code ##r and the deco ##der , as we want all relevant information of the input sequence to be captured by the sentence em ##bed ##ding . we use a single en ##code ##r and deco ##der in our system , which are shared by all languages involved . for that purpose , we build a joint byte - pair encoding ( bp ##e ) vocabulary with 50 ##k operations , which is learned on the con ##cate ##nation of all training corp ##ora . this way , the en ##code ##r has no explicit signal on what the input language is , encouraging it to learn language independent representations . in contrast , the deco ##der takes a language id em ##bed ##ding that specifies the language to generate , which is con ##cate ##nated to the input and sentence em ##bed ##ding ##s at every time step . scaling up to almost hundred languages , which use very different syntax , writing scripts and linguistic concepts , naturally calls for an en ##code ##r with sufficient capacity . in this paper , we limit our study to a stacked bi ##ls ##tm with 1 to 5 layers , each 512 - dimensional . the resulting sentence representations ( after con ##cate ##nat ##ing both directions ) are 102 ##4 dimensional . the deco ##der has always one layer of dimension 204 ##8 . the input em ##bed ##ding size is set to 320 , while the language id em ##bed ##ding has 32 dimensions . training [ - 1 ##pt ] corpus [ - 2 ##pt ] size ta ##to ##eb ##a [ - 1 ##pt ] test set [ - 2 ##pt ] size training [ - 1 ##pt ] corpus [ - 2 ##pt ] size ta ##to ##eb ##a [ - 1 ##pt ] test set [ - 2 ##pt ] size sub ##section : training strategies in preceding work , each sentence at the input was jointly translated into all other languages . while this approach was shown to learn high - quality representations , it poses two obvious draw ##backs when trying to scale to a large number of languages . first , it requires an n - way parallel corpus , which is difficult to obtain for all languages . second , it has a quad ##ratic cost with respect to the number of languages , making training prohibit ##ively slow as the number of languages is increased . in our preliminary experiments , we observed that similar results can be obtained by using less target languages - two seem to be enough . at the same time , we relax the requirement for n - way parallel corp ##ora by considering independent alignment ##s with the two target languages , e . g . we do not require each source sentence to be translated into the two target languages . training minimize ##s the cross - entropy loss on the training corpus , alternating over all combinations of the languages involved . for that purpose , we use adam with a constant learning rate of 0 . 001 and drop ##out set to 0 . 1 , and train for a fixed number of epoch ##s . our implementation is based on fairs ##e ##q , and we make use of its multi - gp ##u support to train on 16 n ##vid ##ia v ##100 gp ##us with a total batch size of 128 , 000 token ##s . unless otherwise specified , we train our model for 17 epoch ##s , which takes about 5 days . stopping training early decreases the overall performance only slightly . sub ##section : training data and pre - processing as described in section [ reference ] , training requires bite ##xt ##s aligned with two target languages . we choose english and spanish for that purpose , as most of the data is aligned with these languages . we collect training corp ##ora for 93 input languages by combining the europa ##rl , united nations , opens ##ub ##titles ##20 ##18 , global voices , tan ##zi ##l and ta ##to ##eb ##a corpus , which are all publicly available on the opus website . appendix [ reference ] provides a more detailed description of this training data , while tables [ reference ] and [ reference ] sum ##mar ##ize the list of all languages used for training , their language family , writing script and the size of the bite ##xt ##s . our training data comprises a total of 223 million parallel sentences . in preliminary experiments , we observed that the domain of the training data played a key role in the performance of our sentence em ##bed ##ding ##s in different tasks . some tasks ( bu ##cc , ml ##do ##c ) tend to perform better when the en ##code ##r is trained on long and formal sentences , whereas other tasks ( x ##nl ##i , ta ##to ##eb ##a ) benefit from training on shorter and more informal sentences . in an attempt to achieve a general purpose sentence en ##code ##r that performs well on all tasks , we aimed at balancing the size of training corp ##ora with long and short sentences . for that purpose , we used at most two million sentences from opens ##ub ##titles , although more data is available for some languages . all pre - processing is done with moses tools : pun ##ct ##uation normal ##ization , removing non - printing characters and token ##ization . as the only exception , chinese and japanese texts were segment ##ed with ji ##eb ##a and me ##ca ##b , respectively . all the languages are kept in their original script with the exception of greek , which we romani ##ze into the latin alphabet . section : experimental evaluation in contrast with the well - established evaluation framework ##s for english sentence representations , there is not yet a commonly accepted standard to evaluate multi ##ling ##ual sentence em ##bed ##ding ##s . the most notable effort in this regard is probably the x ##nl ##i corpus , an nl ##i test set similar to multi ##nl ##i for which the premises and h ##yp ##oth ##eses were translated into 14 languages by professional translators . we train an nl ##i class ##ifier on top of our multi ##ling ##ual sentence em ##bed ##ding using english training data , and evaluate its zero - shot transfer performance in the remaining languages ( section [ reference ] ) . so as to obtain a more complete picture of the behavior of our multi ##ling ##ual sentence representations , we also evaluate them in cross - ling ##ual document classification ( ml ##do ##c , section [ reference ] ) , and bite ##xt mining ( bu ##cc , section [ reference ] ) . however , all these data ##set ##s only cover a subset of our 93 languages , so we also introduce a new test set for multi ##ling ##ual similarity search in 122 languages , including several languages for which we have no training data but whose language family is covered ( section [ reference ] ) . we remark that we use the same pre - trained bi ##ls ##tm en ##code ##r for all tasks and languages without any fine - tuning . sub ##section : x ##nl ##i : cross - ling ##ual nl ##i nl ##i has become a widely used task to evaluate sentence representations s ##nl ##i : 2015 , multi ##nl ##i : 2017 . given two sentences , a premise and a hypothesis , the task consists in deciding whether there is an en ##tail ##ment , contradiction or neutral relationship between them . x ##nl ##i is a recent effort to create a data ##set similar to the english multi ##nl ##i for several languages . 2 , 500 development and 5 , 000 test sentences have been translated from english into 14 languages by professional translators , making results across different languages directly comparable . note that no human translated training data is provided ; instead , different systems are to use english training data from multi ##nl ##i , and their transfer performance is evaluated on the rest of languages . we train a class ##ifier on top of our multi ##ling ##ual en ##code ##r using the usual combination of the two sentence em ##bed ##ding ##s : , where and are the premise and hypothesis . for that purpose , we use a feed - forward neural network with two hidden layers of size 512 and 38 ##4 , trained with adam . all hyper ##para ##meter ##s were opt ##imi ##zed on the english x ##nl ##i development corpus , and then , the same class ##ifier was applied to all languages of the x ##nl ##i test set . as such , we did not use any training or development data in any of the foreign languages . note , moreover , that the multi ##ling ##ual sentence em ##bed ##ding ##s are fixed and not fine - tuned on the task or the language . we report our results in table [ reference ] , along with several baseline ##s from con ##nea ##u : 2018 : em ##nl ##p _ x ##nl ##i and the recently released multi ##ling ##ual bert model devlin : 2018 : ar ##xi ##v _ bert . as it can be seen , our proposed method establishes a new state - of - the - art in zero - shot cross - ling ##ual transfer ( i . e . training a class ##ifier on english data and applying it to all other languages ) for all languages but spanish . our transfer results are strong and homogeneous across all languages : for 11 of them , the zero - short performance is ( at most ) 5 % lower than the one on english , including distant languages like arabic , chinese and vietnamese , and we also achieve remarkable good results on low - resource languages like sw ##ah ##ili . in contrast , bert achieve ##s excellent results on english , out ##per ##form ##ing our system by 7 . 5 points , but its zero - shot cross - ling ##ual transfer performance is much weaker . for instance , the loss in accuracy for both arabic and chinese is 2 . 5 points for our system , compared to 19 . 3 and 17 . 6 points for bert . finally , we also out ##per ##form all baseline ##s of con ##nea ##u : 2018 : em ##nl ##p _ x ##nl ##i by a substantial margin , with the additional advantage that we use a single pre - trained en ##code ##r , whereas x - bi ##ls ##tm learns a separate en ##code ##r for each language by align ##ing it to the english one . for complete ##ness , we also provide results that include the use of machine translation ( mt ) . this can be done in two ways : 1 ) translate the test data into english and apply the english nl ##i class ##ifier , or 2 ) translate the english training data and train a language specific nl ##i class ##ifier for each language . it should be stressed that we are not evaluating multi ##ling ##ual sentence em ##bed ##ding ##s anymore , but rather the quality of the mt system and a mono ##ling ##ual model . moreover , the use of mt inc ##urs in an important overhead with either strategy : translating test makes inference substantially more expensive , whereas translating train results in a separate model for each language . as shown in table [ reference ] , our approach out ##per ##forms all translation baseline ##s of con ##nea ##u : 2018 : em ##nl ##p _ x ##nl ##i with the exception of urdu . we also out ##per ##form mt bert for arabic and thai , and are very close for urdu . finally , it is worth mentioning that , thanks to its multi ##ling ##ual nature , our system can also handle premises and hypothesis in different languages . as reported in appendix [ reference ] , the proposed method obtain ##s very strong results in these settings , even for distant language combinations like french - chinese . sub ##section : ml ##do ##c : cross - ling ##ual classification cross - ling ##ual document classification is a typical application of multi ##ling ##ual representations . in order to evaluate our sentence em ##bed ##ding ##s in this task , we use the ml ##do ##c data ##set of sc ##h ##wen ##k : 2018 : l ##re ##c _ ml ##do ##c , which is an improved version of the reuters bench ##mark lewis : reuters : 2004 , with uniform class prior ##s and a wider language coverage . there are 1 , 000 training and development documents and 4 , 000 test documents for each language , divided in 4 different gender ##s . just as with the x ##nl ##i evaluation , we consider the zero - shot transfer scenario : we train a class ##ifier on top of our multi ##ling ##ual en ##code ##r using the english training data , opt ##imi ##zing hyper - parameters on the english development set , and evaluating the resulting system in the remaining languages . we use a feed - forward neural network with one hidden layer of 10 units . as shown in table [ reference ] , our system obtain ##s the best published results for 5 of the 7 transfer languages . we believe that our weaker performance on japanese can be attributed to the domain and sentence length mis ##mat ##ch between ml ##do ##c and the parallel corpus we use for this language ( opens ##ub ##titles ) . sub ##section : bu ##cc : bite ##xt mining bite ##xt mining is another natural application for multi ##ling ##ual sentence em ##bed ##ding ##s . given two comparable corp ##ora in different languages , the task consists in identifying sentence pairs that are translations of each other . for that purpose , one would commonly score sentence pairs by taking the co ##sin ##e similarity of their respective em ##bed ##ding ##s , so parallel sentences can be extracted through nearest neighbor retrieval and filtered by setting a fixed threshold over this co ##sin ##e score sc ##h ##wen ##k : 2018 : ac ##l _ mine . however , it was recently shown that this approach suffers from scale inc ##ons ##iste ##ncy issues guo ##20 ##18 ##ef ##fect ##ive , and arte ##t ##x ##e ##20 ##18 ##mar ##gin proposed the following alternative score addressing it : where and are the source and target sentences , and denotes the nearest neighbors of in the other language . the paper explores different margin functions , with ratio ( ) yielding the best results . this notion of margin is related to cs ##ls as proposed in con ##nea ##u : 2018 : ic ##lr _ muse . the reader is referred to arte ##t ##x ##e ##20 ##18 ##mar ##gin for a detailed discussion . we use this method to evaluate our sentence em ##bed ##ding ##s on the bu ##cc mining task z ##weig ##en ##baum ##20 ##17 ##over ##view , z ##weig ##en ##baum ##20 ##18 ##over ##view , using exact same hyper - parameters as arte ##t ##x ##e ##20 ##18 ##mar ##gin . the goal is to extract parallel sentences from a comparable corpus between english and four foreign languages : german , french , russian and chinese . the data ##set consists of 150 k to 1 . 2 m sentences for each language , split into a sample , training and test set , with about 2 - 3 % of the sentences being parallel . as shown in our results in table [ reference ] , our sentence em ##bed ##ding ##s establish a new state - of - the - art for all language pairs with the exception of english - chinese test . quite remarkably , we also out ##per ##form arte ##t ##x ##e ##20 ##18 ##mar ##gin themselves , who use two separate models covering 4 languages each ( english / french / spanish / german and english / french / russian / chinese ) . the average performance over the four languages increased from 93 . 27 to 93 . 92 . not only are our results better , but our model also covers many more languages , so it can potentially be used to mine bite ##xt for any combination of the 93 languages supported . sub ##section : ta ##to ##eb ##a : similarity search while x ##nl ##i , ml ##do ##c and bu ##cc are well established bench ##marks with comparative results available , they only cover a small subset of our 93 languages . so as to better assess the performance of our model in all these different languages , we introduce a new test set of similarity search for 122 languages based on the ta ##to ##eb ##a corpus . the data ##set consists of up to 1 , 000 english - aligned sentence pairs for each language . appendix [ reference ] describes how the data ##set was constructed in more details . evaluation is done by finding the nearest neighbor for each sentence in the other language according to co ##sin ##e similarity and computing the error rate . we report our results in tables [ reference ] and [ reference ] . contrasting these results with those of x ##nl ##i , one would assume that similarity error rates below 5 % are indicative of strong downstream performance . this is the case for 37 languages , while there are 48 languages with an error rate below 10 % and 55 with less than 20 % , covering 22 different families and 15 different scripts . there are only 15 languages with error rates above 50 % . we believe that our competitive results for many low - resource languages are indicative of the benefits of joint training , which is also supported by our ab ##lation results in section [ reference ] . in relation to that , appendix [ reference ] reports similarity search results for 29 additional languages without any training data , showing that our en ##code ##r can also general ##ize to unseen languages to some extent as long as it was trained in related languages . section : ab ##lation experiments in this section , we explore different variants of our approach and study the impact on the performance for all our evaluation tasks . we report average results across all languages . for x ##nl ##i , we also report the accuracy on english . sub ##section : en ##code ##r depth table [ reference ] reports the performance on the different tasks for en ##code ##rs with one , three or five layers . we were not able to achieve good convergence with deeper models . it can be seen that all tasks benefit from deeper models , in particular x ##nl ##i and ta ##to ##eb ##a , suggesting that a single layer bi ##ls ##tm has not enough capacity to en ##code so many languages . sub ##section : multi ##tas ##k learning multi ##tas ##k learning has been shown to be helpful to learn english sentence em ##bed ##ding ##s . the most important task in this approach is arguably nl ##i , so we explored adding an additional nl ##i objective to our system with different weight ##ing schemes . as shown in table [ reference ] , the nl ##i objective leads to a better performance on the english nl ##i test set , but this comes at the cost of a worse cross - ling ##ual transfer performance in x ##nl ##i and ta ##to ##eb ##a . the effect in bu ##cc is ne ##gli ##gible . sub ##section : number of training languages so as to better understand how our architecture scales to a large amount of languages , we train a separate model on a subset of 18 evaluation languages , and compare it to our main model trained on 93 languages . we replaced the ta ##to ##eb ##a corpus with the w ##mt 2014 test set to evaluate the multi ##ling ##ual similarity error rate . this covers english , czech , french , german and spanish , so results between both models are directly comparable . as shown in table [ reference ] , the full model equals or out ##per ##forms the one covering the evaluation languages only for all tasks but ml ##do ##c . this suggests that the joint training also yields to overall better representations . section : conclusions in this paper , we propose an architecture to learn multi ##ling ##ual sentence em ##bed ##ding ##s for 93 languages . we use a single language - ag ##nostic bi ##ls ##tm en ##code ##r for all languages , which is trained on publicly available parallel corp ##ora and applied to different downstream tasks without any fine - tuning . our model sets a new state - of - the - art for most languages in zero - shot cross - ling ##ual natural language inference ( x ##nl ##i ) , cross - ling ##ual document classification ( ml ##do ##c ) , and bite ##xt mining ( bu ##cc ) . we also introduce a new test set of cross - ling ##ual similarity search in 122 languages , and show that our approach is competitive even for low - resource languages . to the best of our knowledge , this is the first successful exploration of massive ##ly multi ##ling ##ual sentence representations . in the future , we would like to explore alternative architecture ##s for the en ##code ##r . in particular , we plan to replace our bi ##ls ##tm with the transform ##er , which has been shown to work better in different settings va ##sw ##ani ##20 ##17 ##att ##ent ##ion , devlin : 2018 : ar ##xi ##v _ bert . moreover , we would like to explore possible strategies to exploit mono ##ling ##ual training data in addition to parallel corp ##ora , such as using pre - trained word em ##bed ##ding ##s , back ##tra ##ns ##lation sen ##nr ##ich ##20 ##16 ##im ##pro ##ving , ed ##uno ##v ##20 ##18 ##under ##standing , or other ideas from un ##su ##per ##vis ##ed machine translation , lamp ##le : 2018 : em ##nl ##p _ un ##su ##pm ##t . finally , we would like to replace our language - specific token ##ization and bp ##e segment ##ation with a language ag ##nostic approach similar to sentence ##piece . the model and code used in this paper will be freely available in the framework of the laser tool ##kit . bibliography : references appendix : training data our training data consists of the combination of the following publicly available parallel corp ##ora : europa ##rl provides high - quality translations for 21 european languages . the size varies from 400 ##k to 2 m sentence pairs , in function of the date the respective country joined the european union . united nations : more than 11 million sentences in the six official languages of the united nations . we only use the first two million sentences in arabic , russian and chinese . opens ##ub ##titles ##20 ##18 : a collection of translations of movie sub ##titles in 57 languages . the corpus size varies from few thousand sentences ( e . g . armenian or kazakh ) to more than 50 million ( e . g . spanish or romanian ) . we keep at most 2 million entries for each language pair . global voices : a parallel corpus of news stories from the global voices website ( 38 languages ) . this is a rather small corpus with less than 100 ##k sentence in most of the languages . tan ##zi ##l : a collection of quran translations in 42 languages . the style and vocabulary is very different from news texts . the average size is 135 ##k sentences . ta ##to ##eb ##a : a community supported collection of english sentences and translations into more than 300 languages . we use this corpus to extract a separate test set of up to 1 , 000 sentences for many languages ( see section [ reference ] and [ reference ] ) . for languages with more than 1 , 000 entries , we use the remaining ones for training . using all these corp ##ora would provide parallel data for more than hundred languages . however , we finally only kept 93 different languages to train the multi ##ling ##ual sentence em ##bed ##ding ##s . in particular , we discarded several constructed languages with little practical use ( k ##ling ##on , kota ##va , lo ##j ##ban , to ##ki po ##na and [UNK] ) . appendix : x ##nl ##i results for all language combinations table [ reference ] reports the acc ##ura ##cies of our system on the x ##nl ##i test set when the premises and hypothesis are in a different language ( e . g . premise in russian and hypothesis in thai ) . the numbers in the diagonal correspond to the main results reported in table [ reference ] . we observe that our approach seems to handle the combination of different languages very well . we do not have evidence that very distant languages perform considerably worse . it rather seems that the combined performance is mostly bounded by the accuracy of the language which performs worst when used alone . as an example , greek - russian achieve ##s very similar results than bulgarian - russian , two slavic languages . comb ##ing french with chinese , two totally different languages , is only 1 . 5 points worse than french / spanish , two very close languages . appendix : ta ##to ##eb ##a data ##set ta ##to ##eb ##a is an open collection of english sentences and high quality translations into more than three hundred languages . the number of available translations is updated every saturday . we downloaded the snaps ##hot on november 19th 2018 and performed the following processing : removal of sentences that contain \" @ \" or \" http \" . this is motivated by the fact that emails and web addresses are not language specific . removal of sentences with less than three words ( before token ##ization ) . these are usually sentences with limited semantic information . removal of sentences that appear multiple times , either in the source or the target . after filtering , we created test sets of up to 1 , 000 aligned sentences with english . this amount of texts is available for 78 languages . limiting the number of sentences to 500 , we increase the coverage to 101 languages , and even 141 languages with 100 parallel sentences . it should be stressed that , in general , the english sentences are not the same for the different languages . this implies that the error rates are not necessarily comparable between the languages . appendix : ta ##to ##eb ##a : result analysis we provide here some analysis on the results given in tables [ reference ] and [ reference ] . we have 48 languages with an error rate below 10 % and 55 with less than 20 % , respectively ( english included ) . the languages with less than 20 % error belong to 20 different families and use 12 different scripts . it is nice to find six languages in this list for which we have only small amounts of bite ##xt ##s ( less than 400 ##k ) , namely es ##per ##anto , galician , hindi , inter ##ling ##ua , malaya ##m and marathi . the two constructed languages probably benefit from their inspiration by other european languages . overall , we observe low similarity error rates on the indo - aryan languages , namely hindi , bengali , marathi and urdu . the performance on berber languages ( \" be ##r \" and \" ka ##b \" ) is remarkable , although we have less than 100 thousand sentences to train them . this is a typical example of languages which are spoken by several millions of people , but for which the amount of written resources is very limited . it is quite unlikely that we would be able to train a good sentence em ##bed ##ding with language specific corp ##ora only . this clearly shows the benefit of joint training on many languages . fifteen languages have similarity error rates of more than 50 % . four of them are low - resource languages with their own script and which are alone in their family : am ##hari ##c , armenian , khmer and georgian . this makes it difficult to benefit from joint training . on the other hand , one can also argue that is surprising that a language like khmer performs much better than random ( 99 . 9 % error rate ) with only 625 training examples . khmer probably benefits of the fact that he have trained our model on other languages of the region which have influenced khmer , namely thai and vietnamese . there are also several turkic languages ( kazakh , tatar , ui ##gh ##ur and u ##zbek ) and celtic languages ( breton and cornish ) with high error rates . we hope to improve their performance in the future . appendix : ta ##to ##eb ##a : results for unseen languages we extend our ta ##to ##eb ##a experiments to 29 languages without any training data ( see table [ reference ] ) . many of them are recognized minority languages spoken in specific regions , e . g . as ##tur ##ian , far ##oese , fr ##isi ##an , ka ##shu ##bian , north mo ##lu ##cca ##n malay , pie ##mont ##ese , sw ##abia ##n or so ##rb ##ian . all share some similarities , at various degrees , with other major languages , but also differ by their own grammar or specific vocabulary . this enables our en ##code ##r to perform reasonably well . we can probably assume that these are mainly spoken languages with limited resources in written form . the six languages which perform worst are mongolian , welsh , x ##hos ##a pam ##pan ##gan , yiddish and gaelic . we include these results here as baseline for future research . premise training [ - 1 ##pt ] corpus [ - 2 ##pt ] size ta ##to ##eb ##a [ - 1 ##pt ] test set [ - 2 ##pt ] size",
        "pred_seq": "x ##set [SEP] [SEP] [SEP] zero transfer [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "xnli dataset"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "zeroshot crosslingual transfer"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "bucc dataset",
                        "english",
                        "bucc",
                        "english training data",
                        "english multinli",
                        "english data",
                        "french",
                        "englishchinese test",
                        "english sentences"
                    ]
                ],
                "Method": [
                    [
                        "sentence embeddings",
                        "crosslingual word embeddings",
                        "sentence embedding",
                        "multilingual sentence embedding",
                        "multilingual encoder",
                        "english sentence embeddings"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "bitext mining"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "mldoc dataset",
                        "mldoc",
                        "spanish"
                    ]
                ],
                "Method": [
                    [
                        "sentence embeddings",
                        "crosslingual word embeddings",
                        "sentence embedding",
                        "multilingual sentence embedding",
                        "multilingual encoder",
                        "english sentence embeddings"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy",
                        "accuracies"
                    ]
                ],
                "Task": [
                    [
                        "crosslingual document classification",
                        "classification",
                        "crosslingual classification"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "xnli dataset",
                        "xnli",
                        "xnli corpus",
                        "english development set"
                    ]
                ],
                "Method": [
                    [
                        "bilstm encoder",
                        "stacked bilstm",
                        "xbilstm",
                        "single layer bilstm",
                        "bilstm"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy",
                        "accuracies"
                    ]
                ],
                "Task": [
                    [
                        "crosslingual natural language inference",
                        "inference"
                    ]
                ]
            }
        ]
    },
    "20": {
        "doctext": "document : image super - resolution via dual - state rec ##urrent networks advances in image super - resolution ( sr ) have recently benefited significantly from rapid developments in deep neural networks . inspired by these recent discoveries , we note that many state - of - the - art deep sr architecture ##s can be reform ##ulated as a single - state rec ##urrent neural network ( rn ##n ) with finite un ##folding ##s . in this paper , we explore new structures for sr based on this compact rn ##n view , leading us to a dual - state design , the dual - state rec ##urrent network ( ds ##rn ) . compared to its single - state counterparts that operate at a fixed spatial resolution , ds ##rn exploits both low - resolution ( l ##r ) and high - resolution ( hr ) signals jointly . rec ##urrent signals are exchanged between these states in both directions ( both l ##r to hr and hr to l ##r ) via delayed feedback . extensive quantitative and qu ##ali ##tative evaluation ##s on bench ##mark data ##set ##s and on a recent challenge demonstrate that the proposed ds ##rn performs favorably against state - of - the - art algorithms in terms of both memory consumption and predict ##ive accuracy . section : introduction in the problem of single - image super - resolution ( sr ) , the aim is to recover a high - resolution ( hr ) image from a single low - resolution ( l ##r ) image . in recent years , sr performance has been significantly improved due to rapid developments in deep neural networks ( d ##nn ##s ) . specifically , con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) and residual learning have been widely applied in much recent sr work . in these approaches , two principles have been consistently observed . the first is that increasing the depth of a cnn model improves sr performance ; a deeper model with more parameters can represent a more complex mapping from l ##r to hr images . in addition , increasing network depth en ##lar ##ges the size of rec ##eptive fields , providing more context ##ual information that can be exploited to rec ##ons ##truct missing hr components . the second principle is that adding residual connections ( globally , locally or jointly ) prevents the problems of vanishing and exploding gradient ##s , facilitating the training of deep models . while these recent models have demonstrated promising results , there are also draw ##backs . one major issue is that increasing the depth of models by adding new layers introduces more parameters , and thus raises the likelihood of model over ##fi ##tting . at the same time , larger models demand more storage space , which is a hurdle to deployment in resource - constrained environments ( e . g . mobile systems ) . to resolve this issue , the deep rec ##urs ##ive residual network ( dr ##rn ) inspired by the deeply - rec ##urs ##ive con ##vo ##lu ##tion ##al network ( dr ##c ##n ) shares weights across different residual units and achieve ##s state - of - the - art performance with a small number of parameters . separate efforts in neural architectural design have recently shown that commonly - used deep structures can be represented more compact ##ly using rec ##urrent neural networks ( rn ##ns ) . specifically , lia ##o and po ##ggio demonstrated that a weight - sharing residual neural network ( res ##net ) is equivalent to a shallow rn ##n . inspired by their findings , we first explore the connections between the neural architecture ##s of existing sr algorithms and their compact rn ##n formulation ##s . we note that previous sr models with rec ##urs ##ive computation and weight sharing , including dr ##rn and dr ##c ##n , work at a single spatial resolution ( bi ##cu ##bic inter ##pol ##ation is first applied to upscale l ##r images to a desired spatial resolution ) . this enables their model structures to be represented as a unified single - state rn ##n . thus , both dr ##rn and dr ##c ##n can be viewed as a finite un ##folding in time of the same rn ##n structure , but with different transition functions . this is illustrated in figure [ reference ] , and will be discussed in detail in section [ reference ] . it is worth mentioning that we follow the terminology used in , where a ' ' state ' ' can be considered as corresponding to a ' ' layer ' ' in the normal rn ##n setting . based on this compact rn ##n view of state - of - the - art sr models , in this paper we explore new structures to extend the frontier of sr . the first approach in improving a conventional rn ##n model is generally to make it multi - layer . we apply this experience in designing the sr architecture in our compact rn ##n view by adding an additional state , rendering our model a dual - state rec ##urrent network ( ds ##rn ) , where the two states operate at different spatial resolutions . specifically , the bottom state captures information at l ##r , while the top state operates in the hr regime . as with a conventional two - layer stacked rn ##n , there is a connection from the bottom to the top state via deco ##n ##vo ##lu ##tion ##al operations . this provides information flow from l ##r to hr at every single un ##roll ##ing time . in addition , to allow information flow from previously predicted hr features to l ##r features , we incorporate a delayed feedback mechanism from the top ( hr ) state to the bottom one . the overall structure of the proposed ds ##rn is shown in figure [ reference ] , which not only utilizes parameters efficiently but also allows both l ##r and hr signals to contribute jointly to learning the mapping ##s . to demonstrate the effectiveness of the proposed method , we compare ds ##rn with other recent image sr approaches on four common bench ##marks as well as on the di ##v ##2 k data ##set from the \" new trends in image restoration and enhancement workshop and challenge on image super - resolution ( nt ##ire sr 2017 ) \" . extensive experimental results valid ##ate that ds ##rn delivers higher parameter efficiency , low memory consumption and high restoration accuracy . section : related work single image sr has been widely studied in the past few decades and has an extensive literature . in recent years , due to the fast development of deep learning , significant progress has been made in this field . dong et al . first exploited a fully con ##vo ##lu ##tion ##al neural network , termed sr ##c ##nn , to predict the nonlinear l ##r - hr mapping . it demonstrated superior performance to many other example - based learning paradigm ##s , such as nearest neighbor , sparse representation , neighborhood em ##bed ##ding , random forest , etc . although all layers of a sr ##c ##nn are trained jointly in an end - to - end fashion , conceptual ##ly the network is split into three stages : patch representation , non - linear mapping , and reconstruction . much of the later work follows a similar network design with more complicated building blocks or advanced optimization techniques . wang et al . proposed a sparse coding network ( sc ##n ) that en ##codes a sparse representation prior for image sr and can be trained end - to - end , demonstrating the benefit of domain expertise in sparse coding for image sr . both external and self examples were utilized to synth ##es ##ize the hr prediction via a neural network in . inspired by the success of very deep models on image ##net challenges , kim et al . proposed a very deep cnn , v ##ds ##r , which stacks 20 con ##vo ##lu ##tion ##al layers with kernel ##s . both residual learning and adjustable gradient clip ##ping are used to prevent vanishing and exploding gradient ##s . however , as the model gets deeper , the number of parameters increases . to control the size of the model , dr ##c ##n introduces 16 rec ##urs ##ive layers , each with the same structure and shared parameters . moreover , dr ##c ##n makes use of skip connections and rec ##urs ##ive supervision to mit ##igate the difficulty of training . tai et al . discovered that many residual sr learning algorithms are based on either global residual learning or local residual learning , which are insufficient for very deep models . instead , they proposed the dr ##rn that applies both global and local learning while remaining parameter efficient via rec ##urs ##ive learning . more recently , tong et al . proposed making use of densely connected networks ( dense ##net ) instead of res ##net as the building block for image sr . they demonstrated that the dense ##net structure is better at combining features at different levels , which boost ##s sr performance . apart from deep models working on bi ##cu ##bic upscale ##d input images , shi et al . used a compact network model to conduct con ##vo ##lu ##tions on l ##r images directly and learned ups ##cal ##ing filters in the last layer , which considerably reduces the computation cost . similarly , dong et al . adopted deco ##n ##vo ##lu ##tion layers to accelerate sr ##c ##nn in combination with smaller filter sizes and more con ##vo ##lu ##tion layers . however , these networks are relatively small and have difficulty capturing complicated mapping ##s owing to limited network capacity . the lap ##la ##cian pyramid super - resolution network ( laps ##rn ) works on l ##r images directly and progressively predict ##s sub - band residual ##s on various scales . lim et al . proposed the enhanced deep super - resolution ( eds ##r ) network and a multi - scale variant , which learns different scaled mapping functions in parallel via weight sharing . it is noteworthy that most sr algorithms minimize the mean squared reconstruction error ( i . e . via loss ) . they often suffer from regression - to - the - mean due to the ill - posed nature of single image sr , resulting in blur ##ry predictions and poor subjective scores . to overcome this draw ##back , genera ##tive ad ##vers ##aria ##l networks have been used along with per ##ce ##pt ##ual loss for sr . subjective evaluation by mean - opinion - score showed huge improvement over other regression - based methods . our work is also strongly related to and built upon the idea of viewing a res ##net as an un ##roll ##ed rn ##n . it was first proposed in , which aids understanding of a family of deep structures from the perspective of rn ##ns . later , chen et al . unified several different residual functions to provide a better understanding of the design of d ##nn ##s with high learning capacity . recently , the equivalence to rn ##ns has been further extended to dense ##net . based on this finding , dual path networks were proposed and showed superior performance to dense ##net and res ##net in a var ##ity of applications . section : single - state rec ##urrent networks in this section , we first rev ##isi ##t the discovery that a res ##net with shared weights can be reform ##ulated as a rec ##urrent system . then , based on this view , we unite the recent development of sr models with such rn ##n reform ##ulation ##s to show dr ##c ##n and dr ##rn are structurally equivalent to an un ##roll ##ed single - state rn ##n . to establish the equivalence , we adopt the commonly used definition of a rn ##n , which is characterized by a set of states and transition functions among the states . a rn ##n often consists of the input state , output state , and the rec ##urrent states . depending on the number rec ##urrent states , we describe rn ##ns as ' ' single - state ' ' ( i . e . one rec ##urrent state ) or ' ' dual - state ' ' ( i . e . two rec ##urrent states ) . an illustration of a single - state rn ##n is shown in figure [ reference ] ( a ) . the input , output , and rec ##urrent states are represented as , and respectively . the arrow link indicates the state transition function . the square on the directed cycle indicates that the rec ##urrent function travels one time step forward during the un ##folding . interested readers are referred to for detailed information on this general formulation of a rn ##n . based on figure [ reference ] ( a ) , we un ##fold along the temporal direction to a fixed length . the unfolded graph is shown in figure [ reference ] ( b ) , and the dynamics of a single - state rn ##n can be characterized by : where the upper script indicates the - th un ##roll ##ing . the parameters of , , and are often time - independent , which means these parameters are reused at every un ##folding step . this allows us to un ##ify res ##net , dr ##c ##n , and dr ##rn as un ##roll ##ed networks with the same rec ##urrent structure but with the different realization ##s of and different rules of parameter sharing . res ##net : we consider a res ##net in its simplest form without any down - sampling or up - sampling operations . in other words , both of the spatial dimensions and feature dimensions remain the same across all intermediate layers . to render figure [ reference ] ( b ) equivalent to a res ##net with residual blocks , one possible technique is to make : be the input image or a function of . , and . thus , the state transition becomes . the rec ##urrent function be the same as a conventional residual block , which contains two con ##vo ##lu ##tion ##al layers with skip connections as shown in figure [ reference ] ( c ) . differences in color indicate different sets of parameters . the prediction state be calculated only at the time as the final output . it is worth mentioning that the only difference between an un ##roll ##ed rn ##n following the above definitions and a conventional res ##net is that the parameters in need to be reused among all residual blocks . dr ##c ##n : to realize the dr ##c ##n express ##ible by the same single - state rn ##n , we define and in the same way as for the res ##net . since dr ##c ##n rec ##urs ##ively applies only a single con ##vo ##lu ##tion ##al layer to the input feature map 16 times , with the parameters of the layer reused across the whole network , we could use a single con ##vo ##lu ##tion ##al layer to express . the graph is illustrated in figure [ reference ] ( d ) . moreover , unlike the res ##net where the output is predicted only at the end of un ##folding , dr ##c ##n utilizes rec ##urs ##ive supervision , which generates an output at every un ##folding . the final hr prediction of dr ##c ##n is the weighted sum of the outputs at every un ##folding . dr ##rn : the rec ##urrent structure of dr ##rn differs only slightly from a res ##net . in a res ##net , the skip connection comes from the previous residual block , whereas in a dr ##rn the skip connection always comes from the first un ##roll ##ed state . figure [ reference ] ( e ) shows the equivalent rec ##urrent function for a dr ##rn with one rec ##urs ##ive block ( i . e . ) using the definition in the original paper . section : dual - state rec ##urrent networks drawing on the connections between state - of - the - art sr models and rn ##ns , we have investigated new compact rn ##n architecture ##s for image sr . specifically , we propose a dual - state design , which adopt ##s two rec ##urrent states enable use of features from both l ##r and hr spaces . the rn ##n view of our ds ##rn is shown in figure [ reference ] ( a ) and is introduced as follows . dual - state design : unlike single - state models working at the same spatial resolution , ds ##rn incorporates information from both the l ##r and hr spaces . specifically , and in figure [ reference ] ( a ) indicate the l ##r state and hr state , respectively . four colored arrows indicate the transition functions between these two states . the blue ( ) , orange ( ) and yellow ( ) links exist in a conventional two - layer rn ##n , providing information flow from l ##r to l ##r , hr to hr , and l ##r to hr , respectively . to further enable two - way information flows between and , we add the green link , which is inspired by the delayed feedback mechanism of traditional multi - layer rn ##ns . here , it introduces a delayed hr to l ##r connection . the overall dynamics of our ds ##rn is given as : figure [ reference ] ( b ) demonstrates the same concept via an unfolded graph , where the top row represents hr state while the bottom one is l ##r . this design choice encourages feature specialization for different resolutions and information sharing across different resolutions . transition functions : our model is characterized by six transition functions . , , , and as illustrated in figure [ reference ] ( b ) . specifically , we use the standard residual block for both self - transitions . a single con ##vo ##lu ##tion ##al layer is used for the down - sampling transition and a single trans ##posed con ##vo ##lu ##tion ##al ( or deco ##n ##vo ##lu ##tion ##al ) layer is used for the up - sampling transition . the strides in both inter - state layers are set to be the same as the sr ups ##cal ##ing factor . un ##folding details : similarly to un ##folding a single - state rn ##n to obtain a res ##net , for image sr , we let have no contribution to calculating the state transition . in other words , for any choice of ( e . g . choose ) . furthermore , we set as the output of two con ##vo ##lu ##tion ##al layers with skip connections , which takes the l ##r input image and transform it into a desired feature space . in addition , is set to zero . finally , we use deep supervision for the hr prediction , as discussed below . deep supervision : the un ##roll ##ed ds ##rn is capable of making a prediction at every time step . denote as a prediction at the un ##folding , where is characterized by a single con ##vo ##lu ##tion ##al layer . then , instead of taking the prediction only at the final un ##folding , we average all the predictions as thus , every un ##roll ##ed layer directly connects to the loss layer to facilitate the training of such a very deep network . moreover , the model predict ##s the residual image and minimize ##s the following mean square error where is the group - truth image in hr and is the residual map between the ground truth and bi ##cu ##bic ups ##amp ##led l ##r image . section : experiments in this section , we first provide implementation details , including both model hyper - parameters and training data aug ##ment ##ation . then we analyze a number of design choices and their contributions to final performance . finally , we compare ds ##rn to other state - of - the - art methods on several bench ##mark data ##set ##s . sub ##section : data ##set ##s to evaluate the proposed ds ##rn algorithm , we train our model using 91 images proposed in and test on the following data ##set ##s : set ##5 , set ##14 , b1 ##00 and urban ##100 . the training data is augmented in a similar way to previous methods , which includes 1 ) random flipping along the vertical or horizontal axis ; 2 ) random rotation by 90 , 180 or 270 ; and 3 ) random scaling by a factor from [ 0 . 5 , 0 . 6 , 0 . 7 , 0 . 8 , 0 . 9 , 1 ] . tensor ##flow is used for our full data processing pipeline ; the l ##r training images are generated by the built - in bi ##cu ##bic down - sampling function . we additionally test our algorithm on the di ##v ##2 k data ##set of the nt ##ire sr 2017 challenge , where we use the provided training and validation sets with all of the aforementioned data aug ##ment ##ations except random scaling . sub ##section : implementation details we use our model to super - resolve only the lu ##mina ##nce channel of images , and use bi ##cu ##bic inter ##pol ##ation to upscale the other two color channels , following . we train independent models for each scale ( 2 , 3 , and 4 ) with 64 filters on the first input con ##vo ##lu ##tion ##al layer and 128 filters in the rest of the network . all layers use con ##vo ##lu ##tion filters . due to our dual - state design , the feature maps of and in each time step have the same spatial dimensions as the l ##r and hr images , respectively . we zero - pad the boundaries of feature maps to ensure the spatial size of each feature map is the same as the input size after the con ##vo ##lu ##tion is applied . all the weights in the network are initial ##ized with a uniform distribution using the method proposed in . we use standard st ##och ##astic gradient descent ( sg ##d ) with momentum 0 . 95 as our opt ##imi ##zer to minimize the ms ##e loss function in equation ( [ reference ] ) . we search for the best initial learning rate from and reduce it by a factor of 10 three times during the entire training process . this learning rate anne ##aling is driven by observing that the loss on the validation set stops decreasing . gradient clip ##ping at is adopted during training to prevent the gradient explosion . we sample image patches with a size of and use a mini - batch size of to train our network . we observe that the rec ##urs ##ion defined in equation ( [ reference ] ) may lead to an exponential increase in the scale of feature values , especially when is large . in , the authors proposed the use of un ##sha ##red batch normal ##ization at every un ##folding time to resolve this issue . batch normal ##ization is not used in our network ; we found that normal ##izing the scale with two scala ##r parameters was sufficient . specifically , we use one un ##sha ##red pre ##lu activation for each rec ##urrent state after every un ##roll ##ing step . all other layers have ordinary re ##lu as the activation function . ours others . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 . 24 sub ##section : model analysis in this section , we analyze our proposed model in the following respects : un ##roll ##ing length : the un ##roll ##ing length changes the maximum effective depth of the un ##roll ##ed network . in particular , for a ds ##rn with times un ##roll ##ing , the maximum number of con ##vo ##lu ##tion layers between input and output of the network is . the multi ##pl ##ier comes from the two layers in a residual block , while the extra 4 is from the auxiliary input and output layers . however , the number of model parameters remains independent of the length of un ##roll ##ing . essentially , controls the trade - off between model capacity and computation cost . we study the influence of by training the model with different un ##roll ##ing lengths . the empirical results are shown in figure [ reference ] . the test performance increases when the number of un ##folding steps increases , but the benefit seems to dim ##ini ##sh after . unless otherwise mentioned , we use for all our models . it is worth mentioning that we also experimented with st ##och ##astic depth by randomly sampling during training , but we observed no improvement in validation accuracy . parameter sharing : we empirical ##ly find parameter sharing to be crucial for training a deep rec ##urs ##ive model . as shown in table [ reference ] , the same model with un ##tie ##d weights performs much more poorly than its weight - sharing counterpart . specifically , we observe around 0 . 2d ##b performance drop across all three ups ##cal ##ing scales when changing from shared weights to un ##tie ##d weights . we spec ##ulate that the model with un ##tie ##d weights suffers a larger risk of model over - fitting and much slower training convergence , both of which dim ##ini ##sh the model ' s restoration accuracy . dual - state and delayed feedback : we compare our ds ##rn with two baseline ##s under the same un ##roll ##ing time steps to understand how each module of our model contributes to the final performance : 1 ) a single - state rn ##n un ##roll ##ed res ##net ; and 2 ) a dual - state rn ##n without delayed feedback connections . the quantitative comparison on the nt ##ire sr 2017 challenge is shown in table [ reference ] . comparing the single - state baseline and the ds ##rn without feedback , it is clear that considering information from both l ##r and hr spaces as two separated states provides performance gains . in addition , comparing our models with and without feedback , we realize that incorporating such an information flow from hr space back to l ##r space consistently improves performance on all three different scales . in all , both the dual - state and delayed feedback designs are beneficial to our model . state visual ##ization since ds ##rn has independent scaling parameters on each un ##roll ##ed state , the model implicit ##ly learns a weighted - average of all the un ##roll ##ed states for the final prediction . empirical ##ly we observe that this strategy performs better than output from the last state only . to demonstrate how the network aggregate ##s different un ##roll ##ed states , we show feature response maps at different un ##roll ##ing steps in figure [ reference ] , demonstrating that the network distribute ##s slightly different features to each un ##roll ##ed state . sub ##section : comparison with the state - of - the - art we provide results of evaluation of our model on several public bench ##mark data ##set ##s in table [ reference ] , with three commonly - used evaluation metric ##s : peak signal - to - noise ratio ( ps ##nr ) , structural similarity ( ss ##im ) and the information fidelity criterion ( if ##c ) . specifically , we perform a comprehensive comparison between our method and 10 other existing sr algorithms , including both deep learning and non - deep - learning based methods . note that many recent deep learning based competitors , including v ##ds ##r , laps ##rn and dr ##rn , use 291 training samples with the additional 200 from the training set of berkeley segment ##ation data ##set , while our model was trained on only the 91 images . still , our ds ##rn method achieve ##s competitive performance across all data ##set ##s and scales . it achieve ##s particularly strong performance in the and settings . in addition , we report quantitative evaluation ##s on the recently developed di ##v ##2 k data ##set and comparisons with top - ranking algorithms in table [ reference ] . our method achieve ##s competitive performance with the best algorithm , eds ##r + , and out ##per ##forms all the other algorithms by a large margin , which demonstrates the effectiveness of our proposed dual - state rec ##urrent structure . to further analyze the proposed ds ##rn against other state - of - the - art sr approaches in a qu ##ali ##tative manner , in figure [ reference ] we present several visual examples of super - resolved images on set ##14 with ups ##cal ##ing among different sr approaches . for these competing methods , we use sr results publicly released by the authors . as shown in figure [ reference ] , our method can construct sharp and detailed structures and is less prone to generating spur ##ious artifacts . furthermore , the proposed ds ##rn benefits from inherent parameter sharing and therefore obtain ##s higher parameter efficiency compared to other methods . in figure [ reference ] , we illustrate the parameters - to - ps ##nr relationship of our model and several state - of - the - art methods , including sr ##c ##nn , v ##ds ##r , dr ##c ##n , dr ##rn and red ##30 . our method represents a favorable trade - off between model size and sr performance , and has modest inference time . the ds ##rn takes 0 . 4 ##s on the x ##4 task with a 288 ##x ##28 ##8 output image size , on an n ##vid ##ia titan x gp ##u . section : conclusion in this work , we have provided a unique formulation that expresses many state - of - the - art sr models as a finite un ##folding of a single - state rn ##n with various rec ##urrent functions . based on this , we extend existing methods by considering a dual - state design ; the two hidden states of our proposed ds ##rn operate at different spatial resolutions . one captures the l ##r information while the other one targets the hr domains . to ensure two - way communication between states , we integrate a delayed feedback mechanism . thus , the predicted features from both l ##r and hr states can be exploited jointly for final predictions . extensive experiments on bench ##mark data ##set ##s have demonstrated that the proposed ds ##rn performs favorably against state - of - the - art sr models in terms of both efficiency and accuracy . for the future work , we will explore use of our proposed ds ##rn to capture temporal depend ##encies for video sr . bibliography : references",
        "pred_seq": "[SEP] [SEP] [SEP] image resolution [SEP] [unused0] [SEP] [SEP] image resolution [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "image superresolution"
                    ]
                ]
            },
            {
                "Material": [],
                "Method": [],
                "Metric": [
                    [
                        "image superresolution"
                    ]
                ],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "b100"
                    ]
                ],
                "Method": [
                    [
                        "dualstate recurrent networks",
                        "dualstate recurrent network",
                        "dsrn",
                        "unrolled singlestate rnn",
                        "dualstate rnn without delayed feedback connections",
                        "dualstate recurrent structure"
                    ]
                ],
                "Metric": [
                    [
                        "peak signaltonoise ratio",
                        "psnr",
                        "psnr relationship"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "sr",
                        "singleimage superresolution",
                        "single image sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "b100"
                    ]
                ],
                "Method": [
                    [
                        "dualstate recurrent networks",
                        "dualstate recurrent network",
                        "dsrn",
                        "unrolled singlestate rnn",
                        "dualstate rnn without delayed feedback connections",
                        "dualstate recurrent structure"
                    ]
                ],
                "Metric": [
                    [
                        "structural similarity",
                        "ssim"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "sr",
                        "singleimage superresolution",
                        "single image sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set14"
                    ]
                ],
                "Method": [
                    [
                        "dualstate recurrent networks",
                        "dualstate recurrent network",
                        "dsrn",
                        "unrolled singlestate rnn",
                        "dualstate rnn without delayed feedback connections",
                        "dualstate recurrent structure"
                    ]
                ],
                "Metric": [
                    [
                        "peak signaltonoise ratio",
                        "psnr",
                        "psnr relationship"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "sr",
                        "singleimage superresolution",
                        "single image sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set14"
                    ]
                ],
                "Method": [
                    [
                        "dualstate recurrent networks",
                        "dualstate recurrent network",
                        "dsrn",
                        "unrolled singlestate rnn",
                        "dualstate rnn without delayed feedback connections",
                        "dualstate recurrent structure"
                    ]
                ],
                "Metric": [
                    [
                        "structural similarity",
                        "ssim"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "sr",
                        "singleimage superresolution",
                        "single image sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set5"
                    ]
                ],
                "Method": [
                    [
                        "dualstate recurrent networks",
                        "dualstate recurrent network",
                        "dsrn",
                        "unrolled singlestate rnn",
                        "dualstate rnn without delayed feedback connections",
                        "dualstate recurrent structure"
                    ]
                ],
                "Metric": [
                    [
                        "peak signaltonoise ratio",
                        "psnr",
                        "psnr relationship"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "sr",
                        "singleimage superresolution",
                        "single image sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set5"
                    ]
                ],
                "Method": [
                    [
                        "dualstate recurrent networks",
                        "dualstate recurrent network",
                        "dsrn",
                        "unrolled singlestate rnn",
                        "dualstate rnn without delayed feedback connections",
                        "dualstate recurrent structure"
                    ]
                ],
                "Metric": [
                    [
                        "structural similarity",
                        "ssim"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "sr",
                        "singleimage superresolution",
                        "single image sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "urban100"
                    ]
                ],
                "Method": [
                    [
                        "dualstate recurrent networks",
                        "dualstate recurrent network",
                        "dsrn",
                        "unrolled singlestate rnn",
                        "dualstate rnn without delayed feedback connections",
                        "dualstate recurrent structure"
                    ]
                ],
                "Metric": [
                    [
                        "peak signaltonoise ratio",
                        "psnr",
                        "psnr relationship"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "sr",
                        "singleimage superresolution",
                        "single image sr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "urban100"
                    ]
                ],
                "Method": [
                    [
                        "dualstate recurrent networks",
                        "dualstate recurrent network",
                        "dsrn",
                        "unrolled singlestate rnn",
                        "dualstate rnn without delayed feedback connections",
                        "dualstate recurrent structure"
                    ]
                ],
                "Metric": [
                    [
                        "structural similarity",
                        "ssim"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "sr",
                        "singleimage superresolution",
                        "single image sr"
                    ]
                ]
            }
        ]
    },
    "21": {
        "doctext": "document : natural language inference over interaction space natural language inference ( nl ##i ) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis . we introduce interactive inference network ( ii ##n ) , a novel class of neural network architecture ##s that is able to achieve high - level understanding of the sentence pair by hierarchical ##ly extract ##ing semantic features from interaction space . we show that an interaction tensor ( attention weight ) contains semantic information to solve natural language inference , and a dense ##r interaction tensor contains richer semantic information . one instance of such architecture , densely interactive inference network ( di ##in ) , demonstrates the state - of - the - art performance on large scale nl ##i cop ##ora and large - scale nl ##i alike corpus . it ' s noteworthy that di ##in achieve a greater than 20 % error reduction on the challenging multi - genre nl ##i ( multi ##nl ##i ; ) data ##set with respect to the strongest published system . section : introduction natural language inference ( nl ##i also known as recognizing textual en ##tia ##il ##ment , or rte ) task requires one to determine whether the logical relationship between two sentences is among en ##tail ##ment ( if the premise is true , then the hypothesis must be true ) , contradiction ( if the premise is true , then the hypothesis must be false ) and neutral ( neither en ##tail ##ment nor contradiction ) . nl ##i is known as a fundamental and yet challenging task for natural language understanding , not only because it requires one to identify the language pattern , but also to understand certain common sense knowledge . in table [ reference ] , three samples from multi ##nl ##i corpus show solving the task requires one to handle the full complexity of lexi ##cal and composition ##al semantics . the previous work on nl ##i ( or rte ) has extensively researched on conventional approaches . recent progress on nl ##i is enabled by the availability of 570 ##k human ann ##ota ##ted data ##set and the advancement of representation learning technique . among the core representation learning techniques , attention mechanism is broadly applied in many nl ##u tasks since its introduction : machine translation , abstract ##ive sum ##mar ##ization , reading comprehension , dial ##og system , etc . as described by , \" an attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors . the output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key \" . attention mechanism is known for its alignment between representations , focusing one part of representation over another , and modeling the dependency regardless of sequence length . observing attention ' s powerful capability , we h ##yp ##oth ##es ##ize that the attention weight can assist with machine to understanding the text . a regular attention weight , the core component of the attention mechanism , en ##codes the cross - sentence word relationship into a alignment matrix . however , a multi - head attention weight can en ##code such interaction into multiple alignment matrices , which shows a more powerful alignment . in this work , we push the multi - head attention to a extreme by building a word - by - word dimension - wise alignment tensor which we call interaction tensor . the interaction tensor en ##codes the high - order alignment relationship between sentences pair . our experiments demonstrate that by capturing the rich semantic features in the interaction tensor , we are able to solve natural language inference task well , especially in cases with para ##ph ##rase , antony ##ms and overlapping words . we dub the general framework as interactive inference network ( ii ##n ) . to the best of our knowledge , it is the first attempt to solve natural language inference task in the interaction space . we further explore one instance of interactive inference network , densely interactive inference network ( di ##in ) , which achieve ##s new state - of - the - art performance on both s ##nl ##i and multi ##nl ##i cop ##ora . to test the general ##ity of the architecture , we interpret the para ##ph ##rase identification task as natural language inference task where matching as en ##tail ##ment , not - matching as neutral . we test the model on quo ##ra question pair data ##set , which contains over 400 ##k real world question pair , and achieve ##s new state - of - the - art performance . we introduce the related work in section 2 , and discuss the general framework of ii ##n along with a specific instance that enjoys state - of - the - art performance on multiple data ##set ##s in section 3 . we describe experiments and analysis in section 4 . finally , we conclude and discuss future work in section 5 . section : related work the early exploration on nl ##i mainly rely on conventional methods and small scale data ##set ##s . the availability of s ##nl ##i data ##set with 570 ##k human ann ##ota ##ted sentence pairs has enabled a good deal of progress on natural language understanding . the essential representation learning techniques for nl ##u such as attention , memory and the use of par ##se structure are studied on the s ##nl ##i which serves as an important bench ##mark for sentence understanding . the models trained on nl ##i task can be divided into two categories : ( i ) sentence encoding - based model which aims to find vector representation for each sentence and class ##ifies the relation by using the con ##cate ##nation of two vector representation along with their absolute element - wise difference and element - wise product . ( ii ) joint feature models which use the cross sentence feature or attention from one sentence to another . after neural attention mechanism is successfully applied on the machine translation task , such technique has became widely used in both natural language process and computer vision domains . many variants of attention technique such as hard - attention , self - attention , multi - hop attention , bid ##ire ##ction ##al attention and multi - head attention are also introduced to tackle more complicated tasks . before this work , neural attention mechanism is mainly used to make alignment , focusing on specific part of the representation . in this work , we want to show that attention weight contains rich semantic information required for understanding the logical relationship between sentence pair . though rn ##n or l ##st ##m are very good for variable length sequence modeling , using con ##vo ##lu ##tion ##al neural network in nl ##u tasks is very desirable because of its parallel ##ism in computation . con ##vo ##lu ##tion ##al structure has been successfully applied in various domain such as machine translation , sentence classification , text matching and sentiment analysis , etc . the con ##vo ##lu ##tion structure is also applied on different level of gran ##ular ##ity such as byte , character , word and sentences levels . section : model sub ##section : interactive inference network the interactive inference network ( ii ##n ) is a hierarchical multi - stage process and consists of five components . each of the components is compatible with different type of implementations . potentially all exiting approaches in machine learning , such as decision tree , support vector machine and neural network approach , can be transfer to replace certain component in this architecture . we focus on neural network approaches below . figure [ reference ] provides a visual illustration of interactive inference network . em ##bed ##ding layer converts each word or phrase to a vector representation and construct the representation matrix for sentences . in em ##bed ##ding layer , a model can map token ##s to vectors with the pre - trained word representation such as glove , word ##2 ##ve ##c and fast ##text . it can also utilize the pre - processing tool , e . g . named entity recognize ##r , part - of - speech recognize ##r , lexi ##cal par ##ser and core ##ference id ##ent ##ifier etc . , to incorporate more lexi ##cal and syn ##ta ##ctic ##al information into the feature vector . encoding layer en ##codes the representations by incorporating the context information or en ##rich ##ing the representation with desirable features for future use . for instance , a model can adopt bid ##ire ##ction ##al rec ##urrent neural network to model the temporal interaction on both direction , rec ##urs ##ive neural network ( also known as tree ##rn ##n ) to model the composition ##ality and the rec ##urs ##ive structure of language , or self - attention to model the long - term dependency on sentence . different components of en ##code ##r can be combined to obtain a better sentence matrix representation . interaction layer creates an word - by - word interaction tensor by both premise and hypothesis representation matrix . the interaction can be modeled in different ways . a common approach is to compute the co ##sin ##e similarity or dot product between each pair of feature vector . on the other hand , a high - order interaction tensor can be constructed with the outer product between two matrix representations . feature extraction layer adopt ##s feature extract ##or to extract the semantic feature from interaction tensor . the con ##vo ##lu ##tion ##al feature extract ##ors , such as alex ##net , v ##gg , inception , res ##net and dense ##net , proven work well on image recognition are completely compatible under such architecture . unlike the work who employs 1 - d sliding window , our cnn architecture allows 2 - d kernel to extract semantic interaction feature from the word - by - word interaction between n - gram pair . sequential or tree - like feature extract ##ors are also applicable in the feature extraction layer . output layer deco ##des the acquired features to give prediction . under the setting of nl ##i , the output layer predict ##s the confidence on each class . sub ##section : densely interactive inference network here we introduce densely interactive inference network ( di ##in ) , which is a relatively simple instant ##iation of ii ##n but produces state - of - the - art performance on multiple data ##set ##s . paragraph : em ##bed ##ding layer : for di ##in , we use the con ##cate ##nation of word em ##bed ##ding , character feature and syn ##ta ##ctic ##al features . the word em ##bed ##ding is obtained by mapping token to high dimensional vector space by pre - trained word vector ( 840 ##b glove ) . the word em ##bed ##ding is updated during training . as in , we filter character em ##bed ##ding with 1 ##d con ##vo ##lu ##tion kernel . the character con ##vo ##lu ##tion ##al feature maps are then max poole ##d over time dimension for each token to obtain a vector . the character features supplies extra information for some out - of - vocabulary ( o ##ov ) words . syn ##ta ##ctic ##al features include one - hot part - of - speech ( po ##s ) tag ##ging feature and binary exact match ( em ) feature . the em value is activated if there are token ##s with same stem or le ##mma in the other sentence as the corresponding token . the em feature is simple while found useful as in reading comprehension task . in analysis section , we study how em feature helps text understanding . now we have premise representation and hypothesis representation , where refers to the sequence length of premise , refers to the sequence length of hypothesis and means the dimension of both representation . the 1 - d con ##vo ##lu ##tion ##al neural network and character features weights share the same set of parameters between premise and hypothesis . paragraph : encoding layer : in the encoding layer , the premise representation and the hypothesis representation are passed through a two - layer highway network , thus having and for new premise representation and new hypothesis representation . these new representation are then passed to self - attention layer to take into account the word order and context information . take premise as example , we model self - attention by where is a weighted sum ##mation of . we choose , where is a train ##able weight , is element - wise multiplication , [ ; ] is vector con ##cate ##nation across row , and the implicit multiplication is matrix multiplication . then both and are fed into a semantic composite fuse gate ( fuse gate in short ) , which acts as a skip connection . the fuse gate is implemented as where , , and , are train ##able weights , is si ##gm ##oid nonlinear operation . we do the same operation on hypothesis representation , thus having . the weights of intra - attention and fuse gate for premise and hypothesis are not shared , but the difference between the weights of are penal ##ized . the penal ##ization aims to ensure the parallel structure learns the similar functionality but is aware of the subtle semantic difference between premise and hypothesis . paragraph : interaction layer : the interaction layer models the interaction between premise encoded representation and hypothesis encoded representation as follows : where is the - th row vector of , and is the - th row vector of . though there are many implementations of interaction , we find very useful . paragraph : feature extraction layer : we adopt dense ##net as con ##vo ##lu ##tion ##al feature extract ##or in di ##in . though our experiments show res ##net works well in the architecture , we choose dense ##net because it is effective in saving parameters . one interesting observation with res ##net is that if we remove the skip connection in residual structure , the model does not converge at all . we found batch normal ##ization delays convergence without contributing to accuracy , therefore we does not use it in our case . a re ##lu activation function is applied after all con ##vo ##lu ##tion unless otherwise noted . once we have the interaction tensor , we use a con ##vo ##lu ##tion with kernel to scale down the tensor in a ratio , , without following re ##lu . if the input channel is then the output channel is . then the generated feature map is feed into three sets of dense block and transition block pair . the dense ##net block contains n layers of con ##vo ##lu ##tion layer with growth rate of g . the transition layer has a con ##vo ##lu ##tion layer with kernel for scaling down purpose , followed by a max pool ##ing layer with stride . the transition scale down ratio in transition layer is . paragraph : output layer : di ##in uses a linear layer to classify final flattened feature representation to three classes . section : experiments in this section , we present the evaluation of our model . we first perform quantitative evaluation , comparing our model with other competitive models . we then conduct some qu ##ali ##tative analyses to understand how di ##in achieve the high level understanding through interaction . sub ##section : data here we introduce three data ##set ##s we evaluate our model on . the evaluation metric for all data ##set is accuracy . paragraph : s ##nl ##i stanford natural language inference ( s ##nl ##i ; ) has 570 ##k human ann ##ota ##ted sentence pairs . the premise data is draw from the capt ##ions of the flick ##r ##30 ##k corpus , and the hypothesis data is manually composed . the labels provided in are \" en ##tail ##ment \" , \" neutral ' , \" contradiction \" and \" - \" . \" - \" shows that ann ##ota ##tors can not reach consensus with each other , thus removed during training and testing as in other works . we use the same data split as in . paragraph : multi ##nl ##i multi - genre nl ##i corpus ( multi ##nl ##i ; ) has 43 ##3 ##k sentence pairs , whose collection process and task detail are modeled closely to s ##nl ##i . the premise data is collected from maximal ##ly broad range of genre of american english such as written non - fiction genres ( slate , ou ##p , government , verb ##ati ##m , travel ) , spoken genres ( telephone , face - to - face ) , less formal written genres ( fiction , letters ) and a specialized one for 9 / 11 . half of these selected genres appear in training set while the rest are not , creating in - domain ( matched ) and cross - domain ( mis ##mat ##ched ) development / test sets . we use the same data split as provided by . since test set labels are not provided , the test performance is obtained through submission on ka ##ggle . com . each team is limited to two submissions per day . paragraph : quo ##ra question pair quo ##ra question pair data ##set contains over 400 ##k real world question pair selected from quo ##ra . com . a binary ann ##ota ##tion which stands for match ( duplicate ) or not match ( not duplicate ) is provided for each question pair . in our case , duplicate question pair can be interpreted as en ##tail ##ment relation and not duplicate as neutral . we use the same split ratio as mentioned in . sub ##section : experiments setting we implement our algorithm with tensor ##flow framework . an ada ##del ##ta opt ##imi ##zer with as 0 . 95 and as is used to opt ##imi ##ze all the train ##able weights . the initial learning rate is set to 0 . 5 and batch size to 70 . when the model does not improve best in - domain performance for 30 , 000 steps , an sg ##d opt ##imi ##zer with learning rate of is used to help model to find a better local opt ##imum . drop ##out layers are applied before all linear layers and after word - em ##bed ##ding layer . we use an exponential decay ##ed keep rate during training , where the initial keep rate is 1 . 0 and the decay rate is 0 . 97 ##7 for every 10 , 000 step . we initial ##ize our word em ##bed ##ding ##s with pre - trained 300 ##d glove 840 ##b vectors while the out - of - vocabulary word are randomly initial ##ized with uniform distribution . the character em ##bed ##ding ##s are randomly initial ##ized with 100 ##d . we crop or pad each token to have 16 characters . the 1 ##d con ##vo ##lu ##tion kernel size for character em ##bed ##ding is 5 . all weights are constraint by l ##2 regular ##ization , and the l ##2 regular ##ization at step is calculated as follows : where determines the maximum l ##2 regular ##ization ratio , and determines at which step the maximum l ##2 regular ##ization ratio would be applied on the l ##2 regular ##ization . we choose as and as 100 , 000 . the ratio of l ##2 penalty between the difference of two en ##code ##r weights is set to . for a dense block in feature extraction layer , the number of layer is set to and growth rate g is set to . the first scale down ratio in feature extraction layer is set to and transitional scale down ratio is set to . the sequence length is set as a hard cut ##off on all experiments : 48 for multi ##nl ##i , 32 for s ##nl ##i and 24 for quo ##ra question pair data ##set . during the experiments on multi ##nl ##i , we use 15 % of data from s ##nl ##i as in . we select the parameter by the best run of development accuracy . our en ##se ##mbling approach considers the majority vote of the predictions given by multiple runs of the same model under different random parameter initial ##ization . sub ##section : experiment on multi ##nl ##i we compare our result with all other published systems in table [ reference ] . besides es ##im , the state - of - the - art model on s ##nl ##i , all other models appear at rep ##eva ##l 2017 workshop . rep ##eva ##l 2017 workshop requires all submitted model to be sentence encoding - based model therefore alignment between sentences and memory module are not eligible for competition . all models except ours share one common feature that they use l ##st ##m as a essential building block as en ##code ##r . our approach , without using any rec ##urrent structure , achieve ##s the new state - of - the - art performance of 80 . 0 % , exceeding current state - of - the - art performance by more than 5 % . unlike the observation from , we find the out - of - domain test performance is consistently lower than in - domain test performance . selecting parameters from the best in - domain development accuracy partially contributes to this result . sub ##section : experiment on s ##nl ##i in table [ reference ] , we compare our model to other model performance on s ##nl ##i . experiments ( 2 - 7 ) are sentence encoding based model . provides a bi ##ls ##tm baseline . adopt ##s two layer gr ##u en ##code ##r with pre - trained \" skip - thoughts \" vectors . to capture sentence - level semantics , use tree - based cnn and propose a stack - augmented par ##ser - interpreter neural network ( spin ##n ) which incorporates par ##sing information in a sequential manner . uses intra - attention on top of bi ##ls ##tm to generate sentence representation , and proposes an memory augmented neural network to en ##code the sentence . the next group of model , experiments ( 8 - 18 ) , uses cross sentence feature . align ##s each sentence word - by - word with attention on top of l ##st ##ms . enforce ##s cross sentence attention word - by - word matching with the prop ##rose ##d mls ##tm model . proposes long short - term memory - network ( l ##st ##m ##n ) with deep attention fusion that links the current word to previous word stored in memory . deco ##mp ##oses the task into sub - problems and conquer them respectively . proposes neural tree index ##er , a full n - ar ##y tree whose sub ##tree ##s can be overlap ##ped . re - read l ##st ##m proposed by considers the attention vector of one sentence as the inner - state of l ##st ##m for another sentence . propose a sequential model that in ##fers locally , and a ensemble with tree - like inference module that further improves performance . we show our model , di ##in , achieve ##s state - of - the - art performance on the competitive leader ##board . sub ##section : experiment on quo ##ra question pair data ##set in this sub ##section , we evaluate the effectiveness of our model for para ##ph ##rase identification as natural language inference task . other than our baseline ##s , we compare with and . bi ##mp ##m models different perspective of matching between sentence pair on both direction , then aggregate ##s matching vector with l ##st ##m . dec ##att ##word and dec ##att ##cha ##r uses automatically collected in - domain para ##ph ##rase data to noisy pre ##train - gram word em ##bed ##ding and - gram sub ##word em ##bed ##ding corresponding ##ly on deco ##mp ##osa ##ble attention model proposed by . in table [ reference ] , our experiment shows di ##in has better performance than all other models and an ensemble score is higher than the former best result for more than 1 percent . sub ##section : analysis paragraph : ab ##lation study we conduct a ab ##lation study on our base model to examine the effectiveness of each component . we study our model on multi ##nl ##i data ##set and we use matched validation score as the standard for model selection . the result is shown in table [ reference ] . we studies how em feature contributes to the system . after removing the exact match binary feature , we find the performance de ##grade to 78 . 2 on matched score on development set and 78 . 0 on mis ##mat ##ched score . as observed in reading comprehension task , the simple exact match feature does help the model to better understand the sentences . in the experiment 3 , we remove the con ##vo ##lu ##tion ##al feature extract ##or and then model is structured as a sentence - encoding based model . the sentence representation matrix is max - poole ##d over time to obtain a feature vector . once we have the feature vector for premise and for hypothesis , we use as final feature vector to classify the relationship . we obtain 73 . 2 for matched score and 73 . 6 on mis ##mat ##ched data . the result is competitive among other sentence - encoding based model . we further study how encoding layer contribute in en ##rich ##ing the feature space in interaction tensor . if we remove encoding layer completely , then we ' ll obtain a 73 . 5 for matched score and 73 . 2 for mis ##mat ##ched score . the result demonstrate the feature extraction layer have powerful capability to capture the semantic feature . in experiment 5 , we remove both self - attention and fuse gate , thus retaining only highway network . the result improves to 77 . 7 and 77 . 3 respectively on matched and mis ##mat ##ched development set . however , in experiment 6 , when we only remove fuse gate , to our surprise , the performance de ##grade to 73 . 5 for matched score and 73 . 8 for mis ##mat ##ched . on the other hand , if we use the addition of the representation after highway network and the representation after self - attention as skip connection as in experiment 7 , the performance increase to 77 . 3 and 76 . 3 . the comparison indicates self - attention layer makes the training harder to converge while a skip connection could ease the gradient flow for both highway layer and self - attention layer . by comparing the base model and the model the in experiment 6 , we show that the fuse gate not only well serves as a skip connection , but also makes good decision upon which information the fuse for both representation . to show that dense interaction tensor contains more semantic information , we replace the dense interaction tensor with dot product similarity matrix between the encoded representation of premise and hypothesis . the result shows that the dot product similarity matrix has an inferior capacity of semantic information . another dimensional ##ity study is provided in supplementary material . in experiment 9 , we share the encoding layer weight , and the result decrease from the baseline . the result shows that the two set of encoding weights learn the subtle difference between premise and hypothesis . paragraph : error analysis to analyze the model prediction , we use ann ##ota ##ted subset of development set provided by that consists of 1 , 000 examples each tagged with zero or more following tags : conditional : whether the sentence contains a conditional . word overlap : whether both sentences share more than 70 % of their token ##s . ne ##gation : whether a ne ##gation shows up in either sentence . ant ##o : whether two sentences contain antony ##m pair . long sentence : whether premise or hypothesis is longer than 30 or 16 token ##s respectively . tense difference : whether any verb in two sentences uses different tense . active / passive : whether there is an active - to - passive ( or vice versa ) transformation from the premise to the hypothesis . para ##ph ##rase : whether the two sentences are close para ##ph ##rase ##s quantity / time reasoning : whether understanding the pair requires quantity or time reasoning . core ##f : whether the hypothesis contains a pro ##no ##un or referring expression that needs to be resolved using the premise . quan ##ti ##fi ##er : whether either sentence contains one of the following quan ##ti ##fi ##er : much , enough , more , most , less , least , no , none , some , any , many , few , several , almost , nearly . mod ##al : whether one of the following mod ##al verbs appears in either sentence : can , could , may , might , must , will , would , should . belief : whether one of the following belief verbs appear in either sentence : know , believe , understand , doubt , think , suppose , recognize , forget , remember , imagine , mean , agree , disagree , deny , promise . for more detailed descriptions , please resort to . the result is shown in table [ reference ] . we find di ##in is consistently better on sentence pair with word overlap , ant ##o , long sentence , para ##ph ##rase and belief tags by a large margin . during investigation , we h ##yp ##oth ##es ##ize exact match feature helps the model to better understand para ##ph ##rase , therefore we study the result from second ab ##lation ab ##lation study where exact match feature is not used . surprisingly , the model without exact model feature does not work worse on para ##ph ##rase , instead , the accuracy on ant ##o drops about 10 % . di ##in is also work well on long sentence , partially because the rec ##eptive field is large enough to cover all token ##s . paragraph : visual ##ization we also visual ##ize the hidden representation from interaction tensor and the feature map from first dense block in figure [ reference ] . we pick a sentence pair whose premise is \" south carolina has no referendum right , so the supreme court canceled the vote and upheld the ban . \" and hypothesis is \" south carolina has a referendum right , so the supreme court was powerless over the state . \" . the upper row of figures are sampled from hidden representation of interaction tensor . we observe the values of neurons are highly correlated row - wise and column - wise in the interaction tensor and different channel of hidden representation shows different aspect of interaction . though in certain channel same words , \" referendum \" , or phrases , \" supreme court \" , cause activation , different word or phrase pair , such as \" ban \" and \" powerless over \" , also cause activation in other activation . it shows the model ' s strong capacity of understanding text in different perspective . the lower row of figure [ reference ] shows the feature map from first dense block . after being con ##vo ##lved from the interaction tensor and previous feature map , new feature maps shows activation in different position , demonstrating different semantic features are found . the first figure in the lower row has similar pattern as normal attention weight whereas others has no obvious pattern . different channels of feature maps indicate different kinds of semantic feature . section : conclusion and future work we show the interaction tensor ( or attention weight ) contains semantic information to understand the natural language . we introduce interactive inference network , a novel class of architecture that allows the model to solve nl ##i or nl ##i alike tasks via extract ##ing semantic feature from interaction tensor end - to - end . one instance of such architecture , densely interactive inference network ( di ##in ) , achieve ##s state - of - the - art performance on multiple data ##set ##s . by ab ##lating each component in di ##in and changing the dimensional ##ity , we show the effectiveness of each component in di ##in . though we have the initial exploration of natural language inference in interaction space , the full potential is not yet clear . we will keep exploring the potential of interaction space . incorporating common - sense knowledge from external resources such as knowledge base to leverage the capacity of the mode is another research goal of ours . sub ##su ##bs ##ection : ac ##k ##now ##led ##gm ##ents we thank yu ##chen lu , chang huang and kai yu for their sincere and insight ##ful advice . bibliography : references appendix : supplementary material paragraph : dimensional ##ity and parameter number study to study the influence of the model dimension which is also the channel number of interaction tensor , we design experiments to find out whether dimension has influence on performance . we also present the parameter count of these models . the dimensional ##ity is 44 ##8 where 300 comes from word em ##bed ##ding , 100 comes from char feature , 47 comes from part of speech tag ##ging and 1 comes from the binary exact match feature . since highway network sets the output dimensional ##ity default as that in input , we design a variant to highway network so that different output size could be obtained . the variant of highway layer is designed as follows : where is the - th vector of input matrix , is the - th vector of output matrix , , , and , , are train ##able weights . the result shows that higher dimension number have better performance when the dimension number is lower certain threshold , however , when the number of dimensional ##ity is greater than the threshold , larger number of parameter and higher dimensional ##ity does n ' t contribute to performance . in the case of s ##nl ##i , due to its simplicity in language pattern , 250 ##d would be su ##ffi ##ce to obtain a good performance . on the other hand , it requires 350 ##d to achieve a competitive performance on multi ##nl ##i . we fail to reproduce our best performance with the new structure on multi ##nl ##i . it shows that the additional layer on highway network does n ' t helps convergence .",
        "pred_seq": "densely network [SEP] [SEP] [SEP] natural inference [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "densely interactive inference network"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "natural language inference"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "quora question pair dataset",
                        "quora question pair"
                    ]
                ],
                "Method": [
                    [
                        "densely interactive inference network",
                        "diin"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy",
                        "development accuracy",
                        "indomain development accuracy"
                    ]
                ],
                "Task": [
                    [
                        "paraphrase identification task",
                        "paraphrase identification",
                        "paraphrase"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "snli",
                        "snli dataset",
                        "stanford natural language inference"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "natural language inference",
                        "nli"
                    ]
                ]
            }
        ]
    },
    "22": {
        "doctext": "document : fra ##tern ##al drop ##out rec ##urrent neural networks ( rn ##ns ) form an important class of architecture ##s among neural networks useful for language modeling and sequential prediction . however , opt ##imi ##zing rn ##ns is known to be harder compared to feed - forward neural networks . a number of techniques have been proposed in literature to address this problem . in this paper we propose a simple technique called fra ##tern ##al drop ##out that takes advantage of drop ##out to achieve this goal . specifically , we propose to train two identical copies of an rn ##n ( that share parameters ) with different drop ##out masks while mini ##mi ##zing the difference between their ( pre - soft ##max ) predictions . in this way our regular ##ization encourages the representations of rn ##ns to be invariant to drop ##out mask , thus being robust . we show that our regular ##ization term is upper bounded by the expectation - linear drop ##out objective which has been shown to address the gap due to the difference between the train and inference phases of drop ##out . we evaluate our model and achieve state - of - the - art results in sequence modeling tasks on two bench ##mark data ##set ##s - penn tree ##bank and wi ##kit ##ex ##t - 2 . we also show that our approach leads to performance improvement by a significant margin in image capt ##ion ##ing ( microsoft coco ) and semi - supervised ( ci ##far - 10 ) tasks . section : introduction rec ##urrent neural networks ( rn ##ns ) like long short - term memory ( l ##st ##m ; ) networks and gate ##d rec ##urrent unit ( gr ##u ; ) are popular architecture ##s for sequence modeling tasks like language generation , translation , speech synthesis , and machine comprehension . however , they are harder to opt ##imi ##ze compared to feed - forward networks due to challenges like variable length input sequences , repeated application of the same transition operator at each time step , and largely - dense em ##bed ##ding matrix that depends on the vocabulary size . due to these optimization challenges in rn ##ns , the application of batch normal ##ization and its variants ( layer normal ##ization , rec ##urrent batch normal ##ization , rec ##urrent normal ##ization propagation ) have not been as successful as their counterparts in feed - forward networks , although they do considerably provide performance gains . similarly , naive application of drop ##out has been shown to be ineffective in rn ##ns . therefore , regular ##ization techniques for rn ##ns is an active area of research . to address these challenges , proposed to apply drop ##out only to the non - rec ##urrent connections in multi - layer rn ##ns . variation ##al drop ##out ( ) uses the same drop ##out mask throughout a sequence during training . drop ##con ##ne ##ct applies the drop ##out operation on the weight matrices . zone ##out ( ) , in a similar spirit with drop ##out , randomly chooses to use the previous time step hidden state instead of using the current one . similarly as a substitute for batch normal ##ization , layer normal ##ization normal ##izes the hidden units within each sample to have zero mean and unit standard deviation . rec ##urrent batch normal ##ization applies batch normal ##ization but with un ##sha ##red mini - batch statistics for each time step . and on the other hand show that activity regular ##ization ( ar ) and temporal activation regular ##ization ( tar ) are also effective methods for regular ##izing l ##st ##ms . another more recent way of regular ##izing rn ##ns , that is similar in spirit to the approach we take , involves mini ##mi ##zing the difference between the hidden states of the original and the auxiliary network . in this paper we propose a simple regular ##ization based on drop ##out that we call fra ##tern ##al drop ##out , where we minimize an equally weighted sum of prediction losses from two identical copies of the same l ##st ##m with different drop ##out masks , and add as a regular ##ization the difference between the predictions ( pre - soft ##max ) of the two networks . we analytical ##ly show that our regular ##ization objective is equivalent to mini ##mi ##zing the variance in predictions from different i . i . d . drop ##out masks ; thus encouraging the predictions to be invariant to drop ##out masks . we also discuss how our regular ##ization is related to expectation linear drop ##out , - model and activity regular ##ization , and empirical ##ly show that our method provides non - trivial gains over these related methods which we explain furthermore in our ab ##lation study ( section [ reference ] ) . section : fra ##tern ##al drop ##out drop ##out is a powerful regular ##ization for neural networks . it is usually more effective on densely connected layers because they suffer more from over ##fi ##tting compared with con ##vo ##lu ##tion layers where the parameters are shared . for this reason drop ##out is an important regular ##ization for rn ##ns . however , drop ##out has a gap between its training and inference phase since the latter phase assumes linear activation ##s to correct for the factor by which the expected value of each activation would be different . in addition , the prediction of models with drop ##out generally vary with different drop ##out mask . however , the desirable property in such cases would be to have final predictions be invariant to drop ##out masks . as such , the idea behind fra ##tern ##al drop ##out is to train a neural network model in a way that encourages the variance in predictions under different drop ##out masks to be as small as possible . specifically , consider we have an rn ##n model denoted by that takes as input , where denotes the model parameters . let be the prediction of the model for input sample at time , for drop ##out mask and current input , where is a function of and the hidden states corresponding to the previous time steps . similarly , let be the corresponding time step loss value for the overall input - target sample pair . then in fra ##tern ##al drop ##out , we simultaneously feed - forward the input sample through two identical copies of the rn ##n that share the same parameters but with different drop ##out masks and at each time step . this yields two loss values at each time step given by , and . then the overall loss function of fra ##tern ##al drop ##out is given by , where is the regular ##ization coefficient , is the dimensions of and is the fra ##tern ##al drop ##out regular ##ization given by , we use monte carlo sampling to approximate where and are the same as the one used to calculate values . hence , the additional computation is ne ##gli ##gible . we note that the regular ##ization term of our objective is equivalent to mini ##mi ##zing the variance in the prediction function with different drop ##out masks as shown below ( proof in the appendix ) . theorem : . let sit and s ##j ##t be i . i . d . drop ##out masks and ##\u2208 ##pt ( z ##t , sit ; \u03b8 ) rm be the prediction function as described above . then , note that a general ##ization of our approach would be to minimize the difference between the predictions of the two networks with different data / model aug ##ment ##ations . however , in this paper we focus on using different drop ##out masks and experiment mainly with rn ##ns . section : related work sub ##section : relation to expectation linear drop ##out ( el ##d ) analytical ##ly showed that the expected error ( over samples ) between a model ' s expected prediction over all drop ##out masks , and the prediction using the average mask , is upper bounded . based on this result , they propose to explicitly minimize the difference ( we have adapted their regular ##ization to our notation ##s ) , where is the drop ##out mask . however , due to feasibility consideration , they instead propose to use the following regular ##ization in practice , specifically , this is achieved by feed - forward ##ing the input twice through the network , with and without drop ##out mask , and mini ##mi ##zing the main network loss ( with drop ##out ) along with the regular ##ization term specified above ( but without back - prop ##aga ##ting the gradient ##s through the network without drop ##out ) . the goal of is to minimize the network loss along with the expected difference between the prediction from individual drop ##out mask and the prediction from the expected drop ##out mask . we note that our regular ##ization objective is upper bounded by the expectation - linear drop ##out regular ##ization as shown below ( proof in the appendix ) . theorem : . . this result shows that mini ##mi ##zing the el ##d objective indirectly minimize ##s our regular ##ization term . finally as indicated above , they apply the target loss only on the network with drop ##out . in fact , in our own ab ##lation studies ( see section [ reference ] ) we find that back - prop ##aga ##ting target loss through the network ( without drop ##out ) makes opt ##imi ##zing the model harder . however , in our setting , simultaneously back - prop ##aga ##ting target loss through both networks yields both performance gain as well as convergence gain . we believe convergence is faster for our regular ##ization because network weights are more likely to get target based updates from back - propagation in our case . this is especially true for weight drop ##out since in this case dropped weights do not get updated in the training iteration . sub ##section : relation to - model propose - model with the goal of improving performance on classification tasks in the semi - supervised setting . they propose a model similar to ours ( considering the equivalent deep feed - forward version of our model ) except they apply target loss only on one of the networks and use time - dependent weight ##ing function ( while we use constant ) . the intuition in their case is to leverage un ##lab ##ele ##d data by using them to minimize the difference in prediction between the two copies of the network with different drop ##out masks . further , they also test their model in the supervised setting but fail to explain the improvements they obtain by using this regular ##ization . we note that in our case we analytical ##ly show that mini ##mi ##zing our regular ##izer ( also used in - model ) is equivalent to mini ##mi ##zing the variance in the model predictions ( remark [ reference ] ) . furthermore , we also show the relation of our regular ##izer to expectation linear drop ##out ( proposition [ reference ] ) . in section [ reference ] , we study the effects of target based loss on both networks , which is not used in the - model . we find that applying target loss on both the networks leads to significantly faster convergence . finally , we bring to attention that temporal em ##bed ##ding ( another model proposed by , claimed to be a better version of - model for semi - supervised , learning ) is intra ##ctable in natural language processing applications because storing averaged predictions over all of the time steps would be memory exhaust ##ive ( since predictions are usually huge - tens of thousands values ) . on a final note , we argue that in the supervised case , using a time - dependent weight ##ing function instead of a constant value is not needed . since the ground truth labels are known , we have not observed the problem mentioned by , that the network gets stuck in a de ##gen ##erate solution when is too large in earlier epoch ##s of training . we note that it is much easier to search for an optimal constant value , which is true in our case , as opposed to tuning the time - dependent function . similarity to - model makes our method related to other semi - supervised works , mainly and . since semi - supervised learning is not a primary focus of this paper , we refer to for more details . we note that the idea of adding a penalty encouraging the representation to be similar for two different masks was previously implemented by the authors of a multi - prediction deep bolt ##zman ##n machines . nevertheless , the idea is not discussed in their paper . another way to address the gap between the train and evaluation mode of drop ##out is to perform monte carlo sampling of masks and average the predictions during evaluation , and this has been used for feed - forward networks . we find that this technique does not work well for rn ##ns . the details of these experiments can be found in the appendix . section : experiments sub ##section : language models in the case of language modeling we test our model on two bench ##mark data ##set ##s - penn tree - bank ( pt ##b ) data ##set and wi ##kit ##ex ##t - 2 ( w ##t ##2 ) data ##set . we use prep ##ro ##ces ##sing as specified by ( for pt ##b corpus ) and moses token ##izer ( for the w ##t ##2 data ##set ) . for both data ##set ##s we use the aw ##d - l ##st ##m 3 - layer architecture described in which we call the baseline model . the number of parameters in the model used for pt ##b is 24 million as compared to 34 million in the case of w ##t ##2 because w ##t ##2 has a larger vocabulary size for which we use a larger em ##bed ##ding matrix . apart from those differences , the architecture ##s are identical . when we use fra ##tern ##al drop ##out , we simply add our regular ##ization on top of this baseline model . word level penn tree ##bank ( pt ##b ) . influenced by , our goal here is to make sure that fra ##tern ##al drop ##out out ##per ##forms existing methods not simply because of extensive hyper - parameter grid search but rather due to its regular ##ization effects . hence , in our experiments we leave a vast majority of hyper - parameters used in the baseline model unchanged i . e . em ##bed ##ding and hidden states sizes , gradient clip ##ping value , weight decay and the values used for all drop ##out layers ( drop ##out on the word vectors , the output between l ##st ##m layers , the output of the final l ##st ##m , and em ##bed ##ding drop ##out ) . however , a few changes are necessary : the coefficients for ar and tar needed to be altered because fra ##tern ##al drop ##out also affects rn ##ns activation ( as explained in sub ##section [ reference ] ) - we did not run grid search to obtain the best values but simply dea ##ct ##ivated ar and tar regular ##izer ##s ; since fra ##tern ##al drop ##out needs twice as much memory , batch size is hal ##ved so the model needs approximately the same amount of memory and hence fits on the same gp ##u . the final change in hyper - parameters is to alter the non - mono ##tone interval used in non - mono ##tonic ##ally triggered averaged sg ##d ( nt - as ##g ##d ) opt ##imi ##zer . we run a grid search on and obtain very similar results for the largest values ( 40 , 50 and 60 ) in the candidate set . hence , our model is trained longer using ordinary sg ##d opt ##imi ##zer as compared to the baseline model . we evaluate our model using the per ##plex ##ity metric and compare the results that we obtain against the existing state - of - the - art results . the results are reported in table [ reference ] . our approach achieve ##s the state - of - the - art performance compared with existing bench ##marks . to confirm that the gains are robust to initial ##ization , we run ten experiments for the baseline model with different seeds ( without fine - tuning ) for pt ##b data ##set to compute confidence intervals . the average best validation per ##plex ##ity is with the minimum value equals . the same for test per ##plex ##ity is and , respectively . our score ( validation and test per ##plex ##ity ) beats or ##dina ##l drop ##out minimum values . we also perform experiments using fra ##tern ##al drop ##out with a grid search on all the hyper - parameters and find that it leads to further improvements in performance . the details of this experiment can be found in section [ reference ] . word level wi ##kit ##ex ##t - 2 ( w ##t ##2 ) . in the case of wi ##kit ##ex ##t - 2 language modeling task , we out ##per ##form the current state - of - the - art using the per ##plex ##ity metric by a significant margin . due to the lack of computational power , we run a single training procedure for fra ##tern ##al drop ##out on w ##t ##2 data ##set because it is larger than pt ##b . in this experiment , we use the best hyper - parameters found for pt ##b data ##set ( , non - mono ##tone interval and hal ##ved batch size ; the rest of the hyper - parameters are the same as described in for w ##t ##2 ) . the final results are presented in table [ reference ] . sub ##section : image capt ##ion ##ing we also apply fra ##tern ##al drop ##out on an image capt ##ion ##ing task . we use the well - known show and tell model as a baseline . we emphasize that in the image capt ##ion ##ing task , the image en ##code ##r and sentence deco ##der architecture ##s are usually learned together . since we want to focus on the benefits of using fra ##tern ##al drop ##out in rn ##ns we use frozen pre ##train ##ed res ##net - 101 model as our image en ##code ##r . it means that our results are not directly comparable with other state - of - the - art methods , however we report results for the original methods so readers can see that our baseline performs well . the final results are presented in table [ reference ] . we argue that in this task smaller values are optimal because the image capt ##ion ##ing en ##code ##r is given all information in the beginning and hence the variance of consecutive predictions is smaller that in un ##con ##dition ##ed natural language processing tasks . fra ##tern ##al drop ##out may benefits here mainly due to averaging gradient ##s for different mask and hence up ##dating weights more frequently . section : ab ##lation studies in this section , the goal is to study existing methods closely related to ours - expectation linear drop ##out , - model and activity regular ##ization . all of our experiments for ab ##lation studies , which apply a single layer l ##st ##m , use the same hyper - parameters and model architecture as . sub ##section : expectation - linear drop ##out ( el ##d ) the relation with expectation - linear drop ##out has been discussed in section [ reference ] . here we perform experiments to study the difference in performance when using the el ##d regular ##ization versus our regular ##ization ( f ##d ) . in addition to el ##d , we also study a modification ( el ##dm ) of el ##d which applies target loss to both copies of l ##st ##ms in el ##d similar to f ##d ( notice in their case they only have drop ##out on one l ##st ##m ) . finally we also evaluate a baseline model without any of these regular ##izations . the learning dynamics curves are shown in figure [ reference ] . our regular ##ization performs better in terms of convergence compared with other methods . in terms of general ##ization , we find that f ##d is similar to el ##d , but baseline and el ##dm are much worse . interesting ##ly , looking at the train and validation curves together , el ##dm seems to be suffering from optimization problems . sub ##section : - model since - model is similar to our algorithm ( even though it is designed for semi - supervised learning in feed - forward networks ) , we study the difference in performance with - model both qu ##ali ##tative ##ly and quantitative ##ly to establish the advantage of our approach . first , we run both single layer l ##st ##m and 3 - layer aw ##d - l ##st ##m on pt ##b task to check how their model compares with ours in the case of language modeling . the results are shown in figure [ reference ] and [ reference ] . we find that our model converge ##s significantly faster than - model . we believe this happens because we back - prop ##aga ##te the target loss through both networks ( in contrast to - model ) that leads to weights getting updated using target - based gradient ##s more often . even though we designed our algorithm specifically to address problems in rn ##ns , to have a fair comparison , we compare with - model on a semi - supervised task which is their goal . specifically , we use the ci ##far - 10 data ##set that consists of images from 10 classes . following the usual splits used in semi - supervised learning literature , we use 4 thousand labeled and 41 thousand un ##lab ##ele ##d samples for training , 5 thousand labeled samples for validation and 10 thousand labeled samples for test set . we use the original res ##net - 56 architecture . we run grid search on , drop ##out rates in and leave the rest of the hyper - parameters unchanged . we additionally check importance of using un ##lab ##ele ##d data . the results are reported in table [ reference ] . we find that our algorithm performs at par with - model . when un ##lab ##ele ##d data is not used , fra ##tern ##al drop ##out provides slightly better results as compared to traditional drop ##out . sub ##section : activity regular ##ization and temporal activity regular ##ization analysis the authors of study the importance of activity regular ##ization ( ar ) and temporal activity regular ##ization ( tar ) in l ##st ##ms given as , where is the l ##st ##m ' s output activation at time step ( hence depends on both current input and the model parameters ) . notice that ar and tar regular ##izations are applied on the output of the l ##st ##m , while our regular ##ization is applied on the pre - soft ##max output of the l ##st ##m . however , since our regular ##ization can be deco ##mp ##osed as and en ##cap ##sul ##ates an term along with the dot product term , we perform experiments to confirm that the gains in our approach is not due to the regular ##ization alone . a similar argument goes for the tar objective . we run a grid search on , , which include the hyper - parameters mentioned in . for our regular ##ization , we use . furthermore , we also compare with a regular ##ization ( pr ) that regular ##izes to further rule - out any gains only from regular ##ization . based on this grid search , we pick the best model on the validation set for all the regular ##izations , and additionally report a baseline model without any of these four mentioned regular ##izations . the learning dynamics is shown in figure [ reference ] . our regular ##ization performs better both in terms of convergence and general ##ization compared with other methods . average hidden state activation is reduced when any of the regular ##izer described is applied ( see figure [ reference ] ) . sub ##section : improvements using fine - tuning we confirm that models trained with fra ##tern ##al drop ##out benefit from the nt - as ##g ##d fine - tuning step ( as also used in ) . however , this is a very time - consuming practice and since different hyper - parameters may be used in this additional part of the learning procedure , the probability of obtaining better results due to the extensive grid search is higher . hence , in our experiments we use the same fine - tuning procedure as implemented in the official repository ( even fra ##tern ##al drop ##out was not used ) . we present the importance of fine - tuning in table [ reference ] . sub ##section : fra ##tern ##al drop ##out and expectation linear drop ##out comparison we perform extensive grid search for the baseline model from sub ##section [ reference ] ( an aw ##d - l ##st ##m 3 - layer architecture ) trained with either fra ##tern ##al drop ##out or expectation linear drop ##out regular ##izations , to further contrast the performance of these two methods . the experiments are run without fine - tuning on the pt ##b data ##set . in each run , all five drop ##out rates are randomly altered ( they are set to their original value , as in , multiplied by a value drawn from the uniform distribution on the interval ) and the rest of the hyper - parameters are drawn as shown in table [ reference ] . as in sub ##section [ reference ] , ar and tar regular ##izer ##s are dea ##ct ##ivated . together we run more than 400 experiments . the results are presented in table [ reference ] . both f ##d and el ##d perform better than the baseline model that instead uses ar and tar regular ##izer ##s . hence , we confirm our previous finding ( see sub ##section [ reference ] ) that both f ##d and el ##d are better . however , as found previously for smaller model in sub ##section [ reference ] , the convergence of f ##d is faster than that of el ##d . additionally , fra ##tern ##al drop ##out is more robust to different hyper - parameters choice ( more runs performing better than the baseline and better average for top performing runs ) . section : conclusion in this paper we propose a simple regular ##ization method for rn ##ns called fra ##tern ##al drop ##out that acts as a regular ##ization by reducing the variance in model predictions across different drop ##out masks . we show that our model achieve ##s state - of - the - art results on bench ##mark language modeling tasks along with faster convergence . we also analytical ##ly study the relationship between our regular ##ization and expectation linear drop ##out . we perform a number of ab ##lation studies to evaluate our model from different aspects and carefully compare it with related methods both qu ##ali ##tative ##ly and quantitative ##ly . section : acknowledge ##ments the authors would like to acknowledge the support of the following agencies for research funding and computing support : ns ##er ##c , ci ##far , and iv ##ado . we would like to thank rosemary nan ke and philippe lac ##ail ##le for their thoughts and comments throughout the project . we would also like to thank stanis\u0142aw [UNK] and evan ra ##ca ##h for useful discussions . bibliography : references appendix : appendix sub ##section : monte carlo evaluation a well known way to address the gap between the train and evaluation mode of drop ##out is to perform monte carlo sampling of masks and average the predictions during evaluation ( mc - eva ##l ) , and this has been used for feed - forward networks . since fra ##tern ##al drop ##out addresses the same problem , we would like to clarify that it is not straight - forward and feasible to apply mc - eva ##l for rn ##ns . in feed - forward networks , we average the output prediction scores from different masks . however , in the case rn ##ns ( for next step predictions ) , there is more than one way to perform such evaluation , but each one is problematic . they are as follows : 1 . online averaging consider that we first make the prediction at time step 1 using different masks by averaging the prediction score . then we use this output to feed as input to the time step 2 , then use different masks at time step 2 to generate the output at time step 2 , and so on . but in order to do so , because of the way rn ##ns work , we also need to feed the previous time hidden state to time step 2 . one way would be to average the hidden states over different masks at time step 1 . but the hidden space can in general be highly nonlinear , and it is not clear if averaging in this space is a good strategy . this approach is not justified . besides , this strategy as a whole is extremely time consuming because we would need to sequential ##ly make predictions with multiple masks at each time step . 2 . sequence averaging let ' s consider that we use a different mask each time we want to generate a sequence , and then we average the prediction scores , and compute the ar ##gm ##ax ( at each time step ) to get the actual generated sequence . in this case , notice it is not guaranteed that the predicted word at time step due to averaging the predictions would lead to the next word ( generated by the same process ) if we were to feed the time step output as input to the time step . for example , with different drop ##out masks , if the probability of time step outputs are : i 40 % ) , he ( 30 % ) , she ( 30 % ) , and the probability of the 2nd time step outputs are : am ( 30 % ) , is ( 60 % ) , was ( 10 % ) . then the averaged prediction score followed by ar ##gm ##ax will result in the prediction ' ' i is ' ' , but this would be incorrect . a similar concern applies for output predictions varying in temporal length . hence , this approach can not be used to generate a sequence ( it has to be done by by sampling a mask and generating a single sequence ) . however , this approach may be used to estimate the probability assigned by the model to a given sequence . nonetheless , we run experiments on the pt ##b data ##set using mc - eva ##l ( the results are summarized in table [ reference ] ) . we start with a simple comparison that compares fra ##tern ##al drop ##out with the averaged mask and the aw ##d - l ##st ##m 3 - layer baseline with a single fixed mask that we call mc ##1 . the mc ##1 model performs much worse than fra ##tern ##al drop ##out . hence , it would be hard to use mc ##1 model in practice because a single sample is inaccurate . we also check mc - eva ##l for a larger number of models ( mc ##50 ) ( 50 models were used since we were not able to fit more models simultaneously on a single gp ##u ) . the final results for mc ##50 are worse than the baseline which uses the averaged mask . for comparison , we also evaluate mc ##10 . note that no fine - tuning is used for the above experiments . sub ##section : reasons for focusing on rn ##ns the fra ##tern ##al drop ##out method is general and may be applied in feed - forward architecture ##s ( as shown in sub ##section [ reference ] for ci ##far - 10 semi ##su ##per ##vis ##ed example ) . however , we believe that it is more powerful in the case of rn ##ns because : variance in prediction accumulate ##s among time steps in rn ##ns and since we share parameters for all time steps , one may use the same value at each step . in feed - forward networks the layers usually do not share parameters and hence one may want to use different values for different layers ( which may be hard to tune ) . the simple way to alleviate this problem is to apply the regular ##ization term on the pre - soft ##max predictions only ( as shown in the paper ) or use the same value for all layers . however , we believe that it may limit possible gains . the best performing rn ##n architecture ##s ( state - of - the - art ) usually use some kind of drop ##out ( em ##bed ##ding drop ##out , word drop ##out , weight drop ##out etc . ) , very often with high drop ##out rates ( even larger than 50 % for input word em ##bed ##ding in nl ##p tasks ) . however , this is not true for feed - forward networks . for instance , res ##net architecture ##s very often do not use drop ##out at all ( probably because batch normal ##ization is often better to use ) . it can be seen in the paper ( sub ##section [ reference ] , semi ##su ##per ##vis ##ed ci ##far - 10 task ) that when un ##lab ##ele ##d data is not used the regular drop ##out hurts performance and using fra ##tern ##al drop ##out seems to improve just a little . on a final note , the monte carlo sampling ( a well known method that ad ##resses the gap bet ##wee ##m the train and evaluation mode of drop ##out ) can not be easily applied for rn ##ns and fra ##tern ##al drop ##out may be seen as an alternative . to conclude , we believe that when the use of drop ##out benefits in a given architecture , applying fra ##tern ##al drop ##out should improve performance even more . as mentioned before , in image recognition tasks , one may experiment with something what we would temporarily dub fra ##tern ##al aug ##ment ##ation ( even though drop ##out is not used , one can use random data aug ##ment ##ation such as random crop or random flip ) . hence , one may force a given neural network to have the same predictions for different aug ##ment ##ations . sub ##section : proof ##s theorem : . let sit and s ##j ##t be i . i . d . drop ##out masks and ##\u2208 ##pt ( z ##t , sit ; \u03b8 ) rm be the prediction function as described above . then , proof : proof . for simplicity of notation , we om ##it the time index . [UNK] theorem : . . proof : proof . let , then then using jensen ' s inequality , [UNK]",
        "pred_seq": "ci 10 [SEP] fra networks [SEP] [SEP] language modeling [SEP] [unused0] wi 2 [SEP] fra networks [SEP] [SEP] language modeling [SEP] [unused0] wi 2 [SEP] fra networks [SEP] [SEP] language modeling [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "cifar10"
                    ]
                ],
                "Method": [
                    [
                        "fraternal dropout recurrent neural networks"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "language modeling"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "wikitext2"
                    ]
                ],
                "Method": [
                    [
                        "fraternal dropout recurrent neural networks"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "language modeling"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "wikitext2"
                    ]
                ],
                "Method": [
                    [
                        "fraternal dropout recurrent neural networks"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "language modeling"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "penn treebank",
                        "penn treebank ptb dataset",
                        "ptb"
                    ]
                ],
                "Method": [
                    [
                        "fraternal dropout",
                        "dropout",
                        "tar",
                        "lstm",
                        "fraternal dropout regularization",
                        "dropout layers",
                        "eld",
                        "fraternal dropout method"
                    ]
                ],
                "Metric": [
                    [
                        "perplexity metric"
                    ]
                ],
                "Task": [
                    [
                        "language modeling",
                        "wikitext2 language modeling task",
                        "language modeling tasks"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "penn treebank",
                        "penn treebank ptb dataset",
                        "ptb"
                    ]
                ],
                "Method": [
                    [
                        "fraternal dropout",
                        "dropout",
                        "tar",
                        "lstm",
                        "fraternal dropout regularization",
                        "dropout layers",
                        "eld",
                        "fraternal dropout method"
                    ]
                ],
                "Metric": [
                    [
                        "train and evaluation mode",
                        "average best validation perplexity",
                        "validation and test perplexity",
                        "train and validation curves",
                        "train and evaluation mode of dropout"
                    ]
                ],
                "Task": [
                    [
                        "language modeling",
                        "wikitext2 language modeling task",
                        "language modeling tasks"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "wikitext2",
                        "wikitext2 wt2 dataset"
                    ]
                ],
                "Method": [
                    [
                        "fraternal dropout",
                        "dropout",
                        "tar",
                        "lstm",
                        "fraternal dropout regularization",
                        "dropout layers",
                        "eld",
                        "fraternal dropout method"
                    ]
                ],
                "Metric": [
                    [
                        "perplexity metric"
                    ]
                ],
                "Task": [
                    [
                        "language modeling",
                        "wikitext2 language modeling task",
                        "language modeling tasks"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "wikitext2",
                        "wikitext2 wt2 dataset"
                    ]
                ],
                "Method": [
                    [
                        "fraternal dropout",
                        "dropout",
                        "tar",
                        "lstm",
                        "fraternal dropout regularization",
                        "dropout layers",
                        "eld",
                        "fraternal dropout method"
                    ]
                ],
                "Metric": [
                    [
                        "train and evaluation mode",
                        "average best validation perplexity",
                        "validation and test perplexity",
                        "train and validation curves",
                        "train and evaluation mode of dropout"
                    ]
                ],
                "Task": [
                    [
                        "language modeling",
                        "wikitext2 language modeling task",
                        "language modeling tasks"
                    ]
                ]
            }
        ]
    },
    "23": {
        "doctext": "document : speech recognition with deep rec ##urrent neural networks rec ##urrent neural networks ( rn ##ns ) are a powerful model for sequential data . end - to - end training methods such as connection ##ist temporal classification make it possible to train rn ##ns for sequence label ##ling problems where the input - output alignment is unknown . the combination of these methods with the long short - term memory rn ##n architecture has proved particularly fruit ##ful , delivering state - of - the - art results in cu ##rs ##ive handwriting recognition . however rn ##n performance in speech recognition has so far been disappointing , with better results returned by deep feed ##for ##ward networks . this paper investigates deep rec ##urrent neural networks , which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that em ##power ##s rn ##ns . when trained end - to - end with suitable regular ##isation , we find that deep long short - term memory rn ##ns achieve a test set error of 17 . 7 % on the tim ##it phone ##me recognition bench ##mark , which to our knowledge is the best recorded score . alex ##grave ##s , abd ##el - , university ##oft ##oro ##nto rec ##urrent neural networks , deep neural networks , speech recognition section : introduction neural networks have a long history in speech recognition , usually in combination with hidden marko ##v models . they have gained attention in recent years with the dramatic improvements in acoustic modelling yielded by deep feed ##for ##ward networks . given that speech is an inherently dynamic process , it seems natural to consider rec ##urrent neural networks ( rn ##ns ) as an alternative model . hmm - rn ##n systems have also seen a recent revival , but do not currently perform as well as deep networks . instead of combining rn ##ns with hmm ##s , it is possible to train rn ##ns ' end - to - end ' for speech recognition . this approach exploits the larger state - space and richer dynamics of rn ##ns compared to hmm ##s , and avoids the problem of using potentially incorrect alignment ##s as training targets . the combination of long short - term memory , an rn ##n architecture with an improved memory , with end - to - end training has proved especially effective for cu ##rs ##ive handwriting recognition . however it has so far made little impact on speech recognition . rn ##ns are inherently deep in time , since their hidden state is a function of all previous hidden states . the question that inspired this paper was whether rn ##ns could also benefit from depth in space ; that is from stack ##ing multiple rec ##urrent hidden layers on top of each other , just as feed ##for ##ward layers are stacked in conventional deep networks . to answer this question we introduce deep long short - term memory rn ##ns and assess their potential for speech recognition . we also present an enhancement to a recently introduced end - to - end learning method that jointly trains two separate rn ##ns as acoustic and linguistic models . sections [ reference ] and [ reference ] describe the network architecture ##s and training methods , sec : experiments provides experimental results and concluding remarks are given in sec : conclusion . section : rec ##urrent neural networks given an input sequence , a standard rec ##urrent neural network ( rn ##n ) compute ##s the hidden vector sequence and output vector sequence by it ##era ##ting the following equations from to : where the terms denote weight matrices ( e . g . is the input - hidden weight matrix ) , the terms denote bias vectors ( e . g . is hidden bias vector ) and is the hidden layer function . is usually an element ##wise application of a si ##gm ##oid function . however we have found that the long short - term memory ( l ##st ##m ) architecture , which uses purpose - built memory cells to store information , is better at finding and exploit ##ing long range context . fig : l ##st ##m illustrates a single l ##st ##m memory cell . for the version of l ##st ##m used in this paper is implemented by the following composite function : where is the log ##istic si ##gm ##oid function , and , , and are respectively the input gate , forget gate , output gate and cell activation vectors , all of which are the same size as the hidden vector . the weight matrices from the cell to gate vectors ( e . g . ) are diagonal , so element in each gate vector only receives input from element of the cell vector . one short ##coming of conventional rn ##ns is that they are only able to make use of previous context . in speech recognition , where whole utter ##ances are transcribed at once , there is no reason not to exploit future context as well . bid ##ire ##ction ##al rn ##ns ( br ##nn ##s ) do this by processing the data in both directions with two separate hidden layers , which are then fed forwards to the same output layer . as illustrated in fig : br ##nn , a br ##nn compute ##s the forward hidden sequence , the backward hidden sequence and the output sequence by it ##era ##ting the backward layer from to , the forward layer from to and then up ##dating the output layer : comb ##ing br ##nn ##s with l ##st ##m gives bid ##ire ##ction ##al l ##st ##m , which can access long - range context in both input directions . a crucial element of the recent success of hybrid hmm - neural network systems is the use of deep architecture ##s , which are able to build up progressively higher level representations of acoustic data . deep rn ##ns can be created by stack ##ing multiple rn ##n hidden layers on top of each other , with the output sequence of one layer forming the input sequence for the next . assuming the same hidden layer function is used for all layers in the stack , the hidden vector sequences are it ##erative ##ly computed from to and to : where we define . the network outputs are deep bid ##ire ##ction ##al rn ##ns can be implemented by replacing each hidden sequence with the forward and backward sequences and , and ensuring that every hidden layer receives input from both the forward and backward layers at the level below . if l ##st ##m is used for the hidden layers we get deep bid ##ire ##ction ##al l ##st ##m , the main architecture used in this paper . as far as we are aware this is the first time deep l ##st ##m has been applied to speech recognition , and we find that it yields a dramatic improvement over single - layer l ##st ##m . section : network training we focus on end - to - end training , where rn ##ns learn to map directly from acoustic to phonetic sequences . one advantage of this approach is that it removes the need for a pre ##de ##fine ##d ( and error - prone ) alignment to create the training targets . the first step is to to use the network outputs to parameter ##ise a different ##iable distribution over all possible phonetic output sequences given an acoustic input sequence . the log - probability of the target output sequence can then be differentiated with respect to the network weights using back ##pro ##pa ##gation through time , and the whole system can be opt ##imi ##sed with gradient descent . we now describe two ways to define the output distribution and hence train the network . we refer throughout to the length of as , the length of as , and the number of possible phone ##mes as . sub ##section : connection ##ist temporal classification the first method , known as connection ##ist temporal classification ( ct ##c ) , uses a soft ##max layer to define a separate output distribution at every step along the input sequence . this distribution covers the phone ##mes plus an extra blank symbol which represents a non - output ( the soft ##max layer is therefore size ) . intuitive ##ly the network decides whether to emi ##t any label , or no label , at every times ##te ##p . taken together these decisions define a distribution over alignment ##s between the input and target sequences . ct ##c then uses a forward - backward algorithm to sum over all possible alignment ##s and determine the normal ##ised probability of the target sequence given the input sequence . similar procedures have been used elsewhere in speech and handwriting recognition to integrate out over possible segment ##ations ; however ct ##c differs in that it ignores segment ##ation altogether and sums over single - times ##te ##p label decisions instead . rn ##ns trained with ct ##c are generally bid ##ire ##ction ##al , to ensure that every depends on the entire input sequence , and not just the inputs up to . in this work we focus on deep bid ##ire ##ction ##al networks , with defined as follows : where is the element of the length un ##nor ##mal ##ised output vector , and is the number of bid ##ire ##ction ##al levels . sub ##section : rn ##n trans ##du ##cer ct ##c defines a distribution over phone ##me sequences that depends only on the acoustic input sequence . it is therefore an acoustic - only model . a recent aug ##ment ##ation , known as an rn ##n trans ##du ##cer combines a ct ##c - like network with a separate rn ##n that predict ##s each phone ##me given the previous ones , thereby yielding a jointly trained acoustic and language model . joint l ##m - acoustic training has proved beneficial in the past for speech recognition . whereas ct ##c determines an output distribution at every input times ##te ##p , an rn ##n trans ##du ##cer determines a separate distribution for every combination of input times ##te ##p and output times ##te ##p . as with ct ##c , each distribution covers the phone ##mes plus . intuitive ##ly the network ' decides ' what to output depending both on where it is in the input sequence and the outputs it has already emitted . for a length target sequence , the complete set of decisions jointly determines a distribution over all possible alignment ##s between and , which can then be integrated out with a forward - backward algorithm to determine . in the original formulation was defined by taking an ' acoustic ' distribution from the ct ##c network , a ' linguistic ' distribution from the prediction network , then multi ##ply ##ing the two together and reno ##rma ##lis ##ing . an improvement introduced in this paper is to instead feed the hidden activation ##s of both networks into a separate feed ##for ##ward output network , whose outputs are then normal ##ised with a soft ##max function to yield . this allows a richer set of possibilities for combining linguistic and acoustic information , and appears to lead to better general ##isation . in particular we have found that the number of del ##eti ##on errors encountered during deco ##ding is reduced . denote by and the upper ##most forward and backward hidden sequences of the ct ##c network , and by the hidden sequence of the prediction network . at each the output network is implemented by feeding and to a linear layer to generate the vector , then feeding and to a hidden layer to yield , and finally feeding to a size soft ##max layer to determine : where is the element of the length un ##nor ##mal ##ised output vector . for simplicity we constrained all non - output layers to be the same size ( ; however they could be varied independently . rn ##n trans ##du ##cer ##s can be trained from random initial weights . however they appear to work better when initial ##ised with the weights of a pre ##train ##ed ct ##c network and a pre ##train ##ed next - step prediction network ( so that only the output network starts from random weights ) . the output layers ( and all associated weights ) used by the networks during pre ##train ##ing are removed during re ##train ##ing . in this work we pre ##train the prediction network on the phonetic transcription ##s of the audio training data ; however for large - scale applications it would make more sense to pre ##train on a separate text corpus . sub ##section : deco ##ding rn ##n trans ##du ##cer ##s can be deco ##ded with beam search to yield an n - best list of candidate transcription ##s . in the past ct ##c networks have been deco ##ded using either a form of best - first deco ##ding known as prefix search , or by simply taking the most active output at every times ##te ##p . in this work however we exploit the same beam search as the trans ##du ##cer , with the modification that the output label pro ##ba ##bilities do not depend on the previous outputs ( so ) . we find beam search both faster and more effective than prefix search for ct ##c . note the n - best list from the trans ##du ##cer was originally sorted by the length normal ##ised log - pro ##ba ##bil ##ty ; in the current work we di ##sp ##ense with the normal ##isation ( which only helps when there are many more del ##eti ##ons than insertion ##s ) and sort by . sub ##section : regular ##isation regular ##isation is vital for good performance with rn ##ns , as their flexibility makes them prone to over ##fi ##tting . two regular ##iser ##s were used in this paper : early stopping and weight noise ( the addition of ga ##uss ##ian noise to the network weights during training ) . weight noise was added once per training sequence , rather than at every times ##te ##p . weight noise tends to ' sim ##plify ' neural networks , in the sense of reducing the amount of information required to transmit the parameters , which improves general ##isation . section : experiments phone ##me recognition experiments were performed on the tim ##it corpus . the standard 46 ##2 speaker set with all sa records removed was used for training , and a separate development set of 50 speakers was used for early stopping . results are reported for the 24 - speaker core test set . the audio data was encoded using a fourier - transform - based filter - bank with 40 coefficients ( plus energy ) distributed on a mel - scale , together with their first and second temporal derivatives . each input vector was therefore size 123 . the data were normal ##ised so that every element of the input vectors had zero mean and unit variance over the training set . all 61 phone ##me labels were used during training and deco ##ding ( so ) , then mapped to 39 classes for scoring . note that all experiments were run only once , so the variance due to random weight initial ##isation and weight noise is unknown . as shown in tab : tim ##it , nine rn ##ns were evaluated , varying along three main dimensions : the training method used ( ct ##c , trans ##du ##cer or pre ##train ##ed trans ##du ##cer ) , the number of hidden levels ( 1 - 5 ) , and the number of l ##st ##m cells in each hidden layer . bid ##ire ##ction ##al l ##st ##m was used for all networks except ct ##c - 3 ##l - 500 ##h - tan ##h , which had units instead of l ##st ##m cells , and ct ##c - 3 ##l - 421 ##h - un ##i where the l ##st ##m layers were un ##idi ##re ##ction ##al . all networks were trained using st ##och ##astic gradient descent , with learning rate , momentum and random initial weights drawn uniformly from . all networks except ct ##c - 3 ##l - 500 ##h - tan ##h and pre ##tra ##ns - 3 ##l - 250 ##h were first trained with no noise and then , starting from the point of highest log - probability on the development set , re ##train ##ed with ga ##uss ##ian weight noise ( ) until the point of lowest phone ##me error rate on the development set . pre ##tra ##ns - 3 ##l - 250 ##h was initial ##ised with the weights of ct ##c - 3 ##l - 250 ##h , along with the weights of a phone ##me prediction network ( which also had a hidden layer of 250 l ##st ##m cells ) , both of which were trained without noise , re ##train ##ed with noise , and stopped at the point of highest log - probability . pre ##tra ##ns - 3 ##l - 250 ##h was trained from this point with noise added . ct ##c - 3 ##l - 500 ##h - tan ##h was entirely trained without weight noise because it failed to learn with noise added . beam search deco ##ding was used for all networks , with a beam width of 100 . the advantage of deep networks is immediately obvious , with the error rate for ct ##c dropping from 23 . 9 % to 18 . 4 % as the number of hidden levels increases from one to five . the four networks ct ##c - 3 ##l - 500 ##h - tan ##h , ct ##c - 1 ##l - 62 ##2 ##h , ct ##c - 3 ##l - 421 ##h - un ##i and ct ##c - 3 ##l - 250 ##h all had approximately the same number of weights , but give radically different results . the three main conclusions we can draw from this are ( a ) l ##st ##m works much better than for this task , ( b ) bid ##ire ##ction ##al l ##st ##m has a slight advantage over un ##idi ##re ##ction ##al l ##st ##man ##d ( c ) depth is more important than layer size ( which supports previous findings for deep networks ) . although the advantage of the trans ##du ##cer is slight when the weights are randomly initial ##ised , it becomes more substantial when pre ##train ##ing is used . section : conclusions and future work we have shown that the combination of deep , bid ##ire ##ction ##al long short - term memory rn ##ns with end - to - end training and weight noise gives state - of - the - art results in phone ##me recognition on the tim ##it database . an obvious next step is to extend the system to large vocabulary speech recognition . another interesting direction would be to combine frequency - domain con ##vo ##lu ##tion ##al neural networks with deep l ##st ##m . bibliography : references",
        "pred_seq": "tim ##mark [SEP] [SEP] [SEP] speech recognition [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "timit phoneme recognition benchmark"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "speech recognition"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "timit corpus",
                        "timit",
                        "timit database"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "test set error",
                        "deletion errors",
                        "error rate"
                    ]
                ],
                "Task": [
                    [
                        "speech recognition",
                        "speech",
                        "phoneme recognition"
                    ]
                ]
            }
        ]
    },
    "24": {
        "doctext": "document : image restoration using very deep con ##vo ##lu ##tion ##al en ##code ##r - deco ##der networks with symmetric skip connections in this paper , we propose a very deep fully con ##vo ##lu ##tion ##al encoding - deco ##ding framework for image restoration such as den ##ois ##ing and super - resolution . the network is composed of multiple layers of con ##vo ##lu ##tion and de - con ##vo ##lu ##tion operators , learning end - to - end mapping ##s from corrupted images to the original ones . the con ##vo ##lu ##tion ##al layers act as the feature extract ##or , which capture the abstraction of image contents while eliminating noises / corruption ##s . de - con ##vo ##lu ##tion ##al layers are then used to recover the image details . we propose to symmetrical ##ly link con ##vo ##lu ##tion ##al and de - con ##vo ##lu ##tion ##al layers with skip - layer connections , with which the training converge ##s much faster and attain ##s a higher - quality local opt ##imum . first , the skip connections allow the signal to be back - prop ##aga ##ted to bottom layers directly , and thus tackles the problem of gradient vanishing , making training deep networks easier and achieving restoration performance gains consequently . second , these skip connections pass image details from con ##vo ##lu ##tion ##al layers to de - con ##vo ##lu ##tion ##al layers , which is beneficial in recovering the original image . significantly , with the large capacity , we can handle different levels of noises using a single model . experimental results show that our network achieve ##s better performance than all previously reported state - of - the - art methods . . / fig ##s / section : introduction the task of image restoration is to recover an clean image from its corrupted observation , which is known to be an ill - posed inverse problem . by acc ##om ##mo ##dating different types of corruption distributions , the same mathematical model applies to problems such as image den ##ois ##ing and super - resolution . recently , deep neural networks ( d ##nn ##s ) have shown their superior performance in image processing and computer vision tasks , ranging from high - level recognition , semantic segment ##ation to low - level den ##ois ##ing , super - resolution , de ##bl ##ur , in ##pa ##int ##ing and recovering raw images from compressed images . despite the progress that d ##nn ##s achieve , there still are some problems . for example , can a deeper network in general achieve better performance ; can we design a single model to handle different levels of corruption . observing recent superior performance of d ##nn ##s on image processing tasks , we propose a con ##vo ##lu ##tion ##al neural network ( cnn ) - based framework for image restoration . we observe that in order to obtain good restoration performance , it is beneficial to train a very deep model . meanwhile , we show that it is possible to achieve good performance with a single network when processing multiple different levels of corruption ##s due to the benefits of large - capacity networks . specifically , the proposed framework learns end - to - end fully con ##vo ##lu ##tion ##al mapping ##s from corrupted images to the clean ones . the network is composed of multiple layers of con ##vo ##lu ##tion and de - con ##vo ##lu ##tion operators . as deeper networks tend to be more difficult to train , we propose to symmetrical ##ly link con ##vo ##lu ##tion ##al and de - con ##vo ##lu ##tion ##al layers with skip - layer connections , with which the training converge ##s much faster and attain ##s a higher - quality local opt ##imum . our main contributions are briefly outlined as follows : 1 ) a very deep network architecture , which consists of a chain of symmetric con ##vo ##lu ##tion ##al and deco ##n ##vo ##lu ##tion ##al layers , for image restoration is proposed in this paper . the con ##vo ##lu ##tion ##al layers act as the feature extract ##or which en ##code the primary components of image contents while eliminating the corruption . the deco ##n ##vo ##lu ##tion ##al layers then deco ##de the image abstraction to recover the image content details . 2 ) we propose to add skip connections between corresponding con ##vo ##lu ##tion ##al and de - con ##vo ##lu ##tion ##al layers . these skip connections help to back - prop ##aga ##te the gradient ##s to bottom layers and pass image details to the top layers , making training of the end - to - end mapping more easier and effective , and thus achieve performance improvement while the network going deeper . 3 ) relying on the large capacity and fitting ability of our very deep network , we propose to handle different level of noises / corruption using a single model . to our knowledge , this is the first approach that achieve ##s good accuracy for processing different levels of noises with a single model . 4 ) experimental results demonstrate the advantages of our network over other recent state - of - the - art methods on image den ##ois ##ing and super - resolution , setting new records on these topics . related work extensive work has been done on image restoration in the literature . see detail reviews in a survey . traditional methods such as total variation , b ##m ##3d algorithm and dictionary learning based methods have shown very good performance on image restoration topics such as image den ##ois ##ing and super - resolution . since image restoration is in general an ill - posed problem , the use of regular ##ization has been proved to be essential . an active ( and probably more promising ) category for image restoration is the d ##nn based methods . stacked den ##ois ##ing auto - en ##code ##r is one of the most well - known d ##nn models which can be used for image restoration . xi ##e et al . combined sparse coding and d ##nn pre - trained with den ##ois ##ing auto - en ##code ##r for low - level vision tasks such as image den ##ois ##ing and in ##pa ##int ##ing . other neural networks based methods such as multi - layer per ##ce ##pt ##ron and cnn for image den ##ois ##ing , as well as d ##nn for image or video super - resolution and compression artifacts reduction have been actively studied in these years . burger et al . presented a patch - based algorithm learned with a plain multi - layer per ##ce ##pt ##ron . they also concluded that with large networks , large training data , neural networks can achieve state - of - the - art image den ##ois ##ing performance . jain and se ##ung proposed fully con ##vo ##lu ##tion ##al cnn for den ##ois ##ing . they found that cnn provide comparable or even superior performance to wave ##let and marko ##v random field ( mr ##f ) methods . cu ##i et al . employed non - local self - similarity ( nl ##ss ) search on the input image in multi - scale , and then used collaborative local auto - en ##code ##r for super - resolution in a layer by layer fashion . dong et al . proposed to directly learn an end - to - end mapping between the low / high - resolution images . wang et al . argued that domain expertise represented by the conventional sparse coding can be combined to achieve further improved results . in general , d ##nn - based methods learn restoration parameters directly from data , which tends to been more effective in real - world image restoration applications . an advantage of d ##nn methods is that these methods are purely data driven and no assumption about the noise distributions are made . section : very deep red - net for image restoration the proposed framework mainly contains a chain of con ##vo ##lu ##tion ##al layers and symmetric deco ##n ##vo ##lu ##tion ##al layers , as shown in figure [ reference ] . we term our method ' ' red - net ' ' \u2014 very deep residual en ##code ##r - deco ##der networks . sub ##section : architecture the framework is fully con ##vo ##lu ##tion ##al and deco ##n ##vo ##lu ##tion ##al . rec ##ti ##fication layers are added after each con ##vo ##lu ##tion and deco ##n ##vo ##lu ##tion . the con ##vo ##lu ##tion ##al layers act as feature extract ##or , which preserve the primary components of objects in the image and meanwhile eliminating the corruption ##s . the deco ##n ##vo ##lu ##tion ##al layers are then combined to recover the details of image contents . the output of the deco ##n ##vo ##lu ##tion ##al layers is the ' ' clean ' ' version of the input image . moreover , skip connections are also added from a con ##vo ##lu ##tion ##al layer to its corresponding mirrored deco ##n ##vo ##lu ##tion ##al layer . the passed con ##vo ##lu ##tion ##al feature maps are sum ##med to the deco ##n ##vo ##lu ##tion ##al feature maps element - wise , and passed to the next layer after rec ##ti ##fication . for low - level image restoration problems , we use neither pool ##ing nor un ##pool ##ing in the network as usually pool ##ing disc ##ards useful image details that are essential for these tasks . motivated by the v ##gg model , the kernel size for con ##vo ##lu ##tion and deco ##n ##vo ##lu ##tion is set to , which has shown excellent image recognition performance . it is worth mentioning that the size of input image can be arbitrary since our network is essentially a pixel - wise prediction . the input and output of the network are images of the same size , where , and are width , height and number of channels . in this paper , we use although it is straightforward to apply to images with . we found that using 64 feature maps for con ##vo ##lu ##tion ##al and deco ##n ##vo ##lu ##tion ##al layers achieve ##s satisfactory results , although more feature maps leads to slightly better performance . der ##iving from the above architecture , we propose two networks , which are 20 - layer and 30 - layer respectively . sub ##su ##bs ##ection : deco ##n ##vo ##lu ##tion deco ##der architecture ##s combining layers of con ##vo ##lu ##tion and deco ##n ##vo ##lu ##tion have been proposed for semantic segment ##ation lately . in contrast to con ##vo ##lu ##tion ##al layers , in which multiple input activation ##s within a filter window are fused to output a single activation , deco ##n ##vo ##lu ##tion ##al layers associate a single input activation with multiple outputs . one can simply replace deco ##n ##vo ##lu ##tion with con ##vo ##lu ##tion , which results in a architecture that is very similar to recently proposed very deep fully con ##vo ##lu ##tion ##al neural networks . however , there exist essential differences between a fully con ##vo ##lu ##tion model and our model . in the fully con ##vo ##lu ##tion case , the noise is eliminated step by step , i . e . , the noise level is reduced after each layer . during this process , the details of the image content may be lost . nevertheless , in our network , con ##vo ##lu ##tion preserves the primary image content . then deco ##n ##vo ##lu ##tion is used to compensate the details . we compare the 5 - layer and 10 - layer fully con ##vo ##lu ##tion ##al network with our network ( combining con ##vo ##lu ##tion and deco ##n ##vo ##lu ##tion , but without skip connection ) . for fully con ##vo ##lu ##tion ##al networks , we use pad ##ding and up - sample the input to make the input and output the same size . for our network , the first 5 layers are con ##vo ##lu ##tion ##al and the second 5 layers are deco ##n ##vo ##lu ##tion ##al . all the other parameters for training are the same , i . e . , trained with sg ##d and learning rate of , noise level . in terms of ps ##nr , using deco ##n ##vo ##lu ##tion works better than the fully con ##vo ##lu ##tion ##al counterpart . we see that , the fully con ##vo ##lu ##tion ##al network reduces noise layer by layer , and our network preserve primary image contents by con ##vo ##lu ##tion and recover some details by using deco ##n ##vo ##lu ##tion . detailed results are in the supplementary materials . sub ##su ##bs ##ection : skip connections an intuitive question is that , is deco ##n ##vo ##lu ##tion able to recover image details from the image abstraction only ? we find that in shallow networks with only a few layers of con ##vo ##lu ##tion , deco ##n ##vo ##lu ##tion is able to recover the details . however , when the network goes deeper or using operations such as max pool ##ing , deco ##n ##vo ##lu ##tion does not work so well , possibly because too much details are already lost in the con ##vo ##lu ##tion . the second question is that , when our network goes deeper , does it achieve performance gain ? we observe that deeper networks often suffer from gradient ##s vanishing and become hard to train \u2014 a problem that is well addressed in the literature . to address the above two problems , inspired by highway networks and deep residual networks , we add skip connections between two corresponding con ##vo ##lu ##tion ##al and deco ##n ##vo ##lu ##tion ##al layers as shown in figure [ reference ] . a building block is shown in figure [ reference ] . there are two reasons for using such connections . first , when the network goes deeper , as mentioned above , image details can be lost , making deco ##n ##vo ##lu ##tion weaker in recovering them . however , the feature maps passed by skip connections carry much image detail , which helps deco ##n ##vo ##lu ##tion to recover a better clean image . second , the skip connections also achieve benefits on back - prop ##aga ##ting the gradient to bottom layer , which makes training deeper network much easier as observed in and . note that our skip layer connections are very different from the ones proposed in and , where the only concern is on the optimization side . in our case , we want to pass information of the con ##vo ##lu ##tion ##al feature maps to the corresponding deco ##n ##vo ##lu ##tion ##al layers . instead of directly learning the mapping ##s from input to the output , we would like the network to fit the residual of the problem , which is denoted as . such a learning strategy is applied to inner blocks of the encoding - deco ##ding network to make training more effective . skip connections are passed every two con ##vo ##lu ##tion ##al layers to their mirrored deco ##n ##vo ##lu ##tion ##al layers . other configurations are possible and our experiments show that this configuration already works very well . using such short ##cut ##s makes the network easier to be trained and gains restoration performance via increasing network depth . the very deep highway networks are essentially feed - forward long short - term memory ( l ##st ##ms ) with forget gates ; and the cnn layers of deep residual network are feed - forward l ##st ##ms without gates . note that our deep residual networks are in general not in the format of standard feed - forward l ##st ##ms . sub ##section : discussions training with symmetric skip connections as mentioned above , using skip connections mainly has two benefits : ( 1 ) passing image detail forward ##ly , which helps recovering clean images and ( 2 ) passing gradient backward ##ly , which helps finding better local minimum . we design experiments to show these observations . we first compare two networks trained for den ##ois ##ing noises of . in the first network , we use 5 layers of con ##vo ##lu ##tion with stride 3 . the input size of training data is , which results in a vector after 5 layers of con ##vo ##lu ##tion . then deco ##n ##vo ##lu ##tion is used to recover the input . the second network uses the same settings as the first one , except for adding skip connections . the results are show in figure [ reference ] ( a ) . we can observe that it is hard for deco ##n ##vo ##lu ##tion to recover details from only a vector encoding the abstraction of the input , which shows that the ability on recovering image details for deco ##n ##vo ##lu ##tion is limited . however , if we use skip connections , the network can still recover the input , because details are passed to top ##per layers in the network . we also train five networks to show that using skip connections help to back - prop ##aga ##te gradient in training to better fit the end - to - end mapping , as shown in figure [ reference ] ( b ) . the five networks are : 10 , 20 and 30 layer networks without skip connections , and 20 , 30 layer networks with skip connections . as we can see , the training loss increases when the network going deeper without short ##cut ##s ( similar phenomenon is also observed in ) , but we obtain smaller loss when using skip connections . comparison with deep residual networks [ ] one may use different types of skip connections in our network , a straightforward alternate is that in . in , the skip connections are added to divide the network into sequential blocks . a benefit of our model is that our skip connections have element - wise correspondence , which can be very important in pixel - wise prediction problems . we carry out experiments to compare the two types of skip connections . here the block size indicates the span of the connections . the results are shown in figure [ reference ] ( c ) . we can observe that our connections often converge to a better opt ##imum , demonstrating that element - wise correspondence can be important . dealing with different levels of noises / corruption an important question is , can we handle different levels of corruption with a single model . almost all existing methods need to train different models for different levels of corruption and estimate the corruption level at first . we use a trained model in , to den ##oise different levels of noises with being 10 , 30 , 50 and 70 . the obtained average ps ##nr on the 14 images are 29 . 95 ##db , 27 . 81 ##db , 18 . 62 ##db and 14 . 84 ##db , respectively . the results show that the parameters trained on a single noise level can not handle different levels of noises well . therefore , in this paper , we aim to train a single model for recovering different levels of corruption , which are different noise levels in the task of image den ##ois ##ing and different scaling parameters in image super - resolution . the large capacity of the network is the key to this success . sub ##section : training learning the end - to - end mapping from corrupted images to clean ones needs to estimate the weights represented by the con ##vo ##lu ##tion ##al and deco ##n ##vo ##lu ##tion ##al kernel ##s . this is achieved by mini ##mi ##zing the euclidean loss between the outputs of the network and the clean image . in specific , given a collection of training sample pairs , where is a corrupted image and is the clean version as the ground ##tr ##uth . we minimize the following mean squared error ( ms ##e ) : we implement and train our network using caf ##fe . in practice , we find that using adam with learning rate for training converge ##s faster than traditional st ##och ##astic gradient descent ( sg ##d ) . the base learning rate for all layers are the same , different from , in which a smaller learning rate is set for the last layer . this trick is not necessary in our network . as general settings in the literature , we use gray - scale image for den ##ois ##ing and the lu ##mina ##nce channel for super - resolution in this paper . 300 images from the berkeley segment ##ation data ##set ( bs ##d ) are used to generate the training set . for each image , patches of size are sampled as ground truth . for den ##ois ##ing , we add additive ga ##uss ##ian noise to the patches multiple times to generate a large training set ( about 0 . 5 m ) . for super - resolution , we first down - sample a patch and then up - sample it to its original size , obtaining a low - resolution version as the input of the network . sub ##section : testing although trained on local patches , our network can perform den ##ois ##ing and super - resolution on images of arbitrary size . given a testing image , one can simply go forward through the network , which is able to obtain a better performance than existing methods . to achieve more smooth results , we propose to process a corrupted image on multiple orientation ##s . different from segment ##ation , the filter kernel ##s in our network only eliminate the corruption ##s , which is not sensitive to the orientation of image contents . therefore , we can rotate and mirror flip the kernel ##s and perform forward multiple times , and then average the output to get a more smooth image . we see that this can lead to slightly better den ##ois ##ing and super - resolution performance . section : experiments in this section , we provide evaluation of den ##ois ##ing and super - resolution performance of our models against a few existing state - of - the - art methods . den ##ois ##ing experiments are performed on two data ##set ##s : 14 common bench ##mark images and the bs ##d ##200 data ##set . we test additive ga ##uss ##ian noises with zero mean and standard deviation 10 , 30 , 50 and 70 respectively . b ##m ##3d , nc ##sr , ep ##ll , pc ##lr , pd ##pd and w ##mm ##n are compared with our method . for super - resolution , we compare our network with sr ##c ##nn , n ##bs ##rf , cs ##c ##n , cs ##c , ts ##e and ar ##fl + on three data ##set : set ##5 , set ##14 and bs ##d ##100 . the scaling parameter are tested with 2 , 3 and 4 . peak signal - to - noise ratio ( ps ##nr ) and structural similarity ( ss ##im ) index are calculated for evaluation . for our method , which is denoted as red - net , we implement three versions : red ##10 contains 5 con ##vo ##lu ##tion ##al and deco ##n ##vo ##lu ##tion ##al layers without short ##cut ##s , red ##20 contains 10 con ##vo ##lu ##tion ##al and deco ##n ##vo ##lu ##tion ##al layers with short ##cut ##s , and red ##30 contains 15 con ##vo ##lu ##tion ##al and deco ##n ##vo ##lu ##tion ##al layers with short ##cut ##s . sub ##section : image den ##ois ##ing evaluation on the 14 images table [ reference ] presents the ps ##nr and ss ##im results of 10 , 30 , 50 , and 70 . we can make some observations from the results . first of all , the 10 layer con ##vo ##lu ##tion ##al and deco ##n ##vo ##lu ##tion ##al network has already achieved better results than the state - of - the - art methods , which demonstrates that combining con ##vo ##lu ##tion and deco ##n ##vo ##lu ##tion for den ##ois ##ing works well , even without any skip connections . moreover , when the network goes deeper , the skip connections proposed in this paper help to achieve even better den ##ois ##ing performance , which exceeds the existing best method w ##nn ##m by 0 . 32 ##db , 0 . 43 ##db , 0 . 49 ##db and 0 . 51 ##db on noise levels of being 10 , 30 , 50 and 70 respectively . while w ##nn ##m is only slightly better than the second best existing method pc ##lr by 0 . 01 ##db , 0 . 06 ##db , 0 . 03 ##db and 0 . 01 ##db respectively , which shows the large improvement of our model . last , we can observe that the more complex the noise is , the more improvement our model achieve ##s than other methods . similar observations can be made on the evaluation of ss ##im . evaluation on bs ##d ##200 for testing efficiency , we convert the images to gray - scale and res ##ize them to smaller ones on bs ##d - 200 . then all the methods are run on these images to get average ps ##nr and ss ##im results of 10 , 30 , 50 , and 70 , as shown in table [ reference ] . for existing methods , their den ##ois ##ing performance does not differ much , while our model achieve ##s 0 . 38 ##db , 0 . 47 ##db , 0 . 49 ##db and 0 . 42 ##db higher of ps ##nr over w ##nn ##m . sub ##section : image super - resolution the evaluation on set ##5 is shown in table [ reference ] . our 10 - layer network out ##per ##forms the compared methods already , and we achieve better performance with deeper networks . the 30 - layer network exceeds the second best method cs ##c ##n for 0 . 52 ##db , 0 . 56 ##db and 0 . 47 ##db on scale 2 , 3 and 4 respectively . the evaluation on set ##14 is shown in table [ reference ] . the improvement on set ##14 in not as significant as that on set ##5 , but we can still observe that the 30 layer network achieve ##s higher ps ##nr than the second best cs ##c ##n for 0 . 23 ##db , 0 . 06 ##db and 0 . 1 ##db . the results on bs ##d ##100 , as shown in table [ reference ] , is similar than that on set ##5 . the second best method is still cs ##c ##n , the performance of which is not as good as our 10 layer network . our deeper network obtain ##s much more performance gain than the others . sub ##section : evaluation with a single model to construct the training set , we extract image patches with different noise levels and scaling parameters for den ##ois ##ing and super - resolution . then a 30 - layer network is trained for the two tasks respectively . the evaluation results are shown in table [ reference ] and table [ reference ] . although training with different levels of corruption , we can observe that the performance of our network only slightly de ##grade ##s comparing to the case in which using separate models for den ##ois ##ing and super - resolution . this may be due the fact that the network has to fit much more complex mapping ##s . except that cs ##c ##n works slightly better on super - resolution with scales 3 and 4 , our network still beats the existing methods , showing that our network works much better in image den ##ois ##ing and super - resolution even using only one single model to deal with complex corruption . section : conclusions in this paper we have proposed a deep encoding and deco ##ding framework for image restoration . con ##vo ##lu ##tion and deco ##n ##vo ##lu ##tion are combined , modeling the restoration problem by extract ##ing primary image content and recovering details . more importantly , we propose to use skip connections , which helps on recovering clean images and tackles the optimization difficulty caused by gradient vanishing , and thus obtain ##s performance gains when the network goes deeper . experimental results and our analysis show that our network achieve ##s better performance than state - of - the - art methods on image den ##ois ##ing and super - resolution . x . - j . mao ' s contribution was made when visiting the university of adelaide . this work was in part supported by arc future fellowship ( ft ##12 ##01 ##00 ##9 ##6 ##9 ) . correspondence should be addressed to c . shen . bibliography : references",
        "pred_seq": "[SEP] [SEP] [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "berkeley segmentation dataset",
                        "bsd",
                        "bsd100"
                    ]
                ],
                "Method": [
                    [
                        "red30"
                    ]
                ],
                "Metric": [
                    [
                        "psnr",
                        "peak signaltonoise ratio"
                    ]
                ],
                "Task": [
                    [
                        "superresolution",
                        "image superresolution"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "berkeley segmentation dataset",
                        "bsd",
                        "bsd100"
                    ]
                ],
                "Method": [
                    [
                        "red30"
                    ]
                ],
                "Metric": [
                    [
                        "structural similarity",
                        "ssim"
                    ]
                ],
                "Task": [
                    [
                        "superresolution",
                        "image superresolution"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set5"
                    ]
                ],
                "Method": [
                    [
                        "red30"
                    ]
                ],
                "Metric": [
                    [
                        "psnr",
                        "peak signaltonoise ratio"
                    ]
                ],
                "Task": [
                    [
                        "superresolution",
                        "image superresolution"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set5"
                    ]
                ],
                "Method": [
                    [
                        "red30"
                    ]
                ],
                "Metric": [
                    [
                        "structural similarity",
                        "ssim"
                    ]
                ],
                "Task": [
                    [
                        "superresolution",
                        "image superresolution"
                    ]
                ]
            }
        ]
    },
    "25": {
        "doctext": "document : a disc ##rim ##ina ##tively learned cnn em ##bed ##ding for person re - identification in this paper , we rev ##isi ##t two popular con ##vo ##lu ##tion ##al neural networks ( cnn ) in person re - identification ( re - id ) , i . e . , verification and identification models . the two models have their respective advantages and limitations due to different loss functions . in this paper , we shed light on how to combine the two models to learn more disc ##rim ##ina ##tive pedestrian des ##cript ##ors . specifically , we propose a siam ##ese network that simultaneously compute ##s the identification loss and verification loss . given a pair of training images , the network predict ##s the identities of the two input images and whether they belong to the same identity . our network learns a disc ##rim ##ina ##tive em ##bed ##ding and a similarity measurement at the same time , thus making full usage of the re - id ann ##ota ##tions . our method can be easily applied on different pre - trained networks . albeit simple , the learned em ##bed ##ding improves the state - of - the - art performance on two public person re - id bench ##marks . further , we show our architecture can also be applied in image retrieval . large - scale person re - identification , con ##vo ##lu ##tion ##al neural networks . section : introduction person re - identification ( re - id ) is usually viewed as an image retrieval problem , which matches pedestrians from different cameras . given a person - of - interest ( query ) , person re - id determines whether the person has been observed by another camera . recent progress in this area has been due to two factors : 1 ) the availability of the large - scale pedestrian data ##set ##s . the data ##set ##s contain the general visual variance of pedestrian and provide a comprehensive evaluation . 2 ) the learned em ##bed ##ding of pedestrian using a con ##vo ##lu ##tion ##al neural network ( cnn ) . recently , the con ##vo ##lu ##tion ##al neural network ( cnn ) has shown potential for learning state - of - the - art feature em ##bed ##ding ##s or deep metric ##s . as shown in fig . [ reference ] , there are two major types of cnn structures , i . e . , verification models and identification models . the two models are different in terms of input , feature extraction and loss function for training . our motivation is to combine the strengths of the two models and learn a more disc ##rim ##ina ##tive pedestrian em ##bed ##ding . verification models take a pair of images as input and determine whether they belong to the same person or not . a number of previous works treat person re - id as a binary - class classification task or a similarity regression task . given a label , the verification network forces two images of the same person to be mapped to nearby points in the feature space . if the images are of different people , the points are far apart . however , the major problem in the verification models is that they only use weak re - id labels , and do not take all the ann ##ota ##ted information into consideration . therefore , the verification network lacks the consideration of the relationship between the image pairs and other images in the data ##set . in the attempt to take full advantages of the re - id labels , identification models which treat person re - identification as a multi - class recognition task , are employed for feature learning . they directly learn the non - linear functions from an input image to the person id and the cross - entropy loss is used following the final layer . during testing , the feature is extracted from a fully connected layer and then normal ##ized . the similarity of two images is thus computed by the euclidean distance between their normal ##ized cnn em ##bed ##ding ##s . the major draw ##back of the identification model is that the training objective is different from the testing procedure , i . e . , it does not account for the similarity measurement between image pairs , which can be problematic during the pedestrian retrieval process . the above - mentioned observations demonstrate that the two types of models have complementary advantages and limitations as shown in table [ reference ] . motivated by these properties , this work proposes to combine the strengths of the two networks and leverage their complementary nature to improve the disc ##rim ##ina ##tive ability of the learned em ##bed ##ding ##s . the proposed model is a siam ##ese network that predict ##s person identities and similarity scores at the same time . compared to previous networks , we take full advantages of the ann ##ota ##ted data in terms of pair - wise similarity and image identities . during testing , the final con ##vo ##lu ##tion ##al activation ##s are extracted for eu ##cl ##ide ##pd ##fan distance based pedestrian retrieval . to sum ##mar ##ize , our contributions are : we propose a siam ##ese network that has two losses : identification loss and verification loss . this network simultaneously learns a disc ##rim ##ina ##tive cnn em ##bed ##ding and a similarity metric , thus improving pedestrian retrieval accuracy . we report competitive accuracy compared to the state - of - art methods on two large - scale person re - id data ##set ##s ( market ##15 ##01 and cu ##h ##k ##0 ##3 ) and one instance retrieval data ##set ( oxford ##5 ##k ) . the paper is organized as follows . we first review some related works in section [ reference ] . in section [ reference ] , we describe how we combine the two losses and define the cnn structure . the implementation details are provided . in section [ reference ] , we present the experimental results on two large - scale person re - identification data ##set ##s and one instance retrieval data ##set . we conclude this paper in section [ reference ] . section : related work in this section we describe previous works relevant to the approach discussed in this paper . they are mainly based on verification models or identification models . sub ##section : verification models in 1993 , bromley et al . first used verification models to deep metric learning in signature verification . verification models usually take a pair of images as input and output a similarity score by calculating the co ##sin ##e distance between low - dimensional features , which can be penal ##ized by the contrast ##ive loss . recently researchers have begun to apply verification models to person re - identification with a focus on data aug ##ment ##ation and image matching . yi et al . split a pedestrian image into three horizontal parts and train three part - cnn ##s to extract features . the similarity of two images is computed by the co ##sin ##e distance of their features . similarly , cheng et al . split the con ##vo ##lu ##tion ##al map into four parts and fuse the part features with the global features . li et al . add a patch - matching layer that multi ##pl ##ies the activation of two images in different horizontal stripes . they use it to find similar locations and treat similarity regression as binary - class penal ##ized by soft ##max loss . later , ahmed et al . improve the verification model by adding a different matching layer that compares the activation of two images in neighboring pixels . besides , wu et al . use smaller filters and a deeper network to extract features . var ##ior et al . combine cnn with some gate functions , similar to long - short - term memory ( l ##st ##m ) in spirit , which aims to adaptive ##ly focus on the similar parts of input image pairs . but it is limited by the computational in ##ef ##fi ##ciency because the query image has to pair with every gallery image to pass through the network . moreover , ding et al . use triple ##t samples for training the network which considers the images from the same people and the different people at the same time . sub ##section : identification models recent data ##set ##s such as cu ##h ##k ##0 ##3 and market ##15 ##01 provide large - scale training sets , which make it possible to train a deeper classification model without over - fitting . every identity has 9 . 6 training images on average in cu ##h ##k ##0 ##3 and has 17 . 2 images in market ##15 ##01 . cnn can learn disc ##rim ##ina ##tive em ##bed ##ding ##s by itself without part - matching . zheng et al . directly use a conventional fine - tuning approach on market ##15 ##01 , pr ##w and mars and out ##per ##form many recent results . wu et al . combine cnn em ##bed ##ding ##s with the hand - crafted features in the fc layer . besides , xiao et al . jointly train a classification model using multiple data ##set ##s and propose a new drop ##out function to deal with the hundreds of classes . in , xiao et al . train a classification model similar to the faster - rc ##nn method and automatically predict the location of the candidate pedestrian from the whole image , which alleviate ##s the pedestrian detection errors . sub ##section : verification - identification models in face recognition , the \" deep ##id networks \" train the network with the verification and identification losses , which is similar to our network . in , sun et al . jointly train face identification and verification . then more verification supervision is added into the model and a deeper network is used . our method is different from their models in the following aspects . first , in face recognition , the training data ##set contains 202 , 59 ##9 face images of 10 , 177 identities while the current largest person re - i d training data ##set contains 12 , 93 ##6 images of 75 ##1 identities . deep ##id networks apply contrast ##ive loss to the verification problem , wil ##e our model uses the cross - entropy loss . we find that the contrast ##ive loss leads to over - fitting when the number of images is limited . in the experiment , we show the proposed method learns more robust person representative and out ##per ##forms using contrast ##ive loss . second , drop ##out can not be applied on the em ##bed ##ding before the contrast ##ive loss , which introduces zero values at random locations . on the contrary , we can add drop ##out regular ##ization on the em ##bed ##ding in the proposed model . third , the deep ##id networks are trained from scratch , while our model benefits from the networks pre ##train ##ed on image ##net . finally , we evaluate our method on the tasks of person re - id and instance retrieval , providing more insights in the verification - classification models . section : proposed method sub ##section : preview fig . [ reference ] ( a ) and fig . [ reference ] ( b ) illustrate the relational graph built by verification and identification models . in a sample batch of size , red edges represent the positive pairs ( the same person ) and blue edges represent the negative pairs ( different persons ) . the dotted edges denote implicit relationships built by the identification loss and the solid edges denote explicit relationships built by the verification loss . in verification models , there are several operations between the two inputs . the explicit relationship between data is built by the pair - wise comparison , such as part matching or contrast ##ive loss . for example , contrast ##ive loss directly calculate ##s the euclidean distance between two em ##bed ##ding ##s . in identification models , the input is independent to each other . but there is implicit relationship between the learned em ##bed ##ding ##s built by the cross - entropy loss . the cross - entropy loss can be formulated as . is the weight of the linear function . are the em ##bed ##ding ##s of the two images from the same class . to maximize , , the network converge ##s when and have similar vector direction with . in , similar observation and visual ##ization are shown . so the learned em ##bed ##ding ##s are eventually close for images within the same class and far away for images in the different classes . the relationship is implicit ##ly built between and bridge ##d by the weight . due to the usage of the weak labels , verification models take limited relationships into consideration . on the other hand , classification models do not explicitly consider similarity measurements . fig . [ reference ] ( c ) illustrates how our model works in a batch . we benefit from simultaneously considering the verification and identification losses . the proposed model thus combines the strength of the two models ( see table [ reference ] ) . sub ##section : overall network our network is basically a con ##vo ##lu ##tion ##al siam ##ese network that combines the verification and identification losses . fig . [ reference ] briefly illustrates the architecture of the proposed network . given an input pair of images res ##ized to , the proposed network simultaneously predict ##s the id ##s of the two images and the similarity score . the network consists of two image ##net pre - trained cnn models , three additional con ##vo ##lu ##tion ##al layers , one square layer and three losses . it is supervised by the identification label and the verification label . the pre - trained cnn model can be caf ##fen ##et , v ##gg ##16 or res ##net - 50 , from which we have removed the final fully - connected ( fc ) layer . the re - id performance of the three models is comprehensive ##ly evaluated in section [ reference ] . here , we do not provide detailed descriptions of the architecture of the cnn models and only take caf ##fen ##et as an example in the following sub ##section ##s . the three optimization objectives include two identification losses and one verification loss . we use the final con ##vo ##lu ##tion ##al activation ##s as the disc ##rim ##ina ##tive des ##cript ##or for person re - id , which is directly supervised by three objectives . sub ##section : identification loss there are two caf ##fen ##ets in our architecture . they share weights and predict the two identity labels of the input image pair simultaneously . in order to fine - tune the network on a new data ##set , we replace the final fully - connected layer ( 1 , 000 - dim ) of the pre - trained cnn model with a con ##vo ##lu ##tion ##al layer . the number of the training identities in market - 150 ##1 is 75 ##1 . so this con ##vo ##lu ##tion ##al layer has kernel ##s of size connected to the output of caf ##fen ##et and then we add a soft ##max unit to normal ##ize the output . the size of the result tensor is . the rec ##ti ##fied linear unit ( re ##lu ) is not added after this con ##vo ##lu ##tion . similar to conventional multi - class recognition approaches , we use the cross - entropy loss for identity prediction , which is here denotes the con ##vo ##lu ##tion ##al operation . is a tensor , is the target class and denotes the parameters of the added con ##vo ##lu ##tion ##al layer . is the predicted probability , is the target probability . for all except . sub ##section : verification loss while some previous works contain a matching function in the intermediate layers , our work directly compares the high - level features for similarity estimation . the high - level feature from the fine - tuned cnn has shown a disc ##rim ##ina ##tive ability and it is more compact than the activation ##s in the intermediate layers . so in our model , the pedestrian des ##cript ##or in the identification model are directly supervised by the verification loss . as shown in fig . [ reference ] , we introduce a non - para ##metric layer called square layer to compare the high - level features . it takes two tensor ##s as inputs and outputs one tensor after sub ##tra ##cting and sq ##ua ##ring element - wise ##ly . the square layer is denoted as , where are the 4 , 09 ##6 - dim em ##bed ##ding ##s and is the output tensor of the square layer . we then add a con ##vo ##lu ##tion ##al layer and the soft ##max output function to em ##bed the resulting tensor to a 2 - dim vector ( , ) which represents the predicted probability of the two input images belonging to the same identity . the con ##vo ##lu ##tion ##al layer takes as input and filters it with kernel ##s of size . the re ##lu is not added after this con ##vo ##lu ##tion . we treat pedestrian verification as a binary classification problem and use the cross - entropy loss that is similar to the one in the identification loss , which is here are the two tensor ##s of size . is the target class ( same / different ) , denotes the parameters of the added con ##vo ##lu ##tion ##al layer and is the predicted probability . if the image pair depicts the same person , ; otherwise , . departing from , we do not use the contrast ##ive loss . on the one hand , the contrast ##ive loss , as a regression loss , forces the same - class em ##bed ##ding ##s to be as close as possible . it may make the model over - fitting because the number of training of each identity is limited in person re - id . on the other hand , drop ##out , which introduces zero values at random locations , can not be applied on the em ##bed ##ding before the contrast ##ive loss . but the cross - entropy loss in our model can work with drop ##out to regular ##ize the model . in section [ reference ] , we show that the result using contrast ##ive loss is 4 . 39 % and 6 . 55 % lower than the one using the cross - entropy loss on rank - 1 accuracy and map respectively . sub ##section : identification vs . verification the proposed network is trained to minimize the three cross - entropy losses jointly . to figure out which objective contributes more , we train the identification model and verification model separately . following the learning rate setting in section [ reference ] , we train the models until convergence . we also train the network with the two losses jointly until two objectives both converge . as the quantitative results shown in table [ reference ] , the fine - tuned cnn model with two kinds of losses out ##per ##forms the one trained individually . this result has been confirmed on the three different network structures . further , we visual ##ize the intermediate feature maps that are trained using res ##net - 50 as the pre ##train ##ed model and try to find the differences between identification loss and verification loss . we select three test images in the market ##15 ##01 . one image is considered to be well detected and the other two images are not well aligned . given one image as input , we get its activation in the intermediate layer \" res ##4 ##f ##x \" , the size of which is . we visual ##ize the sum of several activation maps . as shown in fig . [ reference ] , the identification and the verification networks exhibit different activation patterns to the pedestrian . we find that if we use only one kind of loss , the network tends to find one disc ##rim ##ina ##tive part . the proposed model takes advantages of both networks , so the new activation map is mostly a union of the two individual maps . this also illustrates the complementary nature of the two baseline networks . the proposed model makes more neurons activated . moreover , as shown in fig . [ reference ] we visual ##ize the em ##bed ##ding by plot them to the 2 - dimension map . in regard to fig . [ reference ] , we find the network usually has strong attention on the center part of the human ( usually clothes ) and it also illustrates the color of the clothes is the major clue for the person re - identification . sub ##section : training and optimization input preparation . we res ##ize all the training images to . the mean image computed from all the training images is sub ##tracted from all the images . during training , all the images are randomly crop ##ped to for caf ##fen ##et and mirrored horizontally . for res ##net - 50 and v ##gg ##16 , we randomly crop images to . we shuffle the data ##set and use a random order of the images . then we sample another image from the same / different class to compose a positive / negative pair . the initial ratio between negative pairs and positive pairs is to alleviate the prediction bias and we multiple it by a factor of every epoch until it reaches , since the number of positive pairs is so limited that the network risks over - fitting . training . we use the mat ##con ##vn ##et package for training and testing the em ##bed ##ding with caf ##fen ##et , v ##gg ##16 and res ##net - 50 , respectively . the maximum number of training epoch ##s is set to 75 for res ##net - 50 , 65 for v ##gg ##16 ##net and 155 for caf ##fen ##et . the batch size ( in image pairs ) is set to 128 for caf ##fen ##et , 48 for v ##gg ##16 and res ##net - 50 . the learning rate is initial ##ized as 0 . 001 and then set to 0 . 000 ##1 for the final 5 epoch ##s . we adopt the mini - batch st ##och ##astic gradient descent ( sg ##d ) to update the parameters of the network . there are three objectives in our network . therefore , we first compute all the gradient ##s produced by every objectives respectively and add the weighted gradient ##s together to update the network . we assign a weight of 1 to the gradient produced by the verification loss and 0 . 5 for the two gradient ##s produced by two identification losses . moreover , we insert the drop ##out function before the final con ##vo ##lu ##tion ##al layer . testing . we adopt an efficient method to extract features as well as the activation in the intermediate layer . because two caf ##fen ##et share weights , our model has nearly the same memory consumption with the pre ##train ##ed model . so we extract features by only act ##ivating one fine - tuned model . given a image , we feed forward the image to one caf ##fen ##et in our network and obtain a 4 , 09 ##6 - dim pedestrian des ##cript ##or . once the des ##cript ##ors for the gallery sets are obtained , they are stored off ##line . given a query image , its des ##cript ##or is extracted online . we sort the co ##sin ##e distance between the query and all the gallery features to obtain the final ranking result . note that the co ##sin ##e distance is equivalent to euclidean distance when the feature is l ##2 - normal ##ized . section : experiments we mainly verify the proposed model on two large - scale data ##set ##s market ##15 ##01 and cu ##h ##k ##0 ##3 . we report the results trained by three network structures . besides , we also report the result on market ##15 ##01 + 500 ##k data ##set . meanwhile , the proposed architecture is also applied on the image retrieval task . we modify our model and test it on a popular image retrieval data ##set , i . e . , oxford buildings . the performance is comparable to the state of the art . sub ##section : data ##set market ##15 ##01 contains 32 , 66 ##8 ann ##ota ##ted bound ##ing boxes of 1 , 501 identities . images of each identity are captured by at most six cameras . according to the data ##set setting , the training set contains 12 , 93 ##6 crop ##ped images of 75 ##1 identities and testing set contains 19 , 73 ##2 crop ##ped images of 750 identities and distract ##ors . they are directly detected by the def ##or ##mable part model ( d ##pm ) instead of using hand - drawn bb ##ox ##es , which is closer to the realistic setting . for each query , we aim to retrieve the ground truth images from the 19 , 73 ##2 candidate images . the searching pool ( gallery ) is important to person re - identification . in the realistic setting , the scale of the gallery is usually large . the distract ##or data ##set of market ##15 ##01 provides extra 500 , 000 bb ##ox ##es , consisting of false alarms on the background as well as the persons not belonging to any of the original 1 , 501 identities . when testing , we add the 500 ##k images to the original gallery , which makes the retrieval more difficult . cu ##h ##k ##0 ##3 data ##set contains 14 , 09 ##7 crop ##ped images of 1 , 46 ##7 identities collected in the cu ##h ##k campus . each identity is observed by two camera views and has 4 . 8 images in average for each view . the author provides two kinds of bound ##ing boxes . we evaluate our model on the bound ##ing boxes detected by d ##pm , which is closer to the realistic setting . following the setting of the data ##set , the data ##set is partition ##ed into a training set of 1 , 36 ##7 persons and a testing set of 100 persons . the experiment is repeated with 20 random splits . both the single - shot and multiple - shot results will be reported . oxford ##5 ##k buildings consists of 50 ##6 ##2 images collected from the internet and corresponding to particular oxford landmarks . some images have complex structures and may contain other buildings . the images corresponding to 11 oxford landmarks are manually ann ##ota ##ted and a set of 55 que ##ries for 11 different landmarks are provided . this bench ##mark contains many high - resolution images and the mean image size of this data ##set is . we use the rank - 1 accuracy and mean average precision ( map ) for performance evaluation on market ##15 ##01 ( + 100 ##k ) and cu ##h ##k ##0 ##3 , while on oxford , we use map . sub ##section : person re - i d evaluation comparison with the cnn baseline . we train the baseline networks according the conventional fine - tuning method . the baseline networks are pre ##train ##ed on image ##net and fine - tuned to predict the person identities . as shown in tab . [ reference ] , we obtain 50 . 89 % , 65 . 02 % and 73 . 69 % rank - 1 accuracy by caf ##fen ##et , v ##gg ##16 and res ##net - 50 , respectively on market ##15 ##01 . note that using the baseline alone exceeds many previous works . our model further improves these baseline ##s on market ##15 ##01 . the improvement can be observed on three network architecture ##s . to be specific , we obtain 11 . 25 % , 5 . 14 % and 5 . 82 % improvement , respectively , using caf ##fen ##et , v ##gg ##16 and res ##net - 50 on market ##15 ##01 . similarly , we observe 35 . 8 % , 49 . 1 % and 71 . 5 % baseline rank - 1 accuracy on cu ##h ##k ##0 ##3 in single - shot setting . as show in tab . [ reference ] , these baseline results exceed some previous works as well . we further get 14 . 0 % , 22 . 7 % and 11 . 9 % improvement on the baseline by our method . these results show that our method can work with different networks and improve their results . it indicates that the proposed model helps the network to learn more disc ##rim ##ina ##tive features . cross - entropy vs . contrast ##ive loss . we replace the cross - entropy loss with the contrast ##ive loss as used in \" deep ##id network \" . however , we find a 4 . 39 % and 6 . 55 % drop in rank - 1 and map . the res ##net - 50 model using the contrast ##ive loss has 75 . 12 % rank - 1 accuracy and 53 . 32 % map . we spec ##ulate that the contrast ##ive loss tends to over - fit on the re - id data ##set because no regular ##ization is added to the verification . cross - entropy loss designed in our model can work with the drop ##out function and avoid the over - fitting . comparison with the state of the art . as shown in table [ reference ] , we compare our method with other state - of - the - art algorithms in terms of mean average precision ( map ) and rank - 1 accuracy on market ##15 ##01 . we report the single - query as well as multiple - query evaluation results . our model ( caf ##fen ##et ) achieve ##s 62 . 14 % rank - 1 accuracy and 39 . 61 % map , which is comparable to the state of the art 65 . 88 % rank - 1 accuracy and 39 . 55 % map . our model using res ##net - 50 produces the best performance 79 . 51 % in rank - 1 accuracy and 59 . 87 % in map , which out ##per ##forms other state - of - the - art algorithms . for cu ##h ##k ##0 ##3 , we evaluate our method in the single - shot setting as shown in tab . [ reference ] . there is only one right image in the searching pool . in the evaluation , we randomly select 100 images from 100 identities under the other camera as gallery . the proposed model yields 83 . 4 % rank - 1 and 86 . 4 % map and out ##per ##forms the state - of - the - art performance . as shown in tab . [ reference ] , we also report the results in the multi - shot setting , which uses all the images from the other camera as gallery and the number of the gallery images is about 500 . we think this setting is much closer to image retrieval and alleviate the unstable effect caused by the random searching pool under single - shot settings . fig . [ reference ] presents some re - id samples on cu ##h ##k ##0 ##3 data ##set . the images in the first column are the query images . the retrieval images are sorted according to the similarity scores from left to right . most ground - truth candidate images are correctly retrieved . although the model retrieve ##s some incorrect candidates on the third row , we find it is a reasonable prediction since the man with red hat and blue coat is similar to the query . the proposed model yields 88 . 3 % rank - 1 and 85 . 0 % map and also out ##per ##forms the state - of - the - art performance in the multi - shot setting . results between camera pairs . cu ##h ##k ##0 ##3 only contains two camera views . so this experiment is evaluated on market ##15 ##01 since it contains six different cameras . we provide the re - identification results between all camera pairs in fig . [ reference ] . although camera - 6 is a low - resolution camera and captures distinct background with the other hd cameras , the re - id accuracy between camera 6 and the others is relatively high . we also compute the cross - camera average map and average rank - 1 accuracy : 48 . 42 % and 54 . 42 % respectively . comparing to the previous reported results , i . e . , 10 . 51 % and 13 . 72 % in , our method largely improves the performance and observes a smaller standard deviation between cameras . it suggests that the disc ##rim ##ina ##tively learned em ##bed ##ding works under different viewpoint ##s . further , fig . [ reference ] shows the barnes - hut t - s ##ne visual ##ization on the learned em ##bed ##ding ##s of our model . by the cluster ##ing algorithm , the persons wearing the similar - color clothes are quit clustered together and are apart from other persons . the learned pedestrian des ##cript ##or pay more attention to the color and it is robust to some illusion and viewpoint variations . in realistic setting , we think color provides the most important information to figure out the person . large - scale experiments . the market ##15 ##01 data ##set also provides an additional distract ##or set with 500 ##k images to en ##lar ##ge the gallery . in general , more candidate images may confuse the image retrieval . the re - id performance of our model ( res ##net ) on the large - scale data ##set is presented in tab . [ reference ] . as the searching pool gets larger , the accuracy drops . with the gallery size of , we still achieve 68 . 26 % rank ##1 accuracy and 45 . 24 % map . a relative drop 24 . 4 % from 59 . 87 % to 45 . 24 % on map is observed , compared to a relative drop 37 . 88 % from 13 . 94 % to 8 . 66 % in our previous work . besides , we also compare our result with the performance of the res ##net baseline . as shown in fig . [ reference ] , it is interesting that the re - id precision of our model decreases more quickly comparing to the baseline model . we spec ##ulate that the market ##15 ##01 training set is relatively small in covering the pedestrian variations encountered in a much larger test set . in fact , the 500 ##k data ##set was collected in a different time ( the same location ) with the market ##15 ##01 data ##set , so the transfer effect is large enough that the learned em ##bed ##ding is inferior to the baseline on the scale of 500 k images . in the future , we will look into this interesting problem and design more robust des ##cript ##ors for the transfer data ##set . sub ##section : instance retrieval we apply the identification - verification model to the generic image retrieval task . oxford ##5 ##k is a testing data ##set containing buildings in the oxford university . we train the network on another scene data ##set proposed in , which comprises of a number of buildings without overlapping with the oxford ##5 ##k . similarly , the model is trained to not only tell which building the image depicts but also determine whether the two input images are from the same architecture . the training data is high - resolution . in order to obtain more information from the high - resolution building images , we modify the final pool ##ing layer of our model to a mac layer , which outputs the maximum value over the whole activation map . this layer helps us to handle large images without res ##izing them to a fixed size and output a fixed - dimension feature to retrieve the images . during training , the input image is randomly crop ##ped to from and mirrored horizontally . during testing , we keep the original size of the images that are not crop ##ped or res ##ized and extract the feature . in table [ reference ] , many previous works are based on caf ##fen ##et or v ##gg ##16 . for fair comparison , we report the baseline results and the results of our model based on these two network structures , respectively . our model which uses caf ##fen ##et as pre ##train ##ed model out ##per ##forms the state of the art . meanwhile , the model using v ##gg ##16 is comparable to the state - of - the - arts methods . the proposed method show a 6 . 0 % and 6 . 6 % improvement over the baseline networks caf ##fen ##et and v ##gg ##16 , respectively . we visual ##ize some retrieval results in fig . [ reference ] . the images in the first column are the query images . the retrieval images are sorted according to the similarity scores from left to right . the main difficulty in the image retrieval is various object sizes in the image . in the first row , we use the roof ( part of the building ) to retrieve the images and the top five images are correct candidate images . the other retrieval samples also show our model is robust to the scale variations . section : conclusion in this work , we propose a siam ##ese network that simultaneously considers the identification loss and the verification loss . the proposed model learns a disc ##rim ##ina ##tive em ##bed ##ding and a similarity measurement at the same time . it out ##per ##forms the state of the art on two popular person re - id bench ##marks and shows potential ability to apply on the generic instance retrieval task . future work includes exploring more novel applications of the proposed method , such as car recognition and fine - grain ##ed classification . besides , we will investigate how to learn a robust des ##cript ##or to further improve the performance of the person re - identification on large - scale testing set . bibliography : references",
        "pred_seq": "[SEP] disc ##ding [SEP] [SEP] person identification [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "discriminatively learned cnn embedding"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "person reidentification"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "market1501"
                    ]
                ],
                "Method": [
                    [
                        "discriminatively learned cnn embedding"
                    ]
                ],
                "Metric": [
                    [
                        "map",
                        "mean average precision"
                    ]
                ],
                "Task": [
                    [
                        "person reidentification",
                        "reid",
                        "identification loss",
                        "identification",
                        "person rei d evaluation",
                        "reidentification"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "market1501"
                    ]
                ],
                "Method": [
                    [
                        "discriminatively learned cnn embedding"
                    ]
                ],
                "Metric": [
                    [
                        "rank1 accuracy",
                        "rank1",
                        "rank1 accuracy"
                    ]
                ],
                "Task": [
                    [
                        "person reidentification",
                        "reid",
                        "identification loss",
                        "identification",
                        "person rei d evaluation",
                        "reidentification"
                    ]
                ]
            }
        ]
    },
    "26": {
        "doctext": "document : learning to adapt structured output space for semantic segment ##ation con ##vo ##lu ##tion ##al neural network - based approaches for semantic segment ##ation rely on supervision with pixel - level ground truth , but may not general ##ize well to unseen image domains . as the labeling process is ted ##ious and labor intensive , developing algorithms that can adapt source ground truth labels to the target domain is of great interest . in this paper , we propose an ad ##vers ##aria ##l learning method for domain adaptation in the context of semantic segment ##ation . considering semantic segment ##ations as structured outputs that contain spatial similarities between the source and target domains , we adopt ad ##vers ##aria ##l learning in the output space . to further enhance the adapted model , we construct a multi - level ad ##vers ##aria ##l network to effectively perform output space domain adaptation at different feature levels . extensive experiments and ab ##lation study are conducted under various domain adaptation settings , including synthetic - to - real and cross - city scenarios . we show that the proposed method performs favorably against the state - of - the - art methods in terms of accuracy and visual quality . section : introduction semantic segment ##ation aims to assign each pixel a semantic label , e . g . , person , car , road or tree , in an image . recently , methods based on con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) have achieved significant progress in semantic segment ##ation with applications for autonomous driving and image editing . the cr ##ux of cnn - based approaches is to ann ##ota ##te a large number of images that cover possible scene variations . however , this trained model may not general ##ize well to unseen images , especially when there is a domain gap between the training ( source ) and test ( target ) images . for instance , the distribution of appearance for objects and scenes may vary in different cities , and even weather and lighting conditions can change significantly in the same city . in such cases , relying only on the supervised model that requires re - ann ##ota ##ting per - pixel ground truths in different scenarios would en ##tail prohibit ##ively high labor cost . to address this issue , knowledge transfer or domain adaptation techniques have been proposed to close the gap between source and target domains , where ann ##ota ##tions are not available in the target domain . for image classification , one effective approach is to align features across two domains such that the adapted features can general ##ize to both domains . similar efforts have been made for semantic segment ##ation via ad ##vers ##aria ##l learning in the feature space . however , different from the image classification task , feature adaptation for semantic segment ##ation may suffer from the complexity of high - dimensional features that needs to en ##code diverse visual cues , including appearance , shape and context . this mo ##tiv ##ates us to develop an effective method for adapting pixel - level prediction tasks rather than using feature adaptation . in semantic segment ##ation , we note that the output space contains rich information , both spatial ##ly and locally . for instance , even if images from two domains are very different in appearance , their segment ##ation outputs share a significant amount of similarities , e . g . , spatial layout and local context ( see figure [ reference ] ) . based on this observation , we address the pixel - level domain adaptation problem in the output ( segment ##ation ) space . in this paper , we propose an end - to - end cnn - based domain adaptation algorithm for semantic segment ##ation . our formulation is based on ad ##vers ##aria ##l learning in the output space , where the intuition is to directly make the predicted label distributions close to each other across source and target domains . based on the genera ##tive ad ##vers ##aria ##l network ( gan ) , the proposed model consists of two parts : 1 ) a segment ##ation model to predict output results , and 2 ) a disc ##rim ##inator to distinguish whether the input is from the source or target segment ##ation output . with an ad ##vers ##aria ##l loss , the proposed segment ##ation model aims to fool the disc ##rim ##inator , with the goal of generating similar distributions in the output space for either source or target images . the proposed method also adapt ##s features as the errors are back - prop ##aga ##ted to the feature level from the output labels . however , one concern is that lower - level features may not be adapted well as they are far away from the high - level output labels . to address this issue , we develop a multi - level strategy by incorporating ad ##vers ##aria ##l learning at different feature levels of the segment ##ation model . for instance , we can use both con ##v ##5 and con ##v ##4 features to predict segment ##ation results in the output space . then two disc ##rim ##inator ##s can be connected to each of the predicted output for multi - level ad ##vers ##aria ##l learning . we perform one - stage end - to - end training for the segment ##ation model and disc ##rim ##inator ##s jointly , without using any prior knowledge of the data in the target domain . in the testing phase , we can simply disc ##ard disc ##rim ##inator ##s and use the adapted segment ##ation model on target images , with no extra computational requirements . due to the high labor cost of ann ##ota ##ting segment ##ation ground truth , there has been great interest in large - scale synthetic data ##set ##s with ann ##ota ##tions , e . g . , gt ##a ##5 and synth ##ia . as a result , one critical setting is to adapt the model trained on synthetic data to real - world data ##set ##s , such as city ##sca ##pes . we follow this setting and conduct extensive experiments to valid ##ate the proposed domain adaptation method . first , we use a strong baseline model that is able to general ##ize to different domains . we note that a strong baseline facilitates real - world applications and can evaluate the limitation of the proposed adaptation approach . based on this baseline model , we show comparisons using ad ##vers ##aria ##l adaptation in the feature and output spaces . furthermore , we show that the multi - level ad ##vers ##aria ##l learning improves the results over single - level adaptation . in addition to the synthetic - to - real setting , we show experimental results on the cross - city data ##set , where ann ##ota ##tions are provided in one city ( source ) , while testing the model on another unseen city ( target ) . overall , our method performs favorably against state - of - the - art algorithms on numerous bench ##mark data ##set ##s under different settings . the contributions of this work are as follows . first , we propose a domain adaptation method for pixel - level semantic segment ##ation via ad ##vers ##aria ##l learning . second , we demonstrate that adaptation in the output ( segment ##ation ) space can effectively align scene layout and local context between source and target images . third , a multi - level ad ##vers ##aria ##l learning scheme is developed to adapt features at different levels of the segment ##ation model , which leads to improved performance . section : related work semantic segment ##ation . state - of - the - art semantic segment ##ation methods are mainly based on the recent advances of deep neural networks . as proposed by long , one can transform a classification cnn ( e . g . , alex ##net , v ##gg , or res ##net ) to a fully - con ##vo ##lu ##tion ##al network ( fc ##n ) for semantic segment ##ation . numerous methods have since been developed to improve this model by utilizing context information or en ##lar ##ging rec ##eptive fields . to train these advanced networks , a substantial amount of dense pixel ann ##ota ##tions must be collected in order to match the model capacity of deep cnn ##s . as a result , weakly and semi - supervised approaches are proposed in recent years to reduce the heavy labeling cost of collecting segment ##ation ground truths . however , in most real - world applications , it is difficult to obtain weak ann ##ota ##tions and the trained model may not general ##ize well to unseen image domains . another approach to tackle the ann ##ota ##tion problem is to construct synthetic data ##set ##s based on rendering , e . g . , gt ##a ##5 and synth ##ia . while the data collection is less costly since the pixel - level ann ##ota ##tion can be done with a partially automated process , these data ##set ##s are usually used in conjunction with real - world data ##set ##s for joint learning to improve the performance . however , when training solely on the synthetic data ##set , the model does not general ##ize well to real - world data , mainly due to the large domain shift between synthetic images and real - world images , i . e . , appearance differences are still significant with current rendering techniques . although synth ##es ##izing more realistic images can decrease the domain shift , it is necessary to use domain adaptation to narrow the performance gap . domain adaptation . domain adaptation methods for image classification have been developed to address the domain - shift problem between the source and target domains . numerous methods are developed based on cnn class ##ifiers due to performance gain . the main insight behind these approaches is to tackle the problem by align ##ing the feature distribution between source and target images . gan ##in propose the domain - ad ##vers ##aria ##l neural network ( dan ##n ) to transfer the feature distribution . a number of variants have since been proposed with different loss functions or class ##ifiers . recently , the pixel ##da method addresses domain adaptation for image classification by transferring the source images to target domain , thereby obtaining a simulated training set for target images . we note that domain adaptation for pixel - level prediction tasks have not been explored widely . hoffman introduce the task of domain adaptation on semantic segment ##ation by applying ad ##vers ##aria ##l learning ( i . e . , dan ##n ) in a fully - con ##vo ##lu ##tion ##al way on feature representations and additional category constraints similar to the constrained cnn . other methods focus on adapting synthetic - to - real or cross - city images by adopting class - wise ad ##vers ##aria ##l learning or label transfer . similar to the pixel ##da method , one concurrent work , cy ##ca ##da uses the cycle ##gan and transfers source domain images to the target domain with pixel alignment , thus generating extra training data combined with feature space ad ##vers ##aria ##l learning . although feature space adaptation has been successfully applied to image classification , pixel - level tasks such as semantic segment ##ation remains challenging based on feature adaptation - based approaches . in this paper , we use the property that pixel - level predictions are structured outputs that contain information spatial ##ly and locally , to propose an efficient domain adaptation algorithm through ad ##vers ##aria ##l learning in the output space . section : algorithm ##ic overview sub ##section : overview of the proposed model our domain adaptation algorithm consists of two modules : a segment ##ation network and the disc ##rim ##inator , where indicates the level of a disc ##rim ##inator in the multi - level ad ##vers ##aria ##l learning . two sets of images from source and target domains are denoted as and . we first forward the source image ( with ann ##ota ##tions ) to the segment ##ation network for opt ##imi ##zing . then we predict the segment ##ation soft ##max output for the target image ( without ann ##ota ##tions ) . since our goal is to make segment ##ation predictions of source and target images ( i . e . , and ) close to each other , we use these two predictions as the input to the disc ##rim ##inator to distinguish whether the input is from the source or target domain . with an ad ##vers ##aria ##l loss on the target prediction , the network prop ##aga ##tes gradient ##s from to , which would encourage to generate similar segment ##ation distributions in the target domain to the source prediction . figure [ reference ] shows the overview of the proposed algorithm . sub ##section : objective function for domain adaptation with the proposed network , we formula ##te the adaptation task containing two loss functions from both modules : where is the cross - entropy loss using ground truth ann ##ota ##tions in the source domain , and is the ad ##vers ##aria ##l loss that adapt ##s predicted segment ##ations of target images to the distribution of source predictions ( see section [ reference ] ) . in ( [ reference ] ) , is the weight used to balance the two losses . section : output space adaptation different from image classification based on features that describe the global visual information of the image , high - dimensional features learned for semantic segment ##ation en ##codes complex representations . as a result , adaptation in the feature space may not be the best choice for semantic segment ##ation . on the other hand , although segment ##ation outputs are in the low - dimensional space , they contain rich information , e . g . , scene layout and context . our intuition is that no matter images are from the source or target domain , their segment ##ations should share strong similarities , spatial ##ly and locally . thus , we utilize this property to adapt low - dimensional soft ##max outputs of segment ##ation predictions via an ad ##vers ##aria ##l learning scheme . sub ##section : single - level ad ##vers ##aria ##l learning disc ##rim ##inator training . before introducing how to adapt the segment ##ation network via ad ##vers ##aria ##l learning , we first describe the training objective for the disc ##rim ##inator . given the segment ##ation soft ##max output , where is the number of categories , we forward to a fully - con ##vo ##lu ##tion ##al disc ##rim ##inator using a cross - entropy loss for the two classes ( i . e . , source and target ) . the loss can be written as : where if the sample is drawn from the target domain , and for the sample from the source domain . segment ##ation network training . first , we define the segment ##ation loss in ( [ reference ] ) as the cross - entropy loss for images from the source domain : where is the ground truth ann ##ota ##tions for source images and is the segment ##ation output . second , for images in the target domain , we forward them to and obtain the prediction . to make the distribution of closer to , we use an ad ##vers ##aria ##l loss in ( [ reference ] ) as : this loss is designed to train the segment ##ation network and fool the disc ##rim ##inator by maxim ##izing the probability of the target prediction being considered as the source prediction . sub ##section : multi - level ad ##vers ##aria ##l learning although performing ad ##vers ##aria ##l learning in the output space directly adapt ##s predictions , low - level features may not be adapted well as they are far away from the output . similar to the deep supervision method that uses auxiliary loss for semantic segment ##ation , we incorporate additional ad ##vers ##aria ##l module in the low - level feature space to enhance the adaptation . the training objective for the segment ##ation network can be extended from ( [ reference ] ) as : where indicates the level used for predicting the segment ##ation output . we note that , the segment ##ation output is still predicted in each feature space , before passing through individual disc ##rim ##inator ##s for ad ##vers ##aria ##l learning . hence , and remain in the same form as in ( [ reference ] ) and ( [ reference ] ) , respectively . based on ( [ reference ] ) , we opt ##imi ##ze the following min - max criterion : the ultimate goal is to minimize the segment ##ation loss in for source images , while maxim ##izing the probability of target predictions being considered as source predictions . road sidewalk building wall fence pole light sign ve ##g terrain sky person rider car truck bus train mb ##ike bike section : network architecture and training disc ##rim ##inator . for the disc ##rim ##inator , we use an architecture similar to but utilize all fully - con ##vo ##lu ##tion ##al layers to retain the spatial information . the network consists of 5 con ##vo ##lu ##tion layers with kernel and stride of 2 , where the channel number is { 64 , 128 , 256 , 512 , 1 } , respectively . except for the last layer , each con ##vo ##lu ##tion layer is followed by a leak ##y re ##lu parameter ##ized by . an up - sampling layer is added to the last con ##vo ##lu ##tion layer for re - scaling the output to the size of the input . we do not use any batch - normal ##ization layers as we jointly train the disc ##rim ##inator with the segment ##ation network using a small batch size . segment ##ation network . it is essential to build upon a good baseline model to achieve high - quality segment ##ation results . we adopt the deep ##lab - v ##2 framework with res ##net - 101 model pre - trained on image ##net as our segment ##ation baseline network . however , we do not use the multi - scale fusion strategy due to the memory issue . similar to the recent work on semantic segment ##ation , we remove the last classification layer and modify the stride of the last two con ##vo ##lu ##tion layers from 2 to 1 , making the resolution of the output feature maps effectively times the input image size . to en ##lar ##ge the rec ##eptive field , we apply dil ##ated con ##vo ##lu ##tion layers in con ##v ##4 and con ##v ##5 layers with a stride of 2 and 4 , respectively . after the last layer , we use the at ##rous spatial pyramid pool ##ing ( as ##pp ) as the final class ##ifier . finally , we apply an up - sampling layer along with the soft ##max output to match the size of the input image . based on this architecture , our segment ##ation model achieve ##s 65 . 1 % mean intersection - over - union ( io ##u ) when trained on the city ##sca ##pes training set and tested on the city ##sca ##pes validation set . multi - level adaptation model . we construct the above - mentioned disc ##rim ##inator and segment ##ation network as our single - level adaptation model . for the multi - level structure , we extract feature maps from the con ##v ##4 layer and add an as ##pp module as the auxiliary class ##ifier . similarly , a disc ##rim ##inator with the same architecture is added for ad ##vers ##aria ##l learning . figure [ reference ] shows the proposed multi - level adaptation model . in this paper , we use two levels due to the balance of its efficiency and accuracy . network training . to train the proposed single / multi - level adaptation model , we find that jointly training the segment ##ation network and disc ##rim ##inator ##s in one stage is effective . in each training batch , we first forward the source image to opt ##imi ##ze the segment ##ation network for in ( [ reference ] ) and generate the output . for the target image , we obtain the segment ##ation output , and pass it along with to the disc ##rim ##inator for opt ##imi ##zing in ( [ reference ] ) . in addition , we compute the ad ##vers ##aria ##l loss in ( [ reference ] ) for the target prediction . for the multi - level training objective in ( [ reference ] ) , we simply repeat the same procedure for each adaptation module . we implement our network using the p ##yt ##or ##ch tool ##box on a single titan x gp ##u with 12 gb memory . to train the segment ##ation network , we use the st ##och ##astic gradient descent ( sg ##d ) opt ##imi ##zer with nest ##ero ##v acceleration where the momentum is 0 . 9 and the weight decay is . the initial learning rate is set as and is decreased using the polynomial decay with power of 0 . 9 as mentioned in . for training the disc ##rim ##inator , we use the adam opt ##imi ##zer with the learning rate as and the same polynomial decay as the segment ##ation network . the momentum is set as 0 . 9 and 0 . 99 . section : experimental results in this section , we present experimental results to valid ##ate the proposed domain adaptation method for semantic segment ##ation under different settings . first , we show evaluation ##s of the model trained on synthetic data ##set ##s ( i . e . , gt ##a ##5 and synth ##ia ) and test the adapted model on real - world images from the city ##sca ##pes data ##set . extensive experiments including comparisons to the state - of - the - art methods and ab ##lation study are also conducted , e . g . , adaptation in the feature / output spaces and single / multi - level ad ##vers ##aria ##l learning . second , we carry out experiments on the cross - city data ##set , where the model is trained on one city and adapted to another city without using ann ##ota ##tions . in all the experiments , the io ##u metric is used . the code and model are available at . sub ##section : gt ##a ##5 the gt ##a ##5 data ##set consists of images with the resolution of synthesized from the video game based on the city of los angeles . the ground truth ann ##ota ##tions are compatible with the city ##sca ##pes data ##set that contains 19 categories . following , we use the full set of gt ##a ##5 and adapt the model to the city ##sca ##pes training set with 297 ##5 images . during testing , we evaluate on the city ##sca ##pes validation set with 500 images . overall results . we present adaptation results in table [ reference ] with comparisons to the state - of - the - art domain adaptation methods . for these approaches , the baseline model is trained using v ##gg - based architecture ##s . to fairly evaluate our method , we first use the same baseline architecture ( v ##gg - 16 ) and train our model with the proposed single - level adaptation module . table [ reference ] shows that our method performs favorably against the other algorithms . while these methods all have feature adaptation modules , our results show that adapting the model in the output space achieve ##s better performance . we note that cy ##ca ##da has a pixel adaptation module by transforming source domain images to the target domain and hence obtain ##s additional training samples . although this strategy achieve ##s a similar performance as ours , one can always apply pixel transformation combined with our output space adaptation to improve the results . on the other hand , we argue that utilizing a stronger baseline model is critical for understanding the importance of different adaptation components as well as for enhancing the performance to enable real - world applications . thus , we use the res ##net - 101 based network introduced in section [ reference ] and train the proposed adaptation model . table [ reference ] shows the baseline results only trained on source images without adaptation , with comparisons to our adapted models under different settings , including feature adaptation and single / multi - level ad ##vers ##aria ##l learning in the output space . figure [ reference ] presents some example results for adapted segment ##ation . we note that for small objects such as poles and traffic signs , they are harder to adapt since they easily get merged with background classes . in addition , another factor to evaluate the adaptation performance is to measure how much gap is narrowed between the adaptation model and the fully - supervised model . hence , we train the model using ann ##ota ##ted ground truths in the city ##sca ##pes data ##set as the oracle results . table [ reference ] shows the gap under different baseline models . we observe that , although the oracle result does not differ a lot between v ##gg - 16 and res ##net - 101 based models , the gap is larger for the v ##gg one . it suggests us that to narrow the gap , using a deeper model with larger capacity is more practical . parameter analysis . during opt ##imi ##zing the segment ##ation network , it is essential to balance the weight between segment ##ation and ad ##vers ##aria ##l losses . we first consider the single - level case in ( [ reference ] ) and conduct experiments to observe the impact of changing . table [ reference ] shows that a smaller may not facilitate the training process significantly , while a larger may prop ##aga ##te incorrect gradient ##s to the network . we empirical ##ly choose as 0 . 001 in the single - level setting . feature level v . s . output space adaptation . in the single - level setting in ( [ reference ] ) , we compare results by using feature - level or output space adaptation via ad ##vers ##aria ##l learning . for feature - level adaptation , we adopt a similar strategy as used in and train our model accordingly . table [ reference ] shows that the proposed adaptation method in the output space performs better than the one in the feature level . in addition , table [ reference ] shows that adaptation in the feature space is more sensitive to , which causes the training process more difficult , while output space adaptation allows for a wider range of . one reason is that as feature adaptation is performed in the high - dimensional space , the problem for the disc ##rim ##inator becomes easier . thus , such an adapted model can not effectively match distributions between source and target domains via ad ##vers ##aria ##l learning . single - level v . s . multi - level ad ##vers ##aria ##l learning . we have shown the merits of adopting ad ##vers ##aria ##l learning in the output space . in addition , we present the results of using multi - level ad ##vers ##aria ##l learning in table [ reference ] . here , we utilize an additional ad ##vers ##aria ##l module ( see figure [ reference ] ) and jointly opt ##imi ##ze ( [ reference ] ) for two levels . to properly balance and , we use the same weight as in the single - level setting for the high - level output space ( i . e . , = 1 and = 0 . 001 ) . since the low - level output carries less information to predict the segment ##ation , we use smaller weights for both the segment ##ation and ad ##vers ##aria ##l loss ( i . e . , = 0 . 1 and = 0 . 000 ##2 ) . evaluation results show that our multi - level ad ##vers ##aria ##l adaptation further improves the segment ##ation accuracy . more results and analysis are presented in the supplementary material . road sidewalk building light sign ve ##g sky person rider car bus mb ##ike bike sub ##section : synth ##ia to adapt from the synth ##ia to city ##sca ##pes data ##set ##s , we use the synth ##ia - rand - city ##sca ##pes set as the source domain which contains 94 ##00 images compatible with the city ##sca ##pes ann ##ota ##ted classes . similar to , we evaluate images on the city ##sca ##pes validation set with 13 classes . for the weight in ( [ reference ] ) and ( [ reference ] ) , we use the same ones as in the case of gt ##a ##5 data ##set . table [ reference ] shows evaluation results of the proposed algorithm against the state - of - the - art methods that use feature adaptation . similar to the experiments with the gt ##a ##5 data ##set , we first utilize the same v ##gg - based model and train our single - level adaptation model for fair comparisons . the experimental results suggest that adapting the model in the output space performs better . second , we compare results using different components of the proposed method with the res ##net based model . we show that the multi - level adaptation module improves the results over the baseline , feature space adaptation and single - level adaptation models . in addition , we present comparisons of mean io ##u gap between adapted and oracle results in table [ reference ] . our method achieve ##s the smallest gap and is the only one that can minimize the gap below 30 % . road sidewalk building light sign ve ##g sky person rider car bus mb ##ike bike sub ##section : cross - city data ##set in addition to the synthetic - to - real adaptation for a larger domain gap , we conduct experiment on the cross - city data ##set with smaller domain gaps between cities . the data ##set contains four different cities : rio , rome , tokyo and taipei , in which each city has 320 ##0 images without ann ##ota ##tions and 100 images with pixel - level ground truths for 13 classes . similar to , we use the city ##sca ##pes training set as the source domain and adapt it to each target city using 320 ##0 images , while 100 ann ##ota ##ted images are used for evaluation . since a smaller domain gap results in smaller output differences , we use smaller weights for the ad ##vers ##aria ##l loss ( i . e . , ) when training our models , while the weights for segment ##ation remain the same as previous experiments . we show our results in table [ reference ] with comparisons to and our baseline models under different settings . again , our final multi - level model achieve ##s consistent improvement for different cities , which demonstrates the advantages of the proposed adaptation method in the output space . note that the state - of - the - art method uses a different baseline model , and we present it as a reference to analyze how much the proposed algorithm can improve . section : concluding remarks in this paper , we exploit the fact that segment ##ations are structured outputs and share many similarities between source and target domains . we tackle the domain adaptation problem for semantic segment ##ation via ad ##vers ##aria ##l learning in the output space . to further enhance the adapted model , we construct a multi - level ad ##vers ##aria ##l network to effectively perform output space domain adaptation at different feature levels . experimental results show that the proposed method performs favorably against numerous baseline models and the state - of - the - art algorithms . we hope that our proposed method can be a generic adaptation model for a wide range of pixel - level prediction tasks . ac ##k ##now ##led ##gm ##ents . w . - c . hung is supported in part by the ns ##f career grant # 114 ##9 ##7 ##8 ##3 , gifts from adobe and n ##vid ##ia . bibliography : references",
        "pred_seq": "synthetic real [SEP] [SEP] [SEP] semantic ##ation [SEP] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] semantic semantic semantic semantic semantic semantic semantic semantic semantic semantic [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "synthetictoreal"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "semantic segmentation"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "singlelevel adaptation model",
                        "multilevel adaptation model",
                        "single"
                    ]
                ],
                "Metric": [
                    [
                        "mean intersectionoverunion",
                        "iou"
                    ]
                ],
                "Task": [
                    [
                        "synthetictoreal",
                        "synthetictoreal setting",
                        "synthetictoreal adaptation"
                    ]
                ]
            }
        ]
    },
    "27": {
        "doctext": "document : pose - driven deep con ##vo ##lu ##tion ##al model for person re - identification feature extraction and matching are two crucial components in person re - identification ( reid ) . the large pose deformation ##s and the complex view variations exhibited by the captured person images significantly increase the difficulty of learning and matching of the features from person images . to overcome these difficulties , in this work we propose a pose - driven deep con ##vo ##lu ##tion ##al ( pd ##c ) model to learn improved feature extraction and matching models from end to end . our deep architecture explicitly leverage ##s the human part cues to alleviate the pose variations and learn robust feature representations from both the global image and different local parts . to match the features from global human body and local body parts , a pose driven feature weight ##ing sub - network is further designed to learn adaptive feature fusion ##s . extensive experimental analyses and results on three popular data ##set ##s demonstrate significant performance improvements of our model over all published state - of - the - art methods . section : introduction person re - identification ( reid ) is an important component in a video surveillance system . here person reid refers to the process of identifying a probe person from a gallery captured by different cameras , and is generally deployed in the following scenario : given a probe image or video sequence containing a specific person under a certain camera , query ##ing the images , locations , and time stamps of this person from other cameras . despite decades of studies , the person reid problem is still far from being solved . this is mainly because of challenging situations like complex view variations and large pose deformation ##s on the captured person images . most of traditional works try to address these challenges with the following two approaches : ( 1 ) representing the visual appearance of a person using customized local invariant features extracted from images or ( 2 ) learning a disc ##rim ##ina ##tive distance metric to reduce the distance among features of images containing the same person . because the human poses and viewpoint ##s are un ##con ##tro ##lla ##ble in real scenarios , hand - coded features may be not robust enough to pose and viewpoint variations . distance metric is computed for each pair of cameras , making distance metric learning based person reid suffers from the computational complexity . in recent years , deep learning has demonstrated strong model capabilities and obtain ##s very promising performances in many computer vision tasks . meanwhile , the release of person reid data ##set ##s like cu ##h ##k 03 , market - 150 ##1 , and mars , both of which contain many ann ##ota ##ted person images , makes training deep models for person reid feasible . therefore , many researchers attempt to leverage deep models in person reid . most of these methods first learn a pedestrian feature and then compute euclidean distance to measure the similarity between two samples . more specifically , existing deep learning based person reid approaches can be summarized into two categories : 1 ) use soft ##max loss with person id labels to learn a global representation , and 2 ) first learn local representations using pre ##de ##fine ##d rigid body parts , then fuse the local and global representations to depict person images . deep learning based methods have demonstrated significant performance improvements over the traditional methods . although these approaches have achieved remarkable results on mainstream person reid data ##set ##s , most of them do not consider pose variation of human body . because pose variations may significantly change the appearance of a person , considering the human pose cues is potential to help person re - identification . although there are several methods that segment the person images according to the pre ##de ##fine ##d configuration , such simple segment ##ation can not capture the pose cues effectively . some recent works attempt to use pose estimation algorithms to predict human pose and then train deep models for person reid . however , they use manually crop ##ped human body parts and their models are not trained from end to end . therefore , the potential of pose information to boost the reid performance has not been fully explored . to better alleviate the challenges from pose variations , we propose a pose - driven deep con ##vo ##lu ##tion ##al ( pd ##c ) model for person reid . the proposed pd ##c model learns the global representation depicting the whole body and local representations depicting body parts simultaneously . the global representation is learned using the soft ##max loss with person id labels on the whole input image . for the learning of local representations , a novel feature em ##bed ##ding sub - net ( fen ) is proposed to learn and read ##just human parts so that parts are af ##fine transformed and re - located at more reasonable regions which can be easily recognizable through two different cameras . in feature em ##bed ##ding sub - net , each body part region is first automatically crop ##ped . the crop ##ped part regions are hence transformed by a pose transformation network ( pt ##n ) to eliminate the pose variations . the local representations are hence learned on the transformed regions . we further propose a feature weight ##ing sub - net ( f ##wn ) to learn the weights of global representations and local representations on different parts . therefore , more reasonable feature fusion is conducted to facilitate feature similarity measurement . some more detailed descriptions to our local representation generation are illustrated in fig . [ reference ] . our method first locate ##s the key body joints from the input image , , illustrated in fig . [ reference ] ( c ) . from the detected joints , six body parts are extracted , , shown in fig . [ reference ] ( d ) . as shown in fig . [ reference ] ( e ) , those parts are extracted and normal ##ized into fixed sizes and orientation ##s . finally , they are fed into the pose transformation network ( pt ##n ) to further eliminate the pose variations . with the normal ##ized and transformed part regions , , fig . [ reference ] ( f ) , local representations are learned by training the deep neural network . different parts commonly convey different levels of disc ##rim ##ina ##tive cues to identify the person . we thus further learn weights for representations on different parts with a sub - network . most of current deep learning based person reid works do not consider the human pose cues and the weights of representation on different parts . this paper proposes a novel deep architecture that transforms body parts into normal ##ized and homo ##log ##ous feature representations to better overcome the pose variations . moreover , a sub - network is proposed to automatically learn weights for different parts to facilitate feature similarity measurement . both the representation and weight ##ing are learned jointly from end to end . since pose estimation is not the focus of this paper , the used pose estimation algorithm , , fully con ##vo ##lu ##tion ##al networks ( fc ##n ) based pose estimation method is simple and trained independently . once the fc ##n is trained , it is incorporated in our framework , which is hence trained in an end - to - end manner , , using images as inputs and person id labels as outputs . experimental results on three popular data ##set ##s show that our algorithm significantly out ##per ##forms many state - of - the - art ones . section : related work traditional algorithms perform person re - identification through two ways : ( a ) acquiring robust local features visually representing a person ' s appearance and then encoding them ; ( b ) closing the gap between a person ' s different features by learning a disc ##rim ##ina ##tive distance metric . some recent works have started to apply deep learning in person reid and achieved promising performance . in the following , we briefly review recent deep learning based person reid methods . deep learning is commonly used to either learn a person ' s representation or the distance metric . when handling a pair of person images , existing deep learning methods usually learn feature representations of each person by using a deep matching function from con ##vo ##lu ##tion ##al features or from the fully connected ( fc ) features . apart from deep metric learning methods , some algorithms first learn image representations directly with the triple ##t loss or the siam ##ese contrast ##ive loss , then utilize euclidean distance for comparison . wang use a joint learning framework to un ##ify single - image representation and cross - image representation using a double ##t or triple ##t cnn . shi propose a moderate positive mining method to use deep distance metric learning for person reid . another novel method learns deep attributes feature for reid with semi - supervised learning . xiao train one network with several person reid data ##set ##s using a domain guided drop ##out algorithm . pre ##de ##fine ##d rigid body parts are also used by many deep learning based methods for the purpose of learning local pedestrian features . different from these algorithms , our work and the ones in use more accurate human pose estimation algorithms to acquire human pose features . however , due to the limited accuracy of pose estimation algorithms as well as reasons like o ##cc ##lusion and lighting change , pose estimation might be not accurate enough . moreover , different parts convey different levels of disc ##rim ##ina ##tive cues . therefore , we normal ##ize the part regions to get more robust feature representation using feature em ##bed ##ding sub - net ( fen ) and propose a feature weight ##ing sub - net ( f ##wn ) to learn the weight for each part feature . in this way , the part with high disc ##rim ##ina ##tive power can be identified and emphasized . this also makes our work different from existing ones , which do not consider the ina ##cc ##ura ##cy of human poses estimation and weight ##ing on different parts features . section : pose - driven deep reid model in this section , we describe the overall framework of the proposed approach , where we mainly introduce the feature em ##bed ##ding sub - net ( fen ) and the feature weight ##ing sub - net ( f ##wn ) . details about the training and test procedures of the proposed approach will also be presented . sub ##section : framework fig . [ reference ] shows the framework of our proposed deep reid model . it can be seen that the global image and part images are simultaneously considered during each round of training . given a training sample , we use an human pose estimation algorithm to acquire the locations of human pose joints . these pose joints are combined into different human body parts . the part regions are first transformed using our feature em ##bed ##ding sub - net ( fen ) and then are combined to form a new modified part image containing the normal ##ized body parts . the global image and the new modified part image are then fed into our cnn together . the two images share the same weights for the first several layers , then have their own network weights in the subsequent layers . at last , we use feature weight ##ing sub - net ( f ##wn ) to learn the weights of part features before fu ##sing them with the global features for final soft ##max loss computation . considering that pedestrian images form different data ##set ##s have different sizes , it is not appropriate to directly use the cnn models pre - trained on the image ##net data ##set . we thus modify and design a network based on the google ##net , as shown in the table [ reference ] . layers from data to inception ( 4 ##e ) in table [ reference ] corresponds to the blue cnn block in fig . [ reference ] , cnn ##g and cnn ##p are inception ( 5 ##a ) and inception ( 5 ##b ) , respectively . the green con ##v matches the subsequent 1 1 con ##vo ##lu ##tion . the loss layers are not shown in table [ reference ] . the batch normal ##ization layers are inserted before every re ##lu layer to accelerate the convergence . we employ a con ##vo ##lu ##tion ##al layer and a global average pool ##ing layer ( gap ) at the end of network to let our network can fit different sizes of input images . in this work , we fix input image size as 512 256 . sub ##section : feature em ##bed ##ding sub - net the feature em ##bed ##ding sub - net ( fen ) is divided into four steps , including locating the joint , generating the original part images , pt ##n , and output ##ting the final modified part images . with a given person image , fen first locate ##s the 14 joints of human body using human pose estimation algorithm . fig . [ reference ] ( c ) shows an example of the 14 joints of human body . according to number , the 14 joints are { } . then we propose six rec ##tangle ##s to cover six different parts of human body , including the head region , the upper body , two arms and two legs . for each human joint , we calculate a response feature map . the horizontal and vertical dimensions of the feature maps are denoted by and , respectively . with the feature maps , the fourteen body joints , can be located by finding the center of mass with the feature values : where in e ##q . [ reference ] are the coordinates of joints , and is the value of pixels in response feature maps . different from , we do not use complex pose estimation networks as the pre - trained network . instead , we use a standard fc ##n trained on the l ##sp data ##set and mp ##ii human pose data ##set . in the second step , the fen uses the 14 human joints to further locate six sub - regions ( head , upper body , left arm , right arm , left leg , and right leg ) as human parts . these parts are normal ##ized through crop ##ping , rotating , and res ##izing to fixed size and orientation . as shown in fig . [ reference ] ( d ) , the 14 located body joints are assigned to six rec ##tangle ##s indicating six parts . the head part , the upper body part , the left arm part , the right arm part , the left leg part , and the right leg part , respectively . for each body part set , the corresponding sub - region bound ##ing box can be obtained based on the location coordinates of all body joints in each part set : an example of the extracted six body sub - regions are visual ##ized in fig . [ reference ] ( d ) . as shown in fig . [ reference ] ( e ) , these body sub - regions are normal ##ized through crop ##ping , rotating , and res ##izing to fixed sizes and orientation ##s . all body parts are rotated to fixed vertical direction . arms and legs are res ##ized to 256 64 , upper body is res ##ized to 256 128 and head is res ##ized to 128 128 . those res ##ized and rotated parts are combined to form the body part image . because 6 body parts have different sizes , black area is una ##vo ##ida ##ble in body part image . simply res ##izing and rotation can not overcome the complex pose variations , especially if the pose estimation ##s are inaccurate . we thus design a pt ##n modified from spatial transform ##er networks ( st ##n ) to learn the angles required for rotating the five body parts . st ##n is a spatial transform ##er module which can be inserted to a neural network to provide spatial transformation capabilities . it thus is potential to adjust the local ##izations and angles of parts . a st ##n is a small net which allows for end - to - end training with standard back - propagation , therefore , the introduction of st ##n does n ' t substantially increase the complexity of training procedure . the st ##n consist of three components : local ##isation network , parameter ##ised sampling grid , and different ##iable image sampling . the local ##isation network takes the input feature map and outputs the parameters of the transformation . for our net , we choose af ##fine transformation so our transformation parameter is 6 - dimensional . the parameter ##ized sampling grid compute ##s each output pixel and the different ##iable image sampling component produces the sampled output image . for more details about st ##n , please refer to . as discussed above , we use a 6 - dimensional parameter to complete af ##fine transformation : where the are the scale and rotation parameters , while the are the translation parameters . the in e ##q . [ reference ] are the target coordinates of the output image and the are the source coordinates of the input image . usually the st ##n compute ##s one af ##fine transform for the whole image , considering a pedestrian ' s different parts have various orientation ##s and sizes from each other , st ##n is not applicable to a part image . inspired by st ##n , we design a pose transform ##er network ( pt ##n ) which compute ##s the af ##fine transformation for each part in part image individually and combines 6 transformed parts together . similar to st ##n , our pt ##n is also a small net and does n ' t substantially increase the complexity of our training procedure . as a consequence , pt ##n has potential to perform better than st ##n for person images . fig . [ reference ] shows the detailed structure of pt ##n . considering a pedestrian ' s head seldom has a large rotation angle , we do n ' t insert a pt ##n net for the pedestrian ' s head part . therefore , we totally have 5 independent pt ##n , namely , , , , . each pt ##n can generate a 6 - dimensional transformation parameter and use to adjust pedestrian ' s part , we can get modified body part . by combining the five transformed parts and a head part together , we obtain the modified part image . sub ##section : feature weight ##ing sub - net the generated part features are combined with the global feature to generate a robust feature representation for precise person re - identification . as the poses generated by the pose detector might be affected by factors like o ##cc ##lusion ##s , pose changes , etc . then inaccurate part detection results could be obtained . examples are shown in fig . [ reference ] . therefore , the part features could be not reliable enough . this happens frequently in real applications with un ##con ##stra ##ined video gathering environment . simply fu ##sing global feature and the part feature may introduces noises . this mo ##tiv ##ates us to introduce feature weight ##ing sub - net ( f ##wn ) to seek a more optimal feature fusion . f ##wn is consisted with a weight layer and a nonlinear transformation , which decides the importance of each dimension in the part feature vector . considering that a single linear weight layer might cause excessive response on some specific dimensions of the part vector , we add a nonlinear function to equal ##ize the response of part feature vector , and the fused feature representation is where the and the are the global and part feature vectors . the and in e ##q . [ reference ] are the weight and bias vectors which have the same dimensions with . the means the had ##ama ##rd product of two vectors , and the means con ##cate ##nation of two vectors together . the impose ##s the hyper ##bolic tangent nonlinear ##ity . is our final person feature generated by and . to allow back - propagation of the loss through the f ##wn , we give the gradient formula : where , , , , , and are the dimensions of and . sub ##section : reid feature extraction the global feature and body - part features are learned by training the pose - driven deep con ##vo ##lu ##tion ##al model . these two types of features are then fused under a unified framework for multi - class person identification . pd ##c extracts the global feature maps from the global body - based representation and learns a 102 ##4 - dimensional feature em ##bed ##ding . similarly , a 102 ##4 - dimension feature is acquired from the modified part image after the fen . the global body feature and the local body part features are compensated into a 204 ##8 - dimensional feature as the final representation . after being weighted by f ##wn , the final representation is used for person reid with euclidean distance . section : experiment sub ##section : data ##set ##s we select three widely used person reid data ##set ##s as our evaluation protocols , including the cu ##h ##k 03 , market 150 ##1 , and viper . note that , because the amount of images in viper is not enough for training a deep model , we combine the training sets of viper , cu ##h ##k 03 and market 150 ##1 together to train the model for viper . cu ##h ##k 03 : this data ##set is made up of 14 , 09 ##6 images of 1 , 46 ##7 different persons taken by six campus cameras . each person only appears in two views . this data ##set provides two types of ann ##ota ##tions , including manually labelled pedestrian bound ##ing boxes and bound ##ing boxes automatically detected by the def ##or ##mable - part - model ( d ##pm ) detector . we denote the two corresponding subset ##s as labeled data ##set and detected data ##set , respectively . the data ##set also provides 20 test sets , each includes 100 identities . we select the first set and use 100 identities for testing and the rest 1 , 36 ##7 identities for training . we report the averaged performance after repeating the experiments for 20 times . market 150 ##1 : this data ##set is made up of 32 , 36 ##8 pedestrian images taken by six manually configured cameras . it has 1 , 501 different persons in it . on average , there are 3 . 6 images for each person captured from each angle . the images can be classified into two types , , crop ##ped images and images of pedestrians automatically detected by the d ##pm . because market 150 ##1 has provided the training set and testing set , we use images in the training set for training our pd ##c network and follow the protocol to report the reid performance . viper : this data ##set is made up of 63 ##2 person images captured from two views . each pair of images depicting a person are collected by different cameras with varying viewpoint ##s and illumination conditions . because the amount of images in viper is not enough to train the deep model , we also perform data aug ##ment ##ation with similar methods in existing deep learning based person reid works . for each training image , we generate 5 augmented images around the image center by performing random 2d transformations . finally , we combine the augmented training images of viper , training images of cu ##h ##k 03 and market 150 ##1 together , as the final training set . sub ##section : implementation details the pedestrian representations are learned through multi - class classification cnn . we use the full body and body parts to learn the representations with soft ##max loss , respectively . we report rank ##1 , rank ##5 , rank ##10 and rank ##20 accuracy of cumulative match curve ( cm ##c ) on the three data ##set ##s to evaluate the reid performance . as for market - 105 ##1 , mean average precision ( map ) is also reported as an additional criterion to evaluate the performance . our model is trained and fine - tuned on caf ##fe . st ##och ##astic gradient descent ( sg ##d ) is used to opt ##imi ##ze our model . images for training are randomly divided into several batch ##es , each of which includes 16 images . the initial learning rate is set as 0 . 01 , and is gradually lowered after each iteration ##s . it should be noted that , the learning rate in part local ##ization network is only 0 . 1 % of that in feature learning network . for each data ##set , we train a model on its corresponding training set as the pre ##train ##ed body - based model . for the overall network training , the network is initial ##ized using pre ##train ##ed body - based model . then , we adopt the same training strategy as described above . we implement our approach with gt ##x titan x gp ##u , intel i ##7 cpu , and 128 gb memory . all images are res ##ized to . the mean value is sub ##tracted from each channel ( b , g , and r ) for training the network . the images of each data ##set are random ##ized in the process of training stage . sub ##section : evaluation of individual components we evaluate five variants of our approach to verify the validity of individual components in our pd ##c , , components like feature em ##bed ##ding sub - net ( fen ) and feature weight ##ing sub - net ( f ##wn ) . comparisons on three data ##set ##s are summarized in table [ reference ] . in the table , \" global only \" means we train our deep model without using any part information . \" global + part \" denotes cnn trained through two streams without fen and f ##wn . based on \" global + part \" , considering fen is denoted as \" global + part + fen \" . similarly , \" global + part + f ##wn \" means considering f ##wn . in addition , \" part only \" denotes only using part features . pd ##c considers all of these components . from the experimental results , it can be observed that , fu ##sing global features and part features achieve ##s better performance than only using one of them . compared with \" global only \" , considering extra part cues , , \" global + part \" , largely improves the reid performance and achieve ##s the rank ##1 accuracy of 85 . 07 % and 76 . 33 % on cu ##h ##k 03 labeled and detected data ##set ##s , respectively . moreover , using fen and f ##wn further boost ##s the rank ##1 identification rate . this shows that training our model using pt ##n and weight layer gets more competitive performance on three data ##set ##s . the above experiments shows that each of the components in our method is helpful for improving the performance . by considering all of these components , pd ##c exhibits the best performance . sub ##section : comparison with related works cu ##h ##k 03 : for the cu ##h ##k 03 data ##set , we compare our pd ##c with some recent methods , including distance metric learning methods : mla ##pg , lo ##mo + x ##q ##da , bow + hs , war ##ca , ld ##ns , feature extraction method : go ##g and deep learning based methods : id ##la , person ##net , d ##g ##drop ##out , si + ci , gate s - cnn , l ##st ##m s - cnn , ed ##m , pie and spin ##dle . we conduct experiments on both the detected data ##set and the labeled data ##set . experimental results are presented in table [ reference ] and table [ reference ] . experimental results show that our approach out ##per ##forms all distance metric learning methods by a large margin . it can be seen that pie , spin ##dle and our pd ##c which all use the human pose cues achieve better performance than the other methods . this shows the advantages of considering extra pose cues in person reid . it is also clear that , our pd ##c achieve ##s the rank ##1 accuracy of 78 . 29 and 88 . 70 on detected and labeled data ##set ##s , respectively . this leads to 11 . 19 and 0 . 20 performance gains over the reported performance of pie and spin ##dle , respectively . market 150 ##1 : on market 150 ##1 , the compared works that learn distance metric ##s for person reid include lo ##mo + x ##q ##da , bow + kiss ##me , war ##ca , ld ##ns , t ##ma and h ##vil . compared works based on deep learning are person ##net , gate s - cnn , l ##st ##m s - cnn , pie and spin ##dle . d ##g ##drop ##out does not report performance on market ##15 ##01 . so we implemented d ##g ##drop ##ut and show experimental results in table [ reference ] . it is clear that our method out ##per ##forms these compared works by a large margin . specifically , pd ##c achieve ##s rank ##1 accuracy of 84 . 14 % , and map of 63 . 41 % using the single query mode . they are higher than the rank ##1 accuracy and map of pie , which performs best among the compared works . this is because our pd ##c not only learns pose invariant features with fen but also learns better fusion strategy with f ##wn to emphasize the more disc ##rim ##ina ##tive features . viper : we also evaluate our method by comparing it with several existing methods on viper . the compared methods include distance metric learning ones : mla ##pg , lo ##mo + x ##q ##da , bow , war ##ca and ld ##ns , and deep learning based ones : id ##la , d ##g ##drop ##out , si + ci , gate s - cnn , l ##st ##m s - cnn , mt ##l - lo ##rae and spin ##dle . from the results shown in table [ reference ] , our pd ##c achieve ##s the rank ##1 accuracy of 51 . 27 % . this out ##per ##forms most of compared methods except spin ##dle which also considers the human pose cues . we assume the reason might be because , spin ##dle involves more training sets to learn the model for viper . therefore , the training set of spin ##dle is larger than ours , , the combination of market 150 ##1 , cu ##h ##k ##0 ##3 and viper . for the other two data ##set ##s , our pd ##c achieve ##s better performance than spin ##dle . sub ##section : evaluation of feature weight ##ing sub - net to test the effectiveness of feature weight ##ing sub - net ( f ##wn ) , we verify the performance of five variants of f ##wn , which are denoted as , = { 0 , 1 , 2 , 3 , 4 } , where is the number of weight layers in f ##wn with nonlinear transformation . for example , means we cascade two weight layers with nonlinear transformation , means we only have one weight layer without nonlinear transformation . the experimental results are shown in table [ reference ] . as we can see that one weight layer with nonlinear transformation gets the best performance on the three data ##set ##s . the reid performance starts to drop as we increase of the number of weight layers , despite more computation ##s are being brought in . it also can be observed that , using one layer with nonlinear transformation gets better performance than one layer without nonlinear transformation , , . this means adding one nonlinear transformation after a weight layer learns more reliable weights for feature fusion and matching . based on the above observations , we adopt as our final model in this paper . examples of features before and after f ##wn are shown fig . [ reference ] . section : conclusions this paper presents a pose - driven deep con ##vo ##lu ##tion ##al model for the person reid . the proposed deep architecture explicitly leverage ##s the human part cues to learn effective feature representations and adaptive similarity measurements . for the feature representations , both global human body and local body parts are transformed to a normal ##ized and homo ##log ##ous state for better feature em ##bed ##ding . for similarity measurements , weights of feature representations from human body and different body parts are learned to adaptive ##ly chase a more disc ##rim ##ina ##tive feature fusion . experimental results on three bench ##mark data ##set ##s demonstrate the superiority of the proposed model over current state - of - the - art methods . ac ##k ##now ##led ##gm ##ents this work is partly supported by national science foundation of china under grant no . 61 ##57 ##20 ##50 , 91 ##53 ##8 ##11 ##1 , 61 ##6 ##20 ##10 ##60 ##0 ##9 , 61 ##42 ##9 ##20 ##1 , 61 ##6 ##7 ##25 ##19 , and the national 1000 youth talents plan . dr . qi tian is supported by ar ##o grant w ##9 ##11 ##n ##f - 15 - 1 - 02 ##90 and faculty research gift awards by nec laboratories of america and b ##lip ##par . bibliography : references",
        "pred_seq": "[SEP] pose model [SEP] [SEP] person identification [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "posedriven deep convolutional model"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "person reidentification"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "market1501",
                        "market 1501",
                        "market1051",
                        "market1501"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "computational complexity",
                        "mean average precision",
                        "map",
                        "map of pie"
                    ]
                ],
                "Task": [
                    [
                        "person reidentification",
                        "reid",
                        "person reid works",
                        "multiclass person identification"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "market1501",
                        "market 1501",
                        "market1051",
                        "market1501"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "rank1",
                        "rank1 identification rate"
                    ]
                ],
                "Task": [
                    [
                        "person reidentification",
                        "reid",
                        "person reid works",
                        "multiclass person identification"
                    ]
                ]
            }
        ]
    },
    "28": {
        "doctext": "document : distributed prior ##iti ##zed experience replay we propose a distributed architecture for deep reinforcement learning at scale , that enables agents to learn effectively from orders of magnitude more data than previously possible . the algorithm deco ##up ##les acting from learning : the actors interact with their own instances of the environment by selecting actions according to a shared neural network , and accumulate the resulting experience in a shared experience replay memory ; the learn ##er replay ##s samples of experience and updates the neural network . the architecture relies on prior ##iti ##zed experience replay to focus only on the most significant data generated by the actors . our architecture substantially improves the state of the art on the arcade learning environment , achieving better final performance in a fraction of the wall - clock training time . section : introduction a broad trend in deep learning is that combining more computation with more powerful models db ##lp : journals / co ##rr / kaiser ##gs ##v ##p ##ju ##17 and larger data ##set ##s image ##net _ cv ##pr ##0 ##9 yields more impressive results . it is reasonable to hope that a similar principle holds for deep reinforcement learning . there are a growing number of examples to justify this optimism : effective use of greater computational resources has been a critical factor in the success of such algorithms as go ##ril ##a go ##ril ##a , a ##3 ##c a ##3 ##c , gp ##u advantage actor critic ga ##3 ##c , distributed pp ##o hee ##ss : d ##pp ##o and alpha ##go alpha ##go . deep learning framework ##s such as tensor ##flow tensor ##flow support distributed training , making large scale machine learning systems easier to implement and deploy . despite this , much current research in deep reinforcement learning concerns itself with improving performance within the computational budget of a single machine , and the question of how to best harness more resources is comparatively under ##ex ##pl ##ored . in this paper we describe an approach to scaling up deep reinforcement learning by generating more data and selecting from it in a prior ##iti ##zed fashion prior ##iti ##zed - replay . standard approaches to distributed training of neural networks focus on parallel ##izing the computation of gradient ##s , to more rapidly opt ##imi ##ze the parameters . in contrast , we distribute the generation and selection of experience data , and find that this alone su ##ffi ##ces to improve results . this is complementary to distributing gradient computation , and the two approaches can be combined , but in this work we focus purely on data - generation . we use this distributed architecture to scale up variants of deep q - networks ( dq ##n ) and deep deter ##mini ##stic policy gradient ( dd ##pg ) , and we evaluate these on the arcade learning environment bench ##mark belle ##mare ##20 ##13 ##ar ##cade , and on a range of continuous control tasks . our architecture achieve ##s a new state of the art performance on atari games , using a fraction of the wall - clock time compared to the previous state of the art , and without per - game hyper ##para ##meter tuning . we empirical ##ly investigate the scala ##bility of our framework , anal ##ys ##ing how prior ##iti ##zation affects performance as we increase the number of data - generating workers . our experiments include an analysis of factors such as the replay capacity , the rec ##ency of the experience , and the use of different data - generating policies for different workers . finally , we discuss implications for deep reinforcement learning agents that may apply beyond our distributed framework . section : background paragraph : distributed st ##och ##astic gradient descent distributed st ##och ##astic gradient descent is widely used in supervised learning to speed up training of deep neural networks , by parallel ##izing the computation of the gradient ##s used to update their parameters . the resulting parameter updates may be applied sync ##hr ##ono ##usly k ##riz ##he ##vsky ##20 ##14 ##one or as ##yn ##ch ##ron ##ously . both approaches have proven effective and are an increasingly standard part of the deep learning tool ##box . inspired by this , applied distributed as ##yn ##ch ##ron ##ous parameter updates and distributed data generation to deep reinforcement learning . as ##yn ##ch ##ron ##ous parameter updates and parallel data generation have also been successfully used within a single - machine , in a multi - threaded rather than a distributed context a ##3 ##c . gp ##u as ##yn ##ch ##ron ##ous actor - critic [ ga ##3 ##c ; ] [ ] ga ##3 ##c and parallel advantage actor - critic [ pa ##ac ; ] [ ] pa ##ac adapt this approach to make efficient use of gp ##us . paragraph : distributed importance sampling a complementary family of techniques for speeding up training is based on variance reduction by means of importance sampling [ cf . ] [ ] hastings ##19 ##70 ##mont ##e . this has been shown to be useful in the context of neural networks hint ##on ##200 ##7 - vs . sampling non - uniformly from a data ##set and weight ##ing updates according to the sampling probability in order to counter ##act the bias thereby introduced can increase the speed of convergence by reducing the variance of the gradient ##s . one way of doing this is to select samples with probability proportional to the norm of the corresponding gradient ##s . in supervised learning , this approach has been successfully extended to the distributed setting alain ##20 ##15 ##var ##iance . an alternative is to rank samples according to their latest known loss value and make the sampling probability a function of the rank rather than of the loss itself los ##hc ##hil ##ov ##20 ##15 ##on ##line . paragraph : prior ##iti ##zed experience replay experience replay experience - replay has long been used in reinforcement learning to improve data efficiency . it is particularly useful when training neural network function approx ##ima ##tors with st ##och ##astic gradient descent algorithms , as in neural fitted q - iteration n ##f ##q and deep q - learning dq ##n . experience replay may also help to prevent over ##fi ##tting by allowing the agent to learn from data generated by previous versions of the policy . prior ##iti ##zed experience replay prior ##iti ##zed - replay extends classic prior ##iti ##zed sweeping ideas prior ##iti ##zed - sweeping to work with deep neural network function approx ##ima ##tors . the approach is strongly related to the importance sampling techniques discussed in the previous section , but using a more general class of biased sampling procedures that focus learning on the most ' surprising ' experiences . biased sampling can be particularly helpful in reinforcement learning , since the reward signal may be sparse and the data distribution depends on the agent ' s policy . as a result , prior ##iti ##zed experience replay is used in many agents , such as prior ##iti ##zed duel ##ing dq ##n duel ##ing , un ##real un ##real , dq ##f ##d dq ##f ##d , and rainbow rainbow . in an ab ##lation study conducted to investigate the relative importance of several algorithm ##ic ingredients rainbow , prior ##iti ##zation was found to be the most important ingredient contributing to the agent ' s performance . section : our contribution : distributed prior ##iti ##zed experience replay in this paper we extend prior ##iti ##zed experience replay to the distributed setting and show that this is a highly scala ##ble approach to deep reinforcement learning . we introduce a few key modifications that enable this scala ##bility , and we refer to our approach as ape - x . as in go ##ril ##a go ##ril ##a , we deco ##mp ##ose the standard deep reinforcement learning algorithm into two parts , which run concurrently with no high - level sync ##hr ##oni ##zation . the first part consists of stepping through an environment , evaluating a policy implemented as a deep neural network , and storing the observed data in a replay memory . we refer to this as acting . the second part consists of sampling batch ##es of data from the memory to update the policy parameters . we term this learning . [ width = trim = 0 ##pt 400 ##pt 0 ##pt 0 ##pt , clip ] images / apex _ architecture . pdf [ t ! ] actor { algorithm ##ic } [ 1 ] t agent in environment instance , storing experiences . call to obtain latest network parameters . initial state from environment . to t an action using the current policy . the action in the environment . data to local buffer . a background thread , periodically send data to replay . buffer ##ed data ( e . g . batch of multi - step transitions ) . priorities for experience ( e . g . absolute td error ) . call to add experience to replay memory . latest network parameters . [ t ! ] learn ##er { algorithm ##ic } [ 1 ] network using batch ##es sampled from memory . to t the parameters t times . a prior ##iti ##zed batch of transitions ( in a background thread ) . learning rule ; e . g . double q - learning or dd ##pg priorities for experience , ( e . g . absolute td error ) . call to update priorities . old experience from replay memory . in principle , both acting and learning may be distributed across multiple workers . in our experiments , hundreds of actors run on cpu ##s to generate data , and a single learn ##er running on a gp ##u samples the most useful experiences ( figure [ reference ] ) . pseudo ##code for the actors and learners is shown in algorithms [ reference ] and [ reference ] . updated network parameters are periodically communicated to the actors from the learn ##er . in contrast to go ##ril ##a , we use a shared , centralized replay memory , and instead of sampling uniformly , we prior ##iti ##ze , to sample the most useful data more often . since priorities are shared , high priority data discovered by any actor can benefit the whole system . priorities can be defined in various ways , depending on the learning algorithm ; two instances are described in the next sections . in prior ##iti ##zed dq ##n prior ##iti ##zed - replay priorities for new transitions were initial ##ized to the maximum priority seen so far , and only updated once they were sampled . this does not scale well : due to the large number of actors in our architecture , waiting for the learn ##er to update priorities would result in a my ##op ##ic focus on the most recent data , which has maximum priority by construction . instead , we take advantage of the computation the actors in ape - x are already doing to evaluate their local copies of the policy , by making them also compute suitable priorities for new transitions online . this ensures that data entering the replay has more accurate priorities , at no extra cost . sharing experiences has certain advantages compared to sharing gradient ##s . low late ##ncy communication is not as important as in distributed sg ##d , because experience data becomes outdated less rapidly than gradient ##s , provided the learning algorithm is robust to off - policy data . across the system , we take advantage of this by batch ##ing all communications with the centralized replay , increasing the efficiency and through ##put at the cost of some late ##ncy . with this approach it is even possible for actors and learners to run in different data - centers without limiting performance . finally , by learning off - policy [ cf . ] [ ] sutton ##bar ##to : 1998 , sutton ##bar ##to : 2017 , we can further take advantage of ape - x ' s ability to combine data from many distributed actors , by giving the different actors different exploration policies , broad ##ening the diversity of the experience they jointly encounter . as we will see in the results , this can be sufficient to make progress on difficult exploration problems . sub ##section : ape - x dq ##n the general framework we have described may be combined with different learning algorithms . first , we combined it with a variant of dq ##n dq ##n with some of the components of rainbow rainbow . more specifically , we used double q - learning double ##q , deep ##dou ##ble ##q ##lea ##rn ##ing with multi - step boots ##tra ##p targets [ cf . ] [ ] sutton : 1988 , sutton ##bar ##to : 1998 , sutton ##bar ##to : 2017 , a ##3 ##c as the learning algorithm , and a duel ##ing network architecture duel ##ing as the function approx ##ima ##tor . this results in computing for all elements in the batch the loss with where is a time index for an experience sampled from the replay starting with state and action , and denotes parameters of the target network dq ##n , a slow moving copy of the online parameters . multi - step returns are truncated if the episode ends in fewer than steps . in principle , q - learning variants are off - policy methods , so we are free to choose the policies we use to generate data . however , in practice , the choice of behaviour policy does affect both exploration and the quality of function approximation . furthermore , we are using a multi - step return with no off - policy correction , which in theory could adverse ##ly affect the value estimation . nonetheless , in ape - x dq ##n , each actor execute ##s a different policy , and this allows experience to be generated from a variety of strategies , relying on the prior ##iti ##zation mechanism to pick out the most effective experiences . in our experiments , the actors use - greedy policies with different values of . low policies allow exploring deeper in the environment , while high policies prevent over - specialization . sub ##section : ape - x d ##pg to test the general ##ity of the framework we also combined it with a continuous - action policy gradient system based on dd ##pg dd ##pg , an implementation of deter ##mini ##stic policy gradient ##s also similar to older methods ad ##hd ##p , ac ##d , and tested it on continuous control tasks from the deep ##mind control suite ta ##ssa ##20 ##18 ##suit ##e . the ape - x d ##pg setup is similar to ape - x dq ##n , but the actor ' s policy is now represented explicitly by a separate policy network , in addition to the q - network . the two networks are opt ##imi ##zed separately , by mini ##mi ##zing different losses on the sampled experience . we denote the policy and q - network parameters by and respectively , and adopt the same convention as above to denote target networks . the q - network outputs an action - value estimate for a given state , and multi - dimensional action . it is updated using temporal - difference learning with a multi - step boots ##tra ##p target . the q - network loss can be written as , where the policy network outputs an action . the policy parameters are updated using policy gradient ascent on the estimated q - value , using gradient \u2014 note that this depends on the policy parameters only through the action that is input to the critic network . further details of the ape - x d ##pg algorithm are available in the appendix . section : experiments sub ##section : atari [ width = ] images / [ width = ] images / in our first set of experiments we evaluate ape - x dq ##n on atari , and show state of the art results on this standard reinforcement learning bench ##mark . we use 360 actor machines ( each using one cpu core ) to feed data into the replay memory as fast as they can generate it ; approximately 139 frames per second ( f ##ps ) each , for a total of 50 k f ##ps , which corresponds to 12 . 5 k transitions ( because of a fixed action repeat of 4 ) . the actors batch experience data locally before sending it to the replay : up to 100 transitions may be buffer ##ed at a time , which are then sent as ##yn ##ch ##ron ##ously in batch ##es of . the learn ##er as ##yn ##ch ##ron ##ously pre ##fe ##tch ##es up to 16 batch ##es of 512 transitions , and compute ##s updates for 19 such batch ##es each second , meaning that gradient ##s are computed for 9 . 7 k transitions per second on average . to reduce memory and bandwidth requirements , observation data is compressed using a p ##ng code ##c when sent and when stored in the replay . the learn ##er deco ##mp ##resses data as it pre ##fe ##tch ##es it , in parallel with computing and applying gradient ##s . the learn ##er also as ##yn ##ch ##ron ##ously handles any requests for parameters from actors . actors copy the network parameters from the learn ##er every 400 frames ( 2 . 8 seconds ) . each actor execute ##s an - greedy policy where with , . each is held constant throughout training . the episode length is limited to 5000 ##0 frames during training . the capacity of the shared experience replay memory is soft - limited to 2 million transitions : adding new data is always permitted , to not slow down the actors , but every 100 learning steps any excess data above this capacity threshold is removed en mass ##e , in fi ##fo order . the median actual size of the memory is 203 ##50 ##50 . data is sampled according to proportional prior ##iti ##zation , with a priority expo ##nent of 0 . 6 and an importance sampling expo ##nent set to 0 . 4 . in figure [ reference ] , on the left , we compare the median human normal ##ized score across all 57 games to several baseline ##s : dq ##n , prior ##iti ##zed dq ##n , distribution ##al dq ##n distribution ##al , rainbow , and go ##ril ##a . in all cases the performance is measured at the end of training under the no - op starts testing regime dq ##n . on the right , we show initial learning curves ( taken from the greed ##iest actor ) for a selection of 6 games ( full learning curves for all games are in the appendix ) . given that ape - x can harness substantially more computation than most baseline ##s , one might expect it to train faster . figure [ reference ] shows that this was indeed the case . perhaps more surprisingly , our agent achieved a substantially higher final performance . in table [ reference ] we compare the median human - normal ##ized performance of ape - x dq ##n on the atari bench ##mark to corresponding metric ##s as reported for other baseline agents in their respective publications . whenever available we report results both for no - op starts and for human starts . the human - starts regime go ##ril ##a corresponds to a more challenging general ##ization test , as the agent is initial ##ized from random starts drawn from games played by human experts . ape - x ' s performance is higher than the performance of any of the baseline ##s according to both metric ##s . sub ##section : continuous control in a second set of experiments we evaluated ape - x d ##pg on four continuous control tasks . in the mani ##pu ##lat ##or domain the agent must learn to bring a ball to a specified location . in the humanoid domain the agent must learn to control a humanoid body to solve three distinct tasks of increasing complexity : standing , walking and running . since here we learn from features , rather than from pixels , the observation space is much smaller than it is in the atari domain . we therefore use small , fully - connected networks ( details in the appendix ) . with 64 actors on this domain , we obtain 14 k total f ##ps ( the same number of transitions per second ; here we do not use action repeats ) . we process 86 batch ##es of 256 transitions per second , or 22 k transitions processed per second . figure [ reference ] shows that ape - x d ##pg achieved very good performance on all four tasks . the figure shows the performance of ape - x d ##pg for different numbers of actors : as the number of actors increases our agent becomes increasingly effective at solving these problems rapidly and re ##lia ##bly , out ##per ##form ##ing a standard dd ##pg baseline trained for over 10 times longer . a parallel paper d ##4 ##pg builds on this work by combining ape - x d ##pg with distribution ##al value functions , and the resulting algorithm is successfully applied to further continuous control tasks . [ width = 0 . 8 ##cl ##ip ] images / section : analysis [ width = 0 . 85 ##tri ##m = 0 ##pt 0 0 ##pt 40 ##pt , clip ] images / in this section we describe additional ape - x dq ##n experiments on atari that helped improve our understanding of the framework , and we investigate the contribution of different components . first , we investigated how the performance scales with the number of actors . we trained our agent with different numbers of actors ( 8 , 16 , 32 , 64 , 128 and 256 ) for 35 hours on a subset of 6 atari games . in all experiments we kept the size of the shared experience replay memory fixed at 1 million transitions . figure [ reference ] shows that the performance consistently improved as the number of actors increased . the appendix contains learning curves for additional games , and a comparison of the scala ##bility of the algorithm with and without prior ##iti ##zed replay . it is perhaps surprising that performance improved so substantially purely by increasing the number of actors , without changing the rate at which the network parameters are updated , the structure of the network , or the update rule . we h ##yp ##oth ##es ##ize that the proposed architecture helps with a common deep reinforcement learning failure mode , in which the policy discovered is a local opt ##imum in the parameter space , but not a global one , e . g . , due to insufficient exploration . using a large number of actors with varying amounts of exploration helps to discover promising new courses of action , and prior ##iti ##zed replay ensures that when this happens , the learning algorithm focuses its efforts on this important information . next , we investigated varying the capacity of the replay memory ( see figure [ reference ] ) . we used a setup with 256 actors , for a median of 37 k total environment frames per second ( approximately 9 k transitions ) . with such a large number of actors , the contents of the memory is replaced much faster than in most dq ##n - like agents . we observed a small benefit to using a larger replay capacity . we h ##yp ##oth ##es ##ize this is due to the value of keeping some high priority experiences around for longer and replay ##ing them . as above , a single learn ##er machine trained the network with median 19 batch ##es per second , each of 512 transitions , for a median of 9 . 7 k transitions processed per second . [ width = trim = 0 ##pt 0 0 ##pt 40 ##pt , clip ] images / finally , we ran additional experiments to di ##sen ##tangle potential effects of two con ##fo ##und ##ing factors in our scala ##bility analysis : rec ##ency of the experience data in the replay memory , and diversity of the data - generating policies . the full description of these experiments is confined to the appendix ; to sum ##mar ##ize , neither factor alone is sufficient to explain the performance we see . we therefore conclude that the results are due substantially to the positive effects of gathering more experience data ; namely better exploration of the environment and better avoidance of over ##fi ##tting . section : conclusion we have designed , implemented , and analyzed a distributed framework for prior ##iti ##zed replay in deep reinforcement learning . this architecture achieved state of the art results in a wide range of discrete and continuous tasks , both in terms of wall - clock learning speed and final performance . in this paper we focused on applying the ape - x framework to dq ##n and d ##pg , but it could also be combined with any other off - policy reinforcement learning update . for methods that use temporal ##ly extended sequences [ e . g . , ] [ ] a ##3 ##c , ace ##r , the ape - x framework may be adapted to prior ##iti ##ze sequences of past experiences instead of individual transitions . ape - x is designed for regimes in which it is possible to generate large quantities of data in parallel . this includes simulated environments but also a variety of real - world applications , such as robotic arm farms , self - driving cars , online recommend ##er systems , or other multi - user systems in which data is generated by many instances of the same environment [ c . f . ] [ ] concurrent - r ##l . in applications where data is costly to obtain , our approach will not be directly applicable . with powerful function approx ##ima ##tors , over ##fi ##tting is an issue : generating more training data is the simplest way of addressing it , but may also provide guidance towards data - efficient solutions . many deep reinforcement learning algorithms are fundamentally limited by their ability to explore effectively in large domains . ape - x uses a naive yet effective mechanism to address this issue : generating a diverse set of experiences and then identifying and learning from the most useful events . the success of this approach suggests that simple and direct approaches to exploration may be feasible , even for sync ##hr ##ono ##us agents . our architecture illustrates that distributed systems are now practical both for research and , potentially , large - scale applications of deep reinforcement learning . we hope that the algorithms , architecture , and analysis we have presented will help to accelerate future efforts in this direction . sub ##su ##bs ##ection : ac ##k ##now ##led ##gm ##ents we would like to acknowledge the contributions of our colleagues at deep ##mind , whose input and support has been vital to the success of this work . thanks in particular to tom sc ##ha ##ul , joseph mod ##ay ##il , sri ##ram sri ##ni ##vas ##an , georg os ##tro ##vs ##ki , josh abrams ##on , todd he ##ster , jean - baptiste les ##pia ##u , alba ##n rr ##ust ##emi and dan bel ##ov . bibliography : references appendix : rec ##ency of experience [ width = ] images / apex _ ic ##lr _ virtual _ actors [ width = ] images / in our main experiments we do not change the size of the replay memory in proportion to the number of actors , so by changing the number of actors we also increased the rate at which the contents of the replay memory is replaced . this means that in the experiments with more actors , transitions in the replay memory are more recent : they are generated by following policies whose parameters are closer to version of the parameters being opt ##imi ##zed by the learn ##er , and in this sense they are more on - policy . could this alone be sufficient to explain the improved performance ? if so , we might be able to recover the results without needing a large number of actor machines . to test this , we constructed an experiment wherein we replicate the rate at which the contents of the replay memory is replaced in the 256 - actor experiments , but instead of actually using 256 actors , we use 32 actors but add each transition they generate to the replay memory 8 times over . in this setup , the contents of the replay memory is similarly generated by policies with a recent version of the network parameters : the only difference is that the data is not as diverse as in the 256 - actor case . we observe ( see figure [ reference ] ) that this does not recover the same performance , and therefore conclude that the rec ##ency of the experience alone is not sufficient to explain the performance of our method . indeed , we see that adding the same data multiple times can sometimes harm performance , since although it increases rec ##ency this comes at the expense of diversity . note : in principle , du ##pl ##ica ##ting the added data in this fashion has a similar effect to reducing the capacity of the replay memory , and indeed , our results with a smaller replay memory in figure [ reference ] do co ##rro ##bor ##ate the finding . however , we test also by du ##pl ##ica ##ting the data primarily in order to exclude any effects arising from the implementation . in particular , in contrast to simply reducing the replay capacity , du ##pl ##ica ##ting each data point means that the computational demands on the replay server in these runs are the same as when we use the corresponding number of real actors . appendix : varying the data - generating policies another factor that could con ##ce ##iva ##bly contribute to the scala ##bility of our algorithm is the fact that each actor has a different . to determine the extent to which this impacts upon the performance , we ran an experiment ( see figure [ reference ] ) with some simple variations on the mechanism we use to choose the policies that generate the data we train on . the first alternative we tested is to choose a small fixed set of 6 values for , instead of the full range that we typically use . in this test , we use prior ##iti ##zed replay as normal , and we find that the results with the full range of are overall slightly better . however , it is not essential for achieving good results within our distributed framework . appendix : atari : additional details the frames received from the environment are prep ##ro ##ces ##sed on the actor side with the standard transformations introduced by dq ##n . this includes grey ##sca ##ling , frame stack ##ing , repeating actions 4 times , and clip ##ping rewards to . the learn ##er waits for at least 5000 ##0 transitions to be accumulated in the replay before starting learning . we use a centered rms ##pro ##p opt ##imi ##zer with a learning rate of 0 . 000 ##25 / 4 , decay of 0 . 95 , epsilon of 1 . 5 ##e - 7 , and no momentum to minimize the multi - step loss ( with ) . gradient norms are clipped to 40 . the target network used in the loss calculation is copied from the online network every 2500 training batch ##es . we use the same network as in the duel ##ing dd ##q ##n agent . appendix : continuous control : additional details the critic network has a layer with 400 units , followed by a tan ##h activation , followed by another layer of 300 units . the actor network has a layer with 300 units , followed by a tan ##h activation , followed by another layer of 200 units . the gradient used to update the actor network is clipped to , element - wise . training uses the adam opt ##imi ##zer ( ) with learning rate of . the target network used in the loss calculation is copied from the online network every 100 training batch ##es . replay sampling priorities are set according to the absolute td error as given by the critic , and are sampled by the learn ##er using proportional prior ##iti ##zed sampling ( see appendix [ reference ] ) with priority expo ##nent . to maintain a fixed replay capacity of , transitions are periodically evicted using proportional prior ##iti ##zed sampling , with priority expo ##nent . this is a different strategy for removing data than in the atari experiments , which simply removed the oldest data first - it remains to be seen which is superior . unlike the original d ##pg algorithm which applies auto ##cor ##rel ##ated noise sampled from a or ##nstein - uh ##len ##beck process ( ) , we apply exploration noise to each action sampled from a normal distribution with . evaluation is performed using the noise ##less deter ##mini ##stic policy . hyper ##para ##meter ##s are otherwise as per dq ##n . bench ##mark ##ing was performed in two continuous control domains ( ( a ) humanoid and ( b ) mani ##pu ##lat ##or , see figure [ reference ] ) implemented in the mu ##jo ##co physics simulator ( ) . humanoid is a humanoid walker with action , state and observation dimensional ##ities , and respectively . three humanoid tasks were considered : walk ( reward for exceeding a minimum velocity ) , run ( reward proportional to movement speed ) and stand ( reward proportional to standing height ) . mani ##pu ##lat ##or is a 2 - dimensional plan ##ar arm with , and , which receives reward for catching a randomly - initial ##ized moving ball . . 5 [ width = . 6 ] . / images / humanoid . p ##ng . 5 [ width = . 6 ] . / images / mani ##pu ##lat ##or . p ##ng appendix : tuning on atari , we performed some limited tuning of the learning rate and batch size : we found that larger batch sizes contribute significantly to performance , when using many actors . we tried batch sizes from { 32 , 128 , 256 , 512 , 102 ##4 } , seeing clear benefits up to 512 . we attempted increasing the learning rate to 0 . 000 ##25 with the larger batch sizes but this des ##ta ##bil ##ized training on some games . we also tried a lower learning rate of 0 . 000 ##25 / 8 , but this did not re ##lia ##bly improve results . likewise for continuous control , we experimented with batch sizes { 32 , 128 , 256 , 512 , 102 ##4 } and learning rates from to . we also experimented with the prior ##iti ##zation expo ##nent ##s from to , with results proving essentially consistent within the range [ 0 . 3 , 0 . 7 ] ( beyond 0 . 7 , training would sometimes become unstable and diver ##ge ) . for the experiments with many actors , we set the period for up ##dating network parameters on the actors to be high enough that the learn ##er was not over ##loaded with requests , and we set the number of transitions that are locally accumulated on each actor to be high enough that the replay server would not be over ##loaded with network traffic , but we did not otherwise tune those parameters and have not observed them to have significant impact on the learning dynamics . appendix : implementation the following section makes explicit some of the more practical details that may be of interest to anyone wishing to implement a similar system . paragraph : data storage the algorithm is implemented using tensor ##flow tensor ##flow . replay data is kept in a distributed in - memory key - value store implemented using custom tensor ##flow ops , similar to the look ##up ops available in core tensor ##flow . the ops allow adding , reading , and removing batch ##es of tensor data efficiently . paragraph : sampling data we also implemented ops for efficiently maintaining and sampling from a prior ##iti ##zed distribution over the keys , using the algorithm for proportional prior ##iti ##zation described in . the probability of sampling a transition is where is the priority of the transition with key . the expo ##nent controls the amount of prior ##iti ##zation , and when uniform sampling is recovered . the proportional variant sets priority where is the td error for transition . whenever a batch of data is added to or removed from the store , or is processed by the learn ##er , this distribution is corresponding ##ly updated , recording any change to the set of valid keys and the priorities associated with them . a background thread on the learn ##er fetch ##es batch ##es of sampled data from the remote replay and deco ##mp ##resses it using the learn ##er ' s cpu , in parallel with the gradient ##s being computed on the gp ##u . the fetch ##ed data is buffer ##ed in a tensor ##flow queue , so that the gp ##u always has data available to train on . paragraph : adding data in order to efficiently construct - step transition data , each actor maintains a circular buffer of capacity containing tu ##ples , where is the current size of the buffer . with each step , the new data is app ##ended and the accumulated per - step discount ##s and partial returns for all entries in the buffer are updated . if the buffer has reached its capacity , , then its first element may be combined with the latest state and value estimates to produce a valid - step transition ( with accompanying q - values ) . however , instead of being directly added to the remote replay memory on each step , the constructed transitions are first stored in a local tensor ##flow queue , in order to reduce the number of requests to the replay server . the queue is periodically flushed , at which stage the absolute - step td - errors ( and thus the initial priorities ) for the queue ##d transitions are computed in batch , using the buffer ##ed q - values to avoid rec ##omp ##utation . the q - value estimates from which the initial priorities are derived are therefore based on the actor ' s copy of the network parameters at the time the corresponding state was obtained from the environment , rather than the latest version on the learn ##er . these q - values need not be stored after this , since the learn ##er does not require them , although they can be helpful for de ##bu ##gging . a unique key is assigned to each transition , which records which actor and environment step it came from , and the de ##que ##ue ##d transition tu ##ples are stored in the remote replay memory . as mentioned in the previous section , the remote sampling distribution is immediately updated with the newly added keys and the corresponding initial priorities computed by the actor . note that , since we store both the start and the end state with each transition , we are storing some data twice : this costs more ram , but sim ##pl ##ifies the code . paragraph : contention it is important that the replay server be able to handle all requests in a timely fashion , in order to avoid slowing down the whole system . possible bottle ##neck ##s include cpu , network bandwidth , and any locks protecting the shared data . in our experiments we found cpu to be the main bottle ##neck , but this was resolved by ensuring all requests and responses use sufficiently large batch ##es . nonetheless , it is ad ##vis ##able to consider all of these potential performance concerns when designing such systems . paragraph : as ##yn ##ch ##ron ##ici ##ty in our framework , since acting and learning proceed with no sync ##hr ##oni ##zation , and performance depends on both , it can be misleading to consider performance with reference to only one of these . for example , the results after a given total number of environment frames have been experienced are highly dependent on the number of updates the learn ##er has performed in that time . for this reason it is important to monitor and report the speeds of all parts of the system and to consider them when analyzing results . paragraph : failure tolerance in distributed systems with many workers , it is inevitable that interruption ##s or failures will occur , either due to occasional hardware issues or because shared resources are needed by higher priority jobs . all state ##ful parts of the system therefore must periodically save their work and be able to resume where they left off when restarted . in our system , actors may be interrupted at any time and this will not prevent continued learning , albeit with a temporarily reduced rate of new data entering the replay memory . if the replay server is interrupted , the data it contains is discarded , and upon res ##uming , the memory is ref ##ille ##d quickly by the actors . in this event , to avoid over ##fi ##tting , the learn ##er will pause training briefly , until the minimum amount of data has once again been accumulated . if the learn ##er is interrupted , progress will stall until it resume ##s . [ width = 0 . 99 ] images / [ width = 0 . 99 ] images / [ height = 4 ##cm ] images / [ width = 0 . 95 ] images / apex _ ic ##lr _ nu ##m _ actors",
        "pred_seq": "d ##o [SEP] distributed replay [SEP] [SEP] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "dppo"
                    ]
                ],
                "Method": [
                    [
                        "distributed prioritized experience replay"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "dqn",
                        "apex"
                    ]
                ],
                "Metric": [
                    [
                        "human normalized score",
                        "median humannormalized performance"
                    ]
                ],
                "Task": [
                    [
                        "atari games",
                        "atari"
                    ]
                ]
            }
        ]
    },
    "29": {
        "doctext": "document : graph ##2 ##se ##q : graph to sequence learning with attention - based neural networks the celebrated sequence to sequence learning ( se ##q ##2 ##se ##q ) technique and its numerous variants achieve excellent performance on many tasks . however , many machine learning tasks have inputs naturally represented as graphs ; existing se ##q ##2 ##se ##q models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence . to address this challenge , we introduce a novel general end - to - end graph - to - sequence neural en ##code ##r - deco ##der model that maps an input graph to a sequence of vectors and uses an attention - based l ##st ##m method to deco ##de the target sequence from these vectors . our method first generates the node and graph em ##bed ##ding ##s using an improved graph - based neural network with a novel aggregation strategy to incorporate edge direction information in the node em ##bed ##ding ##s . we further introduce an attention mechanism that align ##s node em ##bed ##ding ##s and the deco ##ding sequence to better cope with large graphs . experimental results on ba ##bi , shortest path , and natural language generation tasks demonstrate that our model achieve ##s state - of - the - art performance and significantly out ##per ##forms existing graph neural networks , se ##q ##2 ##se ##q , and tree ##2 ##se ##q models ; using the proposed bi - directional node em ##bed ##ding aggregation strategy , the model can converge rapidly to the optimal performance . section : introduction the celebrated sequence to sequence learning ( se ##q ##2 ##se ##q ) technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation ba ##hd ##ana ##u ##20 ##14 ##ne ##ural , ge ##hri ##ng ##20 ##17 ##con ##vo ##lu ##tion ##al , natural language generation ( nl ##g ) db ##lp : con ##f / ac ##l / song ##p ##z ##wg ##17 and speech recognition zhang ##20 ##17 ##very . most of the proposed se ##q ##2 ##se ##q models can be viewed as a family of en ##code ##r - deco ##ders db ##lp : con ##f / ni ##ps / su ##tsk ##ever ##v ##l ##14 , cho ##20 ##14 ##lea ##rn ##ing , ba ##hd ##ana ##u ##20 ##14 ##ne ##ural , where an en ##code ##r reads and en ##codes a source input in the form of sequences into a continuous vector representation of fixed dimension , and a deco ##der takes the encoded vectors and outputs a target sequence . many other enhancement ##s including bid ##ire ##ction ##al rec ##urrent neural networks ( bi - rn ##n ) schuster ##19 ##9 ##7 ##bid ##ire ##ction ##al or bid ##ire ##ction ##al long short - term memory networks ( bi - l ##st ##m ) graves ##200 ##5 ##frame ##wise as en ##code ##r , and attention mechanism ba ##hd ##ana ##u ##20 ##14 ##ne ##ural , lu ##ong ##20 ##15 ##ef ##fect ##ive , have been proposed to further improve its practical performance for general or domain - specific applications . despite their flexibility and expressive power , a significant limitation with the se ##q ##2 ##se ##q models is that they can only be applied to problems whose inputs are represented as sequences . however , the sequences are probably the simplest structured data , and many important problems are best expressed with a more complex structure such as graphs that have more capacity to en ##code complicated pair - wise relationships in the data . for example , one task in nl ##g applications is to translate a graph - structured semantic representation such as abstract meaning representation to a text expressing its meaning ban ##ares ##cu ##20 ##13 ##ab ##stra ##ct . in addition , path planning for a mobile robot hu ##200 ##4 ##k ##now ##led ##ge and path finding for question answering in ba ##bi task li ##20 ##15 ##gated can also be cast as graph - to - sequence problems . on the other hand , even if the raw inputs are originally expressed in a sequence form , it can still benefit from the enhanced inputs with additional information ( to formula ##te graph inputs ) . for example , for semantic par ##sing tasks ( text - to - am ##r or text - to - sql ) , they have been shown better performance by aug ##ment ##ing the original sentence sequences with other structural information such as dependency par ##sing trees pu ##st ##20 ##15 ##par ##sing . intuitive ##ly , the ideal solution for graph - to - sequence tasks is to build a more powerful en ##code ##r which is able to learn the input representation regardless of its inherent structure . to cope with graph - to - sequence problems , a simple and straightforward approach is to directly convert more complex structured graph data into sequences i ##yer ##20 ##16 ##sum ##mar ##izing , gomez ##20 ##16 ##au ##tom ##atic , liu ##20 ##17 ##ret ##ros ##yn ##the ##tic , and apply sequence models to the resulting sequences . however , the se ##q ##2 ##se ##q model often fails to perform as well as hoped on these problems , in part because it inevitably suffers significant information loss due to the conversion of complex structured data into a sequence , especially when the input data is naturally represented as graphs . recently , a line of research efforts have been devoted to incorporate additional information by extract ##ing syn ##ta ##ctic information such as the phrase structure of a source sentence ( tree ##2 ##se ##q ) er ##ig ##uchi ##20 ##16 ##tree , by utilizing attention mechanisms for input sets ( set ##2 ##se ##q ) vin ##yal ##s ##20 ##15 ##ord ##er , and by encoding sentences rec ##urs ##ively as trees soc ##her ##20 ##10 ##lea ##rn ##ing , tai ##20 ##15 ##im ##pro ##ved . although these methods achieve promising results on certain classes of problems , most of the presented techniques largely depend on the underlying application and may not be able to general ##ize to a broad class of problems in a general way . to address this issue , we propose graph ##2 ##se ##q , a novel general attention - based neural network model for graph - to - sequence learning . the graph ##2 ##se ##q model follows the conventional en ##code ##r - deco ##der approach with two main components , a graph en ##code ##r and a sequence deco ##der . the proposed graph en ##code ##r aims to learn expressive node em ##bed ##ding ##s and then to re ##asse ##mble them into the corresponding graph em ##bed ##ding ##s . to this end , inspired by a recent graph representation learning method hamilton ##20 ##17 ##ind ##uc ##tive , we propose an ind ##uc ##tive graph - based neural network to learn node em ##bed ##ding ##s from node attributes through aggregation of neighborhood information for directed and und ##ire ##cted graphs , which explores two distinct aggregator ##s on each node to yield two representations that are con ##cate ##nated to form the final node em ##bed ##ding . in addition , we further design an attention - based rn ##n sequence deco ##der that takes the graph em ##bed ##ding as its initial hidden state and outputs a target prediction by learning to align and translate jointly based on the context vectors associated with the corresponding nodes and all previous predictions . our code and data are available at . graph ##2 ##se ##q is simple yet general and is highly ex ##tens ##ible where its two building blocks , graph en ##code ##r and sequence deco ##der , can be replaced by other models such as graph con ##vo ##lu ##tion ##al ( attention ) networks ki ##pf ##20 ##16 ##se ##mi , ve ##lick ##ovic ##20 ##17 ##graph or their extensions sc ##hli ##cht ##kr ##ull ##20 ##17 ##mo ##del ##ing , and l ##st ##m hoc ##hre ##iter ##19 ##9 ##7 ##long . we highlight three main contributions of this paper as follows : we propose a novel general attention - based neural networks model to elegant ##ly address graph - to - sequence learning problems that learns a mapping between graph - structured inputs to sequence outputs , which current se ##q ##2 ##se ##q and tree ##2 ##se ##q may be inadequate to handle . we propose a novel graph en ##code ##r to learn a bi - directional node em ##bed ##ding ##s for directed and und ##ire ##cted graphs with node attributes by employing various aggregation strategies , and to learn graph - level em ##bed ##ding by exploit ##ing two different graph em ##bed ##ding techniques . equally importantly , we present an attention mechanism to learn the alignment ##s between nodes and sequence elements to better cope with large graphs . experimental results show that our model achieve ##s state - of - the - art performance on three recently introduced graph - to - sequence tasks and significantly out ##per ##forms existing graph neural networks , se ##q ##2 ##se ##q , and tree ##2 ##se ##q models . section : related work our model draws inspiration from the research fields of graph representation learning , neural networks on graphs , and neural en ##code ##r - deco ##der models . graph representation learning . graph representation learning has been proven extremely useful for a broad range of the graph - based analysis and prediction tasks , go ##yal ##20 ##17 ##graph . the main goal for graph representation learning is to learn a mapping that em ##bed ##s nodes as points in a low - dimensional vector space . these representation learning approaches can be roughly categorized into two classes including matrix factor ##ization - based algorithms and random - walk based methods . a line of research learn the em ##bed ##ding ##s of graph nodes through matrix factor ##ization rowe ##is ##200 ##0 ##non ##line ##ar , bel ##kin ##200 ##2 ##la ##pl ##ac ##ian , ahmed ##20 ##13 ##dis ##tri ##bu ##ted , cao ##20 ##15 ##gra ##re ##p , ou ##20 ##16 ##as ##ym ##metric . these methods directly train em ##bed ##ding ##s for individual nodes of training and testing data jointly and thus inherently trans ##ductive . another family of work is the use of random walk - based methods to learn low - dimensional em ##bed ##ding ##s of nodes by exploring neighborhood information for a single large - scale graph duran ##20 ##17 ##lean ##ring , hamilton ##20 ##17 ##ind ##uc ##tive , tang ##20 ##15 ##line , grover ##20 ##16 ##no ##de ##2 ##ve ##c , per ##oz ##zi ##20 ##14 ##dee ##pw ##al ##k , ve ##lick ##ovic ##20 ##17 ##graph . graphs ##age hamilton ##20 ##17 ##ind ##uc ##tive is such a technique that learns node em ##bed ##ding ##s through aggregation from a node local neighborhood using node attributes or degrees for ind ##uc ##tive learning , which has better capability to generate node em ##bed ##ding ##s for previously unseen data . our graph en ##code ##r is an extension to graphs ##age with two major distinctions . first , we non - trivial ##ly general ##ize it to cope with both directed and und ##ire ##cted graphs by splitting original node into forward nodes ( a node directs to ) and backward nodes ( direct to a node ) according to edge direction and applying two distinct aggregation functions to these types of nodes . second , we exploit two different schemes ( pool ##ing - based and super ##no ##de - based ) to re ##asse ##mble the learned node em ##bed ##ding ##s to generate graph em ##bed ##ding , which is not studied in graphs ##age . we show the advantages of our graph en ##code ##r over graphs ##age in our experiments . neural networks on graphs . over the past few years , there has been a surge of approaches that seek to learn the representations of graph nodes , or entire ( sub ) graphs , based on graph neural networks ( g ##nn ) that extend well - known network architecture ##s including rn ##n and cnn to graph data go ##ri ##200 ##5 ##ne ##w , scars ##elli ##200 ##9 ##graph , li ##20 ##15 ##gated , br ##una ##20 ##13 ##sp ##ect ##ral , du ##ven ##aud ##20 ##15 ##con ##vo ##lu ##tion ##al , ni ##ep ##ert ##20 ##16 ##lea ##rn ##ing , , yang ##20 ##16 ##re ##vis ##iting , ki ##pf ##20 ##16 ##se ##mi , chen ##20 ##18 ##fast ##gc ##n . a line of research is the neural networks that operate on graphs as a form of rn ##n go ##ri ##200 ##5 ##ne ##w , scars ##elli ##200 ##9 ##graph , and recently extended by li et al . li ##20 ##15 ##gated by introducing modern practices of rn ##n ( using of gr ##u updates ) in the original g ##nn framework . another important stream of work that has recently drawn fast increasing interest is graph con ##vo ##lu ##tion ##al networks ( g ##c ##n ) built on spectral graph theory , introduced by and then extended by with fast localized con ##vo ##lu ##tion . most of these approaches can not scale to large graphs , which is improved by using a localized first - order approximation of spectral graph con ##vo ##lu ##tion ki ##pf ##20 ##16 ##se ##mi and further e ##qui ##pping with important sampling for der ##iving a fast g ##c ##n chen ##20 ##18 ##fast ##gc ##n . the closely relevant work to our graph en ##code ##r is g ##c ##n ki ##pf ##20 ##16 ##se ##mi , which is designed for semi - supervised learning in trans ##ductive setting that requires full graph lap ##la ##cian to be given during training and is typically applicable to a single large und ##ire ##cted graph . an extension of g ##c ##n can be shown to be mathematical ##ly related to one variant of our graph en ##code ##r on und ##ire ##cted graphs . we compare the difference between our graph en ##code ##r and g ##c ##n in our experiments . another relevant work is gate ##d graph sequence neural networks ( g ##gs - n ##ns ) li ##20 ##15 ##gated . although it is also designed for output ##ting a sequence , it is essentially a prediction model that learns to predict a sequence embedded in graph while our approach is a genera ##tive model that learns a mapping between graph inputs and sequence outputs . a good analogy that can be drawn between our proposed graph ##2 ##se ##q and g ##gs - n ##ns is the relationship between con ##vo ##lu ##tion ##al se ##q ##2 ##se ##q and rn ##n . neural en ##code ##r - deco ##der models . one of the most successful en ##code ##r - deco ##der architecture ##s is the sequence to sequence learning db ##lp : con ##f / ni ##ps / su ##tsk ##ever ##v ##l ##14 , cho ##20 ##14 ##lea ##rn ##ing , ba ##hd ##ana ##u ##20 ##14 ##ne ##ural , lu ##ong ##20 ##15 ##ef ##fect ##ive , ge ##hri ##ng ##20 ##17 ##con ##vo ##lu ##tion ##al , which are originally proposed for machine translation . recently , the classical se ##q ##2 ##se ##q model and its variants have been applied to several applications in which these models can perform mapping ##s from objects to sequences , including mapping from an image to a sentence vin ##yal ##s ##20 ##15 ##sho ##w , models for computation map from problem statements of a python program to their solutions ( the answers to the program ) za ##rem ##ba ##20 ##14 ##lea ##rn ##ing , the traveling salesman problem for the set of points vin ##yal ##s ##20 ##15 ##point ##er and deep genera ##tive model for molecules generation from existing known molecules in drug discovery . it is easy to see that the objects that are mapped to sequences in the listed examples are often naturally represented in graphs rather than sequences . recently , many research efforts and the key contributions have been made to address the limitations of se ##q ##2 ##se ##q when dealing with more complex data , that leverage external information using specialized neural models attached to underlying targeted applications , including tree ##2 ##se ##q er ##ig ##uchi ##20 ##16 ##tree , set ##2 ##se ##q vin ##yal ##s ##20 ##15 ##ord ##er , rec ##urs ##ive neural networks soc ##her ##20 ##10 ##lea ##rn ##ing , and tree - structured l ##st ##m tai ##20 ##15 ##im ##pro ##ved . due to more recent advances in graph representations and graph con ##vo ##lu ##tion ##al networks , a number of research has investigated to utilize various g ##nn to improve the performance over the se ##q ##2 ##se ##q models in the domains of machine translation and graph generation bas ##ting ##s ##20 ##17 ##graph , beck ##20 ##18 ##graph , simon ##ovsky ##20 ##18 ##graph ##va ##e , li ##20 ##18 ##lea ##rn ##ing . there are several distinctions between these work and ours . first , our model is the first general - purpose en ##code ##r - deco ##der model for graph - to - sequence learning that is applicable to different applications while the aforementioned research has to utilize domain - specific information . second , we design our own graph em ##bed ##ding techniques for our graph deco ##der while most of other work directly apply existing g ##nn to their problems . section : graph - to - sequence model as shown in figure [ reference ] , our graph - to - sequence model includes a graph en ##code ##r , a sequence deco ##der , and a node attention mechanism . following the conventional en ##code ##r - deco ##der architecture , the graph en ##code ##r first generates node em ##bed ##ding ##s , and then construct ##s graph em ##bed ##ding ##s based on the learned node em ##bed ##ding ##s . finally , the sequence deco ##der takes both the graph em ##bed ##ding ##s and node em ##bed ##ding ##s as input and employs attention over the node em ##bed ##ding ##s whilst generating sequences . in this section , we first introduce the node - em ##bed ##ding generation algorithm which derives the bi - directional node em ##bed ##ding ##s by ag ##gre ##gating information from both forward and backward neighborhoods of a node in a graph . upon these node em ##bed ##ding ##s , we propose two methods for generating graph em ##bed ##ding ##s capturing the whole - graph information . sub ##section : node em ##bed ##ding generation inspired by , we design a new ind ##uc ##tive node em ##bed ##ding algorithm that generates bi - directional node em ##bed ##ding ##s by ag ##gre ##gating information from a node local forward and backward neighborhood within hop ##s for both directed and und ##ire ##cted graphs . in order to make it more clear , we take the em ##bed ##ding generation process for node as an example to explain our node em ##bed ##ding generation algorithm : we first transform node ' s text attribute to a feature vector , av , by looking up the em ##bed ##ding matrix w . note that for some tasks where ' s text attribute may be a word sequence , one neural network layer , such as an l ##st ##m layer , could be additionally used to generate av . we cat ##ego ##rize the neighbors of into forward neighbors , , and backward neighbors , , according to the edge direction . in particular , returns the nodes that directs to and returns the nodes that direct to ; we aggregate the forward representations of ' s forward neighbors { [UNK] - k ##1 , } into a single vector , [UNK] ( v ) k , where is the iteration index . in our experiments , we find that the aggregator choice , , may heavily affect the overall performance and we will discuss it later . notice that at iteration , this aggregator only uses the representations generated at . the initial forward representation of each node is its feature vector calculated in step ( 1 ) ; we con ##cate ##nate ' s current forward representation , [UNK] - k ##1 , with the newly generated neighborhood vector , [UNK] ( v ) k . this con ##cate ##nated vector is fed into a fully connected layer with nonlinear activation function , which updates the forward representation of , [UNK] , to be used at the next iteration ; we update the backward representation of , [UNK] , using the similar procedure as introduced in step ( 3 ) and ( 4 ) except that operating on the backward representations instead of the forward representations ; we repeat steps ( 3 ) ( 5 ) times , and the con ##cate ##nation of the final forward and backward representation is used as the final bi - directional representation of . since the neighbor information from different hop ##s may have different impact on the node em ##bed ##ding , we learn a distinct aggregator at each iteration . aggregator architecture ##s . since a node neighbors have no natural ordering , the aggregator function should be invariant to per ##mut ##ations of its inputs , ensuring that our neural network model can be trained and applied to ar ##bit ##rar ##ily ordered node - neighborhood feature sets . in practice , we examined the following three aggregator functions : mean aggregator : this aggregator function takes the element - wise mean of the vectors in { [UNK] - k ##1 , } and { [UNK] - k ##1 , } . l ##st ##m aggregator : similar to hamilton ##20 ##17 ##ind ##uc ##tive , we also examined a more complex aggregator based on an long short term memory ( l ##st ##m ) architecture . note that l ##st ##ms are not inherently symmetric since they process their inputs sequential ##ly . we use l ##st ##ms to operate on uno ##rder ##ed sets by simply applying them to a single random per ##mut ##ation of the node neighbors . pool ##ing aggregator : in this aggregator , each neighbor ' s vector is fed through a fully - connected neural network , and an element - wise max - pool ##ing operation is applied : where max denotes the element - wise max operator , and is a nonlinear activation function . by applying max - pool ##ing , the model can capture different information across the neighborhood set . sub ##section : graph em ##bed ##ding generation most existing works of graph con ##vo ##lu ##tion neural networks focus more on node em ##bed ##ding ##s rather than graph em ##bed ##ding ##s since their focus is on the node - wise classification task . however , graph em ##bed ##ding ##s that convey the entire graph information are essential to the downstream deco ##der . in this work , we introduce two approaches ( i . e . , pool ##ing - based and node - based ) to generate these graph em ##bed ##ding ##s from the node em ##bed ##ding ##s . pool ##ing - based graph em ##bed ##ding . in this approach , we investigated three pool ##ing techniques : max - pool ##ing , min - pool ##ing and average - pool ##ing . in our experiments , we fed the node em ##bed ##ding ##s to a fully - connected neural network and applied each pool ##ing method element - wise . we found no significant performance difference across the three different pool ##ing approaches ; we thus adopt the max - pool ##ing method as our default pool ##ing approach . node - based graph em ##bed ##ding . in this approach , we add one super node , , into the input graph , and all other nodes in the graph direct to . we use the aforementioned node em ##bed ##ding generation algorithm to generate the em ##bed ##ding of by ag ##gre ##gating the em ##bed ##ding ##s of the neighbor nodes . the em ##bed ##ding of that captures the information of all nodes is regarded as the graph em ##bed ##ding . sub ##section : attention based deco ##der the sequence deco ##der is a rec ##urrent neural network ( rn ##n ) that predict ##s the next token , given all the previous words , the rn ##n hidden state for time , and a context vector that directs attention to the en ##code ##r side . in particular , the context vector depends on a set of node representations ( , \u2026 , ) which the graph en ##code ##r maps the input graph to . each node representation contains information about the whole graph with a strong focus on the parts surrounding the - th node of the input graph . the context vector is computed as a weighted sum of these node representations and the weight of each node representation is computed by : where is an which scores how well the input node around position and the output at position match . the score is based on the rn ##n hidden state and the - th node representation of the input graph . we parameter ##ize the alignment model as a feed - forward neural network which is jointly trained with other components of the proposed system . our model is jointly trained to maximize the conditional log - probability of the correct description given a source graph . in the inference phase , we use the beam search to generate a sequence with the beam size = 5 . section : experiments we conduct experiments to demonstrate the effectiveness and efficiency of the proposed method . following the experimental settings in li ##20 ##15 ##gated , we firstly compare its performance with classical l ##st ##m , g ##gs - n ##n , and g ##c ##n based methods on two selected tasks including ba ##bi task 19 and the shortest path task . we then compare graph ##2 ##se ##q against other se ##q ##2 ##se ##q based methods on a real - world application - natural language generation task . note that the parameters of all baseline ##s are set based on performance on the development set . experimental settings . our proposed model is trained using the adam opt ##imi ##zer db ##lp : journals / co ##rr / king ##ma ##b ##14 , with mini - batch size 30 . the learning rate is set to 0 . 001 . we apply the drop ##out strategy db ##lp : journals / j ##ml ##r / sri ##vas ##ta ##vah ##ks ##s ##14 with a ratio of 0 . 5 at the deco ##der layer to avoid over ##fi ##tting . gradient ##s are clipped when their norm is bigger than 20 . for the graph en ##code ##r , the default hop size is set to 6 , the size of node initial feature vector is set to 40 , the non - linear ##ity function is re ##lu db ##lp : journals / j ##ml ##r / g ##lor ##ot ##bb ##11 , the parameters of aggregator ##s are randomly initial ##ized . the deco ##der has 1 layer and hidden state size is 80 . since graph ##2 ##se ##q with mean aggregator and pool ##ing - based graph em ##bed ##ding ##s generally performs better than other configurations ( we def ##er this discussion to sec . [ reference ] ) , we use this setting as our default model in the following sections . sub ##section : ba ##bi task 19 setup . the ba ##bi artificial intelligence ( ai ) tasks db ##lp : journals / co ##rr / weston ##bc ##m ##15 are designed to test reasoning capabilities that an ai system possesses . among these tasks , task 19 ( path finding ) is arguably the most challenging task ( see , e . g . , db ##lp : con ##f / ni ##ps / su ##kh ##ba ##ata ##rs ##w ##f ##15 which reports an accuracy of less than 20 % for all methods that do not use strong supervision ) . we apply the transformation procedure introduced in li ##20 ##15 ##gated to transform the description as a graph as shown in figure [ reference ] . the left part shows an instance of ba ##bi task 19 : given a set of sentences describing the relative geographical positions for a pair of objects and , we aim to find the geographical path between and . the question is then treated as finding the shortest path between two nodes , and , which represent and in the graph . to tackle this problem with graph ##2 ##se ##q , we ann ##ota ##te with text attribute start and with text attribute end . for other nodes , we assign their id ##s in the graph as their text attributes . it is worth noting that , in our model , the start and end token ##s are node features whose vector representations are first randomly initial ##ized and then learned by the model later . in contrast , in g ##gs - n ##n , the vector representations of staring and end nodes are set as one - hot vectors , which is specially designed for the shortest path task . to aggregate the edge information into the node em ##bed ##ding , for each edge , we additionally add a node representing this edge into the graph and assign the edge ' s text as its text attribute . we generate 1000 training examples , 1000 development examples and 1000 test examples where each example is a graph - path pair . we use a standard l ##st ##m model hoc ##hre ##iter ##19 ##9 ##7 ##long and g ##gs - n ##n li ##20 ##15 ##gated as our baseline ##s . since g ##c ##n ki ##pf ##20 ##16 ##se ##mi itself can not output a sequence , we also create a baseline that combines g ##c ##n with our sequence deco ##der . figure ##path finding example . results . from table [ reference ] , we can see that the l ##st ##m model fails on this task while our model makes perfect predictions , which under ##lines the importance of the use of graph en ##code ##r to directly en ##code a graph instead of using sequence model on the converted inputs from a graph . comparing to g ##gs - n ##n that uses carefully designed initial em ##bed ##ding ##s for different types of nodes such as start and end , our model uses a purely end - to - end approach which generates the initial node feature vectors based on random initial ##ization of the em ##bed ##ding ##s for words in text attributes . however , we still significantly out ##per ##form g ##gs - n ##n , demonstrating the expressive power of our graph en ##code ##r that considers information flows in both forward and backward directions . we observe similar results when comparing our whole graph ##2 ##se ##q model to g ##c ##n with our deco ##der , which mainly because the current form of g ##c ##n ki ##pf ##20 ##16 ##se ##mi is designed for und ##ire ##cted graph and thus may have information loss when converting directed graph to und ##ire ##cted one as suggested in ki ##pf ##20 ##16 ##se ##mi . sub ##section : shortest path task setup . we further evaluate our model on the shortest path ( sp ) task whose goal is to find the shortest directed path between two nodes in a graph , introduced in li ##20 ##15 ##gated . for this task , we created data ##set ##s by generating random graphs , and choosing pairs random nodes a and b which are connected by a unique shortest directed path . since we can control the size of generated graphs , we can easily test the performance changes of each model when increasing the size of graphs as well . two such data ##set ##s , sp - s and sp - l , were created , containing s mall ( node size = 5 ) and l ar ##ge graphs ( node size = 100 ) , respectively . we restricted the length of the generated shortest paths for sp - s to be at least 2 and at least 4 for sp - l . for each data ##set , we used 1000 training examples and 1000 development examples for parameter tuning , and evaluated on 1000 test examples . we choose the same baseline ##s as introduced in the previous section . results . table [ reference ] shows that the l ##st ##m model still fails on both of these two data ##set ##s . our graph ##2 ##se ##q model achieve ##s comparable performance with g ##gs - n ##n that both models could achieve 100 % accuracy on the sp - s data ##set while achieve ##s much better on larger graphs on the sp - l data ##set . this is because our graph en ##code ##r is more expressive in learning the graph structural information with our dual - direction aggregator ##s , which is the key to maintaining good performance when the graph size grows larger , while the performance of g ##gs - n ##n significantly de ##grade ##s due to hardness of capturing the long - range dependence in a graph with large size . compared to g ##c ##n , it achieve ##s better performance than g ##gs - n ##n but still much lower than our graph ##2 ##se ##q , in part because of both the poor effectiveness of graph en ##code ##r and inca ##pa ##bility of handling with directed graph . sub ##section : natural language generation task setup . we finally evaluate our model on a real - world application - natural language generation ( nl ##g ) task where we translate a structured semantic representation \u2014 in this case a structured query language ( sql ) query \u2014 to a natural language description expressing its meaning . as indicated in db ##lp : journals / is / sp ##ili ##op ##ou ##lou ##h ##9 ##2 , the structure of sql query is essentially a graph . thus we naturally cast this task as an application of the graph - to - sequence model which takes a graph representing the semantic structure as input and outputs a sequence . figure [ reference ] illustrates the process of translation of an sql query to a corresponding natural language description via our graph ##2 ##se ##q model . we use the b ##le ##u - 4 score to evaluate our model on the wi ##kis ##q ##l data ##set z ##hong ##se ##q ##2 ##s ##q ##l ##20 ##17 , a corpus of 87 , 72 ##6 hand - ann ##ota ##ted instances of natural language questions , sql que ##ries , and sql tables . wi ##kis ##q ##l was created as the bench ##mark data ##set for the table - based question answering task ( for which the state - of - the - art performance is 82 . 6 % execution accuracy yu ##20 ##18 ##type ##s ##q ##l ) ; here we reverse the use of the data ##set , treating the sql query as the input and having the goal of generating the correct english question . these wi ##kis ##q ##l sql que ##ries are split into training , development and test sets , which contain 61 ##29 ##7 que ##ries , 91 ##45 que ##ries and 1728 ##4 que ##ries , respectively . since the sql - to - text task can be cast as \" machine translation \" type of problems , we implemented several baseline ##s to address this task . the first one is an attention - based sequence - to - sequence ( se ##q ##2 ##se ##q ) model proposed by ba ##hd ##ana ##u ##20 ##14 ##ne ##ural ; the second one additionally introduces the copy mechanism in the deco ##der side gu ##20 ##16 ##in ##corp ##ora ##ting ; the third one is a tree - to - sequence ( tree ##2 ##se ##q ) model proposed by er ##ig ##uchi ##20 ##16 ##tree as our baseline ; the fourth one is to combine a g ##c ##n ki ##pf ##20 ##16 ##se ##mi with our pg ##e graph em ##bed ##ding ##s with our sequence deco ##der ; the fifth one is to combine a g ##gs - n ##n li ##20 ##15 ##gated with our sequence deco ##der . to apply these baseline ##s , we convert an sql query to a sequence or a tree using some template ##s which we discuss in detail in the appendix . results . from table [ reference ] , we can see that our graph ##2 ##se ##q model performs significantly better than the se ##q ##2 ##se ##q , tree ##2 ##se ##q , and graph ##2 ##se ##q baseline ##s . this result is expected since the structure of sql query is essentially a graph despite its expressions in sequence and a graph en ##code ##r is able to capture much more information directly in graph . among all graph ##2 ##se ##q models , our graph ##2 ##se ##q model performed best , in part due to a more effective graph en ##code ##r . tree ##2 ##se ##q achieve ##s better performance compared to se ##q ##2 ##se ##q since its tree - based en ##code ##r explicitly takes the syn ##ta ##ctic structure of a sql query into consideration . two variants of the graph ##2 ##se ##q models can substantially out ##per ##form tree ##2 ##se ##q , which demonstrates that a general graph to sequence model that is independent of different structural information in complex data is very useful . interesting ##ly , we also observe that graph ##2 ##se ##q - pg ##e ( pool ##ing - based graph em ##bed ##ding ) performs better than graph ##2 ##se ##q - ng ##e ( node - based graph em ##bed ##ding ) . one potential reason is that the node - based graph em ##bed ##ding method artificial ##ly added a super node in graph which changes the original graph topology and brings unnecessary noise into the graph . figure ##a running example of the nl ##g task . sub ##section : impacts of aggregator , hop size and attention mechanism on ga ##rp ##h ##2 ##se ##q model setup . we now investigate the impact of the aggregator and the hop size on the graph ##2 ##se ##q model . following the previous sp task , we further create three synthetic data ##set ##s : i ) sd ##pd ##ag whose graphs are directed ac ##y ##cl ##ic graphs ( da ##gs ) ; ii ) sd ##pd ##c ##g whose graphs are directed cyclic graphs ( dc ##gs ) that always contain cycles ; iii ) sd ##pse ##q whose graphs are essentially sequential lines . for each data ##set , we randomly generated 1000 ##0 graphs with the graph size 100 and split them as 800 ##0 / 1000 / 1000 for the training / development / test set . for each graph , we generated an sd ##p query by choosing two random nodes with the constraints that there should be a unique shortest path connecting these two nodes , and that its length should be at least 4 . we create six variants of the graph ##2 ##se ##q model coupling with different aggregation strategies in the node em ##bed ##ding generation . the first three ( graph ##2 ##se ##q - ma , - la , - pa ) use the m ea ##n a g ##gre ##gat ##or , l st ##m a g ##gre ##gat ##or and p o ##olin ##g a g ##gre ##gat ##or to aggregate node neighbor information , respectively . unlike these three models that aggregate the information of both forward and backward nodes , the other two models ( graph ##2 ##se ##q - ma - f , - ma - b ) only consider one - way information ag ##gre ##gating the information from the forward nodes or the information from the backward nodes with the mean aggregator , respectively . we use the path accuracy to evaluate these models . the hop size is set to 10 . impacts of the aggregator . table [ reference ] shows that on the sd ##p data ##set , both graph ##2 ##se ##q - ma and graph ##2 ##se ##q - pa achieve the best performance . on more complicated structured data , such as sd ##p and sd ##p , graph ##2 ##se ##q - ma ( our default model ) also performs better than other variants . we can also see that graph ##2 ##se ##q - ma performs better than graph ##2 ##se ##q - ma - f and graph ##2 ##se ##q - ma - b on sd ##p and sd ##p since it captures more information from both directions to learn better node em ##bed ##ding ##s . however , graph ##2 ##se ##q - ma - f and graph ##2 ##se ##q - ma - b achieve comparable performance to graph ##2 ##se ##q - ma on sd ##p . this is because in almost 95 % of the graphs , 90 % of the nodes could reach each other by travers ##ing the graph for a given hop size , which dramatically restore ##s its information loss . figure ##test results on sd ##p . impact of hop size . to study the impact of the hop size , we create a sd ##p data ##set , sd ##p and results are shown in figure [ reference ] . we see that the performance of all variants of graph ##2 ##se ##q converge ##s to its optimal performance when increasing the number of hop size . specifically , graph ##2 ##se ##q - ma achieve ##s significantly better performance than its counterparts considering only one direction propagation , especially when the hop size is small . as the hop size increases , the performance differences dim ##ini ##sh . this is the desired property since graph ##2 ##se ##q - ma can use much smaller hop size ( about the half ) to achieve the same performance of graph ##2 ##se ##q - ma - f or graph ##2 ##se ##q - ma - b with a larger size . this is particularly useful for large graphs where increasing hop size may need considerable computing resources and long run - time . we also compare graph ##2 ##se ##q with g ##c ##n , where the hop size means the number of layers in the settings of g ##c ##n . surprisingly , even graph ##2 ##se ##q - ma - f or graph ##2 ##se ##q - ma - b can significantly out ##per ##form g ##c ##n with the same hope size despite its rough equivalence between these two architecture ##s . it again illustrates the importance of the methods that could take into account both directed and und ##ire ##cted graphs . for additional experimental results on the impact of hop size for graphs of different sizes , please refer to the table 4 in appendix c . impact of attention mechanism . to investigate the impact of attention mechanism to the graph ##2 ##se ##q model , we still evaluate our model on sd ##p , sd ##p and sd ##p data ##set ##s but without considering the attention strategy . as shown in table 4 , we find that the attention strategy significantly improves the performance of all variants of graph ##2 ##se ##q by at least 14 . 9 % . this result is expected since for larger graphs it is more difficult for the en ##code ##r to com ##press all necessary information into a fixed - length vector ; as intended , applying the attention mechanism in deco ##ding enabled our proposed graph ##2 ##se ##q model to successfully handle large graphs . section : conclusion in this paper , we study the graph - to - sequence problem , introducing a new general and flexible graph ##2 ##se ##q model that follows the en ##code ##r - deco ##der architecture . we showed that , using our proposed bi - directional node em ##bed ##ding aggregation strategy , the graph en ##code ##r could successfully learn representations for three representative classes of directed graph , i . e . , directed ac ##y ##cl ##ic graphs , directed cyclic graphs and sequence - styled graphs . experimental results on three tasks demonstrate that our model significantly out ##per ##forms existing graph neural networks , se ##q ##2 ##se ##q , and tree ##2 ##se ##q baseline ##s on both synthetic and real application data ##set ##s . we also showed that introducing an attention mechanism over node representation into the deco ##ding substantially enhance ##s the ability of our model to produce correct target sequences from large graphs . since much symbolic data is represented as graphs and many tasks express their desired outputs as sequences , we expect graph ##2 ##se ##q to be broadly applicable to un ##ify symbolic ai and beyond . bibliography : references appendix : pseudo - code of the graph - to - sequence algorithm [ h ] node em ##bed ##ding generation algorithm { algorithm ##ic } [ 1 ] graph ##g ( v , e ) ; node initial feature vector av , [UNK] ; hop ##s k ; weight matrices w ##k , [UNK] { 1 , \u2026 , k } ; non - linear ##ity ##\u03c3 ; aggregator functions [UNK] , [UNK] , [UNK] { 1 , \u2026 , k } ; neighborhood functions [UNK] , [UNK] vector representations z ##v for all ##\u2208 ##v ##v ##\u2190 av , [UNK] av , [UNK] [UNK] ( { [UNK] - k ##1 , [UNK] ( v ) } ) \u2190 ##\u03c3 ( w ##\u22c5 ##k con ##cat ( [UNK] - k ##1 , [UNK] ( v ) k ) ) [UNK] ( v ) k ##\u2190 [UNK] ( { [UNK] - k ##1 , [UNK] ( v ) } ) \u2190 ##\u03c3 ( w ##\u22c5 ##k con ##cat ( [UNK] - k ##1 , [UNK] ( v ) k ) ) \u2190 con ##cat ( [UNK] , [UNK] ) , [UNK] algorithm [ reference ] describes the em ##bed ##ding generation process where the entire graph and initial feature vectors for all nodes av , , are provided as input . here denotes the current hop in the outer loop . the [UNK] denotes node ' s forward representation which aggregate ##s the information of nodes in . similarly , the [UNK] denotes node ' s backward representation which is generated by ag ##gre ##gating the information of nodes in . each step in the outer loop of algorithm [ reference ] proceeds as follows . first , each node in a graph aggregate ##s the forward representations of the nodes in its immediate neighborhood , { [UNK] - k ##1 , } , into a single vector , [UNK] ( v ) k ( line 5 ) . note that this aggregation step depends on the representations generated at the previous iteration of the outer loop , , and the forward representations are defined as the input node feature vector . after ag ##gre ##gating the neighboring feature vectors , we con ##cate ##nate the node current forward representation , [UNK] - k ##1 , with the aggregate ##d neighborhood vector , [UNK] ( v ) k . then this con ##cate ##nated vector is fed through a fully connected layer with nonlinear activation function , which updates the forward representation of the current node to be used at the next step of the algorithm ( line 6 ) . we apply similar process to generate the backward representations of the nodes ( line 7 , 8 ) . finally , the representation of each node z ##v is the con ##cate ##nation of the forward representation ( i . e . , [UNK] ) and the backward representation ( i . e . , [UNK] ) at the last iteration . appendix : structured representation of the sql query to apply graph ##2 ##se ##q , se ##q ##2 ##se ##q and tree ##2 ##se ##q models on the natural language generation task , we need to convert the sql query to a graph , sequence and tree , respectively . in this section , we describe these representations of the sql query . sub ##section : sequence representation we apply a simple template to construct the sql query sequence : \" select + aggregation function > + split symbol + selected column > + where + condition ##0 > + split symbol + condition ##1 > + \u2026 \" . sub ##section : tree representation we apply the sql par ##ser tool to convert an sql query to a tree which is illustrated in figure [ reference ] . specifically , the root of this tree has two child nodes , namely select list and where clause . the child nodes of select list node are the selected columns in the sql query . the where clause node has all occurred logical operators in the sql query as its children . the children of a logical operator node are the columns on which this operator works . sub ##section : graph representation we use the following method to transform the sql query to a graph : select clause . for the select clause such as \" select company \" , we first create a node assigned with text attribute select . this select node connects with column nodes whose text attributes are the selected column names such as company . for the sql que ##ries that contain aggregation functions such as count or max , we add one aggregation node which is connected with the column node \u2014 their text attributes are the aggregation function names . where clause . the where clause usually contains more than one condition . for each condition , we use the same process as for the select clause to create nodes . for example , in figure [ reference ] , we create node assets and for the first condition , the node sales and for the second condition . we then integrate the constraint nodes that have the same text attribute ( e . g . , in figure [ reference ] ) . for a logical operator such as and , or and not , we create a node that connects with all column nodes that the operator works on ( e . g . , and in figure [ reference ] ) . these logical operator nodes then connect with select node . appendix : more results on the impact of hop size in algorithm [ reference ] , we can see that there are three key factors in the node em ##bed ##ding generation . the first factor is the aggregator choice which determines how information from neighborhood nodes is combined . the other two are the hop size ( ) and the neighborhood function ( , ) , which together determine which neighbor nodes should be aggregate ##d to generate each node em ##bed ##ding . to study the impact of the hop size in our model , we create two sd ##p data ##set ##s , sd ##p and sd ##p , where each graph has 100 nodes or 1000 nodes , respectively . both of these two data ##set ##s contain 800 ##0 training examples , 1000 dev examples and 1000 test examples . we evaluated three models , graph ##2 ##se ##q - ma - f , graph ##2 ##se ##q - ma - b and graph ##2 ##se ##q - ma , on these two data ##set ##s ; results are listed in table [ reference ] . we see that graph ##2 ##se ##q - ma - f and graph ##2 ##se ##q - ma - b could show significant performance improvements with increasing the hop size . specifically , on the sd ##p data ##set , graph ##2 ##se ##q - ma - f and graph ##2 ##se ##q - ma - b achieve their best performance when the hop size reaches 7 ; further increases do not improve the overall performance . a similar situation is also observed on the sd ##p data ##set ; performance converge ##s at the hop size of 85 . interesting ##ly , the average diameter ##s of the graphs in the two data ##set ##s are 6 . 8 and 80 . 2 , respectively , suggesting that the ideal hop size for best graph ##2 ##se ##q - ma - f performance should be the graph diameter . this should not be surprising ; if the hop size equals the graph diameter , each node is guaranteed to aggregate the information of all reach ##able nodes on the graph within its em ##bed ##ding . note that in the experiments on sd ##p , in the ( \u00bf 10 ) hop , we always use the aggregator in the 10 - th hop , because introducing too many aggregator ##s ( i . e . , parameters ) may make the model over - fitting . like graph ##2 ##se ##q - ma - f , graph ##2 ##se ##q - ma also benefited from increasing the hop size . however , on both data ##set ##s , graph ##2 ##se ##q - ma could reach peak performance at a smaller hop size than graph ##2 ##se ##q - ma - f . for example , on the sd ##p data ##set , graph ##2 ##se ##q - ma achieve ##s 99 . 2 % accuracy once the hop size is greater than 4 while graph ##2 ##se ##q - ma - f requires a hop size greater than 7 to achieve comparable accuracy ; similar observations hold for the sd ##p data ##set . moreover , we can see that the minimum required hop size that graph ##2 ##se ##q - ma could achieve its best performance is approximately the average ra ##di ##i ( c . f . diameter ) of the graphs , which are 3 . 4 and 40 . 1 , respectively . recall that the main difference between graph ##2 ##se ##q - ma and graph ##2 ##se ##q - ma - f ( or graph ##2 ##se ##q - ma - b ) lies in whether the system aggregate ##s information prop ##aga ##ted from backward nodes ; the performance difference indicates that by incorporating forward and backward nodes ' information , it is possible for the model to achieve the best performance by travers ##ing less of the graph . this is useful in practice , especially for large graphs where increasing hop size may consume considerable computing resources and run - time . table [ reference ] also makes clear the utility of the attention strategy ; the performance of both graph ##2 ##se ##q - ma - f and graph ##2 ##se ##q - ma decreases by at least 9 . 8 % on sd ##p and 14 . 9 % on sd ##p . this result is expected , since for larger graphs it is more difficult for the en ##code ##r to com ##press all necessary information into a fixed - length vector ; as intended , applying the attention mechanism in deco ##ding enabled our proposed graph ##2 ##se ##q model to handle large graphs successfully . as shown in algorithm [ reference ] , the neighborhood function takes a given node as input and returns its directly connected neighbor nodes , which are then fed to the node em ##bed ##ding generator . intuitive ##ly , to obtain a better representation of a node , this function should return all its neighbor nodes in the graph . however , this may result in high training times on large graphs . to address this , hamilton ##20 ##17 ##ind ##uc ##tive proposes a sampling method which randomly selects a fixed number of neighbor nodes from which to aggregate information at each hop . we use this sampling method to manage the neighbor node size at each aggregation step .",
        "pred_seq": "ba ##bi [SEP] graph ##q [SEP] [SEP] neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural neural machine machine machine machine machine machine machine machine machine machine machine machine machine translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation translation [CLS]",
        "pred_templates": [],
        "gold_templates": [
            {
                "Material": [
                    [
                        "wikisql dataset",
                        "wikisql"
                    ]
                ],
                "Method": [
                    [
                        "graph2seqpge"
                    ]
                ],
                "Metric": [
                    [
                        "bleu4 score"
                    ]
                ],
                "Task": [
                    [
                        "sqltotext task"
                    ]
                ]
            }
        ]
    },
    "30": {
        "doctext": "document : you only look once : unified , real - time object detection we present yo ##lo , a new approach to object detection . prior work on object detection rep ##ur ##pose ##s class ##ifiers to perform detection . instead , we frame object detection as a regression problem to spatial ##ly separated bound ##ing boxes and associated class pro ##ba ##bilities . a single neural network predict ##s bound ##ing boxes and class pro ##ba ##bilities directly from full images in one evaluation . since the whole detection pipeline is a single network , it can be opt ##imi ##zed end - to - end directly on detection performance . our unified architecture is extremely fast . our base yo ##lo model processes images in real - time at 45 frames per second . a smaller version of the network , fast yo ##lo , processes an as ##tou ##nding 155 frames per second while still achieving double the map of other real - time detectors . compared to state - of - the - art detection systems , yo ##lo makes more local ##ization errors but is less likely to predict false positive ##s on background . finally , yo ##lo learns very general representations of objects . it out ##per ##forms other detection methods , including d ##pm and r - cnn , when general ##izing from natural images to other domains like artwork . section : introduction humans glance at an image and instantly know what objects are in the image , where they are , and how they interact . the human visual system is fast and accurate , allowing us to perform complex tasks like driving with little conscious thought . fast , accurate algorithms for object detection would allow computers to drive cars without specialized sensors , enable assist ##ive devices to convey real - time scene information to human users , and unlock the potential for general purpose , responsive robotic systems . current detection systems rep ##ur ##pose class ##ifiers to perform detection . to detect an object , these systems take a class ##ifier for that object and evaluate it at various locations and scales in a test image . systems like def ##or ##mable parts models ( d ##pm ) use a sliding window approach where the class ##ifier is run at evenly spaced locations over the entire image . more recent approaches like r - cnn use region proposal methods to first generate potential bound ##ing boxes in an image and then run a class ##ifier on these proposed boxes . after classification , post - processing is used to ref ##ine the bound ##ing boxes , eliminate duplicate detection ##s , and res ##core the boxes based on other objects in the scene . these complex pipeline ##s are slow and hard to opt ##imi ##ze because each individual component must be trained separately . we ref ##ram ##e object detection as a single regression problem , straight from image pixels to bound ##ing box coordinates and class pro ##ba ##bilities . using our system , you only look once ( yo ##lo ) at an image to predict what objects are present and where they are . yo ##lo is refreshing ##ly simple : see figure [ reference ] . a single con ##vo ##lu ##tion ##al network simultaneously predict ##s multiple bound ##ing boxes and class pro ##ba ##bilities for those boxes . yo ##lo trains on full images and directly opt ##imi ##zes detection performance . this unified model has several benefits over traditional methods of object detection . first , yo ##lo is extremely fast . since we frame detection as a regression problem we do n ' t need a complex pipeline . we simply run our neural network on a new image at test time to predict detection ##s . our base network runs at 45 frames per second with no batch processing on a titan x gp ##u and a fast version runs at more than 150 f ##ps . this means we can process streaming video in real - time with less than 25 mill ##ise ##con ##ds of late ##ncy . furthermore , yo ##lo achieve ##s more than twice the mean average precision of other real - time systems . for a demo of our system running in real - time on a web ##cam please see our project web ##page : . second , yo ##lo reasons globally about the image when making predictions . unlike sliding window and region proposal - based techniques , yo ##lo sees the entire image during training and test time so it implicit ##ly en ##codes context ##ual information about classes as well as their appearance . fast r - cnn , a top detection method , mistakes background patches in an image for objects because it ca n ' t see the larger context . yo ##lo makes less than half the number of background errors compared to fast r - cnn . third , yo ##lo learns general ##iza ##ble representations of objects . when trained on natural images and tested on artwork , yo ##lo out ##per ##forms top detection methods like d ##pm and r - cnn by a wide margin . since yo ##lo is highly general ##iza ##ble it is less likely to break down when applied to new domains or unexpected inputs . yo ##lo still la ##gs behind state - of - the - art detection systems in accuracy . while it can quickly identify objects in images it struggles to precisely local ##ize some objects , especially small ones . we examine these trade ##offs further in our experiments . all of our training and testing code is open source . a variety of pre ##train ##ed models are also available to download . section : unified detection we un ##ify the separate components of object detection into a single neural network . our network uses features from the entire image to predict each bound ##ing box . it also predict ##s all bound ##ing boxes across all classes for an image simultaneously . this means our network reasons globally about the full image and all the objects in the image . the yo ##lo design enables end - to - end training and real - time speeds while maintaining high average precision . our system divides the input image into an grid . if the center of an object falls into a grid cell , that grid cell is responsible for detecting that object . each grid cell predict ##s bound ##ing boxes and confidence scores for those boxes . these confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predict ##s . formally we define confidence as . if no object exists in that cell , the confidence scores should be zero . otherwise we want the confidence score to equal the intersection over union ( io ##u ) between the predicted box and the ground truth . each bound ##ing box consists of 5 predictions : , , , , and confidence . the coordinates represent the center of the box relative to the bounds of the grid cell . the width and height are predicted relative to the whole image . finally the confidence prediction represents the io ##u between the predicted box and any ground truth box . each grid cell also predict ##s conditional class pro ##ba ##bilities , . these pro ##ba ##bilities are conditioned on the grid cell containing an object . we only predict one set of class pro ##ba ##bilities per grid cell , regardless of the number of boxes . at test time we multi ##ply the conditional class pro ##ba ##bilities and the individual box confidence predictions , which gives us class - specific confidence scores for each box . these scores en ##code both the probability of that class appearing in the box and how well the predicted box fits the object . for evaluating yo ##lo on pascal vo ##c , we use , . pascal vo ##c has 20 labelled classes so . our final prediction is a tensor . sub ##section : network design we implement this model as a con ##vo ##lu ##tion ##al neural network and evaluate it on the pascal vo ##c detection data ##set . the initial con ##vo ##lu ##tion ##al layers of the network extract features from the image while the fully connected layers predict the output pro ##ba ##bilities and coordinates . our network architecture is inspired by the google ##net model for image classification . our network has 24 con ##vo ##lu ##tion ##al layers followed by 2 fully connected layers . instead of the inception modules used by google ##net , we simply use reduction layers followed by con ##vo ##lu ##tion ##al layers , similar to lin et al . the full network is shown in figure [ reference ] . we also train a fast version of yo ##lo designed to push the boundaries of fast object detection . fast yo ##lo uses a neural network with fewer con ##vo ##lu ##tion ##al layers ( 9 instead of 24 ) and fewer filters in those layers . other than the size of the network , all training and testing parameters are the same between yo ##lo and fast yo ##lo . the final output of our network is the tensor of predictions . sub ##section : training we pre ##train our con ##vo ##lu ##tion ##al layers on the image ##net 1000 - class competition data ##set . for pre ##train ##ing we use the first 20 con ##vo ##lu ##tion ##al layers from figure [ reference ] followed by a average - pool ##ing layer and a fully connected layer . we train this network for approximately a week and achieve a single crop top - 5 accuracy of 88 % on the image ##net 2012 validation set , comparable to the google ##net models in caf ##fe ' s model zoo . we use the dark ##net framework for all training and inference . we then convert the model to perform detection . ren et al . show that adding both con ##vo ##lu ##tion ##al and connected layers to pre ##train ##ed networks can improve performance . following their example , we add four con ##vo ##lu ##tion ##al layers and two fully connected layers with randomly initial ##ized weights . detection often requires fine - grain ##ed visual information so we increase the input resolution of the network from to . our final layer predict ##s both class pro ##ba ##bilities and bound ##ing box coordinates . we normal ##ize the bound ##ing box width and height by the image width and height so that they fall between 0 and 1 . we para ##met ##rize the bound ##ing box and coordinates to be offset ##s of a particular grid cell location so they are also bounded between 0 and 1 . we use a linear activation function for the final layer and all other layers use the following leak ##y rec ##ti ##fied linear activation : we opt ##imi ##ze for sum - squared error in the output of our model . we use sum - squared error because it is easy to opt ##imi ##ze , however it does not perfectly align with our goal of maxim ##izing average precision . it weights local ##ization error equally with classification error which may not be ideal . also , in every image many grid cells do not contain any object . this pushes the \" confidence \" scores of those cells towards zero , often over ##powering the gradient from cells that do contain objects . this can lead to model instability , causing training to diver ##ge early on . to remedy this , we increase the loss from bound ##ing box coordinate predictions and decrease the loss from confidence predictions for boxes that do n ' t contain objects . we use two parameters , and to accomplish this . we set and . sum - squared error also equally weights errors in large boxes and small boxes . our error metric should reflect that small deviation ##s in large boxes matter less than in small boxes . to partially address this we predict the square root of the bound ##ing box width and height instead of the width and height directly . yo ##lo predict ##s multiple bound ##ing boxes per grid cell . at training time we only want one bound ##ing box predict ##or to be responsible for each object . we assign one predict ##or to be \" responsible \" for predicting an object based on which prediction has the highest current io ##u with the ground truth . this leads to specialization between the bound ##ing box predict ##ors . each predict ##or gets better at predicting certain sizes , aspect ratios , or classes of object , improving overall recall . during training we opt ##imi ##ze the following , multi - part loss function : where denotes if object appears in cell and denotes that the th bound ##ing box predict ##or in cell is \" responsible \" for that prediction . note that the loss function only penal ##izes classification error if an object is present in that grid cell ( hence the conditional class probability discussed earlier ) . it also only penal ##izes bound ##ing box coordinate error if that predict ##or is \" responsible \" for the ground truth box ( i . e . has the highest io ##u of any predict ##or in that grid cell ) . we train the network for about 135 epoch ##s on the training and validation data sets from pascal vo ##c 2007 and 2012 . when testing on 2012 we also include the vo ##c 2007 test data for training . throughout training we use a batch size of 64 , a momentum of and a decay of . our learning rate schedule is as follows : for the first epoch ##s we slowly raise the learning rate from to . if we start at a high learning rate our model often diver ##ges due to unstable gradient ##s . we continue training with for 75 epoch ##s , then for 30 epoch ##s , and finally for 30 epoch ##s . to avoid over ##fi ##tting we use drop ##out and extensive data aug ##ment ##ation . a drop ##out layer with rate = . 5 after the first connected layer prevents co - adaptation between layers . for data aug ##ment ##ation we introduce random scaling and translations of up to 20 % of the original image size . we also randomly adjust the exposure and sat ##uration of the image by up to a factor of in the hs ##v color space . sub ##section : inference just like in training , predicting detection ##s for a test image only requires one network evaluation . on pascal vo ##c the network predict ##s 98 bound ##ing boxes per image and class pro ##ba ##bilities for each box . yo ##lo is extremely fast at test time since it only requires a single network evaluation , unlike class ##ifier - based methods . the grid design enforce ##s spatial diversity in the bound ##ing box predictions . often it is clear which grid cell an object falls in to and the network only predict ##s one box for each object . however , some large objects or objects near the border of multiple cells can be well localized by multiple cells . non - maximal suppression can be used to fix these multiple detection ##s . while not critical to performance as it is for r - cnn or d ##pm , non - maximal suppression adds 2 - 3 % in map . sub ##section : limitations of yo ##lo yo ##lo impose ##s strong spatial constraints on bound ##ing box predictions since each grid cell only predict ##s two boxes and can only have one class . this spatial constraint limits the number of nearby objects that our model can predict . our model struggles with small objects that appear in groups , such as flock ##s of birds . since our model learns to predict bound ##ing boxes from data , it struggles to general ##ize to objects in new or unusual aspect ratios or configurations . our model also uses relatively coarse features for predicting bound ##ing boxes since our architecture has multiple downs ##amp ##ling layers from the input image . finally , while we train on a loss function that approximate ##s detection performance , our loss function treats errors the same in small bound ##ing boxes versus large bound ##ing boxes . a small error in a large box is generally benign but a small error in a small box has a much greater effect on io ##u . our main source of error is incorrect local ##izations . section : comparison to other detection systems object detection is a core problem in computer vision . detection pipeline ##s generally start by extract ##ing a set of robust features from input images ( ha ##ar , si ##ft , hog , con ##vo ##lu ##tion ##al features ) . then , class ##ifiers or local ##izer ##s are used to identify objects in the feature space . these class ##ifiers or local ##izer ##s are run either in sliding window fashion over the whole image or on some subset of regions in the image . we compare the yo ##lo detection system to several top detection framework ##s , highlighting key similarities and differences . def ##or ##mable parts models . def ##or ##mable parts models ( d ##pm ) use a sliding window approach to object detection . d ##pm uses a di ##s ##jo ##int pipeline to extract static features , classify regions , predict bound ##ing boxes for high scoring regions , etc . our system replaces all of these di ##spar ##ate parts with a single con ##vo ##lu ##tion ##al neural network . the network performs feature extraction , bound ##ing box prediction , non - maximal suppression , and context ##ual reasoning all concurrently . instead of static features , the network trains the features in - line and opt ##imi ##zes them for the detection task . our unified architecture leads to a faster , more accurate model than d ##pm . r - cnn . r - cnn and its variants use region proposals instead of sliding windows to find objects in images . selective search generates potential bound ##ing boxes , a con ##vo ##lu ##tion ##al network extracts features , an sv ##m scores the boxes , a linear model adjust ##s the bound ##ing boxes , and non - max suppression eliminate ##s duplicate detection ##s . each stage of this complex pipeline must be precisely tuned independently and the resulting system is very slow , taking more than 40 seconds per image at test time . yo ##lo shares some similarities with r - cnn . each grid cell proposes potential bound ##ing boxes and scores those boxes using con ##vo ##lu ##tion ##al features . however , our system puts spatial constraints on the grid cell proposals which helps mit ##igate multiple detection ##s of the same object . our system also proposes far fewer bound ##ing boxes , only 98 per image compared to about 2000 from selective search . finally , our system combines these individual components into a single , jointly opt ##imi ##zed model . other fast detectors fast and faster r - cnn focus on speeding up the r - cnn framework by sharing computation and using neural networks to propose regions instead of selective search . while they offer speed and accuracy improvements over r - cnn , both still fall short of real - time performance . many research efforts focus on speeding up the d ##pm pipeline . they speed up hog computation , use cascade ##s , and push computation to gp ##us . however , only 30 ##h ##z d ##pm actually runs in real - time . instead of trying to opt ##imi ##ze individual components of a large detection pipeline , yo ##lo throws out the pipeline entirely and is fast by design . detectors for single classes like faces or people can be highly opt ##imi ##zed since they have to deal with much less variation . yo ##lo is a general purpose detector that learns to detect a variety of objects simultaneously . deep multi ##box . unlike r - cnn , s ##ze ##ged ##y et al . train a con ##vo ##lu ##tion ##al neural network to predict regions of interest instead of using selective search . multi ##box can also perform single object detection by replacing the confidence prediction with a single class prediction . however , multi ##box can not perform general object detection and is still just a piece in a larger detection pipeline , requiring further image patch classification . both yo ##lo and multi ##box use a con ##vo ##lu ##tion ##al network to predict bound ##ing boxes in an image but yo ##lo is a complete detection system . over ##fe ##at . ser ##man ##et et al . train a con ##vo ##lu ##tion ##al neural network to perform local ##ization and adapt that local ##izer to perform detection . over ##fe ##at efficiently performs sliding window detection but it is still a di ##s ##jo ##int system . over ##fe ##at opt ##imi ##zes for local ##ization , not detection performance . like d ##pm , the local ##izer only sees local information when making a prediction . over ##fe ##at can not reason about global context and thus requires significant post - processing to produce coherent detection ##s . multi ##gra ##sp . our work is similar in design to work on grasp detection by red ##mon et al . our grid approach to bound ##ing box prediction is based on the multi ##gra ##sp system for regression to grasp ##s . however , grasp detection is a much simpler task than object detection . multi ##gra ##sp only needs to predict a single grasp ##able region for an image containing one object . it does n ' t have to estimate the size , location , or boundaries of the object or predict it ' s class , only find a region suitable for grasping . yo ##lo predict ##s both bound ##ing boxes and class pro ##ba ##bilities for multiple objects of multiple classes in an image . section : experiments first we compare yo ##lo with other real - time detection systems on pascal vo ##c 2007 . to understand the differences between yo ##lo and r - cnn variants we explore the errors on vo ##c 2007 made by yo ##lo and fast r - cnn , one of the highest performing versions of r - cnn . based on the different error profiles we show that yo ##lo can be used to res ##core fast r - cnn detection ##s and reduce the errors from background false positive ##s , giving a significant performance boost . we also present vo ##c 2012 results and compare map to current state - of - the - art methods . finally , we show that yo ##lo general ##izes to new domains better than other detectors on two artwork data ##set ##s . sub ##section : comparison to other real - time systems many research efforts in object detection focus on making standard detection pipeline ##s fast . however , only sad ##eg ##hi et al . actually produce a detection system that runs in real - time ( 30 frames per second or better ) . we compare yo ##lo to their gp ##u implementation of d ##pm which runs either at 30 ##h ##z or 100 ##h ##z . while the other efforts do n ' t reach the real - time milestone we also compare their relative map and speed to examine the accuracy - performance trade ##offs available in object detection systems . fast yo ##lo is the fastest object detection method on pascal ; as far as we know , it is the fastest extant object detector . with map , it is more than twice as accurate as prior work on real - time detection . yo ##lo pushes map to while still maintaining real - time performance . we also train yo ##lo using v ##gg - 16 . this model is more accurate but also significantly slower than yo ##lo . it is useful for comparison to other detection systems that rely on v ##gg - 16 but since it is slower than real - time the rest of the paper focuses on our faster models . fastest d ##pm effectively speeds up d ##pm without sac ##ri ##fi ##cing much map but it still misses real - time performance by a factor of 2 . it also is limited by d ##pm ' s relatively low accuracy on detection compared to neural network approaches . r - cnn minus r replaces selective search with static bound ##ing box proposals . while it is much faster than r - cnn , it still falls short of real - time and takes a significant accuracy hit from not having good proposals . fast r - cnn speeds up the classification stage of r - cnn but it still relies on selective search which can take around 2 seconds per image to generate bound ##ing box proposals . thus it has high map but at f ##ps it is still far from real - time . the recent faster r - cnn replaces selective search with a neural network to propose bound ##ing boxes , similar to s ##ze ##ged ##y et al . in our tests , their most accurate model achieve ##s 7 f ##ps while a smaller , less accurate one runs at 18 f ##ps . the v ##gg - 16 version of faster r - cnn is 10 map higher but is also 6 times slower than yo ##lo . the ze ##ile ##r - fergus faster r - cnn is only 2 . 5 times slower than yo ##lo but is also less accurate . sub ##section : vo ##c 2007 error analysis to further examine the differences between yo ##lo and state - of - the - art detectors , we look at a detailed breakdown of results on vo ##c 2007 . we compare yo ##lo to fast r - cnn since fast r - cnn is one of the highest performing detectors on pascal and it ' s detection ##s are publicly available . we use the methodology and tools of ho ##ie ##m et al . for each category at test time we look at the top n predictions for that category . each prediction is either correct or it is classified based on the type of error : correct : correct class and local ##ization : correct class , similar : class is similar , other : class is wrong , background : for any object figure [ reference ] shows the breakdown of each error type averaged across all 20 classes . yo ##lo struggles to local ##ize objects correctly . local ##ization errors account for more of yo ##lo ' s errors than all other sources combined . fast r - cnn makes much fewer local ##ization errors but far more background errors . 13 . 6 % of it ' s top detection ##s are false positive ##s that do n ' t contain any objects . fast r - cnn is almost 3 ##x more likely to predict background detection ##s than yo ##lo . sub ##section : combining fast r - cnn and yo ##lo yo ##lo makes far fewer background mistakes than fast r - cnn . by using yo ##lo to eliminate background detection ##s from fast r - cnn we get a significant boost in performance . for every bound ##ing box that r - cnn predict ##s we check to see if yo ##lo predict ##s a similar box . if it does , we give that prediction a boost based on the probability predicted by yo ##lo and the overlap between the two boxes . the best fast r - cnn model achieve ##s a map of 71 . 8 % on the vo ##c 2007 test set . when combined with yo ##lo , its map increases by 3 . 2 % to 75 . 0 % . we also tried combining the top fast r - cnn model with several other versions of fast r - cnn . those ensembles produced small increases in map between . 3 and . 6 % , see table [ reference ] for details . the boost from yo ##lo is not simply a by ##pro ##du ##ct of model en ##se ##mbling since there is little benefit from combining different versions of fast r - cnn . rather , it is precisely because yo ##lo makes different kinds of mistakes at test time that it is so effective at boost ##ing fast r - cnn ' s performance . unfortunately , this combination does n ' t benefit from the speed of yo ##lo since we run each model sep ##erate ##ly and then combine the results . however , since yo ##lo is so fast it does n ' t add any significant computational time compared to fast r - cnn . sub ##section : vo ##c 2012 results on the vo ##c 2012 test set , yo ##lo scores 57 . 9 % map . this is lower than the current state of the art , closer to the original r - cnn using v ##gg - 16 , see table [ reference ] . our system struggles with small objects compared to its closest competitors . on categories like bottle , sheep , and tv / monitor yo ##lo scores 8 - 10 % lower than r - cnn or feature edit . however , on other categories like cat and train yo ##lo achieve ##s higher performance . our combined fast r - cnn + yo ##lo model is one of the highest performing detection methods . fast r - cnn gets a 2 . 3 % improvement from the combination with yo ##lo , boost ##ing it 5 spots up on the public leader ##board . sub ##section : general ##iza ##bility : person detection in artwork [ b ] . 45 [ b ] . 55 academic data ##set ##s for object detection draw the training and testing data from the same distribution . in real - world applications it is hard to predict all possible use cases and the test data can diver ##ge from what the system has seen before . we compare yo ##lo to other detection systems on the picasso data ##set and the people - art data ##set , two data ##set ##s for testing person detection on artwork . figure [ reference ] shows comparative performance between yo ##lo and other detection methods . for reference , we give vo ##c 2007 detection ap on person where all models are trained only on vo ##c 2007 data . on picasso models are trained on vo ##c 2012 while on people - art they are trained on vo ##c 2010 . r - cnn has high ap on vo ##c 2007 . however , r - cnn drops off considerably when applied to artwork . r - cnn uses selective search for bound ##ing box proposals which is tuned for natural images . the class ##ifier step in r - cnn only sees small regions and needs good proposals . d ##pm maintains its ap well when applied to artwork . prior work theo ##rize ##s that d ##pm performs well because it has strong spatial models of the shape and layout of objects . though d ##pm does n ' t de ##grade as much as r - cnn , it starts from a lower ap . yo ##lo has good performance on vo ##c 2007 and its ap de ##grade ##s less than other methods when applied to artwork . like d ##pm , yo ##lo models the size and shape of objects , as well as relationships between objects and where objects commonly appear . artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects , thus yo ##lo can still predict good bound ##ing boxes and detection ##s . section : real - time detection in the wild yo ##lo is a fast , accurate object detector , making it ideal for computer vision applications . we connect yo ##lo to a web ##cam and verify that it maintains real - time performance , including the time to fetch images from the camera and display the detection ##s . the resulting system is interactive and engaging . while yo ##lo processes images individually , when attached to a web ##cam it functions like a tracking system , detecting objects as they move around and change in appearance . a demo of the system and the source code can be found on our project website : . section : conclusion we introduce yo ##lo , a unified model for object detection . our model is simple to construct and can be trained directly on full images . unlike class ##ifier - based approaches , yo ##lo is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly . fast yo ##lo is the fastest general - purpose object detector in the literature and yo ##lo pushes the state - of - the - art in real - time object detection . yo ##lo also general ##izes well to new domains making it ideal for applications that rely on fast , robust object detection . acknowledge ##ments : this work is partially supported by on ##r n ##00 ##01 ##4 - 13 - 1 - 07 ##20 , ns ##f ii ##s - 133 ##80 ##54 , and the allen distinguished investigator award . bibliography : references",
        "pred_seq": "[SEP] [SEP] [SEP] real detection [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "realtime object detection"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "pascal voc",
                        "voc 2007 test data",
                        "voc 2007",
                        "pascal",
                        "voc 2007 test set",
                        "voc 2007 data"
                    ]
                ],
                "Method": [
                    [
                        "yolo"
                    ]
                ],
                "Metric": [
                    [
                        "fps"
                    ]
                ],
                "Task": [
                    [
                        "realtime object detection",
                        "realtime detection",
                        "realtime detection in wild"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "pascal voc",
                        "voc 2007 test data",
                        "voc 2007",
                        "pascal",
                        "voc 2007 test set",
                        "voc 2007 data"
                    ]
                ],
                "Method": [
                    [
                        "yolo"
                    ]
                ],
                "Metric": [
                    [
                        "mean average precision",
                        "maximizing average precision",
                        "map"
                    ]
                ],
                "Task": [
                    [
                        "object detection",
                        "detection",
                        "frame detection",
                        "detection task",
                        "detection pipeline",
                        "grasp detection",
                        "person detection"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "pascal voc",
                        "voc 2007 test data",
                        "voc 2007",
                        "pascal",
                        "voc 2007 test set",
                        "voc 2007 data"
                    ]
                ],
                "Method": [
                    [
                        "yolo"
                    ]
                ],
                "Metric": [
                    [
                        "mean average precision",
                        "maximizing average precision",
                        "map"
                    ]
                ],
                "Task": [
                    [
                        "realtime object detection",
                        "realtime detection",
                        "realtime detection in wild"
                    ]
                ]
            }
        ]
    },
    "31": {
        "doctext": "in this work , we build on recent advances in distribution ##al reinforcement learning to give a generally applicable , flexible , and state - of - the - art distribution ##al variant of dq ##n . we achieve this by using quan ##tile regression to approximate the full quan ##tile function for the state - action return distribution . by rep ##ara ##meter ##izing a distribution over the sample space , this yields an implicit ##ly defined return distribution and gives rise to a large class of risk - sensitive policies . we demonstrate improved performance on the 57 atari 260 ##0 games in the ale , and use our algorithm ' s implicit ##ly defined distributions to study the effects of risk - sensitive policies in atari games . section : introduction distribution ##al reinforcement learning focuses on the intrinsic random ##ness of returns within the reinforcement learning ( r ##l ) framework . as the agent interact ##s with the environment , ir ##red ##ucible random ##ness see ##ps in through the st ##och ##astic ##ity of these interactions , the approximation ##s in the agent ' s representation , and even the inherently chaotic nature of physical interaction . distribution ##al r ##l aims to model the distribution over returns , whose mean is the traditional value function , and to use these distributions to evaluate and opt ##imi ##ze a policy . any distribution ##al r ##l algorithm is characterized by two aspects : the parameter ##ization of the return distribution , and the distance metric or loss function being opt ##imi ##zed . together , these choices control assumptions about the random returns and how approximation ##s will be traded off . cat ##egorical dq ##n [ c ##51 ] c ##51 combines a cat ##egorical distribution and the cross - entropy loss with the [UNK] - mini ##mi ##zing projection . for this , it assumes returns are bounded in a known range and trades off mean - preservation at the cost of over ##est ##imating variance . c ##51 out ##per ##formed all previous improvements to dq ##n on a set of 57 atari 260 ##0 games in the arcade learning environment , which we refer to as the atari - 57 bench ##mark . subsequently , several papers have built upon this successful combination to achieve significant improvements to the state - of - the - art in atari - 57 , and challenging continuous control tasks . these algorithms are restricted to assign ##ing pro ##ba ##bilities to an a prior ##i fixed , discrete set of possible returns . da ##bn ##ey ##20 ##17 ##q ##r propose an alternate pair of choices , parameter ##izing the distribution by a uniform mixture of dir ##ac ##s whose locations are adjusted using quan ##tile regression . their algorithm , q ##r - dq ##n , while restricted to a discrete set of quan ##tile ##s , automatically adapt ##s return quan ##tile ##s to minimize the was ##ser ##stein distance between the bell ##man updated and current return distributions . this flexibility allows q ##r - dq ##n to significantly improve on c ##51 ' s atari - 57 performance . in this paper , we extend the approach of da ##bn ##ey ##20 ##17 ##q ##r , from learning a discrete set of quan ##tile ##s to learning the full quan ##tile function , a continuous map from pro ##ba ##bilities to returns . when combined with a base distribution , such as , this forms an implicit distribution capable of approx ##imating any distribution over returns given sufficient network capacity . our approach , implicit quan ##tile networks ( iq ##n ) , is best viewed as a simple distribution ##al general ##ization of the dq ##n algorithm , and provides several benefits over q ##r - dq ##n . first , the approximation error for the distribution is no longer controlled by the number of quan ##tile ##s output by the network , but by the size of the network itself , and the amount of training . second , iq ##n can be used with as few , or as many , samples per update as desired , providing improved data efficiency with increasing number of samples per training update . third , the implicit representation of the return distribution allows us to expand the class of policies to more fully take advantage of the learned distribution . specifically , by taking the base distribution to be non - uniform , we expand the class of policies to - greedy policies on arbitrary distortion risk measures . we begin by reviewing distribution ##al reinforcement learning , related work , and introducing the concepts surrounding risk - sensitive r ##l . in subsequent sections , we introduce our proposed algorithm , iq ##n , and present a series of experiments using the atari - 57 bench ##mark , investigating the robust ##ness and performance of iq ##n . despite being a simple distribution ##al extension to dq ##n , and for ##going any other improvements , iq ##n significantly out ##per ##forms q ##r - dq ##n and nearly matches the performance of rainbow , which combines many orthogonal advances . in fact , in human - starts as well as in the hardest atari games ( where current r ##l agents still under ##per ##form human players ) iq ##n improves over rainbow . section : background / related work we consider the standard r ##l setting , in which the interaction of an agent and an environment is modeled as a marko ##v decision process , where and denote the state and action spaces , the ( state - and action - dependent ) reward function , the transition kernel , and a discount factor . a policy maps a state to a distribution over actions . for an agent following policy , the discount ##ed sum of future rewards is denoted by the random variable , where , , , and . the action - value function is defined as , and can be characterized by the bell ##man equation the objective in r ##l is to find an optimal policy , which maximize ##s , i . e . for all and all . one approach is to find the unique fixed point of the bell ##man optimal ##ity operator : to this end , q - learning it ##erative ##ly improves an estimate , , of the optimal action - value function , , by repeatedly applying the bell ##man update : the action - value function can be approximate ##d by a parameter ##ized function ( e . g . a neural network ) , and trained by mini ##mi ##zing the squared temporal difference ( td ) error , over samples observed while following an - greedy policy over . this policy acts greed ##ily with respect to with probability and uniformly at random otherwise . dq ##n uses a con ##vo ##lu ##tion ##al neural network to parameter ##ize and the q - learning algorithm to achieve human - level play on the atari - 57 bench ##mark . sub ##section : distribution ##al r ##l in distribution ##al r ##l , the distribution over returns ( the law of ) is considered instead of the scala ##r value function that is its expectation . this change in perspective has yielded new insights into the dynamics of r ##l , and been a useful tool for analysis . empirical ##ly , distribution ##al r ##l algorithms show improved sample complexity and final performance , as well as increased robust ##ness to hyper ##para ##meter variation . an analogous distribution ##al bell ##man equation of the form can be derived , where denotes that two random variables and have equal probability laws , and the random variables and are distributed according to and , respectively . mori ##mura ##10 ##para ##metric defined the distribution ##al bell ##man operator explicitly in terms of conditional pro ##ba ##bilities , parameter ##ized by the mean and scale of a ga ##uss ##ian or lap ##lace distribution , and minimize ##d the ku ##ll ##back - lei ##bler ( k ##l ) diver ##gence between the bell ##man target and the current estimated return distribution . however , the distribution ##al bell ##man operator is not a contraction in the k ##l . as with the scala ##r setting , a distribution ##al bell ##man optimal ##ity operator can be defined by with distributed according to . while the distribution ##al bell ##man operator for policy evaluation is a contraction in the - was ##ser ##stein distance , this no longer holds for the control case . convergence to the optimal policy can still be established , but requires a more involved argument . c ##51 parameter ##ize the return distribution as a cat ##egorical distribution over a fixed set of e ##qui ##dis ##tan ##t points and minimize the k ##l diver ##gence to the projected distribution ##al bell ##man target . their algorithm , c ##51 , out ##per ##formed previous dq ##n variants on the atari - 57 bench ##mark . subsequently , hesse ##l ##20 ##18 ##rain ##bow combined c ##51 with enhancement ##s such as prior ##iti ##zed experience replay , - step updates , and the duel ##ing architecture , leading to the rainbow agent , current state - of - the - art in atari - 57 . the cat ##egorical parameter ##ization , using the projected k ##l loss , has also been used in recent work to improve the critic of a policy gradient algorithm , d ##4 ##pg , achieving significantly improved robust ##ness and state - of - the - art performance across a variety of continuous control tasks . sub ##section : - was ##ser ##stein metric the - was ##ser ##stein metric , for , plays a key role in recent results in distribution ##al r ##l . it has also been a topic of increasing interest in genera ##tive modeling , because unlike the k ##l diver ##gence , the was ##ser ##stein metric inherently trades off approximate solutions with likelihood ##s . the - was ##ser ##stein distance is the metric on inverse cumulative distribution functions ( c . d . f . ) , also known as quan ##tile functions . for random variables and with quan ##tile functions and , respectively , the - was ##ser ##stein distance is given by the class of optimal transport metric ##s express distances between distributions in terms of the minimal cost for transporting mass to make the two distributions identical . this cost is given in terms of some metric , , on the underlying space . the - was ##ser ##stein metric corresponds to . we are particularly interested in the was ##ser ##stein metric ##s due to the predominant use of spaces in mean - value reinforcement learning . sub ##section : quan ##tile regression for distribution ##al r ##l c ##51 showed that the distribution ##al bell ##man operator is a contraction in the - was ##ser ##stein metric , but as the proposed algorithm did not itself minimize the was ##ser ##stein metric , this left a theory - practice gap for distribution ##al r ##l . recently , this gap was closed , in both directions . first and most relevant to this work , da ##bn ##ey ##20 ##17 ##q ##r proposed the use of quan ##tile regression for distribution ##al r ##l and showed that by choosing the quan ##tile targets suit ##ably the resulting projected distribution ##al bell ##man operator is a contraction in the - was ##ser ##stein metric . concurrently , rowland ##20 ##18 ##analysis showed the original class of cat ##egorical algorithms are a contraction in the [UNK] distance , the metric on cumulative distribution functions . by est ##imating the quan ##tile function at precisely chosen points , q ##r - dq ##n minimize ##s the was ##ser ##stein distance to the distribution ##al bell ##man target . this estimation uses quan ##tile regression , which has been shown to converge to the true quan ##tile function value when minimize ##d using st ##och ##astic approximation . in q ##r - dq ##n , the random return is approximate ##d by a uniform mixture of dir ##ac ##s , with each assigned a fixed quan ##tile target , for , where . these quan ##tile estimates are trained using the hub ##er ##19 ##64 ##ro ##bus ##t quan ##tile regression loss , with threshold , on the pair ##wise td - errors at the time of this writing , q ##r - dq ##n achieve ##s the best performance on atari - 57 , human - normal ##ized mean and median , of all agents that do not combine distribution ##al r ##l , prior ##iti ##zed replay , and - step updates . sub ##section : risk in reinforcement learning distribution ##al r ##l algorithms have been theoretically justified for the was ##ser ##stein and [UNK] metric ##s , and learning the distribution over returns , in and of itself , empirical ##ly results in significant improvements to data efficiency , final performance , and stability . however , in each of these recent works the policy used was based entirely on the mean of the return distribution , just as in standard reinforcement learning . a natural question arises : can we expand the class of policies using information provided by the distribution over returns ( i . e . to the class of risk - sensitive policies ) ? furthermore , when would this larger policy class be beneficial ? here , ' risk ' refers to the uncertainty over possible outcomes , and risk - sensitive policies are those which depend upon more than the mean of the outcomes . at this point , it is important to highlight the difference between intrinsic uncertainty , captured by the distribution over returns , and para ##metric uncertainty , the uncertainty over the value estimate typically associated with bay ##esian approaches such as ps ##rl and ka ##lman td . distribution ##al r ##l seeks to capture the former , which classic approaches to risk are built upon . expected utility theory states that if a decision policy is consistent with a particular set of four ax ##ioms regarding its choices then the decision policy behave ##s as though it is maxim ##izing the expected value of some utility function , this is perhaps the most per ##vas ##ive notion of risk - sensitivity . a policy maxim ##izing a linear utility function is called risk - neutral , whereas con ##cave or convex utility functions give rise to risk - ave ##rse or risk - seeking policies , respectively . many previous studies on risk - sensitive r ##l adopt the utility function approach . a crucial ax ##iom of expected utility is independence : given random variables , and , such that ( preferred over ) , any mixture between and is preferred to the same mixture between and . stated in terms of the cumulative probability functions , . this ax ##iom in particular has troubled many researchers because it is consistently violated by human behavior . the alla ##is paradox is a frequently used example of a decision problem where people violate the independence ax ##iom of expected utility theory . however , as ya ##ari ##19 ##8 ##7 ##du ##al showed , this ax ##iom can be replaced by one in terms of convex combinations of outcome values , instead of mixture ##s of distributions . specifically , if as before , then for any and random variable , . this leads to an alternate , dual , theory of choice than that of expected utility . under these ax ##ioms the decision policy behave ##s as though it is maxim ##izing a distorted expectation , for some continuous mono ##tonic function : such a function is known as a distortion risk measure , as it di ##stor ##ts the cumulative pro ##ba ##bilities of the random variable . that is , we have two fundamentally equivalent approaches to risk - sensitivity . either , we choose a utility function and follow the expectation of this utility . or , we choose a re ##weight ##ing of the distribution and compute expectation under this distortion measure . indeed , ya ##ari ##19 ##8 ##7 ##du ##al further showed that these two functions are inverse ##s of each other . the choice between them amounts to a choice over whether the behavior should be invariant to mixing with random events or to convex combinations of outcomes . distortion risk measures include , as special cases , cumulative probability weight ##ing used in cumulative prospect theory , conditional value at risk , and many other methods . recently maj ##um ##dar ##20 ##17 ##sho ##uld argued for the use of distortion risk measures in robotics . section : implicit quan ##tile networks we now introduce the implicit quan ##tile network ( iq ##n ) , a deter ##mini ##stic para ##metric function trained to rep ##ara ##meter ##ize samples from a base distribution , e . g . , to the respective quan ##tile values of a target distribution . iq ##n provides an effective way to learn an implicit representation of the return distribution , yielding a powerful function approx ##ima ##tor for a new dq ##n - like agent . let be the quan ##tile function at for the random variable . for notation ##al simplicity we write , thus for the resulting state - action return distribution sample is . we propose to model the state - action quan ##tile function as a mapping from state - actions and samples from some base distribution , typically , to , viewed as samples from the implicit ##ly defined return distribution . let be a distortion risk measure , with identity corresponding to risk - neutrality . then , the distorted expectation of under is given by notice that the distorted expectation is equal to the expected value of weighted by , that is , . the immediate implication of this is that for any , there exists a sampling distribution for such that the mean of is equal to the distorted expectation of under , that is , any distorted expectation can be represented as a weighted sum over the quan ##tile ##s . denote by the risk - sensitive greedy policy for two samples , and policy , the sampled temporal difference ( td ) error at step is then , the iq ##n loss function is given by where and denote the respective number of ii ##d samples used to estimate the loss . a corresponding sample - based risk - sensitive policy is obtained by approx ##imating in equation [ reference ] by samples of : implicit quan ##tile networks differ from the approach of da ##bn ##ey ##20 ##17 ##q ##r in two ways . first , instead of approx ##imating the quan ##tile function at fixed values of we approximate it with for some different ##iable functions , , and . if we ignore the distribution ##al interpretation for a moment and view each as a separate action - value function , this highlights that implicit quan ##tile networks are a type of universal value function approx ##ima ##tor ( uv ##fa ) . there may be additional benefits to implicit quan ##tile networks beyond the obvious increase in representation ##al fidelity . as with uv ##fa ##s , we might hope that training over many different ' s ( goals in the case of the uv ##fa ) leads to better general ##ization between values and improved sample complexity than attempting to train each separately . second , , , and are sampled from continuous , independent , distributions . besides , we also explore risk - sent ##ive policies , with non - linear . the independent sampling of each , results in the sample td errors being decor ##rel ##ated , and the estimated action - values go from being the true mean of a mixture of dir ##ac ##s to a sample mean of the implicit distribution defined by rep ##ara ##meter ##izing the sampling distribution via the learned quan ##tile function . sub ##section : implementation consider the neural network structure used by the dq ##n agent . let be the function computed by the con ##vo ##lu ##tion ##al layers and the subsequent fully - connected layers mapping to the estimated action - values , such that . for our network we use the same functions and as in dq ##n , but include an additional function computing an em ##bed ##ding for the sample point . we combine these to form the approximation , where denotes the element - wise ( had ##ama ##rd ) product . as the network for is not particularly deep , we use the multi ##pl ##icative form , , to force interaction between the con ##vo ##lu ##tion ##al features and the sample em ##bed ##ding . alternative functional forms , e . g . con ##cate ##nation or a ' residual ' function , are con ##ce ##iva ##ble , and can be parameter ##ized in different ways . to investigate these , we compared performance across a number of architectural variants on six atari 260 ##0 games ( as ##ter ##ix , assault , breakout , ms . pac ##man , qb ##ert , space invaders ) . full results are given in the appendix . despite minor variation in performance , we found the general approach to be robust to the various choices . based upon the results we used the following function in our later experiments , for em ##bed ##ding dimension : after settling on a network architecture , we study the effect of the number of samples , and , used in the estimate terms of equation [ reference ] . we h ##yp ##oth ##es ##ized that , the number of samples of , would affect the sample complexity of iq ##n , with larger values leading to faster learning , and that with one would potentially approach the performance of dq ##n . this would support the hypothesis that the improved performance of many distribution ##al r ##l algorithms rests on their effect as auxiliary loss functions , which would vanish in the case of . furthermore , we believed that , the number of samples of , would affect the variance of the gradient estimates much like a mini - batch size hyper ##para ##meter . our prediction was that would have the greatest effect on variance of the long - term performance of the agent . we used the same set of six games as before , with our chosen architecture , and varied . in figure [ reference ] we report the average human - normal ##ized scores on the six games for each configuration . figure [ reference ] ( left ) shows the average performance over the first ten million frames , while ( right ) shows the average performance over the last ten million ( from 190 m to 200 m ) . as expected , we found that has a dramatic effect on early performance , shown by the continual improvement in score as the value increases . additionally , we observed that affected performance very differently than expected : it had a strong effect on early performance , but minimal impact on long - term performance past . overall , while using more samples for both distributions is generally favorable , appears to be sufficient to achieve the majority of improvements offered by iq ##n for long - term performance , with variation past this point largely insignificant . to our surprise we found that even for , which is comparable to dq ##n in the number of loss components , the longer term performance is still quite strong ( dq ##n ) . in an informal evaluation , we did not find iq ##n to be sensitive to , the number of samples used for the policy , and have fixed it at for all experiments . section : risk - sensitive reinforcement learning in this section , we explore the effects of varying the distortion risk measure , , away from identity . this only affects the policy , , used both in equation [ reference ] and for acting in the environment . as we have argued , evaluating under different distortion risk measures is equivalent to changing the sampling distribution for , allowing us to achieve various forms of risk - sensitive policies . we focus on a handful of sampling distributions and their corresponding distortion measures . the first one is the cumulative probability weight ##ing parameter ##ization proposed in cumulative prospect theory : in particular , we use the parameter value found by wu ##19 ##9 ##6 ##cu ##rva ##ture to most closely match human subjects . this choice is interesting as , unlike the others we consider , it is neither globally convex nor con ##cave . for small values of it is locally con ##cave and for larger values of it becomes locally convex . recall that con ##ca ##vity corresponds to risk - ave ##rse and convex ##ity to risk - seeking policies . second , we consider the distortion risk measure proposed by wang ##200 ##0 ##class , where and are taken to be the standard normal cumulative distribution function and its inverse : for , this produces risk - ave ##rse policies and we include it due to its simple interpretation and ability to switch between risk - ave ##rse and risk - seeking distortion ##s . third , we consider a simple power formula for risk - ave ##rse ( ) or risk - seeking ( ) policies : finally , we consider conditional value - at - risk ( cv ##ar ) : cv ##ar has been widely studied in and out of reinforcement learning . its implementation as a modification to the sampling distribution of is particularly simple , as it changes to . another interesting sampling distribution , not included in our experiments , is denoted and corresponds to sampled by averaging samples from . in figure [ reference ] ( right ) we give an example of a distribution ( neutral ) and how each of these distortion measures affects the implied distribution due to changing the sampling distribution of . and reduce the impact of the tails of the distribution , while and heavily shift the distribution mass towards the tails , creating a risk - ave ##rse or risk - seeking preference . additionally , while cv ##ar entirely ignores all values corresponding to , gives these non - zero , but vanishing ##ly small , probability . by using these sampling distributions we can induce various risk - sensitive policies in iq ##n . we evaluate these on the same set of six atari 260 ##0 games previously used . our algorithm simply changes the policy to maximize the distorted expectations instead of the usual sample mean . figure [ reference ] ( left ) shows our results in this experiment , with average scores reported under the usual , risk - neutral , evaluation criterion . intuitive ##ly , we expected to see a qu ##ali ##tative effect from risk - sensitive training , e . g . strengthened exploration from a risk - seeking objective . although we did see qu ##ali ##tative differences , these did not always match our expectations . for two of the games , as ##ter ##ix and assault , there is a very significant advantage to the risk - ave ##rse policies . although tends to perform almost identical ##ly to the standard risk - neutral policy , and the risk - seeking performs as well or worse than risk - neutral , we find that both risk - ave ##rse policies improve performance over standard iq ##n . however , we also observe that the more risk - ave ##rse of the two , , suffers some loss in performance on two other games ( qb ##ert and space invaders ) . additionally , we note that the risk - seeking policy significantly under ##per ##forms the risk - neutral policy on three of the six games . it remains an open question as to exactly why we see improved performance for risk - ave ##rse policies . there are many possible explanations for this phenomenon , e . g . that risk - ave ##rs ##ion en ##codes a he ##uri ##stic to stay alive longer , which in many games is correlated with increased rewards . section : full atari - 57 results finally , we evaluate iq ##n on the full atari - 57 bench ##mark , comparing with the state - of - the - art performance of rainbow , a distribution ##al r ##l agent that combines several advances in deep r ##l , the closely related algorithm q ##r - dq ##n , prior ##iti ##zed experience replay dq ##n , and the original dq ##n agent . note that in this section we use the risk - neutral variant of the iq ##n , that is , the policy of the iq ##n agent is the regular - greedy policy with respect to the mean of the state - action return distribution . it is important to remember that rainbow builds upon the distribution ##al r ##l algorithm c ##51 , but also includes prior ##iti ##zed experience replay , double dq ##n , duel ##ing network architecture , noisy networks , and multi - step updates . in particular , besides the distribution ##al update , - step updates and prior ##iti ##zed experience replay were found to have significant impact on the performance of rainbow . our other competitive baseline is q ##r - dq ##n , which is currently state - of - the - art for agents that do not combine distribution ##al updates , - step updates , and prior ##iti ##zed replay . thus , between q ##r - dq ##n and the much more complex rainbow we compare to the two most closely related , and best performing , agents in published work . in particular , we would expect that iq ##n would benefit from the additional enhancement ##s in rainbow , just as rainbow improved significantly over c ##51 . figure [ reference ] shows the mean ( left ) and median ( right ) human - normal ##ized scores during training over the atari - 57 bench ##mark . iq ##n dramatically improves over q ##r - dq ##n , which itself improves on many previously published results . at 100 million frames iq ##n has reached the same level of performance as q ##r - dq ##n at 200 million frames . table [ reference ] gives a comparison between the same methods in terms of their best , human - normal ##ized , scores per game under the 30 random no - op start condition . these are averages over the given number of seeds . additionally , using human - starts , iq ##n achieve ##s median human - normal ##ized score , whereas rainbow reaches , see table [ reference ] . finally , we took a closer look at the games in which each algorithm continues to under ##per ##form humans , and computed , on average , how far below human - level they perform . we refer to this value as the human - gap ##than ##ks to joseph mod ##ay ##il for proposing this metric . metric and give results in table [ reference ] . interesting ##ly , c ##51 out ##per ##forms q ##r - dq ##n in this metric , and iq ##n out ##per ##forms all others . this shows that the remaining gap between rainbow and iq ##n is entirely from games on which both algorithms are already super - human . the games where the most progress in r ##l is needed happen to be the games where iq ##n shows the greatest improvement over q ##r - dq ##n and rainbow . section : discussion and conclusions we have proposed a general ##ization of recent work based around using quan ##tile regression to learn the distribution over returns of the current policy . our general ##ization leads to a simple change to the dq ##n agent to enable distribution ##al r ##l , the natural integration of risk - sensitive policies , and significantly improved performance over existing methods . the iq ##n algorithm provides , for the first time , a fully integrated distribution ##al r ##l agent without prior assumptions on the parameter ##ization of the return distribution . iq ##n can be trained with as little as a single sample from each state - action value distribution , or as many as computational limits allow to improve the algorithm ' s data efficiency . furthermore , iq ##n allows us to expand the class of control policies to a large class of risk - sensitive policies connected to distortion risk measures . finally , we show substantial gains on the atari - 57 bench ##mark over q ##r - dq ##n , and even hal ##ving the distance between q ##r - dq ##n and rainbow . despite the significant empirical successes in this paper there are many areas in need of additional theoretical analysis . we highlight a few particularly relevant open questions we were unable to address in the present work . first , sample - based convergence results have been recently shown for a class of cat ##egorical distribution ##al r ##l algorithms . could existing sample - based r ##l convergence results be extended to the q ##r - based algorithms ? second , can the contraction mapping results for a fixed grid of quan ##tile ##s given by da ##bn ##ey ##20 ##17 ##q ##r be extended to the more general class of approximate quan ##tile functions studied in this work ? finally , and particularly sal ##ient to our experiments with distortion risk measures , theoretical guarantees for risk - sensitive r ##l have been building over recent years , but have been largely limited to special cases and restricted classes of risk - sensitive policies . can the convergence of the distribution of returns under the bell ##man operator be leverage ##d to show convergence to a fixed - point in distorted expectations ? in particular , can the control results of c ##51 be expanded to cover some class of risk - sensitive policies ? there remain many intriguing directions for future research into distribution ##al r ##l , even on purely empirical fronts . hesse ##l ##20 ##18 ##rain ##bow recently showed that distribution ##al r ##l agents can be significantly improved , when combined with other techniques . creating a rainbow - iq ##n agent could yield even greater improvements on atari - 57 . we also recall the surprisingly rich return distributions found by bart ##hma ##ron ##20 ##18 ##d ##4 ##pg , and h ##yp ##oth ##es ##ize that the continuous control setting may be a particularly fruit ##ful area for the application of distribution ##al r ##l in general , and iq ##n in particular . bibliography : references section : appendix sub ##section : architecture and hyper ##para ##meter ##s we considered multiple architectural variants for parameter ##izing an iq ##n . all of these build on the q - network of a regular dq ##n , which can be seen as the composition of a con ##vo ##lu ##tion ##al stack and an ml ##p , and extend it by an em ##bed ##ding of the sample point , , and a merging function , resulting in the function for the em ##bed ##ding , we considered a number of variants : a learned linear em ##bed ##ding , a learned ml ##p em ##bed ##ding with a single hidden layer of size , and a learned linear function of co ##sin ##e basis functions of the form . each of those was followed by either a re ##lu or si ##gm ##oid nonlinear ##ity . for the merging function , the simplest choice would be a simple vector con ##cate ##nation of and . note however , that the ml ##p which takes in the output of and outputs the action - value quan ##tile ##s , only has a single hidden layer in the dq ##n network . therefore , to force a sufficiently early interaction between the two representations , we also considered a multi ##pl ##icative function , where denotes the element - wise ( had ##ama ##rd ) product of two vectors , as well as a ' residual ' function . early experiments showed that a simple linear em ##bed ##ding of was insufficient to achieve good performance , and the residual version of did n ' t show any marked difference to the multi ##pl ##icative variant , so we do not include results for these here . for the other configurations , figure [ reference ] shows pair ##wise comparisons between 1 ) a co ##sin ##e basis function em ##bed ##ding and a completely learned ml ##p em ##bed ##ding , 2 ) an em ##bed ##ding size ( hidden layer size or number of co ##sin ##e basis elements ) 32 and 64 , 3 ) re ##lu and si ##gm ##oid nonlinear ##ity following the em ##bed ##ding , and 4 ) con ##cate ##nation and a multi ##pl ##icative interaction between and . each comparison ' violin plot ' can be understood as a marginal ##ization over the other variants of the architecture , with the human - normal ##ized performance at the end of training , averaged across six atari 260 ##0 games , on the y - axis . each white dot corresponds to a configuration ( each represented by two seeds ) , the black dots show the position of our preferred configuration . the width of the colored regions corresponds to a kernel density estimate of the number of configurations at each performance level . our final choice is a multi ##pl ##icative interaction with a linear function of a co ##sin ##e em ##bed ##ding , with and a re ##lu nonlinear ##ity ( see equation [ reference ] ) , as this configuration yielded the highest performance consistently over multiple seeds . also noteworthy is the overall robust ##ness of the approach to these variations : most of the configurations consistently out ##per ##form the q ##r - dq ##n baseline shown as a grey horizontal line for comparison . we give pseudo - code for the iq ##n loss in algorithm [ reference ] . all other hyper ##para ##meter ##s for this agent correspond to the ones used by da ##bn ##ey ##20 ##17 ##q ##r . in particular , the bell ##man target is computed using a target network . notice that iq ##n will generally be more computational ##ly expensive per - sample than q ##r - dq ##n . however , in practice iq ##n requires many fewer samples per update than q ##r - dq ##n so that the actual running times are comparable . [ h ##t ] implicit quan ##tile network loss and functions , # compute greedy next action # sample quan ##tile threshold ##s # compute distribution ##al temporal differences # compute hub ##er quan ##tile loss sub ##section : evaluation the human - normal ##ized scores reported in this paper are given by the formula where , and are the per - game raw scores ( und ##is ##co ##unt ##ed returns ) for the given agent , a reference human player , and random agent baseline . the ' human - gap ' metric referred to at the end of section [ reference ] builds on the human - normal ##ized score , but emphasizes the remaining improvement for the agent to reach super - human performance . it is given by , with a value of corresponding to random play , and a value of corresponding to super - human level of performance . to avoid de ##gen ##era ##cies in the case of , the quantity is being clipped above at .",
        "pred_seq": "atari games [SEP] [SEP] [SEP] atari games [SEP] [unused0] atari games [SEP] [SEP] [SEP] atari games [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "atari 2600 games"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": []
            },
            {
                "Material": [
                    [
                        "atari 2600 games"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "assault"
                    ]
                ],
                "Method": [
                    [
                        "implicit quantile networks",
                        "distributional generalization of dqn algorithm",
                        "iqn",
                        "distributional bellman equation",
                        "implicit quantile network",
                        "stateaction quantile function",
                        "iqn loss"
                    ]
                ],
                "Metric": [
                    [
                        "humannormalized scores",
                        "humannormalized scores",
                        "humannormalized score"
                    ]
                ],
                "Task": [
                    [
                        "atari games",
                        "atari57"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "asterix"
                    ]
                ],
                "Method": [
                    [
                        "implicit quantile networks",
                        "distributional generalization of dqn algorithm",
                        "iqn",
                        "distributional bellman equation",
                        "implicit quantile network",
                        "stateaction quantile function",
                        "iqn loss"
                    ]
                ],
                "Metric": [
                    [
                        "humannormalized scores",
                        "humannormalized scores",
                        "humannormalized score"
                    ]
                ],
                "Task": [
                    [
                        "atari games",
                        "atari57"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "breakout"
                    ]
                ],
                "Method": [
                    [
                        "implicit quantile networks",
                        "distributional generalization of dqn algorithm",
                        "iqn",
                        "distributional bellman equation",
                        "implicit quantile network",
                        "stateaction quantile function",
                        "iqn loss"
                    ]
                ],
                "Metric": [
                    [
                        "humannormalized scores",
                        "humannormalized scores",
                        "humannormalized score"
                    ]
                ],
                "Task": [
                    [
                        "atari games",
                        "atari57"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "atari 2600 games",
                        "atari games"
                    ]
                ],
                "Method": [
                    [
                        "implicit quantile networks",
                        "distributional generalization of dqn algorithm",
                        "iqn",
                        "distributional bellman equation",
                        "implicit quantile network",
                        "stateaction quantile function",
                        "iqn loss"
                    ]
                ],
                "Metric": [
                    [
                        "humannormalized scores",
                        "humannormalized scores",
                        "humannormalized score"
                    ]
                ],
                "Task": [
                    [
                        "atari games",
                        "atari57"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "qbert"
                    ]
                ],
                "Method": [
                    [
                        "implicit quantile networks",
                        "distributional generalization of dqn algorithm",
                        "iqn",
                        "distributional bellman equation",
                        "implicit quantile network",
                        "stateaction quantile function",
                        "iqn loss"
                    ]
                ],
                "Metric": [
                    [
                        "humannormalized scores",
                        "humannormalized scores",
                        "humannormalized score"
                    ]
                ],
                "Task": [
                    [
                        "atari games",
                        "atari57"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "space invaders"
                    ]
                ],
                "Method": [
                    [
                        "implicit quantile networks",
                        "distributional generalization of dqn algorithm",
                        "iqn",
                        "distributional bellman equation",
                        "implicit quantile network",
                        "stateaction quantile function",
                        "iqn loss"
                    ]
                ],
                "Metric": [
                    [
                        "humannormalized scores",
                        "humannormalized scores",
                        "humannormalized score"
                    ]
                ],
                "Task": [
                    [
                        "atari games",
                        "atari57"
                    ]
                ]
            }
        ]
    },
    "32": {
        "doctext": "document : se ##q ##gan : sequence genera ##tive ad ##vers ##aria ##l nets with policy gradient as a new way of training genera ##tive models , genera ##tive ad ##vers ##aria ##l net ( gan ) that uses a disc ##rim ##ina ##tive model to guide the training of the genera ##tive model has enjoyed considerable success in generating real - valued data . however , it has limitations when the goal is for generating sequences of discrete token ##s . a major reason lies in that the discrete outputs from the genera ##tive model make it difficult to pass the gradient update from the disc ##rim ##ina ##tive model to the genera ##tive model . also , the disc ##rim ##ina ##tive model can only assess a complete sequence , while for a partially generated sequence , it is non - trivial to balance its current score and the future one once the entire sequence has been generated . in this paper , we propose a sequence generation framework , called se ##q ##gan , to solve the problems . modeling the data generator as a st ##och ##astic policy in reinforcement learning ( r ##l ) , se ##q ##gan bypass ##es the generator differentiation problem by directly performing gradient policy update . the r ##l reward signal comes from the gan disc ##rim ##inator judged on a complete sequence , and is passed back to the intermediate state - action steps using monte carlo search . extensive experiments on synthetic data and real - world tasks demonstrate significant improvements over strong baseline ##s . section : introduction generating sequential synthetic data that mimic ##s the real one is an important problem in un ##su ##per ##vis ##ed learning . recently , rec ##urrent neural networks ( rn ##ns ) with long short - term memory ( l ##st ##m ) cells have shown excellent performance ranging from natural language generation to handwriting generation . the most common approach to training an rn ##n is to maximize the log predict ##ive likelihood of each true token in the training sequence given the previous observed token ##s . however , as argued in , the maximum likelihood approaches suffer from so - called exposure bias in the inference stage : the model generates a sequence it ##erative ##ly and predict ##s next token conditioned on its previously predicted ones that may be never observed in the training data . such a disc ##re ##pan ##cy between training and inference can inc ##ur acc ##um ##ula ##tively along with the sequence and will become prominent as the length of sequence increases . to address this problem , proposed a training strategy called scheduled sampling ( ss ) , where the genera ##tive model is partially fed with its own synthetic data as prefix ( observed token ##s ) rather than the true data when deciding the next token in the training stage . nevertheless , showed that ss is an inconsistent training strategy and fails to address the problem fundamentally . another possible solution of the training / inference disc ##re ##pan ##cy problem is to build the loss function on the entire generated sequence instead of each transition . for instance , in the application of machine translation , a task specific sequence score / loss , bilingual evaluation under ##st ##udy ( b ##le ##u ) , can be adopted to guide the sequence generation . however , in many other practical applications , such as poem generation and chat ##bot , a task specific loss may not be directly available to score a generated sequence accurately . general ad ##vers ##aria ##l net ( gan ) proposed by is a promising framework for all ##ev ##iating the above problem . specifically , in gan a disc ##rim ##ina ##tive net learns to distinguish whether a given data instance is real or not , and a genera ##tive net learns to confuse by generating high quality data . this approach has been successful and been mostly applied in computer vision tasks of generating samples of natural images . unfortunately , applying gan to generating sequences has two problems . firstly , gan is designed for generating real - valued , continuous data but has difficulties in directly generating sequences of discrete token ##s , such as texts . the reason is that in gan ##s , the generator starts with random sampling first and then a deter ##mist ##ic transform , gov ##er ##ment ##ed by the model parameters . as such , the gradient of the loss from w . r . t . the outputs by is used to guide the genera ##tive model ( para ##mt ##ers ) to slightly change the generated value to make it more realistic . if the generated data is based on discrete token ##s , the \" slight change \" guidance from the disc ##rim ##ina ##tive net makes little sense because there is probably no corresponding token for such slight change in the limited dictionary space . secondly , gan can only give the score / loss for an entire sequence when it has been generated ; for a partially generated sequence , it is non - trivial to balance how good as it is now and the future score as the entire sequence . in this paper , to address the above two issues , we follow and consider the sequence generation procedure as a sequential decision making process . the genera ##tive model is treated as an agent of reinforcement learning ( r ##l ) ; the state is the generated token ##s so far and the action is the next token to be generated . unlike the work in that requires a task - specific sequence score , such as b ##le ##u in machine translation , to give the reward , we employ a disc ##rim ##inator to evaluate the sequence and feedback the evaluation to guide the learning of the genera ##tive model . to solve the problem that the gradient can not pass back to the genera ##tive model when the output is discrete , we regard the genera ##tive model as a st ##och ##astic para ##met ##rized policy . in our policy gradient , we employ monte carlo ( mc ) search to approximate the state - action value . we directly train the policy ( genera ##tive model ) via policy gradient , which naturally avoids the differentiation difficulty for discrete data in a conventional gan . extensive experiments based on synthetic and real data are conducted to investigate the efficacy and properties of the proposed se ##q ##gan . in our synthetic data environment , se ##q ##gan significantly out ##per ##forms the maximum likelihood methods , scheduled sampling and pg - b ##le ##u . in three real - world tasks , i . e . poem generation , speech language generation and music generation , se ##q ##gan significantly out ##per ##forms the compared baseline ##s in various metric ##s including human expert judgement . section : related work deep genera ##tive models have recently drawn significant attention , and the ability of learning over large ( un ##lab ##ele ##d ) data end ##ows them with more potential and vital ##ity . first proposed to use the contrast ##ive diver ##gence algorithm to efficiently training deep belief nets ( db ##n ) . proposed den ##ois ##ing auto ##en ##code ##r ( da ##e ) that learns the data distribution in a supervised learning fashion . both db ##n and da ##e learn a low dimensional representation ( encoding ) for each data instance and generate it from a deco ##ding network . recently , variation ##al auto ##en ##code ##r ( va ##e ) that combines deep learning with statistical inference intended to represent a data instance in a late ##nt hidden space , while still utilizing ( deep ) neural networks for non - linear mapping . the inference is done via variation ##al methods . all these genera ##tive models are trained by maxim ##izing ( the lower bound of ) training data likelihood , which , as mentioned by , suffers from the difficulty of approx ##imating intra ##ctable pro ##ba ##bilis ##tic computation ##s . proposed an alternative training methodology to genera ##tive models , i . e . gan ##s , where the training procedure is a mini ##max game between a genera ##tive model and a disc ##rim ##ina ##tive model . this framework bypass ##es the difficulty of maximum likelihood learning and has gained striking successes in natural image generation . however , little progress has been made in applying gan ##s to sequence discrete data generation problems , e . g . natural language generation . this is due to the generator network in gan is designed to be able to adjust the output continuously , which does not work on discrete data generation . on the other hand , a lot of efforts have been made to generate structured sequences . rec ##urrent neural networks can be trained to produce sequences of token ##s in many applications such as machine translation . the most popular way of training rn ##ns is to maximize the likelihood of each token in the training data whereas pointed out that the disc ##re ##pan ##cy between training and generating makes the maximum likelihood estimation sub ##op ##ti ##mal and proposed scheduled sampling strategy ( ss ) . later theo ##rized that the objective function underneath ss is improper and explained the reason why gan ##s tend to generate natural - looking samples in theory . consequently , the gan ##s have great potential but are not practically feasible to discrete pro ##ba ##bilis ##tic models currently . as pointed out by , the sequence data generation can be formulated as a sequential decision making process , which can be potentially be solved by reinforcement learning techniques . modeling the sequence generator as a policy of picking the next token , policy gradient methods can be adopted to opt ##imi ##ze the generator once there is an ( implicit ) reward function to guide the policy . for most practical sequence generation tasks , e . g . machine translation , the reward signal is meaningful only for the entire sequence , for instance in the game of go , the reward signal is only set at the end of the game . in those cases , state - action evaluation methods such as monte carlo ( tree ) search have been adopted . by contract , our proposed se ##q ##gan extends gan ##s with the r ##l - based generator to solve the sequence generation problem , where a reward signal is provided by the disc ##rim ##inator at the end of each episode via monte carlo approach , and the generator picks the action and learns the policy using estimated overall rewards . section : sequence genera ##tive ad ##vers ##aria ##l nets the sequence generation problem is denoted as follows . given a data ##set of real - world structured sequences , train a - parameter ##ized genera ##tive model to produce a sequence , where is the vocabulary of candidate token ##s . we interpret this problem based on reinforcement learning . in times ##te ##p , the state is the current produced token ##s and the action is the next token to select . thus the policy model is st ##och ##astic , whereas the state transition is deter ##mini ##stic after an action has been chosen , i . e . for the next state if the current state and the action ; for other next states , . additionally , we also train a - parameter ##ized disc ##rim ##ina ##tive model to provide a guidance for improving generator . is a probability indicating how likely a sequence is from real sequence data or not . as illustrated in figure [ reference ] , the disc ##rim ##ina ##tive model is trained by providing positive examples from the real sequence data and negative examples from the synthetic sequences generated from the genera ##tive model . at the same time , the genera ##tive model is updated by employing a policy gradient and mc search on the basis of the expected end reward received from the disc ##rim ##ina ##tive model . the reward is estimated by the likelihood that it would fool the disc ##rim ##ina ##tive model . the specific formulation is given in the next sub ##section . sub ##section : se ##q ##gan via policy gradient following , when there is no intermediate reward , the objective of the generator model ( policy ) is to generate a sequence from the start state to maximize its expected end reward : where is the reward for a complete sequence . note that the reward is from the disc ##rim ##inator , which we will discuss later . is the action - value function of a sequence , i . e . the expected acc ##um ##ula ##tive reward starting from state , taking action , and then following policy . the rational of the objective function for a sequence is that starting from a given initial state , the goal of the generator is to generate a sequence which would make the disc ##rim ##inator consider it is real . the next question is how to estimate the action - value function . in this paper , we use the reinforce algorithm and consider the estimated probability of being real by the disc ##rim ##inator as the reward . formally , we have : however , the disc ##rim ##inator only provides a reward value for a finished sequence . since we actually care about the long - term reward , at every times ##te ##p , we should not only consider the fitness of previous token ##s ( prefix ) but also the resulted future outcome . this is similar to playing the games such as go or chess where players sometimes would give up the immediate interests for the long - term victory . thus , to evaluate the action - value for an intermediate state , we apply monte carlo search with a roll - out policy to sample the unknown last token ##s . we represent an - time monte carlo search as where and is sampled based on the roll - out policy and the current state . in our experiment , is set the same as the generator , but one can use a simplified version if the speed is the priority . to reduce the variance and get more accurate assessment of the action value , we run the roll - out policy starting from current state till the end of the sequence for times to get a batch of output samples . thus , we have : where , we see that when no intermediate reward , the function is it ##erative ##ly defined as the next - state value starting from state and rolling out to the end . a benefit of using the disc ##rim ##inator as a reward function is that it can be dynamic ##ally updated to further improve the genera ##tive model it ##erative ##ly . once we have a set of more realistic generated sequences , we shall re - train the disc ##rim ##inator model as follows : each time when a new disc ##rim ##inator model has been obtained , we are ready to update the generator . the proposed policy based method relies upon opt ##imi ##zing a para ##met ##rized policy to directly maximize the long - term reward . following , the gradient of the objective function w . r . t . the generator ' s parameters can be derived as the above form is due to the deter ##mini ##stic state transition and zero intermediate rewards . the detailed derivation is provided in the appendix . using likelihood ratios , we build an un ##bia ##sed estimation for e ##q . ( [ reference ] ) ( on one episode ) : where is the observed intermediate state sampled from . since the expectation can be approximate ##d by sampling methods , we then update the generator ' s parameters as : where denotes the corresponding learning rate at - th step . also the advanced gradient algorithms such as adam and rms ##pro ##p can be adopted here . in summary , algorithm [ reference ] shows full details of the proposed se ##q ##gan . at the beginning of the training , we use the maximum likelihood estimation ( ml ##e ) to pre - train on training set . we found the supervised signal from the pre - trained disc ##rim ##inator is inform ##ative to help adjust the generator efficiently . after the pre - training , the generator and disc ##rim ##inator are trained alternatively . as the generator gets progressed via training on g - steps updates , the disc ##rim ##inator needs to be re - trained periodically to keeps a good pace with the generator . when training the disc ##rim ##inator , positive examples are from the given data ##set , whereas negative examples are generated from our generator . in order to keep the balance , the number of negative examples we generate for each d - step is the same as the positive examples . and to reduce the variability of the estimation , we use different sets of negative samples combined with positive ones , which is similar to boots ##tra ##pping . [ t ] sequence genera ##tive ad ##vers ##aria ##l nets [ 1 ] generator policy g ##\u03b8 ; roll - out policy g ##\u03b2 ; disc ##rim ##inator [UNK] ; a sequence data ##set = s { x : 1 t } initial ##ize g ##\u03b8 , [UNK] with random weights ##\u03b8 , [UNK] . g ##\u03b8 using ml ##e on s negative samples using g ##\u03b8 for training [UNK] [UNK] via mini ##mi ##zing the cross entropy a sequence y : 1 ##t = ( y ##1 , \u2026 , y ##t ) [UNK] in : 1 t q ( a = y ##t ; s = y : 1 - t ##1 ) by e ##q . ( ) generator parameters via policy gradient e ##q . ( ) current g ##\u03b8 to generate negative examples and combine with given positive examples s disc ##rim ##inator [UNK] for k epoch ##s by e ##q . ( ) converge ##s sub ##section : the genera ##tive model for sequences we use rec ##urrent neural networks ( rn ##ns ) as the genera ##tive model . an rn ##n maps the input em ##bed ##ding representations of the sequence into a sequence of hidden states by using the update function rec ##urs ##ively . moreover , a soft ##max output layer maps the hidden states into the output token distribution where the parameters are a bias vector and a weight matrix . to deal with the common vanishing and exploding gradient problem of the back ##pro ##pa ##gation through time , we leverage the long short - term memory ( l ##st ##m ) cells to implement the update function in e ##q . ( [ reference ] ) . it is worth noticing that most of the rn ##n variants , such as the gate ##d rec ##urrent unit ( gr ##u ) and soft attention mechanism , can be used as a generator in se ##q ##gan . sub ##section : the disc ##rim ##ina ##tive model for sequences deep disc ##rim ##ina ##tive models such as deep neural network ( d ##nn ) , con ##vo ##lu ##tion ##al neural network ( cnn ) and rec ##urrent con ##vo ##lu ##tion ##al neural network ( rc ##nn ) have shown a high performance in complicated sequence classification tasks . in this paper , we choose the cnn as our disc ##rim ##inator as cnn has recently been shown of great effectiveness in text ( token sequence ) classification . most disc ##rim ##ina ##tive models can only perform classification well for an entire sequence rather than the unfinished one . in this paper , we also focus on the situation where the disc ##rim ##inator predict ##s the probability that a finished sequence is real . we first represent an input sequence as : where is the - dimensional token em ##bed ##ding and is the con ##cate ##nation operator to build the matrix . then a kernel applies a con ##vo ##lu ##tion ##al operation to a window size of words to produce a new feature map : where operator is the sum ##mation of element ##wise production , is a bias term and is a non - linear function . we can use various numbers of kernel ##s with different window sizes to extract different features . finally we apply a max - over - time pool ##ing operation over the feature maps . to enhance the performance , we also add the highway architecture based on the poole ##d feature maps . finally , a fully connected layer with si ##gm ##oid activation is used to output the probability that the input sequence is real . the optimization target is to minimize the cross entropy between the ground truth label and the predicted probability as formulated in e ##q . ( [ reference ] ) . detailed implementations of the genera ##tive and disc ##rim ##ina ##tive models are provided in the appendix . section : synthetic data experiments to test the efficacy and add our understanding of se ##q ##gan , we conduct a simulated test with synthetic data . to simulate the real - world structured sequences , we consider a language model to capture the dependency of the token ##s . we use a randomly initial ##ized l ##st ##m as the true model , aka , the oracle , to generate the real data distribution for the following experiments . sub ##section : evaluation metric the benefit of having such oracle is that firstly , it provides the training data ##set and secondly evaluate ##s the exact performance of the genera ##tive models , which will not be possible with real data . we know that ml ##e is trying to minimize the cross - entropy between the true data distribution and our approximation , i . e . . however , the most accurate way of evaluating genera ##tive models is that we draw some samples from it and let human observers review them based on their prior knowledge . we assume that the human observer has learned an accurate model of the natural distribution . then in order to increase the chance of passing turing test , we actually need to minimize the exact opposite average negative log - likelihood , with the role of and exchanged . in our synthetic data experiments , we can consider the oracle to be the human observer for real - world problems , thus a perfect evaluation metric should be where and denote our genera ##tive model and the oracle respectively . at the test stage , we use to generate 100 , 000 sequence samples and calculate for each sample by and their average score . also significance tests are performed to compare the statistical properties of the generation performance between the baseline ##s and se ##q ##gan . sub ##section : training setting to set up the synthetic data experiments , we first initial ##ize the parameters of an l ##st ##m network following the normal distribution as the oracle describing the real data distribution . then we use it to generate 10 , 000 sequences of length 20 as the training set for the genera ##tive models . in se ##q ##gan algorithm , the training set for the disc ##rim ##inator is comprised by the generated examples with the label 0 and the instances from with the label 1 . for different tasks , one should design specific structure for the con ##vo ##lu ##tion ##al layer and in our synthetic data experiments , the kernel size is from 1 to and the number of each kernel size is between 100 to 200 . drop ##out and l ##2 regular ##ization are used to avoid over - fitting . four genera ##tive models are compared with se ##q ##gan . the first model is a random token generation . the second one is the ml ##e trained l ##st ##m . the third one is scheduled sampling . the fourth one is the policy gradient with b ##le ##u ( pg - b ##le ##u ) . in the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous token ##s into l ##st ##m , towards a less guided scheme which mostly feeds the l ##st ##m with its generated token ##s . a curriculum rate is used to control the probability of replacing the true token ##s with the generated ones . to get a good and stable performance , we decrease by 0 . 00 ##2 for every training epoch . in the pg - b ##le ##u algorithm , we use b ##le ##u , a metric measuring the similarity between a generated sequence and references ( training data ) , to score the finished samples from monte carlo search . sub ##section : results the performance of generating sequences from the compared policies is provided in table [ reference ] . since the evaluation metric is fundamentally ins ##truct ##ive , we can see the impact of se ##q ##gan , which out ##per ##forms other baseline ##s significantly . a significance t - test on the score distribution of the generated sequences from the compared models is also performed , which demonstrates the significant improvement of se ##q ##gan over all compared models . the learning curves shown in figure [ reference ] illustrate the superiority of se ##q ##gan explicitly . after about 150 training epoch ##s , both the maximum likelihood estimation and the schedule sampling methods converge to a relatively high score , whereas se ##q ##gan can improve the limit of the generator with the same structure as the baseline ##s significantly . this indicates the prospect of applying ad ##vers ##aria ##l training strategies to discrete sequence genera ##tive models to breakthrough the limitations of ml ##e . additionally , se ##q ##gan out ##per ##forms pg - b ##le ##u , which means the disc ##rim ##ina ##tive signal in gan is more general and effective than a pre ##de ##fine ##d score ( e . g . b ##le ##u ) to guide the genera ##tive policy to capture the underlying distribution of the sequence data . sub ##section : discussion in our synthetic data experiments , we find that the stability of se ##q ##gan depends on the training strategy . more specifically , the g - steps , d - steps and parameters in algorithm [ reference ] have a large effect on the convergence and performance of se ##q ##gan . figure [ reference ] shows the effect of these parameters . in figure [ reference ] , the g - steps is much larger than the d - steps and epoch number , which means we train the generator for many times until we update the disc ##rim ##inator . this strategy leads to a fast convergence but as the generator improves quickly , the disc ##rim ##inator can not get fully trained and thus will provide a misleading signal gradually . in figure [ reference ] , with more disc ##rim ##inator training epoch ##s , the unstable training process is alleviate ##d . in figure [ reference ] , we train the generator for only one epoch and then before the disc ##rim ##inator gets fooled , we update it immediately based on the more realistic negative examples . in such a case , se ##q ##gan learns stab ##ly . the d - steps in all three training strategies described above is set to 1 , which means we only generate one set of negative examples with the same number as the given data ##set , and then train the disc ##rim ##inator on it for various epoch ##s . but actually we can utilize the potentially unlimited number of negative examples to improve the disc ##rim ##inator . this trick can be considered as a type of boots ##tra ##pping , where we combine the fixed positive examples with different negative examples to obtain multiple training sets . figure [ reference ] shows this technique can improve the overall performance with good stability , since the disc ##rim ##inator is shown more negative examples and each time the positive examples are emphasized , which will lead to a more comprehensive guidance for training generator . this is in line with the theorem in . when analyzing the convergence of genera ##tive ad ##vers ##aria ##l nets , an important assumption is that the disc ##rim ##inator is allowed to reach its opt ##imum given . only if the disc ##rim ##inator is capable of different ##iating real data from unnatural data consistently , the supervised signal from it can be meaningful and the whole ad ##vers ##aria ##l training process can be stable and effective . section : real - world scenarios to complement the previous experiments , we also test se ##q ##gan on several real - world tasks , i . e . poem composition , speech language generation and music generation . sub ##section : text generation for text generation scenarios , we apply the proposed se ##q ##gan to generate chinese poems and barack obama political speeches . in the poem composition task , we use a corpus of 16 , 39 ##4 chinese qu ##at ##rain ##s , each containing four lines of twenty characters in total . to focus on a fully automatic solution and stay general , we did not use any prior knowledge of special structure rules in chinese poems such as specific ph ##ono ##logical rules . in the obama political speech generation task , we use a corpus , which is a collection of 11 , 09 ##2 paragraph ##s from obama ' s political speeches . we use b ##le ##u score as an evaluation metric to measure the similarity degree between the generated texts and the human - created texts . b ##le ##u is originally designed to automatically judge the machine translation quality . the key point is to compare the similarity between the results created by machine and the references provided by human . specifically , for poem evaluation , we set n - gram to be 2 ( b ##le ##u - 2 ) since most words ( dependency ) in classical chinese poems consist of one or two characters and for the similar reason , we use b ##le ##u - 3 and b ##le ##u - 4 to evaluate obama speech generation performance . in our work , we use the whole test set as the references instead of trying to find some references for the following line given the previous line . the reason is in generation tasks we only provide some positive examples and then let the model catch the patterns of them and generate new ones . in addition to b ##le ##u , we also choose poem generation as a case for human judgement since a poem is a creative text construction and human evaluation is ideal . specifically , we mix the 20 real poems and 20 each generated from se ##q ##gan and ml ##e . then 70 experts on chinese poems are invited to judge whether each of the 60 poem is created by human or machines . once regarded to be real , it gets + 1 score , otherwise 0 . finally , the average score for each algorithm is calculated . the experiment results are shown in tables [ reference ] and [ reference ] , from which we can see the significant advantage of se ##q ##gan over the ml ##e in text generation . particularly , for poem composition , se ##q ##gan performs com ##para ##bly to real human data . sub ##section : music generation for music composition , we use nottingham data ##set as our training data , which is a collection of 69 ##5 music of folk tunes in midi file format . we study the solo track of each music . in our work , we use 88 numbers to represent 88 pitches , which correspond to the 88 keys on the piano . with the pitch sampling for every 0 . 4 ##s , we transform the midi files into sequences of numbers from 1 to 88 with the length 32 . to model the fitness of the discrete piano key patterns , b ##le ##u is used as the evaluation metric . to model the fitness of the continuous pitch data patterns , the mean squared error ( ms ##e ) is used for evaluation . from table [ reference ] , we see that se ##q ##gan out ##per ##forms the ml ##e significantly in both metric ##s in the music generation task . section : conclusion in this paper , we proposed a sequence generation method , se ##q ##gan , to effectively train genera ##tive ad ##vers ##aria ##l nets for structured sequences generation via policy gradient . to our best knowledge , this is the first work extending gan ##s to generate sequences of discrete token ##s . in our synthetic data experiments , we used an oracle evaluation mechanism to explicitly illustrate the superiority of se ##q ##gan over strong baseline ##s . for three real - world scenarios , i . e . , poems , speech language and music generation , se ##q ##gan showed excellent performance on generating the creative sequences . we also performed a set of experiments to investigate the robust ##ness and stability of training se ##q ##gan . for future work , we plan to build monte carlo tree search and value network to improve action decision making for large scale data and in the case of longer - term planning . section : ac ##k ##now ##led ##gm ##ents we sincerely thank tian ##xing he for many helpful discussions and comments on the manuscript . bibliography : references appendix : appendix in section 1 , we present the step - by - step derivation of e ##q . ( 6 ) in the paper . in section 2 , the detailed realization of the genera ##tive model and the disc ##rim ##ina ##tive model is discussed , including the model parameter settings . in section 3 , an interesting ab ##lation study is provided , which is a supplementary to the discussions of the synthetic data experiments . sub ##section : proof for e ##q . ( 6 ) for read ##ability , we provide the detailed derivation of e ##q . ( 6 ) here by following . as mentioned in sequence genera ##tive ad ##vers ##aria ##l nets section , the state transition is deter ##mini ##stic after an action has been chosen , i . e . for the next state if the current state and the action ; for other next states , . in addition , the intermediate reward is 0 . we re - write the action value and state value as follows : for the start state , the value is calculated as which is the objective function to maximize in e ##q . ( 1 ) of the paper . then we can obtain the gradient of the objective function , defined in e ##q . ( 1 ) , w . r . t . the generator ' s parameters : which is the result in e ##q . ( 6 ) of the paper . sub ##section : model implementations in this section , we present a full version of the discussed genera ##tive model and disc ##rim ##ina ##tive model in our paper submission . sub ##su ##bs ##ection : the genera ##tive model for sequences we use rec ##urrent neural networks ( rn ##ns ) as the genera ##tive model . an rn ##n maps the input em ##bed ##ding representations of the sequence into a sequence of hidden states by using the update function rec ##urs ##ively . moreover , a soft ##max output layer maps the hidden states into the output token distribution where the parameters are a bias vector and a weight matrix . the vanishing and exploding gradient problem in back ##pro ##pa ##gation through time ( bp ##tt ) issues a challenge of learning long - term depend ##encies to rec ##urrent neural network . to address such problems , gate ##d rn ##ns have been designed based on the basic idea of creating paths through time that have derivatives that neither vanish nor explode . among various gate ##d rn ##ns , we choose the long short - term memory ( l ##st ##m ) to be our genera ##tive networks with the update equations : where is the vector con ##cate ##nation and is the element ##wise product . for simplicity , we use the standard l ##st ##m as the generator , while it is worth noticing that most of the rn ##n variants , such as the gate ##d rec ##urrent unit ( gr ##u ) and soft attention mechanism , can be used as a generator in se ##q ##gan . the standard way of training an rn ##n is the maximum likelihood estimation ( ml ##e ) , which involves mini ##mi ##zing the negative log - likelihood for a generated sequence given input . however , when applying ml ##e to genera ##tive models , there is a disc ##re ##pan ##cy between training and generating , which mo ##tiv ##ates our work . sub ##su ##bs ##ection : the disc ##rim ##ina ##tive model for sequences deep disc ##rim ##ina ##tive models such as deep neural network ( d ##nn ) , con ##vo ##lu ##tion ##al neural network ( cnn ) and rec ##urrent con ##vo ##lu ##tion ##al neural network ( rc ##nn ) have shown a high performance in complicated sequence classification tasks . in this paper , we choose the cnn as our disc ##rim ##inator as cnn has recently been shown of great effectiveness in text ( token sequence ) classification . as far as we know , except for some specific tasks , most disc ##rim ##ina ##tive models can only perform classification well for a whole sequence rather than the unfinished one . in case of some specific tasks , one may design a class ##ifier to provide intermediate reward signal to enhance the performance of our framework . but to make it more general , we focus on the situation where disc ##rim ##inator can only provide final reward , i . e . , the probability that a finished sequence was real . we first represent an input sequence as : where is the - dimensional token em ##bed ##ding and is the vertical con ##cate ##nation operator to build the matrix . then a kernel applies a con ##vo ##lu ##tion ##al operation to a window size of words to produce a new feature map : where operator is the sum ##mation of element ##wise production , is a bias term and is a non - linear function . we can use various numbers of kernel ##s with different window sizes to extract different features . specifically , a kernel with window size applied to the con ##cate ##nated em ##bed ##ding ##s of input sequence will produce a feature map finally we apply a max - over - time pool ##ing operation over the feature map and pass all poole ##d features from different kernel ##s to a fully connected soft ##max layer to get the probability that a given sequence is real . we perform an empirical experiment to choose the kernel window sizes and numbers as shown in table [ reference ] . for different tasks , one should design specific structures for the disc ##rim ##inator . to enhance the performance , we also add the highway architecture before the final fully connected layer : where , and are highway layer weights , denotes an af ##fine transform followed by a non - linear activation function such as a rec ##ti ##fied linear unit ( re ##lu ) and is the \" transform gate \" with the same dimensional ##ity as and . finally , we apply a si ##gm ##oid transformation to get the probability that a given sequence is real : where and is the output layer weight and bias . when opt ##imi ##zing disc ##rim ##ina ##tive models , supervised training is applied to minimize the cross entropy , which is widely used as the objective function for classification and prediction tasks : where is the ground truth label of the input sequence and is the predicted probability from the disc ##rim ##ina ##tive models . sub ##section : more ab ##lation study in the discussion sub ##section of synthetic data experiments section of our paper , we discussed the ab ##lation study of three hyper ##para ##meter ##s of se ##q ##gan , i . e . , g - steps , d - steps and epoch number . here we provide another ab ##lation study which is ins ##truct ##ive for the better training of se ##q ##gan . as described in our paper , we start the ad ##vers ##aria ##l training process after the convergence of ml ##e supervised pre - training . here we further conduct experiments to investigate the performance of se ##q ##gan when the supervised pre - training is insufficient . as shown in figure [ reference ] , if we pre - train the genera ##tive model with conventional ml ##e methods for only 20 epoch ##s , which is far from convergence , then the ad ##vers ##aria ##l training process improves the generator quite slowly and un ##sta ##bly . the reason is that in se ##q ##gan , the disc ##rim ##ina ##tive model provides reward guidance when training the generator and if the generator acts almost randomly , the disc ##rim ##inator will identify the generated sequence to be un ##real with high confidence and almost every action the generator takes receives a low ( unified ) reward , which does not guide the generator towards a good improvement direction , resulting in an ineffective training procedure . this indicates that in order to apply ad ##vers ##aria ##l training strategies to sequence genera ##tive models , a sufficient pre - training is necessary .",
        "pred_seq": "se ##gan [SEP] se ##gan [SEP] [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "seqgan"
                    ]
                ],
                "Method": [
                    [
                        "seqgan"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "chinese poems",
                        "chinese quatrains",
                        "poem"
                    ]
                ],
                "Method": [
                    [
                        "seqgan",
                        "sequence generative adversarial nets",
                        "sequence generation framework",
                        "sequence generation procedure",
                        "sequence generation method"
                    ]
                ],
                "Metric": [
                    [
                        "bleu2"
                    ]
                ],
                "Task": [
                    [
                        "handwriting generation",
                        "sequence generation",
                        "poem generation",
                        "music generation",
                        "natural image generation",
                        "discrete data generation",
                        "sequence data generation",
                        "sequence generation tasks",
                        "go",
                        "generator",
                        "ie poem composition",
                        "text generation",
                        "text generation scenarios",
                        "obama speech generation",
                        "generation tasks",
                        "music generation task",
                        "poems",
                        "speech language",
                        "generating"
                    ]
                ]
            },
            {
                "Material": [],
                "Method": [
                    [
                        "seqgan",
                        "sequence generative adversarial nets",
                        "sequence generation framework",
                        "sequence generation procedure",
                        "sequence generation method"
                    ]
                ],
                "Metric": [
                    [
                        "bleu3"
                    ]
                ],
                "Task": [
                    [
                        "handwriting generation",
                        "sequence generation",
                        "poem generation",
                        "music generation",
                        "natural image generation",
                        "discrete data generation",
                        "sequence data generation",
                        "sequence generation tasks",
                        "go",
                        "generator",
                        "ie poem composition",
                        "text generation",
                        "text generation scenarios",
                        "obama speech generation",
                        "generation tasks",
                        "music generation task",
                        "poems",
                        "speech language",
                        "generating"
                    ]
                ]
            },
            {
                "Material": [],
                "Method": [
                    [
                        "seqgan",
                        "sequence generative adversarial nets",
                        "sequence generation framework",
                        "sequence generation procedure",
                        "sequence generation method"
                    ]
                ],
                "Metric": [
                    [
                        "bleu4"
                    ]
                ],
                "Task": [
                    [
                        "handwriting generation",
                        "sequence generation",
                        "poem generation",
                        "music generation",
                        "natural image generation",
                        "discrete data generation",
                        "sequence data generation",
                        "sequence generation tasks",
                        "go",
                        "generator",
                        "ie poem composition",
                        "text generation",
                        "text generation scenarios",
                        "obama speech generation",
                        "generation tasks",
                        "music generation task",
                        "poems",
                        "speech language",
                        "generating"
                    ]
                ]
            }
        ]
    },
    "33": {
        "doctext": "joint maximum purity forest with application to image super - resolution section : abstract - in this paper , we propose a novel random - forest scheme , namely joint maximum purity forest ( j ##mp ##f ) , for classification , cluster ##ing , and regression tasks . in the j ##mp ##f scheme , the original feature space is transformed into a compact ##ly pre - clustered feature space , via a trained rotation matrix . the rotation matrix is obtained through an it ##erative quan ##ti ##zation process , where the input data belonging to different classes are clustered to the respective vertices of the new feature space with maximum purity . in the new feature space , orthogonal hyper ##planes , which are employed at the split - nodes of decision trees in random forests , can tackle the cluster ##ing problems effectively . we evaluated our proposed method on public bench ##mark data ##set ##s for regression and classification tasks , and experiments showed that j ##mp ##f remarkably out ##per ##forms other state - of - the - art random - forest - based approaches . furthermore , we applied j ##mp ##f to image super - resolution , because the transformed , compact features are more disc ##rim ##ina ##tive to the cluster ##ing - regression scheme . experiment results on several public bench ##mark data ##set ##s also showed that the j ##mp ##f - based image super - resolution scheme is consistently superior to recent state - of - the - art image super - resolution algorithms . section : from achieving the optimal hyper ##planes as sv ##m does ( i . e . , there is no orthogonal constraint in sv ##m ) in some original feature space , as shown in fig . 1 ( a ) . in this paper , we aim to solve this orthogonal ##con ##stra ##int limitation . with the fixed orthogonal hyper ##planes , we propose to rotate the feature space , this is equivalent to rotating the hyper ##planes , in such a way that global maximum purity on the clustered data can be achieved , as illustrated in fig . 2 . this strategy can achieve a joint maximum purity for all the split - nodes when training a random forest . image super - resolution can be performed based on cluster ##ing / classification , according to the recent emerging cluster ##ing - regression stream [ reference ] [ reference ] [ reference ] , and the j ##mp ##f scheme can achieve remarkable performance on both the classification and regression tasks . therefore , j ##mp ##f is applied to single - image super - resolution in this paper . in our algorithm , principal component analysis ( pc ##a ) is applied to the features for dimensional ##ity reduction . the projected feature space is then rotated to a compact , pre ##cl ##ust ##ered feature space via a learned rotation matrix . finally , for all the split - nodes trained for a random forest , their threshold ##s are directly set to the inherent zero - center orthogonal hyper ##planes in the rotated feature space to meet the maximum - purity criterion . experiment results show that j ##mp ##f can achieve more accurate cluster ##ing / classification performance on random forests , and applying j ##mp ##f to image super - resolution can achieve superior quality , compared to state - of - the - art methods . having introduced the main idea of our proposed algorithm , the remainder of this paper is organized as follows . in section ii , we will describe our proposed scheme , the joint maximum purity forest scheme , and present in detail how to compute the rotation matrix via cluster ##ing data into the feature - space vertices . section iii will evaluate our proposed method and compare its performance with recent state - of - the - art random - forest - based approaches on regression and classification tasks . in section iv , we will valid ##ate the performance of j ##mp ##f scheme on single - image super - resolution . conclusions are given in section v . section : ii . joint maximum purity forest scheme section : ii . 1 random forest and our insights a random forest is an ensemble of binary decision trees ( ) : \u2192 ##\u211d , where ( = 1 , 2 , \u2026 , ) is the index of the trees , \u2208 ##\u211d is the m - dimension feature space , and ##\u211d = [ 0 , 1 ] represents the space of class probability distributions over the label space = { 1 , . . . , } . as shown in fig . 1 ( b ) , the vertical dotted line forms a hyper ##plane , = 0 , chosen in the first split - node for separating training samples , and the horizontal dotted line is the hyper ##plane , = 0 , for the second split - node to cluster all the feature data assigned to this node . this results in separating the three data samples ( red , green and blue ) into three leaf - nodes . it can be seen from fig . 1 ( b ) that , for each split - node , the optimal hyper ##plane with more general ##ization capability is the one which can achieve maximum purity in cluster ##ing samples into two groups . for example , the vertical dotted line is the first optimal hyper ##plane because it clusters all the red training samples into the right node , while all the blue and green samples are clustered into the left node . furthermore , the left margin and the right margin are equal . although there is no guarantee that optimal hyper ##planes can be determined for all the split - nodes in a random forest , approximate ##d optimal hyper ##planes can be obtained through a random bag ##ging strategy . the training of a whole random forest is to train all of its decision trees , by choosing the candidate features and threshold ##s for each of the split - nodes , where the feature dimensions and threshold ##s are determined using a random bag ##ging strategy . in the prediction stage , each decision tree returns a class probability ( | ) for a given query sample ##\u2208 ##\u211d , and the final class label y * is then obtained via averaging , as follows : * = ar ##g [UNK] ( | ) . the splitting function for a split - node is denoted as ( ; \u03b8 ) , where is a sample and ##\u03b8 is typically parameter ##ized by two values : ( i ) a feature [UNK] { 1 , . . . , } , and ( ii ) a [UNK] . the splitting function is defined as follows : where the outcome defines to which child node the sample is routed , and 0 and 1 are the two labels for the left and right child nodes , respectively . each node chooses the best splitting function ##\u03b8 * out of a randomly sampled set { \u03b8 } by opt ##imi ##zing the following function : where and are the sets of samples that are routed to the left and the right child nodes , and | | represents the number of samples in the set . during the training of a random forest , the decision trees are provided with a random subset of the training data ( i . e . bag ##ging ) , and are trained independently of each other . therefore , the decision trees are working as independent experts . taking random - forest - based classification as an example , training a single decision tree involves rec ##urs ##ively splitting each node , such that the training data in each newly created child node is clustered according to their corresponding class labels , so the purity at each node is increasing along a tree . each tree is grown until a stopping criterion is reached ( e . g . the number of samples in a node is less than a threshold or the tree depth reaches a maximum value ) and the class probability distributions are estimated in the leaf - nodes . after fulfilling one of these criteria , a density model ( ) in the leaf - node is estimated by all samples falling into this leaf - node for predicting the target value in the testing stage . a simple way to estimate the probability distribution ( ) is averaging all the samples in the leaf - node , while there are also variant methods , such as fitting a ga ##uss ##ian distribution or kernel density estimation , ridge regression [ reference ] [ reference ] [ reference ] , and so on . ( ) is the local score for a set of samples ( is either or ) , which normally is calculated using entropy as in e ##q ##n . ( 4 ) , but it can be replaced by variance [ reference ] [ reference ] [ reference ] or the gin ##i index [ reference ] . where k is the number of classes , and ( | ) is the probability for class , given the set . for the regression problem , the differential entropy : over continuous outputs can be employed , where ( | ) denotes the conditional probability of a target variable given the input sample . assuming ( . , . ) to be a ga ##uss ##ian distribution and having only a finite set of samples , the differential entropy can be written in closed form as where det ( \u03c3 ) is the deter ##mina ##nt of the estimated co ##var ##iance matrix of the target variables in . for training each decision tree in a random forest , the goal on each split - node is to maximize the information gain ( i ##g ) by reducing the entropy after splitting . i ##g is defined as follows : since each decision tree is a binary tree and each step is to split a current node ( a parent set ) into two children nodes ( and sets ) , i ##g can be described as follows : [UNK] is the optimal hyper ##plane of the split - node , and e ##q ##n . ( 8 ) is the target function of each split ##no ##de when training each decision tree of a random forest . as we can see from fig . 1 ( b ) , all the optimal hyper ##planes from split - nodes are achieved independently and locally . since each optimal hyper ##plane is obtained from a subset of feature - dimension candidates with the randomly bag ##ging strategy , there is no guarantee of obtaining a global opt ##imum with respect to all the hyper ##planes in all the split - nodes . an intuitive thinking , which was inspired by the data distribution in fig . 1 ( b ) , is to achieve a global opt ##imum by jointly considering all the hyper ##planes of all the split - nodes , in the form as follows : where is the total number of split - nodes that a training sample has routed through a decision tree . as there is no mathematical solution to the problem described in e ##q ##n . [ reference ] , an alternative way ( i . e . , an approximate method ) to numerical ##ly solving e ##q ##n . ( 9 ) is to jointly maximize the purity of the clustered data groups at each of the split - nodes . this also means that all the data is clustered into the corners ( feature - space vertices ) of the feature space , as shown in fig . 2 . section : ii . 2 the joint maximum purity forest scheme to calculate the threshold for each split - node in each decision tree when training a random forest , we are attempting to determine an orthogonal hyper ##plane for a three - category classification problem , as shown in fig . 1 . since the hyper ##planes for the split - nodes of a decision tree are required to be orthogonal to each other , seeking an optimal orthogonal hyper ##plane locally can not guarantee obtaining maximum purity for the whole tree globally . as shown in fig . 2 , it is easy to determine the vertical hyper ##plane for maximum purity , but it is hard to obtain the horizontal hyper ##plane for maximum purity in the original feature space . to achieve an optimal classification performance for the whole decision tree , all the split - nodes should be considered globally or simultaneously . as shown in fig . 2 , a number of split - nodes , which have their hyper ##planes orthogonal to each other , are required to separate the samples into different nodes . however , if we can transform the samples ( zero ##cent ##ered feature data ) to locate them at the respective corners of the feature space , i . e . { \u2212 ##1 , 1 } for md ##ime ##ns ##ional features , the feature data can be easily and accurately separated by the orthogonal ( either vertical or horizontal ) hyper ##planes , which contain the space center { 0 } , as illustrated in fig . 1 ( b ) . the insight behind this is that the data is clustered into the feature - space vertices ( the corners in a 2 - d feature space means that the data points belong to { \u2212 ##1 , 1 } as the coordinate range is set to [ \u2212 ##1 , 1 ] ) . to tackle the original feature data , which is not ideally clustered in the vertices or corners of the feature space or close to them , as shown in fig . 1 ( a ) , an intuitive idea is to rotate the feature space ( this is equivalent to rotating the hyper ##planes ) . this transformation clusters the feature data compact ##ly into feature - space vertices { \u2212 ##1 , 1 } with a total of 2 vertices . therefore , a possible solution to the problem described in e ##q ##n . ( 10 ) is to rotate the data features by a rotation [UNK] , as shown in fig . 2 , through which the original feature space is transformed into a more compact clustered feature space , where all the feature data is clustered close to the feature - space vertices . this solution can be mathematical ##ly defined as follows : where ##\u2208 ##\u211d ##\u00d7 contains n samples , each of which is a - dimensional feature vector arranged in a row , and is zero - centered , i . e . all the feature vectors are dem ##ean ##ed by sub ##tra ##cting the mean vector from each feature vector . this idea of cluster ##ing data into the feature - space vertices can also be found in locality - sensitive hash ##ing ( l ##sh ) [ reference ] and image representation [ reference ] . in [ reference ] , a simple and efficient alternating mini ##mi ##zation scheme was proposed to find a rotation matrix for zero - centered feature data , which minimize ##s the quan ##ti ##zation errors by mapping the feature data to the vertices of a zero - centered binary hyper ##cu ##be . the method is termed as it ##erative quan ##ti ##zation ( it ##q ) , which can work on multi - class spectral cluster ##ing and orthogonal pro ##cr ##ust ##es problem . yu et al . [ reference ] proposed using a ci ##rc ##ula ##nt matrix to speed up the computation , because the ci ##rc ##ula ##nt structure enables the use of fast fourier transformation ( ff ##t ) . as the computation of the rotation matrix in the training and testing stage is ign ##ora ##ble , we choose a similar scheme to it ##q [ reference ] to determine the rotation matrix ( we throw away the final quan ##ti ##zation matrix described in e ##q ##n . [ reference ] , which is used for hash ##ing in [ reference ] ) , through which the original feature space can be transformed into a new compact clustered feature space : = [UNK] , where the data is located at the respective vertices in the new feature space . after this transformation , a random forest with globally joint maximum purity of all the clustered data can be trained , through all the hyper ##planes in the split - nodes of each decision tree . based on this idea , our proposed scheme is called joint maximum purity forest ( j ##mp ##f ) . section : ii . 3 learning the rotation matrix via cluster ##ing data into feature - space vertices assuming that ##\u2208 ##\u211d is one point in the - dimensional feature space ( zero - centered data ) , the respective vertices in the zero - centered binary hyper ##cu ##be space can be denoted as ( ) \u2208 { \u2212 ##1 , 1 } , and there is a total of 2 vertices in the - dimensional feature space . it is easy to see from we denote a binary code matrix ##\u2208 { \u2212 ##1 , 1 } \u00d7 , whose rows = ( ) \u2208 . for a matrix or a vector , ( . ) applies the sign operation to it element - wise . our objective is to minimize the error between the feature and the feature - space vertices , i . e . , min \u2016 \u2212 \u2016 . as we can see in fig . 2 , when the feature space is rotated , the feature points will be more concentrated around their nearest vertices , which means that the quan ##ti ##zation error will become smaller . therefore , the mini ##mi ##zation problem of min \u2016 \u2212 \u2016 is equivalent to mini ##mi ##zing the error of the zero ##cent ##ered data with respect to the fr ##obe ##nius norm , as in the following formulation : therefore , the task of this mini ##mi ##zation problem is to determine an optimal rotation [UNK] to satisfy e ##q ##n . [ reference ] . since there are two variables in e ##q ##n . ( 11 ) , the expectation - maxim ##ization ( e - m ) algorithm is applied to cluster data into the feature - space vertices , such that a local minimum of the binary code matrix and the rotation [UNK] are computed simultaneously . the idea of rotating feature data to minimize the error between the transformed data and the features ##pace vertices can also be found in [ reference ] , which showed that the rotation [UNK] can be initial ##ized randomly , and then it ##erated to converge to the required rotation matrix . two iteration steps will be performed : in every iteration , each feature vector in the feature space is firstly quan ##tized to the nearest vertex of the binary hyper ##cu ##be , i . e . to a vertex in , and then the rotation [UNK] is updated to minimize the quan ##ti ##zation error by fixing . these two alternating steps are described in detail below : ( 1 ) [UNK] and update : because the zero - centered data matrix is fixed , mini ##mi ##zing e ##q ##n . ( 12 ) is equivalent to maxim ##izing the following term : where is an element of = [UNK] . to maximize e ##q ##n . ( 13 ) with respect to , = 1 whenever ##\u2265 0 and = \u2212 ##1 otherwise , i . e . = ( [UNK] ) \u2208 { \u2212 ##1 , 1 } . ( 2 ) fix and [UNK] : the problem of fixing to obtain a rotation matrix based on the objective function e ##q ##n . ( 11 ) is relative to the classic orthogonal pro ##cr ##ust ##es problem [ reference ] [ reference ] [ reference ] , in which a rotation matrix is determined to align one point set with another . in our algorithm , these two point sets are the zero - centered data set and the quan ##tized matrix . therefore , a closed - form solution [UNK] is available , by applying sv ##d on the ##\u00d7 matrix to obtain ##\u03c9 ( \u03c9 is a diagonal matrix ) , then [UNK] = to [UNK] . section : ii . 4 proof of the orthogonal pro ##cr ##ust ##es problem : for complete ##ness , we prove the orthogonal pro ##cr ##ust ##es problem , for which the solution can be found in [ reference ] [ reference ] [ reference ] : proof : thus , [UNK] \u2016 [UNK] \u2016 equals to maxim ##izing : the last inequality holds because z is also an or ##th ##ono ##rma ##l matrix , [UNK] , = 1 , , \u2264 1 . the objective function can be maximize ##d if z = , i . e . section : [UNK] = [UNK] section : iii . joint maximum purity forest for regression and classification section : iii . 1 the work ##flow of joint maximum purity forest random forest is a machine - learning method using an ensemble of random ##ized decision trees for classification . each tree in a random forest consists of split - nodes and leaf - nodes , which can be trained rec ##urs ##ively . a random forest is constructed rec ##urs ##ively , where each node attempts to find a splitting function or a hyper ##plane to separate its samples into two leaf - nodes , such that the information gain is opt ##imi ##zed . a tree stops growing if the maximum depth is reached or if a node has achieved maximum purity , i . e . it contains only samples from one class . then , each leaf - node collects the statistics of the samples falling in it . in the evaluation phase , the probability of a query sample x belonging to class k is given by averaging all the trees , or by other methods . most random - forest - based models [ reference ] [ reference ] [ reference ] [ reference ] share a similar work ##flow , as shown in fig . 3 , in which the main task on training a tree in a random forest is to decide threshold ##s in the split - nodes and learn the reg ##ress ##ors or classes in the leaf - nodes . rigid regression or linear regression is often employed in the leaf - nodes for the prediction task , because rigid regression has a closed - form solution , while linear regression is an efficient optimization tool , and the li ##bl ##ine ##ar package [ reference ] can be used to fine - tune its configurations . compared to conventional random forests , our j ##mp ##f scheme has one more step , as shown in the left of fig . 3 , the rotation matrix . the j ##mp ##f scheme transforms the original feature space by rotating it into a more compact , pre - clustered feature space , using a trained rotation matrix learned through cluster ##ing feature vectors it ##erative ##ly into the vertices of a new feature space . the whole work ##flow of our proposed algorithm , the j ##mp ##f scheme , is outlined in fig . 3 . the source code of our algorithm is available to download at : https : / / gi ##th ##ub . com / harley ##h ##k / j ##mp ##f . a section : iii . 2 the inherent zero - center hyper ##planes as threshold ##s for split - nodes in training a random forest , the two main operations for training ( splitting ) each split - node are to choose splitting feature ( s ) , and to determine the threshold , using a random bag ##ging strategy , which can avoid over - fitting in training class ##ifiers . in the rotated compact pre - clustered feature space , the inherent zero ##cent ##er hyper ##planes are inherently the optimal threshold ##s ( to meet the max - purity criterion on two clustered data groups ) after training the rotation matrix . therefore , these inherent zero - center hyper ##planes can directly be set as the threshold ##s to achieve optimal classification performance on training a random forest . compared to conventional random forests , our proposed j ##mp ##f only needs to choose which feature ( s ) to split data at split - nodes . this can speed up the training process for a random forest . experimental results in the next sub ##section will valid ##ate this performance . section : iii . 3 : experimental results on j ##mp ##f regression and classification to evaluate the performances of the proposed j ##mp ##f , we test it with 15 standard machine - learning tasks , 7 for classification and 8 for regression . the data ##set ##s used in the experiments are summarized in table - 1 . we use standard performance evaluation metric ##s : error rate for classification and root mean squared error ( rms ##e ) for regression , unless otherwise specified . we firstly evaluate the proposed approach on two real applications , one for classification ( table - 2 ) and one for regression ( table - 3 ) . our proposed j ##mp ##f is compared with the original random forest before ref ##ine ##ment ( denoted as rf ) , and two state - of - the - art variants : alternating decision forests ( ad ##f ) [ reference ] and alternating regression forests ( ar ##f ) [ reference ] , for classification and regression , respectively . furthermore , we compare with j ##mp ##f + ad ##f / ar ##f , for demonstrating that our algorithm can be combined with other methods . we follow the experiment settings in [ reference ] [ reference ] . we set the maximum tree depth d at 15 , and the minimum sample number in a splitting node is set at 5 . the experiments were repeated five times , and the average error and standard deviation were measured . the results are presented in table - 2 and table - 3 , for the classification and regression tasks , respectively . in terms of accuracy , our proposed j ##mp ##f significantly out ##per ##forms the standard random forest on all classification and regression tasks . compared to rf , j ##mp ##f achieve ##s an average of 23 . 57 % improvement on the classification tasks , and an average of 23 . 13 % improvement on the regression tasks . our method also consistently out ##per ##forms the state - of - the ##art variants : ad ##f / ar ##f . moreover , the performance of our j ##mp ##f algorithm can be further improved by integrating with ad ##f and ar ##f , denoted as j ##mp ##f + ad ##f / ar ##f . as shown in table - 2 and table - [ reference ] , j ##mp ##f : proposed algorithm , j ##mp ##f + ar ##f : our proposed algorithm embedded into ar ##f . is the error scale . the number of randomly chosen hyper ##planes # [UNK] is 3 . the percentage ##s in brackets for j ##mp ##f and j ##mp ##f + ar ##f are the reduction rates in rms ##e compared with the rf algorithm . section : iii . 4 : discussions on experimental results the computational complexity of j ##mp ##f is similar to that of the standard random forest . as illustrated in the work ##flow of j ##mp ##f in fig . 3 , only one additional step , which compute ##s the rotation matrix , is required , when compared to the standard random forest . for a small data ##set ( e . g . , feature dimension size less than 500 and data size less than 10 , 000 ) , the computation required to compute the rotation matrix for cluster ##ing data into the feature - space vertices is acceptable in the training stage ( about 10 seconds per level , using mat ##lab ) and ne ##gli ##gible in the testing stage . when the dimension size becomes larger , pc ##a dimensional ##ity reduction can be employed . if the size of the data ##set increases , such that using pc ##a still involves heavy computation , bag ##ging can be used to achieve comparable accuracy and the whole extra computation will be insignificant . , the number hyper ##plane ( s ) # [UNK] on training the random forest is 3 ) . to study the stability of j ##mp ##f , we choose the letter ##ori ##g data ##set for classification and the kin ##8 nm data ##set for regression , and the respective results are shown in fig . 4 ( a ) and fig . 4 ( b ) , respectively . in the experiments , the number of trees , i . e . , the number of weak class ##ifiers in the random forest , varies from 10 to 200 , and we have three observations . firstly , as shown in fig . 4 , when the number of trees increases , the performance of all the algorithms improves . for classification , as shown in fig . 4 ( a ) , when the number of trees is larger than 100 , the errors are converge ##d to become steady . on the contrary , for the regression task as shown in fig . 4 ( b ) , the errors are almost stable , ranged from 10 to 200 . secondly , the results show that j ##mp ##f consistently out ##per ##forms ad ##f and rf , ir ##res ##pe ##ctive of the number of trees used . finally , fig . 4 clearly shows that j ##mp ##f can integrate with ad ##f or ar ##f to further improve its performance . section : iv . image super - resolution based on joint maximum purity forest section : iv . 1 overview of image super - resolution and related works image super - resolution ( sr ) , which recover ##s a high - resolution ( hr ) image from one single image or a number of low - resolution ( l ##r ) images , has been a hot research topic in the field of image processing for decades . sr is a well - known ill - posed problem , which needs artistic skills from mathematics and machine learning . prior methods on sr are mainly based on edge preserving , such as new edge - directed inter ##pol ##ation ( ned ##i ) [ reference ] , soft - decision adaptive inter ##pol ##ation ( sai ) [ reference ] , directional filtering and data - fusion ( d ##f ##df ) [ reference ] , modified edge - directed inter ##pol ##ation ( med ##i ) [ reference ] , etc . the neighbor - em ##bed ##ding ( ne ) methods [ reference ] [ reference ] set the milestone on the patch - learning - based super ##res ##ol ##ution approach . in this approach , each l ##r patch is approximate ##d as a linear regression of its nearest l ##r neighbors in a collected data ##set , while its hr counterpart can be reconstructed with the same coefficients of corresponding hr neighbors , based on the non - linear manifold structure . although the ne method is simple and practical , it requires a huge data ##set ( millions of patches ) to achieve good reconstruction quality and it is computational ##ly intensive , because k - n ##n is used in searching neighboring patches in the huge data ##set . instead of using the patches extracted directly from natural images , yang et al . [ reference ] employed sparse coding [ reference ] [ reference ] to represent patch images , of large size , efficiently , which opens the era for sparse coding in the image inverse problems . the sparse - coding super - resolution ( sc ##sr ) approach is a framework that the hr counterpart of an l ##r patch can be reconstructed aided by two learned di ##ction ##aries , with the sparse constraint on the coefficients via the following formulation ##s : the compact l ##r and hr di ##ction ##aries can be jointly learned with a spa ##rs ##ity constraint , using the following sparse representation : where and are the l ##r patch and the corresponding hr patch , respectively ; and d and d are the l ##r and hr di ##ction ##aries learned from the l ##r and the corresponding hr patch samples , respectively . the value of in \u2016 \u2016 is the spa ##rs ##ity factor of the coefficients . \u2016 \u2016 is - norm , which means the non - zero count of the coefficients in . for each l ##r patch of an input l ##r image , the problem of finding the sparse coefficients can be formulated as follows : where is a linear or non - linear feature - extraction operator on the l ##r patches , which makes the l ##r patches more disc ##rim ##ina ##tive from each other . typically , can be chosen as a high - pass filter , and a simple high - pass filter can be obtained by sub ##tra ##cting the input from the output of a low - pass filter , as in an early work [ reference ] . in the ideal regular ##ization term for the sparse constraint on the coefficients ##\u03b1 is the - norm ( non ##con ##ve ##x ) , but , based on greedy matching , it leads to an np - hard problem . alternatively , yang et al . [ reference ] relaxed it to - norm , as shown in the following formulation : the la ##gra ##nge multi ##pl ##ier provides an equivalent formulation as follows : where the parameter balance ##s the spa ##rs ##ity of the solution and the fidelity of the approximation to . however , the effectiveness of spa ##rs ##ity was challenged in [ reference ] [ reference ] , as to whether real spa ##rs ##ity can help image classification and restoration , or locality property can achieve the same effect . tim ##oft ##e et al . [ reference ] proposed an anchored neighborhood regression ( an ##r ) framework , which relax ##es the sparse decomposition optimization ( - norm ) of [ reference ] [ reference ] to a ridge regression ( - norm ) problem . an important step in the an ##r model is the relaxation of the - norm in e ##q ##n . ( 23 ) to the - norm least - squares mini ##mi ##zation constraint , as follows : where d and d are the l ##r and hr patch - based di ##ction ##aries , respectively . this - norm constraint problem can be solved with a closed - form solution from the ridge regression [ reference ] theory . based on the ti ##kh ##ono ##v regular ##ization / ridge - regression theory , the closed - form solution of the coefficients is given : we assume that the hr patches share the same coefficient ##\u03b1 from their counterpart l ##r patches , i . e . , = d . from e ##q ##n . ( 25 ) , we have : therefore , the hr patches can be reconstructed by : = y , where can be considered a projection matrix , which can be calculated off ##line , as follows : ridge regression allows the coefficients to be calculated by multi ##ply ##ing the constant projection matrix with the new extracted feature , as described in e ##q ##n . ( 26 ) and e ##q ##n . [ reference ] . more importantly , the projection matrix can be pre - computed , and this off ##line learning enables significant speed - up at the prediction stage . tim ##oft ##e et al . [ reference ] further extended the an ##r approach to the a + approach , which learns reg ##ress ##ors from all the training samples , rather than from a small quantity of neighbors of the anchor atoms as an ##r does . later , there are numerous variants and extended approaches , based on an ##r and a + [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . by investigating the an ##r model , li et al . [ reference ] found that the weights of the supporting atoms can be of different values to represent their similarities to the anchor atom . based on this idea , the normal collaborative representation ( cr ) model in an ##r is generalized to a weighted model , named as weighted collaborative representation ( wc ##r ) model , as follows : where is a diagonal matrix . the weights on the diagonal atoms are proportional to their similarities to the anchor atom . similarly , the new closed - form solution for the coefficients can be calculated off ##line , as follows : and the new projection matrix is given as follows : the wc ##r model can further improve the an ##r or a + model in terms of image quality , but it is still a time - consuming problem to find the most similar anchor atoms in a dictionary , and this always hind ##ers its applications where fast speed is greatly required . sc ##hul ##ter et al . [ reference ] adopted the random forest as a class ##ifier , and the reg ##ress ##ors are learned from the patches in the leaf - nodes . with the same number of reg ##ress ##ors , these random - forest - based methods [ reference ] [ reference ] [ reference ] [ reference ] can perform on a par with the a + method in terms of accuracy . however , they achieve an increase in speed , because the sub ##line ##ar search property of random forest can remarkably reduce the reg ##ress ##ors ' search complexity . recently , deep learning has become a hot research topic , which has been successfully applied to image super - resolution [ reference ] [ reference ] [ reference ] [ reference ] and achieved promising performance , particularly in terms of image quality . in [ reference ] [ reference ] , a con ##vo ##lu ##tion ##al neural - network - based image super - resolution ( sr ##c ##nn ) was proposed , in which an end - to - end mapping between l ##r and hr images is learned through a deep con ##vo ##lu ##tion ##al neural network ( cnn ) . the recent emerging stream [ reference ] [ reference ] on single - image sr is to formula ##te the problem as a cluster ##ing ##re ##gre ##ssion problem , which can be solved with machine - learning tools . these approaches are learning ##base ##d methods , which attempt to rec ##ons ##truct an hr image from patches with the help of an external database . these methods first deco ##mp ##ose an image into patches , then classify them into clusters . reg ##ress ##ors are then trained for each of the clusters , which generate mapping ##s from an input l ##r patch ' s feature to its corresponding hr patch ( see fig . 5 ) . in the testing stage , an l ##r query image follows the same procedures to cut into patches and to extract features , which are then assigned to their corresponding clusters using the k - n ##n algorithm [ reference ] [ reference ] or random forest [ reference ] [ reference ] [ reference ] . the respective hr patches are constructed through reg ##ress ##ors learned for the clusters ( see fig . 6 ) . this kind of cluster ##ing - regression algorithms , based on random forest [ reference ] [ reference ] [ reference ] , has achieved state - of - the - art performance in single image super - resolution , both in terms of accuracy and efficiency , because of the use of ensemble learning and sub ##line ##ar search . as j ##mp ##f achieve ##s promising results on both classification and regression tasks , it can be employed for image super - resolution for better performances . an overview of the training and testing processes of the proposed j ##mp ##f - based image sr method is illustrated in fig . 5 and fig . 6 , respectively . in our method , the first and second - order gradient ##s are extracted as features from each patch , followed by pc ##a for dimensional ##ity reduction . these features are then rotated into a more compact , pre - clustered feature space . finally , all the threshold ##s are directly set to the inherent zero - center hyper ##planes when training the random forest , and similar to other algorithms , the reg ##ress ##ors at the leaf - nodes are computed using the rigid regression algorithms . this approach is named as j ##mp ##f - based image super - resolution method . section : iv . 3 the working processes of j ##mp ##f - based image super - resolution j ##mp ##f has been shown to achieve a better performance for cluster ##ing and classification than other random forest methods . since image super - resolution can be considered as a cluster ##ing / classification problem , using j ##mp ##f is likely to result in better performance . this is mainly due to the features transformed to the vertices in the new feature space , so the features become more disc ##rim ##ina ##tive . the image super - resolution training and testing processes of our proposed j ##mp ##f - based method are described in algorithm 1 and algorithm 2 , respectively . section : iv . 4 experimental results on j ##mp ##f - based image super - resolution in this section , we evaluate our image sr algorithm on some standard super - resolution data ##set ##s , including set 5 , set ##14 , and b1 ##00 [ reference ] , and compare it with a number of classical or state - of - the - art methods . these include bi ##cu ##bic inter ##pol ##ation , sparse representation sr ( ze ##yde ) [ reference ] , anchored neighborhood regression ( an ##r ) [ reference ] , a + [ reference ] , standard random forest ( rf ) [ reference ] , and alternating regression forests ( ar ##f ) [ reference ] . we set the same parameters for all the random - forest - based algorithms : the number of trees in the random forest is 10 , and the maximum depth of each tree is 15 . experiment results are tab ##ulated in tables - 4 and tables - 5 , where j ##mp ##f is our proposed j ##mp ##f - based image super - resolution method , and j ##mp ##f is a trimmed version , such that the threshold ##s for the split ##no ##des are not the inherent zero - center hyper ##planes , but set by the standard random - forest bag ##ging strategy . we use the same training images ( 91 images ) for all the algorithms as previous works [ reference ] [ reference ] [ reference ] [ reference ] do . however , for j ##mp ##f + , 100 more images from the general - 100 data ##set [ reference ] are used , so as to check whether or not more training samples can further improve our proposed algorithm . table - 5 : detailed results of the proposed method , compared with state - of - the - art methods on the data ##set set ##5 , in terms of ps ##nr ( db ) using three different mag ##ni ##fication factors ( \u00d7 ##2 , \u00d7 ##3 , \u00d7 ##4 ) . table - 4 tab ##ulates the performances , in terms of the average peak signal to noise ratio ( ps ##nr ) scores , of our proposed algorithm and other image sr methods , on the 3 data ##set ##s with different mag ##ni ##fication factors . for the set ##5 and set ##14 data ##set ##s , with different mag ##ni ##fication factors , our proposed j ##mp ##f - based algorithm can achieve a comparable performance to other recent state - of - the - art methods , such as a + and ar ##f . as those random - forest - based algorithms may not be stable on small data ##set ##s , when evaluation works on extensive data ##set ##s , such as b1 ##00 , our proposed algorithm j ##mp ##f can stab ##ly out ##per ##form a + and ar ##f for all mag ##ni ##fication factors ( \u00d7 ##2 , \u00d7 ##3 , \u00d7 ##4 ) . moreover , the objective quality metric ##s on ps ##nr also show that the j ##mp ##f algorithm can achieve a better performance when more samples are used for training , as shown from j ##mp ##f + in table - 4 . table - 5 provides more details of the performances in data ##set ##s set ##5 . to compare the visual quality of our proposed j ##mp ##f - based sr algorithm to other methods , fig . 7 , shows the reconstructed hr images using different methods . some regions in the reconstructed images are also enlarged , so as to show the details in the images . in general , our proposed method can produce better quality images , particularly in areas with rich texture , which ve ##ri ##fies the feature discrimination of the proposed j ##mp ##f scheme . section : v . conclusions in this paper , we have proposed a novel random - forest scheme , namely the joint maximum purity forest ( j ##mp ##f ) scheme , which rotate ##s the feature space into a compact , clustered feature space , by jointly maxim ##izing the purity of all the feature - space vertices . in the new pre - clustered feature space , orthogonal hyper ##planes can be effectively used in the split - nodes of a decision tree , which can improve the performance of the trained random forest . compared to the standard random forests and the recent state - of - the - art variants , such as alternating decision forests ( ad ##f ) [ reference ] and alternating regression forests ( ar ##f ) [ reference ] , our proposed random - forest method inherit ##s the merits of random forests ( fast training and testing , multi - class capability , etc . ) , and also yields promising results on both classification and regression tasks . experiments have shown that our method achieve ##s an average improvement of about 20 % for classification and regression on publicly bench ##mark ##ed data ##set ##s . furthermore , our proposed scheme can integrate with other methods , such as ad ##f and ar ##f , to further improve the performance . we have also applied j ##mp ##f to single - image super - resolution . we tackle image super - resolution as a cluster ##ing - regression problem , and focus on the cluster ##ing stage , which happens at the split - nodes of each decision tree . by employing the j ##mp ##f strategy , we rotate the feature space into a pre - clustered feature space , which can cluster samples into different sub - spaces more compact ##ly in an un ##su ##per ##vis ##ed problem . the compact pre - clustered feature space can provide the optimal threshold ##s for split - nodes in decision trees , which are the zero - centered orthogonal hyper ##planes . our experiment results on intensive image bench ##mark data ##set ##s , such as b1 ##00 , show that the proposed j ##mp ##f - based image super - resolution approach can consistently out ##per ##form recent state - of - the - art algorithms , in terms of ps ##nr and visual quality . our method also inherit ##s the advantages of random forests , which have fast speed on both the training and inference processes . section :",
        "pred_seq": "[SEP] joint forest [SEP] [SEP] image resolution [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "joint maximum purity forest"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "image superresolution"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "b100"
                    ]
                ],
                "Method": [
                    [
                        "joint maximum purity forest scheme",
                        "joint maximum purity forest",
                        "jmpf"
                    ]
                ],
                "Metric": [
                    [
                        "psnr",
                        "peak signal to noise ratio"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "singleimage superresolution",
                        "sr",
                        "single image superresolution",
                        "image superresolution training"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set14 datasets"
                    ]
                ],
                "Method": [
                    [
                        "joint maximum purity forest scheme",
                        "joint maximum purity forest",
                        "jmpf"
                    ]
                ],
                "Metric": [
                    [
                        "psnr",
                        "peak signal to noise ratio"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "singleimage superresolution",
                        "sr",
                        "single image superresolution",
                        "image superresolution training"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "set5"
                    ]
                ],
                "Method": [
                    [
                        "joint maximum purity forest scheme",
                        "joint maximum purity forest",
                        "jmpf"
                    ]
                ],
                "Metric": [
                    [
                        "psnr",
                        "peak signal to noise ratio"
                    ]
                ],
                "Task": [
                    [
                        "image superresolution",
                        "singleimage superresolution",
                        "sr",
                        "single image superresolution",
                        "image superresolution training"
                    ]
                ]
            }
        ]
    },
    "34": {
        "doctext": "document : cell - aware stacked l ##st ##ms for modeling sentences we propose a method of stack ##ing multiple long short - term memory ( l ##st ##m ) layers for modeling sentences . in contrast to the conventional stacked l ##st ##ms where only hidden states are fed as input to the next layer , our architecture accepts both hidden and memory cell states of the preceding layer and fuse ##s information from the left and the lower context using the soft ga ##ting mechanism of l ##st ##ms . thus the proposed stacked l ##st ##m architecture mod ##ulates the amount of information to be delivered not only in horizontal rec ##ur ##rence but also in vertical connections , from which useful features extracted from lower layers are effectively conveyed to upper layers . we dub this architecture cell - aware stacked l ##st ##m ( cas - l ##st ##m ) and show from experiments that our models achieve state - of - the - art results on bench ##mark data ##set ##s for natural language inference , para ##ph ##rase detection , and sentiment classification . section : introduction in the field of natural language processing ( nl ##p ) , the most prevalent neural approach to obtaining sentence representations is to use rec ##urrent neural networks ( rn ##ns ) , where words in a sentence are processed in a sequential and rec ##urrent manner . along with their intuitive design , rn ##ns have shown outstanding performance across various nl ##p tasks e . g . language modeling , machine translation , text classification , and par ##sing . among several variants of the original rn ##n , gate ##d rec ##urrent architecture ##s such as long short - term memory ( l ##st ##m ) and gate ##d rec ##urrent unit ( gr ##u ) have been accepted as de - facto standard choices for rn ##ns due to their capability of addressing the vanishing and exploding gradient problem and considering long - term depend ##encies . gate ##d rn ##ns achieve these properties by introducing additional ga ##ting units that learn to control the amount of information to be transferred or forgotten , and are proven to work well without relying on complex optimization algorithms or careful initial ##ization . meanwhile , the common practice for further enhancing the expressive ##ness of rn ##ns is to stack multiple rn ##n layers , each of which has distinct parameter sets ( stacked rn ##n ) . in stacked rn ##ns , the hidden states of a layer are fed as input to the subsequent layer , and they are shown to work well due to increased depth or their ability to capture hierarchical time series which are inherent to the nature of the problem being modeled . 0 . 25 0 . 25 however this setting of stack ##ing rn ##ns might hind ##er the possibility of more sophisticated rec ##ur ##rence - based structures since the information from lower layers is simply treated as input to the next layer , rather than as another class of state that participates in core rn ##n computation ##s . especially for gate ##d rn ##ns such as l ##st ##ms and gr ##us , this means that layer - to - layer connections can not fully benefit from the carefully constructed ga ##ting mechanism used in temporal transitions . some recent work on stack ##ing rn ##ns suggests alternative methods that encourage direct and effective interaction between rn ##n layers by adding residual connections , by short ##cut connections , or by using cell states of l ##st ##ms . in this paper , we propose a method of constructing multi - layer l ##st ##ms where cell states are used in controlling the vertical information flow . this system utilizes states from the left and the lower context equally in computation of the new state , thus the information from lower layers is elaborate ##ly filtered and reflected through a soft ga ##ting mechanism . our method is easy - to - implement , effective , and can replace conventional stacked l ##st ##ms without much modification of the overall architecture . we call the proposed architecture cell - aware stacked l ##st ##m , or cas - l ##st ##m , and evaluate our method on multiple bench ##mark data ##set ##s : s ##nl ##i , multi ##nl ##i , quo ##ra question pairs , and ss ##t . from experiments we show that the cas - l ##st ##ms consistently out ##per ##form typical stacked l ##st ##ms , opening the possibility of performance improvement of architecture ##s that use stacked l ##st ##ms . our contribution is summarized as follows . we bring the idea of utilizing states coming from multiple directions to construction of stacked l ##st ##m and apply the idea to the research of sentence representation learning . there is some prior work addressing the idea of incorporating more than one type of state , however to the best of our knowledge there is little work on applying the idea to sentence encoding and text classification . we conduct extensive evaluation of the proposed method and empirical ##ly prove its effectiveness on encoding sentences . our models achieve new state - of - the - art results on s ##nl ##i and quo ##ra question pairs data ##set ##s , and are on par with the best performing models on multi ##nl ##i and ss ##t data ##set ##s . this paper is organized as follows . we give a detailed description about the proposed method in \u00a7 [ reference ] . experimental results are given in \u00a7 [ reference ] . we study prior work related to our objective in \u00a7 [ reference ] and conclude in \u00a7 [ reference ] . section : model description in this section , we give a detailed formulation of the architecture ##s used in experiments . sub ##section : notation throughout this paper , we denote matrices as bold ##face capital letters ( ) , vectors as bold ##face lower ##case letters ( ) , and scala ##rs as normal it ##alic letters ( ) . for l ##st ##m states , we denote a hidden state as and a cell state as . also , a layer index of or is denoted by super ##script and a time index is denoted by a sub ##script , i . e . indicates the hidden state at time and layer . means the element - wise multiplication between two vectors . we write - th component of vector as . all vectors are assumed to be column vectors . sub ##section : stacked l ##st ##ms while there exist various versions of l ##st ##m formulation , in this work we use the following , one of the most common versions : where , , and , , are train ##able parameters . and are the si ##gm ##oid activation and the hyper ##bolic tangent activation function respectively . also we assume that where is the - th input to the network . the input gate and the forget gate control the amount of information transmitted from and , the candidate cell state and the previous cell state , to the new cell state . similarly the output gate soft - selects which portion of the cell state is to be used in the final hidden state . we can clearly see that cell states ( , , ) play a crucial role in forming horizontal rec ##ur ##rence . however the current formulation does not consider , the cell state from - th layer , in computation and thus the lower context is reflected only through the ru ##diment ##ary way , hind ##ering the possibility of controlling vertical information flow . sub ##section : cell - aware stacked l ##st ##ms now we extend the stacked l ##st ##m formulation defined above to address the problem noted in the previous sub ##section . to enhance the interaction between layers in a way similar to how l ##st ##ms keep and forget the information from the previous time step , we introduce the additional forget gate that determines whether to accept or ignore the signals coming from the previous layer . therefore the proposed cell - aware stacked l ##st ##m is formulated as follows : where and . can either be a vector of constant ##s or parameters . when , the equations defined in the previous sub ##section are used . therefore , it can be said that each non - bottom layer of cas - l ##st ##m accepts two sets of hidden and cell states \u2014 one from the left context and the other from the below context . the left and the below context participate in computation with the equivalent procedure so that the information from lower layers can be efficiently prop ##aga ##ted . fig . [ reference ] compares cas - l ##st ##m to the conventional stacked l ##st ##m architecture , and fig . [ reference ] depicts the computation flow of the cas - l ##st ##m . we argue that considering in computation is beneficial for the following reasons . first , contains additional information compared to since it is not filtered by . thus a model that directly uses does not rely solely on for extract ##ing information , due to the fact that it has access to the raw information , as in temporal connections . in other words , no longer has to take all responsibility for selecting useful features for both horizontal and vertical transitions , and the burden of selecting information is shared with . another advantage of using the lies in the fact that it directly connects and . this direct connection helps and stabilize ##s training , since the terminal error signals can be easily back ##pro ##pa ##gated to model parameters . fig . [ reference ] illustrates paths between the two cell states . we find experimental ##ly that there is little difference between letting be constant and letting it be train ##able parameters , thus we set in all experiments . we also experimented with the architecture without i . e . two cell states are combined by un ##weight ##ed sum ##mation similar to multi ##dim ##ens ##ional rn ##ns , and found that it leads to performance degradation and unstable convergence , likely due to mis ##mat ##ch in the range of cell state values between layers ( for the first layer and for the others ) . experimental results on various are presented in \u00a7 [ reference ] . paragraph : connection to tree - structured rn ##ns . the idea of having multiple states is also related to tree - structured rn ##ns . among them , tree - structured l ##st ##ms ( tree - l ##st ##ms ) are similar to ours in that they use both hidden and cell states from children nodes . in tree - l ##st ##ms , states for all children nodes are regarded as input , and they participate in the computation equally through weight - shared ( in child - sum tree - l ##st ##ms ) or weight - un ##sha ##red ( in - ar ##y tree - l ##st ##ms ) projection . from this perspective , each cas - l ##st ##m layer ( where ) can be seen as a binary tree - l ##st ##m where the structures it operates on are fixed to right - branching trees . the use of cell state in computation could be one reason that tree - l ##st ##ms perform better than sequential l ##st ##ms even when trivial trees ( strictly left - or right - branching ) are given . paragraph : connection to multi ##dim ##ens ##ional rn ##ns . multi ##dim ##ens ##ional rn ##ns ( md ##rn ##n ) are an extension of 1 ##d sequential rn ##ns that can accept multi ##dim ##ens ##ional input e . g . images , and have been successfully applied to image segment ##ation and handwriting recognition . notably multi ##dim ##ens ##ional l ##st ##ms ( md ##ls ##tm ) have an analogous formulation to ours except the term and the fact that we use distinct weights per column ( or ' layer ' in our case ) . from this view , cas - l ##st ##m can be seen as a certain kind of md ##ls ##tm that accepts a 2d input . grid l ##st ##ms also take inputs but emi ##t outputs , which is different from our case where a single set of hidden and cell states is produced . sub ##section : sentence en ##code ##rs the sentence en ##code ##r network we use in our experiments takes words ( assumed to be one - hot vectors ) as input . the words are projected to corresponding word representations : where . then is fed to a - layer cas - l ##st ##m model , resulting in the representations . the sentence representation , , is computed by max - pool ##ing over time as in the work of con ##nea ##u ##20 ##17 ##in ##fers ##ent con ##nea ##u ##20 ##17 ##in ##fers ##ent . similar to their results , from preliminary experiments we found that the max - pool ##ing performs consistently better than mean - and last - pool ##ing . to make models more expressive , a bid ##ire ##ction ##al cas - l ##st ##m network may also be used . in the bid ##ire ##ction ##al case , the forward representations and the backward representations are con ##cate ##nated and max - poole ##d to yield the sentence representation . we call this bid ##ire ##ction ##al architecture bi - cas - l ##st ##m in experiments . sub ##section : top - layer class ##ifiers for the natural language inference experiments , we use the following he ##uri ##stic function proposed by mo ##u ##20 ##16 ##s ##nl ##i mo ##u ##20 ##16 ##s ##nl ##i in feature extraction : where means vector con ##cate ##nation , and and are applied element - wise . and we use the following function in para ##ph ##rase identification experiments : as in the work of ji ##20 ##13 ##dis ##cr ##imi ##nat ##ive ji ##20 ##13 ##dis ##cr ##imi ##nat ##ive . for sentiment classification , we use the sentence representation itself . we feed the feature extracted from as input to the ml ##p class ##ifier with re ##lu activation followed by the fully - connected soft ##max layer to predict the label distribution : where , is the number of label classes , and the dimension of the ml ##p output , section : experiments we evaluate our method on natural language inference ( nl ##i ) , para ##ph ##rase identification ( pi ) , and sentiment classification . we also conduct analysis on gate values and experiments on model variants . for detailed experimental settings , we refer readers to the supplemental material . for the nl ##i and pi tasks , there exists recent work specializing in sentence pair classification . however in this work we con ##fine our model to the architecture that en ##codes each sentence using a shared en ##code ##r without any inter - sentence interaction , in order to focus on the effectiveness of the models in extract ##ing semantics . but note that the app ##lica ##bility of cas - l ##st ##m is not limited to sentence encoding based approaches . sub ##section : natural language inference for the evaluation of performance of the proposed method on the nl ##i task , s ##nl ##i and multi ##nl ##i data ##set ##s are used . the objective of both data ##set ##s is to predict the relationship between a premise and a hypothesis sentence : en ##tail ##ment , contradiction , and neutral . s ##nl ##i and multi ##nl ##i data ##set ##s are composed of about 570 ##k and 430 ##k premise - hypothesis pairs respectively . glove pre ##train ##ed word em ##bed ##ding ##s are used and remain fixed during training . the dimension of en ##code ##r states ( ) is set to 300 and a 102 ##4 ##d ml ##p with one or two hidden layers is used . we apply drop ##out to the word em ##bed ##ding ##s and the ml ##p layers . the features used as input to the ml ##p class ##ifier are extracted following e ##q . [ reference ] . table [ reference ] and [ reference ] contain results of the models on s ##nl ##i and multi ##nl ##i data ##set ##s . in s ##nl ##i , our best model achieve ##s the new state - of - the - art accuracy of 87 . 0 % with relatively fewer parameters . similarly in multi ##nl ##i , our models match the accuracy of state - of - the - art models in both in - domain ( matched ) and cross - domain ( mis ##mat ##ched ) test sets . note that only the glove word vectors are used as word representations , as opposed to some models that introduce character - level features . it is also notable that our proposed architecture does not restrict the selection of pool ##ing method ; the performance could further be improved by replacing max - pool ##ing with other advanced algorithms e . g . intra - sentence attention and generalized pool ##ing . sub ##section : para ##ph ##rase identification we use quo ##ra question pairs data ##set in evaluating the performance of our method on the pi task . the data ##set consists of over 400 ##k question pairs , and each pair is ann ##ota ##ted with whether the two sentences are para ##ph ##rase of each other or not . similar to the nl ##i experiments , glove pre ##train ##ed vectors , 300 ##d en ##code ##rs , and 102 ##4 ##d ml ##p are used . the number of cas - l ##st ##m layers is fixed to 2 in pi experiments . two sentence vectors are aggregate ##d using e ##q . [ reference ] and fed as input to the ml ##p . the results on the quo ##ra question pairs data ##set are summarized in table [ reference ] . again we can see that our models out ##per ##form other models by large margin , achieving the new state of the art . 0 . 22 0 . 22 0 . 22 0 . 22 0 . 22 0 . 22 sub ##section : sentiment classification in evaluating sentiment classification performance , the stanford sentiment tree ##bank ( ss ##t ) is used . it consists of about 12 , 000 binary - par ##sed sentences where constituents ( phrases ) of each par ##se tree are ann ##ota ##ted with a sentiment label ( very positive , positive , neutral , negative , very negative ) . following the convention of prior work , all phrases and their labels are used in training but only the sentence - level data are used in evaluation . in evaluation we consider two settings , namely ss ##t - 2 and ss ##t - 5 , the two differing only in their level of gran ##ular ##ity with regard to labels . in ss ##t - 2 , data samples ann ##ota ##ted with ' neutral ' are ignored from training and evaluation . the two positive labels ( very positive , positive ) are considered as the same label , and similarly for the two negative labels . as a result 98 , 79 ##4 / 87 ##2 / 1 , 82 ##1 data samples are used in training / validation / test , and the task is considered as a binary classification problem . in ss ##t - 5 , data are used as - is and thus the task is a 5 - class classification problem . all 318 , 58 ##2 / 1 , 101 / 2 , 210 data samples for training / validation / test are used in the ss ##t - 5 setting . we use 300 ##d glove vectors , 2 - layer 150 ##d or 300 ##d en ##code ##rs , and a 300 ##d ml ##p class ##ifier for the models , however unlike previous experiments we tune the word em ##bed ##ding ##s during training . the results on ss ##t are listed in table [ reference ] . our models achieve the new state - of - the - art accuracy on ss ##t - 2 and competitive accuracy on ss ##t - 5 , without utilizing par ##se tree information . sub ##section : forget gate analysis to inspect the effect of the additional forget gate , we investigate how the values of vertical forget gates are distributed . we sample 1 , 000 random sentences from the development set of the s ##nl ##i data ##set , and use the 3 - layer cas - l ##st ##m model trained on the s ##nl ##i data ##set to compute gate values . if all values from a vertical forget gate were to be 0 , this would mean that the introduction of the additional forget gate is meaningless and the model would reduce to a plain stacked l ##st ##m . on the contrary if all values were 1 , meaning that the vertical forget gates were always open , it would be impossible to say that the information is mod ##ulated effectively . fig . [ reference ] and [ reference ] represent his ##to ##gram ##s of the vertical forget gate values from the second and the third layer . from the figures we can valid ##ate that the trained model does not fall into the de ##gen ##erate case where vertical forget gates are ignored . also the figures show that the values are right - sk ##ew ##ed , which we conjecture to be a result of focusing more on a strong interaction between adjacent layers . to further verify that the gate values are diverse enough within each time step , we compute the distribution of the range of values per time step , , where . we plot the his ##to ##gram ##s in fig . [ reference ] and [ reference ] . from the figure we see that a vertical forget gate controls the amount of information flow effectively , making the decision of retaining or disc ##arding signals . finally , to investigate the argument presented in \u00a7 [ reference ] that the additional forget gate helps the previous output gate with reducing the burden of extract ##ing all needed information , we inspect the distribution of the values from . this distribution indicates how differently the vertical forget gate and the previous output gate select information from . from fig . [ reference ] and [ reference ] we can see that the two gates make fairly different decisions , from which we demonstrate that the direct path between and enables a model to utilize signals overlooked by . sub ##section : model variations in this sub ##section , we see the influence of each component of a model on performance by removing or replacing its components . the s ##nl ##i data ##set is used for experiments , and the best performing configuration is used as a baseline for modifications . we consider the following variants : ( i ) models that use plain stacked l ##st ##ms , ( ii ) models with different , ( iii ) models without , and ( iv ) models that integrate lower contexts via pee ##ph ##ole connections . variant ( iv ) integrate ##s lower contexts via the following equations : where represent pee ##ph ##ole weights that take cell states into account . among the above equations , those that use the lower cell state are e ##q . [ reference ] and [ reference ] . we can see that affects the value of only via pee ##ph ##ole connections , which makes independent of . table [ reference ] sum ##mar ##izes the results of model variants . we can again see that the use of cell states clearly improves sentence modeling performance ( baseline vs . ( i ) and ( iv ) vs . ( i ) ) . also from the results of baseline and ( ii ) , we valid ##ate that the selection of does not significantly affect performance but introducing is beneficial ( baseline vs . ( iii ) ) possibly due to its effect on normal ##izing information from multiple sources , as mentioned in \u00a7 [ reference ] . finally , from the comparison between baseline and ( iv ) , we show that the proposed way of combining the left and the lower contexts leads to better modeling of sentence representations than that of zhang ##20 ##16 ##hi ##gh ##way zhang ##20 ##16 ##hi ##gh ##way in encoding sentences . section : related work paragraph : stacked rec ##urrent neural networks . there is some prior work on methods of stack ##ing rn ##ns beyond the plain stacked rn ##ns . residual l ##st ##ms add residual connections between the hidden states computed at each l ##st ##m layer , and short ##cut - stacked l ##st ##ms con ##cate ##nate hidden states from all previous layers to make the back ##pro ##pa ##gation path short . in our method , the lower context is aggregate ##d via a ga ##ting mechanism , and we believe it mod ##ulates the amount of information to be transmitted in a more efficient and effective way than vector addition or con ##cate ##nation . also , compared to con ##cate ##nation , our method does not significantly increase the number of parameters . highway l ##st ##ms and depth - gate ##d l ##st ##ms are similar to our proposed models in that they use cell states from the previous layer , and they are successfully applied to the field of automatic speech recognition and language modeling . however in contrast to cas - l ##st ##m , where the additional forget gate aggregate ##s the previous layer states , and thus contexts from the left and below participate in computation e ##qui ##ta ##bly , in highway l ##st ##ms and depth - gate ##d l ##st ##ms the previous layer states are considered only through pee ##ph ##ole connections . the comparison of our models and this architecture is presented in \u00a7 [ reference ] . paragraph : multi ##dim ##ens ##ional rec ##urrent neural networks . there is another line of research that aims to extend rn ##ns to operate on multi ##dim ##ens ##ional inputs . grid l ##st ##ms are a general - dimensional l ##st ##m architecture that accepts sets of hidden and cell states as input and yields sets of states as output , in contrast to our architecture , which emi ##ts a single set of states . 2d and 3d grid l ##st ##ms bring a performance gain on character - level language modeling and machine translation respectively . multi ##dim ##ens ##ional rn ##ns have a similar formulation as ours , except that they do not normal ##ize cell states and weights for all columns ( layers ) are tied . however they are often employed to model multi ##dim ##ens ##ional data such as images of hand ##written text with rn ##ns , rather than stack ##ing rn ##n layers for modeling sequential data . paragraph : deep rec ##urrent transitions . rather than stack ##ing rec ##urrent layers , some work focuses on increasing the depth of horizontal rec ##ur ##rence . pas ##can ##u ##20 ##14 ##con ##st ##ru ##ct pas ##can ##u ##20 ##14 ##con ##st ##ru ##ct have investigated various architecture ##s to increase the depth of rn ##ns , inter ali ##a deep transition rn ##ns address the problem of deep hidden - to - hidden transitions . graves ##20 ##16 ##ada ##ptive graves ##20 ##16 ##ada ##ptive proposed an adaptive computation time algorithm that learns how many micro time steps to take between receiving an input and emi ##tting an output . fast - slow rn ##ns process data on different times ##cal ##es by letting a fast cell it ##erate for a fixed number of time steps before a slow cell receives the next input . multi ##sca ##le rn ##ns e . g . clock ##work rn ##ns and hierarchical multi ##sca ##le rn ##ns can be also regarded as architecture ##s with increased rec ##ur ##rence depth . however as noted by z ##illy ##20 ##17 ##rh ##n z ##illy ##20 ##17 ##rh ##n , increase in rec ##urrent depth results in a longer maximum path than stack ##ing rec ##urrent layers and makes training difficult without careful initial ##ization or architectural choice . section : conclusion in this paper , we proposed a method of stack ##ing multiple l ##st ##m layers for modeling sentences , dubbed cas - l ##st ##m . it uses not only hidden states but also cell states from the previous layer , for the purpose of controlling the vertical information flow in a more elaborate way . we evaluated the proposed method on various bench ##mark tasks : natural language inference , para ##ph ##rase identification , and sentiment classification . our models achieve the new state - of - the - art accuracy on s ##nl ##i and quo ##ra question pairs data ##set ##s and obtain comparable results on multi ##nl ##i and ss ##t data ##set ##s . the proposed architecture can replace any stacked l ##st ##m under one weak restriction \u2014 the size of states should be identical across all layers . for future work we plan to apply the cas - l ##st ##m architecture beyond sentence modeling tasks . various problems e . g . sequence labeling , sequence generation , and language modeling might benefit from sophisticated modulation on context integration . ag ##gre ##gating diverse contexts from sequential data , e . g . those from forward and backward reading of text , could also be an intriguing research direction . section : ac ##k ##now ##led ##gm ##ents we thank dan ed ##mist ##on for the review of the manuscript . bibliography : references",
        "pred_seq": "[SEP] cell ##ms [SEP] [SEP] natural inference [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "cellaware stacked lstms"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "natural language inference"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "quora question pairs",
                        "quora question pairs datasets"
                    ]
                ],
                "Method": [
                    [
                        "bidirectional caslstm network"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "paraphrase identification",
                        "pi",
                        "pi tasks",
                        "pi task"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "snli",
                        "snli dataset"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "natural language inference",
                        "nli",
                        "nli task"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "sst",
                        "sst datasets",
                        "sst2"
                    ]
                ],
                "Method": [
                    [
                        "bidirectional caslstm network"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "sentiment classification"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "sst datasets",
                        "sst5"
                    ]
                ],
                "Method": [
                    [
                        "bidirectional caslstm network"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "sentiment classification"
                    ]
                ]
            }
        ]
    },
    "35": {
        "doctext": "document : robust face detection via learning small faces on hard images recent anchor - based deep face detectors have achieved promising performance , but they are still struggling to detect hard faces , such as small , blurred and partially o ##cc ##lu ##ded faces . a reason is that they treat all images and faces equally , without putting more effort on hard ones ; however , many training images only contain easy faces , which are less helpful to achieve better performance on hard images . in this paper , we propose that the robust ##ness of a face detector against hard faces can be improved by learning small faces on hard images . our intuition ##s are ( 1 ) hard images are the images which contain at least one hard face , thus they facilitate training robust face detectors ; ( 2 ) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking . we build an anchor - based deep face detector , which only output a single feature map with small anchors , to specifically learn small faces and train it by a novel hard image mining strategy . extensive experiments have been conducted on wider face , f ##dd ##b , pascal faces , and af ##w data ##set ##s to show the effectiveness of our method . our method achieve ##s ap ##s of 95 . 7 , 94 . 9 and 89 . 7 on easy , medium and hard wider face val data ##set respectively , which sur ##pass the previous state - of - the - arts , especially on the hard subset . code and model are available at . section : introduction face detection is a fundamental and important computer vision problem , which is critical for many face - related tasks , such as face alignment , tracking and recognition . stem from the recent successful development of deep neural networks , massive cnn - based face detection approaches have been proposed and achieved the state - of - the - art performance . however , face detection remains a challenging task due to o ##cc ##lusion , illumination , makeup , as well as pose and scale variance , as shown in the bench ##mark data ##set wider face . current state - of - the - art cnn - based face detectors attempt to address these challenges by employing more powerful backbone models , exploit ##ing feature pyramid - style architecture ##s to combine features from multiple detection feature maps , designing dense ##r anchors and utilizing larger context ##ual information . these methods and techniques have been shown to be successful to build a robust face detector , and improve the performance towards human - level for most images . in spite of their success for most images , an evident performance gap still exists especially for those hard images which contain small , blurred and partially o ##cc ##lu ##ded faces . we realize that these hard images have become the main barriers for face detectors to achieve human - level detection performance . in figure [ reference ] , we show that , even on the train set of wider face , the official pre - trained ss ##h still fails on some of the images with extremely hard faces . we show two such hard training images in the upper right corner in figure [ reference ] . on the other hand , most training images with easy faces can be almost perfectly detected ( see the illustration in the right lower corner of figure [ reference ] ) . as shown in left part of figure [ reference ] , over two thirds of the training images already obtained perfect detection accuracy , which indicates that those easy images are less useful towards training a robust face detector . to address this issue , in this paper , we propose a robust face detector by putting more training focus on those hard images . this issue is most related to anchor - level hard example mining discussed in oh ##em . however , due to the spa ##rs ##ity of ground - truth faces and positive anchors , traditional anchor - level hard example mining mainly focuses on mining hard negative anchors , and mining hard anchors on well - detected images exhibits less effectiveness since there is no useful information that can be further exploited in these easy images . to address this issue , we propose to mine hard examples at image level in parallel with anchor level . more specifically , we propose to dynamic ##ally assign difficulty scores to training images during the learning process , which can determine whether an image is already well - detected or still useful for further training . this allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process . we show this strategy can make our detector more robust towards hard faces , without involving more complex network architecture and computation overhead . apart from mining the hard images , we also propose to improve the detection quality by exclusively exploit ##ing small faces . small faces are typically hard and have attracted extensive research attention . existing methods aim at building a scale - invariant face detector to learn and in ##fer on both small and big faces , with multiple levels of detection features and anchors of different sizes . compared with these methods , our detector is more efficient since it is specially designed to aggressively lever ##aging the small faces during training . more specifically , large faces are automatically ignored during training due to our anchor design , so that the model can fully focus on the small hard faces . additionally , experiments demonstrate that this design effectively achieve ##s improvements on detecting all faces in spite of its simple and shallow architecture . to conclude , in this paper , we propose a novel face detector with the following contributions : we propose a hard image mining strategy , to improve the robust ##ness of our detector to those extremely hard faces . this is done without any extra modules , parameters or computation overhead added on the existing detector . we design a single shot detector with only one detection feature map , which focuses on small faces with a specific range of sizes . this allows our model to be simple and focus on difficult small faces without struggling with scale variance . our face detector establishes state - of - the - art performance on all popular face detection data ##set ##s , including wider face , f ##dd ##b , pascal faces , and af ##w . we achieve 95 . 7 , 94 . 9 and 89 . 7 on easy , medium and hard wider face val data ##set . our method also achieve ##s ap ##s of 99 . 00 and 99 . 60 on pascal faces and af ##w respectively , as well as a t ##pr of 98 . 7 on f ##dd ##b . the remainder of this paper is organized as follows . in section [ reference ] , we discuss some studies have been done which are related to our paper . in section [ reference ] , we dive into details of our proposed method , and we discuss experiment results and ab ##lation experiments in section [ reference ] . finally , conclusions are drawn in section [ reference ] . section : related work face detection has received extensive research attention . with the emergence of modern cnn and object detector , there are many face detectors proposed to achieve promising performances , by adapting general object detection framework into face detection domain . we briefly review hard example mining , face detection architecture , and anchor design & matching . sub ##section : hard example mining hard example mining is an important strategy to improve model quality , and has been studied extensively in image classification and general object detection . the main idea is to find some hard positive and hard negative examples at each step , and put more effort into training on those hard examples . recently , with modern detection framework ##s proposed to boost the performance , oh ##em and focal loss have been proposed to select hard examples . oh ##em computed the gradient ##s of the networks by selecting the proposals with highest losses in every mini ##bat ##ch ; while focal loss aimed at naturally putting more focus on hard and mis ##class ##ified examples by adding a factor to the standard cross entropy criterion . however , these algorithms mainly focused on anchor - level or proposal - level mining . it can not handle the im ##balance of easy and hard images in the data ##set . in our paper , we propose to exploit hard example mining on image level , hard image mining , to improve the quality of face detector on extremely hard faces . more specifically , we assign difficulty scores to training images while training with an sg ##d mechanism , and re - sample the training images to build a new training subset at the next epoch . sub ##section : face detection architecture recent state - of - the - art face detectors are generally built based on faster - rc ##nn , r - fc ##n or ss ##d . ss ##h exploited the r ##p ##n ( region proposal network ) from faster - rc ##nn to detect faces , by building three detection feature maps and designing six anchors with different sizes attached to the detection feature maps . s f ##d and pyramid ##box , on the other hand , adopted ss ##d as their detection architecture with six different detection feature maps . different from s f ##d , pyramid ##box exploited a feature pyramid - style structure to combine features from different detection feature maps . our proposed method , on the other hand , only builds single level detection feature map , based on v ##gg ##16 , for classification and bound ##ing - box regression , which is both simple and effective . sub ##section : anchor design and matching usually , anchors are designed to have different sizes to detect objects with different scales , in order to build a scale - invariant detector . ss ##d as well as its follow - up detectors s f ##d and pyramid ##box , had six sets of anchors with different sizes , ranging from ( ) to ( ) , and their network architecture ##s had six levels of detection feature maps , with resolutions ranging from to , respectively . similarly , ss ##h had the same anchor setting , and those anchors were attached to three levels of detection feature maps with resolutions ranging from to . the difference between ss ##h and s d ##f is that in ss ##h , anchors with two neighboring sizes shared the same detection feature map , while in s d ##f , anchors with different sizes are attached to different detection feature maps . s ##ni ##p discussed an alternative approach to handle scales . it showed that cnn ##s are not robust to changes in scale , so training and testing on the same scales of an image pyramid can be a more optimal strategy . in our paper , we exploit this idea by limiting the anchor sizes to be ( ) , ( ) and ( ) . then those faces with either too small or too big sizes will not be matched to any of the anchors , thus will be ignored during the training and testing . by removing those large anchors with sizes larger than ( ) , our network focuses more on small faces which are potentially more difficult . to deal with large faces , we use multi ##sca ##le training and testing to res ##ize them to match our anchors . experiments show this design performs well on both small and big faces , although it has fewer detection feature maps and anchor sizes . section : proposed method in this section , we introduce our proposed method for effective face detection . we first discuss the architecture of our detector in section [ reference ] , then we elaborate our hard image mining strategy in section [ reference ] , as well as some other useful training techniques in section [ reference ] . sub ##section : single - level small face detection framework the framework of our face detector is illustrated in figure [ reference ] . we use v ##gg ##16 network as our backbone cnn , and combine con ##v ##4 _ 3 and con ##v ##5 _ 3 features , to build the detection feature map with both low - level and high - level semantic information . similar to ss ##h , we apply 1 1 con ##vo ##lu ##tion layers after con ##v ##4 _ 3 and con ##v ##5 _ 3 to reduce dimension , and then apply a 3 3 con ##vo ##lu ##tion layer on the con ##cate ##nation of these two dimension reduced features . the output feature of the 3 3 con ##vo ##lu ##tion layer is the final detection feature map , which will be fed into the detection head for classification and bound ##ing - box regression . the detection feature map has a resolution of of the original image ( of size ) . we attach three anchors at each point in the grid as default face detection boxes . then we do classification and bound ##ing - box regression on those anchors . unlike many other face detectors which build multiple feature maps to detect face with a variant range of scales , inspired by s ##ni ##p , faces are trained and in ##fer ##red with roughly the same scales . we only have one detection feature map , with three sets of anchors attached to it . the anchors have sizes of ( ) , ( ) and ( ) , and the aspect ratio is set to be 1 . by making this configuration , our network only trains and in ##fers on small and medium size of faces ; and we propose to handle large faces by shrinking the images in the test phase . we argue that there is no speed or accuracy degradation for large faces , since in ##fer ##ring on a tiny image ( with short side containing 100 or 300 pixels ) is very fast , and the shrink ##ed large face will still have enough information to be recognized . to handle the difference of anchor sizes attached to the same detection feature map , we propose a detection head which uses different dil ##ation rates for anchors with different sizes , as shown in figure [ reference ] . the intuition is that in order to detect faces with different sizes , different effective rec ##eptive fields are required . this naturally requires the backbone feature map to be invariant to scales . to this end , we adopt different dil ##ation rates for anchors with different sizes . for anchors with size ( ) , ( ) and ( ) , we use a con ##vo ##lu ##tion with kernel size of and dil ##ation rate of , and to gather context features at different scales . these three con ##vo ##lu ##tion layers share weights to reduce the model size . with this design , the input of the con ##vo ##lu ##tion , will be aligned to the same location of faces , regardless of the size of faces and anchors . ab ##lation experiments show the effectiveness of this multi - dil ##ation design . sub ##section : hard image mining different from oh ##em discussed in section [ reference ] , which selects proposals or anchors with the highest losses , we propose a novel hard image mining strategy at image level . the intuition is that most images in the data ##set are very easy , and we can achieve a very high ap even on the hard subset of the wider face val data ##set with our baseline model . we believe not all training images should be treated equally , and well - recognized images will not help towards training a more robust face detector . to put more attention on training hard images instead of easy ones , we use a subset of all training images , to contain hard ones for training . at the beginning of each epoch , we build based on the difficulty scores obtained in the previous epoch . we initially use all training images to train our model ( ) . this is due to the fact that our initial image ##net pre - trained model will only give random guess towards face detection . in this case , there is no easy image . in other words , every image is considered as hard image and fed to the network for training at the first epoch . during the training procedure , we dynamic ##ally assign different difficulty scores to training images , which is defined by the metric worst positive anchor score ( w ##pas ) : where is the set of positive anchors for image , with io ##u over 0 . 5 against ground - truth boxes , is the classification log ##it and , are the log ##its of anchor for image to be fore ##ground face and background . all images are initially marked as hard , and any image with w ##pas greater than a threshold will be marked as easy image . at the beginning of each epoch , we first randomly shuffle the training data ##set to generate the complete training list for the following epoch of training . then given an image marked as easy , we remove it from with a probability of . the remaining training list , which focuses more on hard images , will be used for training at this epoch . note that for multi - gp ##u training , each gp ##u will maintain its training list independently . in our experiments , we set the probability to be 0 . 7 , and the threshold to be 0 . 85 . sub ##section : training strategy sub ##su ##bs ##ection : multi - scale training and anchor matching since we only have anchors covering a limited range of face scales , we train our model by varying the sizes of training images . during the training phase , we res ##ize the training images so that the short side of the image contains pixels , where is randomly selected from . we also set an upper bound of 2000 pixels to the long side of the image considering the gp ##u memory limitation . for each anchor , we assign a label based on how well it matches with any ground - truth face bound ##ing box . if an anchor has an io ##u ( intersection over union ) over 0 . 5 against a ground - truth face bound ##ing box , we assign to that anchor . on the other hand , if the io ##u against any ground - truth face bound ##ing box is lower than 0 . 3 , we assign to that anchor . all other anchors will be given as the label , and thus will be ignored in the classification loss . by doing so , we only train on faces with designated scales . those faces with no anchor matching will be simply ignored , since we do not assign the anchor with largest io ##u to it ( thus assign the corresponding anchor label ) as faster - rc ##nn does . this anchor matching strategy will ignore the large faces , and our model can put more capacity on learning different face patterns on hard small faces instead of memo ##riz ##ing the change in scales . for the regression loss , all anchors with io ##u greater than 0 . 3 against ground - truth faces will be taken into account and contribute to the smooth loss . we use a smaller threshold ( 0 . 3 ) because ( 1 ) this will allow imperfect ##ly matched anchors to be able to local ##ize the face , which may be useful during the testing and ( 2 ) the regression task has less supervision since unlike classification , there are no negative anchors for computing loss and the positive anchors are usually sparse . sub ##su ##bs ##ection : anchor - level hard example mining oh ##em has been proven to be useful for object detection and face detection in . during our training , in parallel with our newly proposed hard image mining , we also exploit the traditional hard anchor mining method to focus more on the hard and mis ##class ##ifice ##d anchors . given a training image with size , there are anchors at the detection head , and we only select 256 of them to be involved in computing the classification loss . for all positive anchors with io ##u greater than against ground - truth boxes , we select the top 64 of them with lowest confidence ##s to be recognized as face . after selecting positive anchors , ( ) negative anchors with highest face confidence are selected to compute the classification loss as the hard negative anchors . note that we only perform oh ##em for classification loss , and we keep all anchors with io ##u greater than 0 . 3 for computing regression loss , without selecting a subset based on either classification loss or bound ##ing - box regression loss . sub ##su ##bs ##ection : data aug ##ment ##ation data aug ##ment ##ation is extremely useful to make the model robust to light , scale changes and small shifts . in our proposed method , we exploit crop ##ping and photo ##metric distortion as data aug ##ment ##ation . given a training image after res ##izing , we crop a patch of it with a probability of . the patch has a height of and a width of which are independently drawn from and , where is the uniform distribution and , are the height and width of the res ##ized training image . all ground - truth boxes whose centers are located inside the patch are kept . after the random crop ##ping , we apply photo ##metric distortion following ss ##d by randomly modifying the brightness , contrast , sat ##uration and hue of the crop ##ped image randomly . [ b ] 0 . 33 easy subset [ b ] 0 . 33 medium subset [ b ] 0 . 33 hard subset [ b ] 0 . 33 [ b ] 0 . 33 [ b ] 0 . 33 section : experiments to verify the effectiveness of our model and proposed method , we conduct extensive experiments on popular face detection data ##set ##s , including wider face , f ##dd ##b , pascal faces and af ##w . it is worth noting that the training is only performed on the train set of wider face , and we use the same model for evaluation on all these data ##set ##s without further fine - tuning . sub ##section : experimental settings we train our model on the train set of wider face , which has 128 ##80 images with 159 ##k faces ann ##ota ##ted . we flip all images horizontally , to double the size of our training data ##set to 257 ##60 . for each training image , we first randomly res ##ize it , and then we use the crop ##ping and photo ##metric distortion data aug ##ment ##ation methods discussed in section [ reference ] to pre - process the res ##ized image . we use an image ##net pre - trained v ##gg ##16 model to initial ##ize our network backbone , and our newly introduced layers are randomly initial ##ized with ga ##uss ##ian initial ##ization . we train the model with the it ##ers ##ize to be 2 , for 46 ##k iteration ##s , with a learning rate of , and then for another 14 ##k iteration ##s with a smaller learning rate of . during training , we use 4 gp ##us to simultaneously to compute the gradient and update the weight by synchronized sg ##d with momentum . the first two blocks of v ##gg ##16 are frozen during the training , and the rest layers of v ##gg ##16 are set to have a double learning rate . since our model is designed and trained on only small faces , we use a multi ##sca ##le image pyramid for testing to deal with faces larger than our anchors . specifically , we res ##ize the testing image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels for evaluation on wider face data ##set . we also follow the testing strategies used in pyramid ##box such as horizontal flip and bound ##ing - box voting . sub ##section : experiment results wider face data ##set includes 322 ##6 images and 39 ##70 ##8 faces labelled in the val data ##set , with three subset ##s - easy , medium and hard . in figure [ reference ] , we show the precision - recall ( pr ) curve and average precision ( ap ) for our model compared with many other state - of - the - arts on these three subset ##s . as we can see , our method achieve ##s the best performance on the hard subset , and out ##per ##forms the current state - of - the - art by a large margin . since the hard set is a super set of small and medium , which contains all faces taller than 10 pixels , the performance on hard set can represent the performance on the full testing data ##set more accurately . our performance on the medium subset is comparable to the most recent state - of - the - art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces , and the architecture of our model is simpler compared with other state - of - the - arts . there is also a wider face test data ##set with no ann ##ota ##tions provided publicly . it contains 1609 ##7 images , and is evaluated by wider face author team . we report the performance of our method at figure [ reference ] for the hard subset . f ##dd ##b data ##set includes 51 ##7 ##1 faces on a set of 284 ##5 images , and we use our model trained on wider face train set to in ##fer on the f ##dd ##b data ##set . we use the raw bound ##ing - box result without fitting it into el ##lip ##se to compute roc . we show the disco ##nti ##nu ##ous roc curve at figure [ reference ] compared with , and our method achieve ##s the state - of - the - art performance of t ##pr = 98 . 7 % given 1000 false positive ##s . pascal faces data ##set includes 133 ##5 labeled faces on a set of 85 ##1 images extracted for the pascal vo ##c data ##set . we show the pr curve at figure [ reference ] compared with , and our method achieve ##s a new the state - of - the - art performance of ap = 99 . 0 . af ##w data ##set includes 47 ##3 faces labelled in a set of 205 images . as shown in figure [ reference ] compared with , our method achieve ##s state - of - the - art and almost perfect performance , with an ap of 99 . 60 . sub ##section : ab ##lation study and diagnosis sub ##su ##bs ##ection : ab ##lation experiments in order to verify the performance of our single level face detector , as well as the effectiveness of our proposed hard image mining , the dil ##ated - head classification and regression structure , we conduct various ab ##lation experiments on the wider face val data ##set . all results are summarized in table [ reference ] . from table [ reference ] , we can see that our single level baseline model can achieve performance comparable to the current state - of - the - art face detector , especially on the hard subset . our model with single detection feature map performs better than the one with three detection feature maps , despite its shallow ##er structure , fewer parameters and anchors . this confirms the effectiveness of our simple face detector with single detection feature map focusing on small faces . we also separately verify our newly proposed hard image mining ( him ) and dil ##ated head architecture ( dh ) described in sub ##section [ reference ] and figure [ reference ] respectively . him can improve the performance on hard subset significantly without involving more complex network architecture nor computation overhead . dh itself can also boost the performance , which shows the effectiveness of designing larger con ##vo ##lu ##tion for larger anchors . combining him and dh together can improve further towards the state - of - the - art performance . sub ##su ##bs ##ection : diagnosis of hard image mining we investigate the effects of our hard image mining mechanism . we show the ratio of and ( the ratio of the number of selected training images to the number of ignored training images ) in figure [ reference ] for each epoch . we can see that at the first epoch , all training images are used to train the model . meanwhile , as the training process continues , more and more training images will be ignored . at the last epoch , over a half images will be ignored and thus will not be included in . sub ##su ##bs ##ection : diagnosis of data aug ##ment ##ation we investigate the effectiveness of the photo ##metric distortion as well as the crop ##ping mechanisms as discussed in sub ##section [ reference ] . the ab ##lation results evaluated on wider face val data ##set are shown in table [ reference ] . both photo ##metric distortion and crop ##ping can contribute to a more robust face detector . sub ##su ##bs ##ection : diagnosis of multi - scale testing our face detector with one detection feature map is design for small face detection , and our anchors are only capable of capturing faces with sizes ranging from ( ) to ( ) . as a result , it is critical to adopt multi - scale testing to deal with large faces . different from ss ##h , s f ##d and pyramid ##box , our testing pyramid includes some extreme small scales ( short side contains only 100 or 300 pixels ) . in table [ reference ] , we show the effectiveness of these extreme small scales to deal with easy and large images . our full evaluation res ##izes the image so that the short side contains 100 , 300 , 600 , 1000 and 1400 pixels respectively , to build an image pyramid . we dia ##gno ##se the impact of the extra small scales ( 100 and 300 ) by removing them from the image pyramid . as shown in table [ reference ] , the extra small scales are crucial to detect easy faces . without res ##izing the short side to contain 100 and 300 pixels , the performance on easy subset is only , which is even lower than the performance on medium and hard which contain much harder faces . we will show in the next sub ##section that these extra small scales ( and ) lead to ne ##gli ##gible computation overhead , due to the lower resolution . sub ##su ##bs ##ection : diagnosis of accuracy / speed trade - off we evaluate the speed of our method as well as some other popular face detectors in table [ reference ] . for fair comparison , we run all methods on the same machine , with one titan x ( maxwell ) gp ##u , and intel core i ##7 - 47 ##70 k 3 . 50 ##gh ##z . all methods except for pyramid ##box are based on caf ##fe ##1 implementation , which is compiled with cu ##da 9 . 0 and cu ##d ##nn 7 . for pyramid ##box , we follow the official fluid code and the default configurations . we use the officially built paddle ##pad ##dle with cu ##da 9 . 0 and cu ##d ##nn 7 . for ss ##h , s f ##d and pyramid , we use the official inference code and configurations . for ss ##h , we use multi - scale testing with the short side containing 500 , 800 , 1200 and 1600 pixels , and for s f ##d , we execute the official evaluation code with both multi - scale testing and horizontal flip . pyramid ##box takes a similar testing configuration as s f ##d . as shown in table [ reference ] , our detector can out ##per ##form ss ##h , s f ##d and pyramid ##box significantly with a smaller inference time . based on that , using horizontal flip can further improve the performance slightly . in terms of gp ##u memory usage , our method uses only a half of what pyramid ##box occupies , while achieving better performance . ours in table [ reference ] indicates our method without extra small scales in inference , , evaluated with scales [ 600 , 1000 , 1400 ] . it is only faster than evaluation with [ 100 , 300 , 600 , 1000 , 1400 ] ( 1 . 59 compared with 1 . 70 ) . this proves that although our face detector is only trained on small faces , it can perform well on large faces , by simply shrinking the testing image with ne ##gli ##gible computation overhead . section : conclusion to conclude , we propose a novel face detector to focus on learning small faces on hard images , which achieve ##s the state - of - the - art performance on all popular face detection data ##set ##s . we propose a hard image mining strategy by dynamic ##ally assign ##ing difficulty scores to training images , and re - sampling subset ##s with hard images for training before each epoch . we also design a single shot face detector with only one detection feature map , to train and test on small faces . with these designs , our model can put more attention on learning small hard faces instead of memo ##riz ##ing change of scales . extensive experiments and ab ##lation ##s have been done to show the effectiveness of our method , and our face detector achieve ##s the state - of - the - art performance on all popular face detection data ##set ##s , including wider face , f ##dd ##b , pascal faces and af ##w . our face detector also enjoys faster multi - scale inference speed and less gp ##u memory usage . our proposed method are flexible and can be applied to other backbone ##s and tasks , which we remain as future work . bibliography : references",
        "pred_seq": "f ##b [SEP] [SEP] ap ##s [SEP] robust detection [SEP] [unused0] pascal faces [SEP] ap ##s [SEP] ap ##s [SEP] robust detection [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "fddb"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "aps"
                    ]
                ],
                "Task": [
                    [
                        "robust face detection"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "pascal faces"
                    ]
                ],
                "Method": [
                    [
                        "aps"
                    ]
                ],
                "Metric": [
                    [
                        "aps"
                    ]
                ],
                "Task": [
                    [
                        "robust face detection"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "afw"
                    ]
                ],
                "Method": [
                    [
                        "anchorbased deep face detectors",
                        "anchorbased deep face detector",
                        "anchor design",
                        "anchors"
                    ]
                ],
                "Metric": [
                    [
                        "aps",
                        "ap",
                        "average precision",
                        "false positives",
                        "ap990",
                        "accuracy speed tradeoff",
                        "multiscale inference speed"
                    ]
                ],
                "Task": [
                    [
                        "robust face detection",
                        "face detection",
                        "face detectors",
                        "humanlevel detection",
                        "face detection domain",
                        "face detector",
                        "effective face detection",
                        "small face detection"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "fddb"
                    ]
                ],
                "Method": [
                    [
                        "anchorbased deep face detectors",
                        "anchorbased deep face detector",
                        "anchor design",
                        "anchors"
                    ]
                ],
                "Metric": [
                    [
                        "aps",
                        "ap",
                        "average precision",
                        "false positives",
                        "ap990",
                        "accuracy speed tradeoff",
                        "multiscale inference speed"
                    ]
                ],
                "Task": [
                    [
                        "robust face detection",
                        "face detection",
                        "face detectors",
                        "humanlevel detection",
                        "face detection domain",
                        "face detector",
                        "effective face detection",
                        "small face detection"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "pascal faces",
                        "face val dataset",
                        "pascal voc dataset"
                    ]
                ],
                "Method": [
                    [
                        "anchorbased deep face detectors",
                        "anchorbased deep face detector",
                        "anchor design",
                        "anchors"
                    ]
                ],
                "Metric": [
                    [
                        "aps",
                        "ap",
                        "average precision",
                        "false positives",
                        "ap990",
                        "accuracy speed tradeoff",
                        "multiscale inference speed"
                    ]
                ],
                "Task": [
                    [
                        "robust face detection",
                        "face detection",
                        "face detectors",
                        "humanlevel detection",
                        "face detection domain",
                        "face detector",
                        "effective face detection",
                        "small face detection"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "hard images",
                        "wider face",
                        "benchmark dataset wider face",
                        "wider face val dataset",
                        "wider face dataset",
                        "hard set",
                        "wider face test dataset",
                        "wider face author team",
                        "wider face train set",
                        "hard subset"
                    ]
                ],
                "Method": [
                    [
                        "anchorbased deep face detectors",
                        "anchorbased deep face detector",
                        "anchor design",
                        "anchors"
                    ]
                ],
                "Metric": [
                    [
                        "aps",
                        "ap",
                        "average precision",
                        "false positives",
                        "ap990",
                        "accuracy speed tradeoff",
                        "multiscale inference speed"
                    ]
                ],
                "Task": [
                    [
                        "robust face detection",
                        "face detection",
                        "face detectors",
                        "humanlevel detection",
                        "face detection domain",
                        "face detector",
                        "effective face detection",
                        "small face detection"
                    ]
                ]
            }
        ]
    },
    "36": {
        "doctext": "con ##vo ##lu ##tion ##al pose machines section : abstract pose machines provide a sequential prediction framework for learning rich implicit spatial models . in this work we show a systematic design for how con ##vo ##lu ##tion ##al networks can be incorporated into the pose machine framework for learning image features and image - dependent spatial models for the task of pose estimation . the contribution of this paper is to implicit ##ly model long - range depend ##encies between variables in structured prediction tasks such as articulated pose estimation . we achieve this by designing a sequential architecture composed of con ##vo ##lu ##tion ##al networks that directly operate on belief maps from previous stages , producing increasingly refined estimates for part locations , without the need for explicit graphical model - style inference . our approach addresses the characteristic difficulty of vanishing gradient ##s during training by providing a natural learning objective function that enforce ##s intermediate supervision , thereby rep ##len ##ishing back - prop ##aga ##ted gradient ##s and conditioning the learning procedure . we demonstrate state - of - the - art performance and out ##per ##form competing methods on standard bench ##marks including the mp ##ii , l ##sp , and fl ##ic data ##set ##s . section : introduction we introduce con ##vo ##lu ##tion ##al pose machines ( cp ##ms ) for the task of articulated pose estimation . cp ##ms inherit the benefits of the pose machine [ reference ] architecture - the implicit learning of long - range depend ##encies between image and multi - part cues , tight integration between learning and inference , a modular sequential design - and combine them with the advantages afforded by con ##vo ##lu ##tion ##al architecture ##s : the ability to learn feature representations for both image and spatial context directly from data ; a different ##iable architecture that allows for globally joint training with back ##pro ##pa ##gation ; and the ability to efficiently handle large training data ##set ##s . cp ##ms consist of a sequence of con ##vo ##lu ##tion ##al networks that repeatedly produce 2d belief maps [ reference ] for the location [ reference ] we use the term belief in a slightly loose sense , however the belief of each part . at each stage in a cp ##m , image features and the belief maps produced by the previous stage are used as input . the belief maps provide the subsequent stage an expressive non - para ##metric encoding of the spatial uncertainty of location for each part , allowing the cp ##m to learn rich image - dependent spatial models of the relationships between parts . instead of explicitly par ##sing such belief maps either using graphical models [ reference ] [ reference ] [ reference ] or specialized post - processing steps [ reference ] [ reference ] , we learn con ##vo ##lu ##tion ##al networks that directly operate on intermediate belief maps and learn implicit image - dependent spatial models of the relationships between parts . the overall proposed multi ##sta ##ge architecture is fully different ##iable and therefore can be trained in an end - to - end fashion using back ##pro ##pa ##gation . at a particular stage in the cp ##m , the spatial context of part beliefs provide strong di ##sam ##bi ##gua ##ting cues to a subsequent stage . as a result , each stage of a cp ##m produces belief maps with increasingly refined estimates for the locations of each part ( see figure 1 ) . in order to capture long ##rang ##e interactions between parts , the design of the network in each stage of our sequential prediction framework is motivated by the goal of achieving a large rec ##eptive field on both the image and the belief maps . we find , through experiments , that large rec ##eptive fields on the belief maps are crucial for learning long range spatial relationships and re ##ma ##ps described are closely related to beliefs produced in message passing inference in graphical models . the overall architecture can be viewed as an un ##roll ##ed mean - field message passing inference algorithm [ reference ] that is learned end - to - end using back ##pro ##pa ##gation . sul ##t in improved accuracy . composing multiple con ##vo ##lu ##tion ##al networks in a cp ##m results in an overall network with many layers that is at risk of the problem of vanishing gradient ##s [ reference ] [ reference ] [ reference ] [ reference ] during learning . this problem can occur because back ##pro ##pa ##gated gradient ##s dim ##ini ##sh in strength as they are prop ##aga ##ted through the many layers of the network . while there exists recent work 2 which shows that supervising very deep networks at intermediate layers aids in learning [ reference ] [ reference ] , they have mostly been restricted to classification problems . in this work , we show how for a structured prediction problem such as pose estimation , cp ##ms naturally suggest a systematic framework that rep ##len ##ish ##es gradient ##s and guides the network to produce increasingly accurate belief maps by enforcing intermediate supervision periodically through the network . we also discuss different training schemes of such a sequential prediction architecture . our main contributions are ( a ) learning implicit spatial models via a sequential composition of con ##vo ##lu ##tion ##al architecture ##s and ( b ) a systematic approach to designing and training such an architecture to learn both image features and image - dependent spatial models for structured prediction tasks , without the need for any graphical model style inference . we achieve state - of - the - art results on standard bench ##marks including the mp ##ii , l ##sp , and fl ##ic data ##set ##s , and analyze the effects of jointly training a multi - staged architecture with repeated intermediate supervision . section : related work the classical approach to articulated pose estimation is the pictorial structures model [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] in which spatial correlation ##s between parts of the body are expressed as a tree - structured graphical model with kin ##ema ##tic prior ##s that couple connected limbs . these methods have been successful on images where all the limbs of the person are visible , but are prone to characteristic errors such as double - counting image evidence , which occur because of correlation ##s between variables that are not captured by a tree - structured model . the work of ki ##efe ##l et al . [ reference ] is based on the pictorial structures model but differs in the underlying graph representation . hierarchical models [ reference ] [ reference ] represent the relationships between parts at different scales and sizes in a hierarchical tree structure . the underlying assumption of these models is that larger parts ( that correspond to full limbs instead of joints ) can often have disc ##rim ##ina ##tive image structure that can be easier to detect and consequently help reason about the location of smaller , harder - to - detect parts . non - tree models [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] incorporate interactions that introduce loops to aug ##ment the tree structure with additional edges that capture symmetry , o ##cc ##lusion and long - range relation - ships . these methods usually have to rely on approximate inference during both learning and at test time , and therefore have to trade off accurate modeling of spatial relationships with models that allow efficient inference , often with a simple para ##metric form to allow for fast inference . in contrast , methods based on a sequential prediction framework [ reference ] learn an implicit spatial model with potentially complex interactions between variables by directly training an inference procedure , as in [ reference ] [ reference ] [ reference ] [ reference ] . there has been a recent surge of interest in models that employ con ##vo ##lu ##tion ##al architecture ##s for the task of articulated pose estimation [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . to ##sh ##ev et al . [ reference ] take the approach of directly reg ##ress ##ing the cart ##esian coordinates using a standard con ##vo ##lu ##tion ##al architecture [ reference ] . recent work reg ##resses image to confidence maps , and resort to graphical models , which require hand - designed energy functions or he ##uri ##stic initial ##ization of spatial probability prior ##s , to remove out ##lier ##s on the reg ##ressed confidence maps . some of them also utilize a dedicated network module for precision ref ##ine ##ment [ reference ] [ reference ] . in this work , we show the reg ##ressed confidence maps are suitable to be input ##ted to further con ##vo ##lu ##tion ##al networks with large rec ##eptive fields to learn implicit spatial depend ##encies without the use of hand designed prior ##s , and achieve state - of - the - art performance over all precision region without careful initial ##ization and dedicated precision ref ##ine ##ment . p ##fi ##ster et al . [ reference ] also used a network module with large rec ##eptive field to capture implicit spatial models . due to the different ##iable nature of con ##vo ##lu ##tions , our model can be globally trained , where tom ##ps ##on et al . [ reference ] and steward et al . [ reference ] also discussed the benefit of joint training . carr ##eira et al . [ reference ] train a deep network that it ##erative ##ly improves part detection ##s using error feedback but use a cart ##esian representation as in [ reference ] which does not preserve spatial uncertainty and results in lower accuracy in the high ##pre ##cision regime . in this work , we show how the sequential prediction framework takes advantage of the preserved uncertainty in the confidence maps to en ##code the rich spatial context , with enforcing the intermediate local supervision ##s to address the problem of vanishing gradient ##s . section : method section : pose machines we denote the pixel location of the p - th anatomical landmark ( which we refer to as a part ) , y p ##\u2208 z ##\u2282 r 2 , where z is the set of all ( u , v ) locations in an image . our goal is to predict the image locations y = ( y 1 , . . . , y p ) for all p parts . a pose machine [ reference ] ( see figure 2a and 2 ##b ) consists of a sequence of multi - class predict ##ors , g t ( \u00b7 ) , that are trained to predict the location of each part in each level of the hierarchy . in each stage t ##\u2208 { 1 . . . t } , the class ##ifiers g t predict beliefs for assign ##ing a location to each part y p = z , [UNK] z , based on features extracted from the image at the location z denoted by x z ##\u2208 r d and context ##ual information from the preceding class ##ifier in the neighbor - [UNK] 26 [UNK] 60 [UNK] 96 [UNK] 160 [UNK] 240 [UNK] 320 [UNK] 400 hood around each y p in stage t . a class ##ifier in the first stage t = 1 , therefore produces the following belief values : where is the score predicted by the class ##ifier g 1 for assign ##ing the p th part in the first stage at image location z . we represent all the beliefs of part p evaluated at every location z = ( u , v ) t in the image as b p t ##\u2208 r w ##\u00d7 ##h , where w and h are the width and height of the image , respectively . that is , b for convenience , we denote the collection of belief maps for all the parts as b t ##\u2208 r w ##\u00d7 ##h ##\u00d7 ( p + 1 ) ( p parts plus one for background ) . in subsequent stages , the class ##ifier predict ##s a belief for assign ##ing a location to each part y p = z , [UNK] z , based on ( 1 ) features of the image data x t z ##\u2208 r d again , and ( 2 ) context ##ual information from the pre ##ce ##eding class ##ifier in the neighborhood around each y p : where ##\u03c8 t > 1 ( \u00b7 ) is a mapping from the beliefs b t ##\u22121 to context features . in each stage , the computed beliefs provide an increasingly refined estimate for the location of each part . note that we allow image features x z for subsequent stage to be different from the image feature used in the first stage x . the pose machine proposed in [ reference ] used boosted random forests for prediction ( { g t } ) , fixed hand - crafted image features across all stages ( x = x ) , and fixed hand - crafted context feature maps ( \u03c8 t ( \u00b7 ) ) to capture spatial context across all stages . section : con ##vo ##lu ##tion ##al pose machines we show how the prediction and image feature computation modules of a pose machine can be replaced by a deep con ##vo ##lu ##tion ##al architecture allowing for both image and context ##ual feature representations to be learned directly from data . con ##vo ##lu ##tion ##al architecture ##s also have the advantage of being completely different ##iable , thereby enabling end ##to - end joint training of all stages of a cp ##m . we describe our design for a cp ##m that combines the advantages of deep con ##vo ##lu ##tion ##al architecture ##s with the implicit spatial modeling afforded by the pose machine framework . section : key ##point local ##ization using local image evidence the first stage of a con ##vo ##lu ##tion ##al pose machine predict ##s part beliefs from only local image evidence . figure 2 ##c shows the network structure used for part detection from local image evidence using a deep con ##vo ##lu ##tion ##al network . the evidence is local because the rec ##eptive field of the first stage of the network is constrained to a small patch around the output pixel location . we use a network structure composed of five con ##vo ##lu ##tion ##al layers followed by two 1 ##\u00d7 1 con ##vo ##lu ##tion ##al layers which results in a fully con ##vo ##lu ##tion ##al arch ##i - figure 3 : spatial context from belief maps of easier - to - detect parts can provide strong cues for local ##izing difficult - to - detect parts . the spatial contexts from shoulder , neck and head can help eliminate wrong ( red ) and strengthen correct ( green ) estimation ##s on the belief map of right elbow in the subsequent stages . te ##cture [ reference ] . in practice , to achieve certain precision , we normal ##ize input crop ##ped images to size 36 ##8 ##\u00d7 36 ##8 ( see section 4 . 2 for details ) , and the rec ##eptive field of the network shown above is 160 ##\u00d7 160 pixels . the network can effectively be viewed as sliding a deep network across an image and reg ##ress ##ing from the local image evidence in each 160 ##\u00d7 160 image patch to a p + 1 sized output vector that represents a score for each part at that image location . section : sequential prediction with learned spatial context features while the detection rate on landmarks with consistent appearance , such as the head and shoulders , can be favorable , the acc ##ura ##cies are often much lower for landmarks lower down the kin ##ema ##tic chain of the human skeleton due to their large variance in configuration and appearance . the landscape of the belief maps around a part location , albeit noisy , can , however , be very inform ##ative . illustrated in figure 3 , when detecting challenging parts such as right elbow , the belief map for right shoulder with a sharp peak can be used as a strong cue . a predict ##or in subsequent stages ( g t > 1 ) can use the spatial context ( \u03c8 t > 1 ( \u00b7 ) ) of the noisy belief maps in a region around the image location z and improve its predictions by lever ##aging the fact that parts occur in consistent geometric configurations . in the second stage of a pose machine , the class ##ifier g 2 accepts as input the image features x 2 z and features computed on the beliefs via the feature function ##\u03c8 for each of the parts in the previous stage . the feature function ##\u03c8 serves to en ##code the landscape of the belief maps from the previous stage in a spatial region around the location z of the different parts . for a con ##vo ##lu ##tion ##al pose machine , we do not have an explicit function that compute ##s context features . instead , we define ##\u03c8 as being the rec ##eptive field of the predict ##or on the beliefs from the previous stage . the design of the network is guided by achieving a rec ##eptive field at the output layer of the second stage network that is large enough to allow the learning of potentially complex and long - range correlation ##s between parts . by simply supplying features on the outputs of the previous stage ( as opposed to specify ##ing potential functions in a graphical model ) , the con ##vo ##lu ##tion ##al layers in the subsequent stage allow the class ##ifier to freely combine context ##ual information by picking the most predict ##ive features . the belief maps from the first stage are generated from a network that examined the image locally with a small rec ##eptive field . in the second stage , we design a network that drastically increases the equivalent rec ##eptive field . large rec ##eptive fields can be achieved either by pool ##ing at the expense of precision , increasing the kernel size of the con ##vo ##lu ##tion ##al filters at the expense of increasing the number of parameters , or by increasing the number of con ##vo ##lu ##tion ##al layers at the risk of encounter ##ing vanishing gradient ##s during training . our network design and corresponding rec ##eptive field for the subsequent stages ( t ##\u2265 2 ) is shown in figure 2d . we choose to use multiple con ##vo ##lu ##tion ##al layers to achieve large rec ##eptive field on the 8 ##\u00d7 downs ##cal ##ed heat ##ma ##ps , as it allows us to be par ##si ##mon ##ious with respect to the number of parameters of the model . we found that our stride - 8 network performs as well as a stride - 4 one even at high precision region , while it makes us easier to achieve larger rec ##eptive fields . we also repeat similar structure for image feature maps to make the spatial context be image - dependent and allow error correction , following the structure of pose machine . we find that accuracy improves with the size of the rec ##eptive field . in figure 4 we show the improvement in accuracy on the fl ##ic data ##set [ reference ] as the size of the rec ##eptive field on the original image is varied by varying the architecture without significantly changing the number of parameters , through a series of experimental trials on input images normal ##ized to a size of 304 ##\u00d7 304 . we see that the accuracy improves as the effective rec ##eptive field increases , and starts to sat ##ura ##te around 250 pixels , which also happens to be roughly the size of the normal ##ized object . this improvement in accuracy with rec ##eptive field size suggests that the network does indeed en ##code long range interactions between parts and that doing so is beneficial . in our best performing setting in figure 2 , we normal ##ize crop ##ped images into a larger size of 36 ##8 ##\u00d7 36 ##8 pixels for better precision , and the rec ##eptive field of the second stage output on the belief maps of the first stage is set to 31 ##\u00d7 31 , which is equivalent ##ly 400 ##\u00d7 400 pixels on the original image , where the radius can usually cover any pair of the parts . with more stages , the effective rec ##eptive field is even larger . in the following section we show our results from up to 6 stages . section : learning in con ##vo ##lu ##tion ##al pose machines the design described above for a pose machine results in a deep architecture that can have a large number of layers . training such a network with many layers can be prone to the problem of vanishing gradient ##s [ reference ] [ reference ] [ reference ] where , as observed by bradley [ reference ] and ben ##gio et al . [ reference ] , the magnitude of back - prop ##aga ##ted gradient ##s decreases in strength with the number of intermediate layers between the output layer and the input layer . fortunately , the sequential prediction framework of the pose machine provides a natural approach to training our deep architecture that addresses this problem . each stage of the pose machine is trained to repeatedly produce the belief maps for the locations of each of the parts . we encourage the network to repeatedly arrive at such a representation by defining a loss function at the output of each stage t that minimize ##s the l 2 distance between the predicted and ideal belief maps for each part . the ideal belief map for a part p is written as b p * ( y p = z ) , which are created by putting ga ##uss ##ian peaks at ground truth locations of each body part p . the cost function we aim to minimize at the output of each stage at each level is therefore given by : the overall objective for the full architecture is obtained by adding the losses at each stage and is given by : we use standard st ##och ##astic gradient descend to jointly train all the t stages in the network . to share the image feature x across all subsequent stages , we share the weights of corresponding con ##vo ##lu ##tion ##al layers ( see figure 2 ) across stages t ##\u2265 2 . section : evaluation section : analysis addressing vanishing gradient ##s . the objective in equation 5 describes a deco ##mp ##osa ##ble loss function that operates on different parts of the network ( see figure 2 ) . specifically , each term in the sum ##mation is applied to the network after each stage t effectively enforcing supervision in intermediate stages through the network . intermediate supervision has the advantage that , even though the full architecture can have many layers , it does not fall prey to the vanishing gradient problem as the intermediate loss functions rep ##len ##ish the gradient ##s at each stage . we verify this claim by observing his ##to ##gram ##s of gradient magnitude ( see figure 5 ) at different depths in the architecture across training epoch ##s for models with and without intermediate supervision . in early epoch ##s , as we move from the output layer to the input layer , we observe on the model larger variance across all layers , suggesting that learning is indeed occurring in all the layers thanks to intermediate supervision . we also notice that as training progresses , the variance in the gradient magnitude distributions decreases pointing to model convergence . benefit of end - to - end learning . we see in figure 6 ##a that replacing the modules of a pose machine with the appropriately designed con ##vo ##lu ##tion ##al architecture provides a large boost of 42 . 4 percentage points over the previous approach of [ reference ] in the high precision regime ( pc ##k @ 0 . 1 ) and 30 . 9 percentage points in the low precision regime ( pc ##k @ 0 . 2 ) . comparison on training schemes . we compare different variants of training the network in figure 6 ##b on the l ##sp data ##set with person - cent ##ric ( pc ) ann ##ota ##tions . to demonstrate the benefit of intermediate supervision with joint training across stages , we train the model in four ways : ( i ) training from scratch using a global loss function that enforce ##s intermediate supervision ( ii ) stage - wise ; where each stage is trained in a feed - forward fashion and stacked ( iii ) as same as ( i ) but initial ##ized with weights from ( ii ) , and ( iv ) as same as ( i ) but with no intermediate supervision . we find that network ( i ) out ##per ##forms all other training methods , showing that intermediate supervision and joint training across stage is indeed crucial in achieving good performance . the stage ##wise training in ( ii ) sat ##ura ##te at sub ##op ##ti ##mal , and the jointly fine - tuning in ( iii ) improves from this sub - optimal to the accuracy level closed to ( i ) , however with effectively longer training iteration ##s . performance across stages . we show a comparison of performance across each stage on the l ##sp data ##set ( pc ) in figure 6 ##c . we show that the performance increases mono ##tonic ##ally until 5 stages , as the predict ##ors in subsequent stages make use of context ##ual information in a large rec ##eptive field on the previous stage beliefs maps to resolve confusion ##s between parts and background . we see dim ##ini ##shing returns at the 6th stage , which is the number we choose for reporting our best results in this paper for l ##sp and mp ##ii data ##set ##s . section : data ##set ##s and quantitative analysis in this section we present our numerical results in various standard bench ##marks including the mp ##ii , l ##sp , and fl ##ic data ##set ##s . to have normal ##ized input samples of 36 ##8 ##\u00d7 36 ##8 for training , we first res ##ize the images to roughly make the samples into the same scale , and then crop or pad the image according to the center positions and rough scale estimation ##s provided in the data ##set ##s if available . in data ##set ##s such as l ##sp without these information , we estimate them according to joint positions or image sizes . for testing , we perform similar res ##izing and crop ##ping ( or pad ##ding ) , but estimate center position and scale only from image sizes when necessary . in addition , we merge the belief maps from different scales ( per ##tur ##bed around the given one ) for final predictions , to handle the ina ##cc ##ura ##cy of the given scale estimation . we define and implement our model using the caf ##fe [ 13 ] libraries for deep learning . we publicly release the source code and details on the architecture , learning parameters , design decisions and data aug ##ment ##ation to ensure full rep ##rod ##uc ##ibility . [ reference ] mp ##ii human pose data ##set . we show in figure 8 our results on the mp ##ii human pose data ##set [ reference ] which consists more than 280 ##00 training samples . we choose to randomly aug ##ment the data with rotation degrees in [ \u2212 ##40 \u2022 , 40 \u2022 ] , scaling with factors in [ 0 . 7 , 1 . 3 ] , and horizon ##al flipping . the evaluation is based on pc ##kh metric [ reference ] where the error tolerance is normal ##ized with respect to head size of the target . because there often are multiple people in the proximity of the interested person ( rough center position is given in the data ##set ) , we made two sets of ideal belief maps for training : one includes all the peaks for every person appearing in the proximity of the primary subject and the second type where we only place peaks for the primary subject . we supply the first set of belief maps to the loss layers in the first stage as the initial stage only relies on local image evidence to make predictions . we supply the second type of belief maps to the wrists elbows ( a ) ( b ) loss layers of all subsequent stages . we also find that supplying to all subsequent stages an additional heat - map with a ga ##uss ##ian peak indicating center of the primary subject is beneficial . our total pc ##kh - 0 . 5 score achieve ##s state of the art at 87 . 95 % ( 88 . 52 % when adding l ##sp training data ) , which is 6 . 11 % higher than the closest competitor , and it is noteworthy that on the ankle ( the most challenging part ) , our pc ##kh - 0 . 5 score is 78 . 28 % ( 79 . 41 % when adding l ##sp training data ) , which is 10 . 76 % higher than the closest competitor . this result shows the capability of our model to capture long distance context given ankles are the far ##thest parts from head and other more recognizable parts . figure 11 shows our accuracy is also consistently significantly higher than other methods across various view angles defined in [ reference ] , especially in those challenging non - frontal views . in summary , our method improves the accuracy in all parts , over all precision ##s , across all view angles , and is the first one achieving such high accuracy without any pre - training from other data , or post - inference par ##sing with hand - design prior ##s or initial ##ization of such a structured prediction task as in [ reference ] [ reference ] . our methods also does not need another module dedicated to location ref ##ine ##ment as in [ reference ] to achieve great high - precision accuracy with a stride - 8 network . leeds sports pose ( l ##sp ) data ##set . we evaluate our method on the extended leeds sports data ##set [ reference ] that consists of 1100 ##0 images for training and 1000 images for testing . we trained on person - cent ##ric ( pc ) ann ##ota ##tions and evaluate our method using the percentage correct key ##points ( pc ##k ) metric [ reference ] . using the same aug ##ment ##ation scheme as for the mp ##i data ##set , our model again achieve ##s state of the art at 84 . 32 % ( 90 . 5 % when adding mp ##ii train - mp ##ii fl ##ic l ##sp figure 10 : qu ##ali ##tative results of our method on the mp ##ii , l ##sp and fl ##ic data ##set ##s respectively . we see that the method is able to handle non - standard poses and resolve am ##bi ##gui ##ties between symmetric parts for a variety of different relative camera views . ing data ) . note that adding mp ##ii data here significantly boost ##s our performance , due to its labeling quality being much better than l ##sp . because of the noisy label in the l ##sp data ##set , pi ##sh ##chu ##lin et al . [ reference ] reproduced the data ##set with original high resolution images and better labeling quality . fl ##ic data ##set . we evaluate our method on the fl ##ic data ##set [ reference ] which consists of 39 ##8 ##7 images for training and 1016 images for testing . we report accuracy as per the metric introduced in sap ##p et al . [ reference ] for the elbow and wrist joints in figure 12 . again , we out ##per ##form all prior art at pc ##k @ 0 . 2 with 97 . 59 % on elbows and 95 . 03 % on wrists . in higher precision region our advantage is even more significant : 14 . 8 percentage points on wrists and 12 . 7 percentage points on elbows at pc ##k @ 0 . 05 , and 8 . 9 percentage points on wrists and 9 . 3 percentage points on elbows at pc ##k @ 0 . 1 . section : discussion con ##vo ##lu ##tion ##al pose machines provide an end - to - end architecture for tack ##ling structured prediction problems in computer vision without the need for graphical - model style inference . we showed that a sequential architecture composed of con ##vo ##lu ##tion ##al networks is capable of implicit ##ly learning a spatial models for pose by communicating increasingly refined uncertainty - preserving beliefs between stages . problems with spatial depend ##encies between variables arise in multiple domains of computer vision such as semantic image labeling , single image depth prediction and object detection and future work will involve extending our architecture to these problems . our approach achieve ##s state of the art accuracy on all primary bench ##marks , however we do observe failure cases mainly when multiple people are in close proximity . handling multiple people in a single end - to - end architecture is also a challenging problem and an interesting avenue for future work . section :",
        "pred_seq": "l ##sp [SEP] [SEP] [SEP] [SEP] [unused0] mp ##ii [SEP] [SEP] [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "lsp"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": []
            },
            {
                "Material": [
                    [
                        "mpii"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "flic datasets",
                        "flic"
                    ]
                ],
                "Method": [
                    [
                        "convolutional pose machines",
                        "pose machines",
                        "cpms",
                        "pose machine",
                        "cpm",
                        "convolutional pose machine"
                    ]
                ],
                "Metric": [
                    [
                        "pck02"
                    ]
                ],
                "Task": [
                    [
                        "pose estimation"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "lsp",
                        "lsp dataset",
                        "lsp training data",
                        "leeds sports pose",
                        "extended leeds sports dataset"
                    ]
                ],
                "Method": [
                    [
                        "convolutional pose machines",
                        "pose machines",
                        "cpms",
                        "pose machine",
                        "cpm",
                        "convolutional pose machine"
                    ]
                ],
                "Metric": [
                    [
                        "high precision regime",
                        "pck01",
                        "low precision regime",
                        "percentage correct keypoints",
                        "pck",
                        "pck005"
                    ]
                ],
                "Task": [
                    [
                        "pose estimation"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "mpii",
                        "mpii datasets",
                        "mpi dataset"
                    ]
                ],
                "Method": [
                    [
                        "convolutional pose machines",
                        "pose machines",
                        "cpms",
                        "pose machine",
                        "cpm",
                        "convolutional pose machine"
                    ]
                ],
                "Metric": [
                    [
                        "pckh05 score"
                    ]
                ],
                "Task": [
                    [
                        "pose estimation"
                    ]
                ]
            }
        ]
    },
    "37": {
        "doctext": "published as a conference paper at ic ##lr 2017 query - reduction networks for question answering section : abstract in this paper , we study the problem of question answering when reasoning over multiple facts is required . we propose query - reduction network ( q ##rn ) , a variant of rec ##urrent neural network ( rn ##n ) that effectively handles both short - term ( local ) and long - term ( global ) sequential depend ##encies to reason over multiple facts . q ##rn considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time . our experiments show that q ##rn produces the state - of - the - art results in ba ##bi q ##a and dial ##og tasks , and in a real goal - oriented dial ##og data ##set . in addition , q ##rn formulation allows parallel ##ization on rn ##n ' s time axis , saving an order of magnitude in time complexity for training and inference . section : introduction in this paper , we address the problem of question answering ( q ##a ) when reasoning over multiple facts is required . for example , consider we know that frogs eat insects and flies are insects . then answering do frogs eat flies ? requires reasoning over both of the above facts . question answering , more specifically context - based q ##a , has been extensively studied in machine comprehension tasks [ reference ] [ reference ] [ reference ] [ reference ] . however , most of the data ##set ##s are primarily focused on lexi ##cal and syn ##ta ##ctic understanding , and hardly concentrate on inference over multiple facts . recently , several data ##set ##s aimed for testing multi - hop reasoning have emerged ; among them are story - based q ##a and the dial ##og task . rec ##urrent neural network ( rn ##n ) and its variants , such as long short - term memory ( l ##st ##m ) [ reference ] and gate ##d rec ##urrent unit ( gr ##u ) [ reference ] , are popular choices for modeling natural language . however , when used for multi - hop reasoning in question answering , purely rn ##n - based models have shown to perform poorly . this is largely due to the fact that rn ##n ' s internal memory is inherently unstable over a long term . for this reason , most recent approaches in the literature have mainly relied on global attention mechanism and shared external memory [ reference ] [ reference ] [ reference ] [ reference ] . the attention mechanism allows these models to focus on a single sentence in each layer . they can sequential ##ly read multiple relevant sentences from the memory with multiple layers to perform multi - hop reasoning . however , one major draw ##back of these standard attention mechanisms is that they are ins ##ens ##itive to the time step ( memory address ) of the sentences when access ##ing them . our proposed model , query - reduction network 1 ( q ##rn ) , is a single rec ##urrent unit that addresses the long - term dependency problem of most rn ##n - based models by sim ##plify ##ing the rec ##urrent update , while taking the advantage of rn ##n ' s capability to model sequential data ( figure 1 ) . q ##rn considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time . for instance in figure 1b , the original question , where is the apple ? , can not be directly answered by any single sentence from the story . after observing the first sentence , sandra got the apple there , q ##rn transforms the original question to a reduced query where is sandra ? , which is presumably 1 code is publicly available at : seo ##min ##jo ##on . gi ##th ##ub . io / q ##rn / figure 1 : ( 1a ) q ##rn unit , ( 1b ) 2 - layer q ##rn on 5 - sentence story , and ( 1 ##c ) entire q ##a system ( q ##rn and input / output modules ) . x , q , [UNK] are the story , question and predicted answer in natural language , respectively . x = x ##1 , . . . , x ##t , q , [UNK] are their corresponding vector representations ( upright font ) . \u03b1 and ##\u03c1 are update gate and reduce functions , respectively . [UNK] is assigned to be h 2 5 , the local query at the last time step in the last layer . also , red - colored text is the in ##fer ##red meanings of the vectors ( see ' interpretations ' of section 5 . 3 ) . easier to answer than the original question given the context provided by the first sentence . 2 unlike rn ##n - based models , q ##rn ' s candidate state ( h t in figure 1a ) does not depend on the previous hidden state ( h t ##\u22121 ) . compared to memory - based approaches [ reference ] [ reference ] [ reference ] [ reference ] , q ##rn can better en ##codes locality information because it does not use a global memory access controller ( circle nodes in figure 2 ) , and the query updates are performed locally . in short , the main contribution of q ##rn is three ##fold . first , q ##rn is a simple variant of rn ##n that reduces the query given the context sentences in a different ##iable manner . second , q ##rn is situated between the attention mechanism and rn ##n , effectively handling time dependency and long - term dependency problems of each technique , respectively . hence it is well - suited for sequential data with both local and global interactions ( note that q ##rn is not the replacement of rn ##n , which is arguably better for modeling complex local interactions ) . third , unlike most rn ##n - based models , q ##rn can be parallel ##ized over time by computing candidate reduced que ##ries ( h t ) directly from local input que ##ries ( q t ) and context sentence vectors ( x t ) . in fact , the parallel ##iza ##bility of q ##rn implies that q ##rn does not suffer from the vanishing gradient problem of rn ##n , hence effectively addressing the long - term dependency . we experimental ##ly demonstrate these contributions by achieving the state - of - the - art results on story - based q ##a and interactive dial ##og data ##set ##s . section : model in story - based q ##a ( or dial ##og data ##set ) , the input is the context as a sequence of sentences ( story or past conversations ) and a question in natural language ( equivalent to the user ' s last utter ##ance in the dial ##og ) . the output is the predicted answer to the question in natural language ( the system ' s next utter ##ance in the dial ##og ) . the only supervision provided during training is the answer to the question . in this paper we particularly focus on end - to - end solutions , i . e . , the only supervision comes from questions and answers , and we restrain from using manually defined rules or external language resources , such as lexi ##con or dependency par ##ser . let x 1 , . . . , x t denote the sequence of sentences , where t is the number of sentences in the story , and let q denote the question . [UNK] denote the predicted answer , and y denote the true answer . our proposed system for end - to - end q ##a task is divided into three modules ( figure 1 ##c ) : input module , q ##rn layers , and output module . input module . input module maps each sentence x t and the question q to d - dimensional vector space , x t ##\u2208 r d and q t ##\u2208 r d . we adopt a previous solution for the input module ( details in section 5 ) . q ##rn layers . q ##rn layers use the sentence vectors and the question vector from the input module to obtain the predicted answer in vector space , [UNK] r d . a q ##rn layer refers to the rec ##urrent application of a q ##rn unit , which can be considered as a variant of rn ##n with two inputs , two outputs , and a hidden state ( reduced query ) , all of which operate in vector space . the details of the q ##rn module is explained throughout this section ( 2 . 1 , 2 . 2 ) . output module . output module [UNK] obtained from q ##rn to a natural language [UNK] . similar to the input module , we adopt a standard solution for the output module ( details in section 5 ) . we first formally define the base model of a q ##rn unit , and then we explain how we connect the input and output modules to it ( section 2 . 1 ) . we also present a few extensions to the network that can improve q ##rn ' s performance ( section 2 . 2 ) . finally , we show that q ##rn can be parallel ##ized over time , giving computational advantage over most rn ##n - based models by one order of magnitude ( section 3 ) . section : q ##rn unit as an rn ##n - based model , q ##rn is a single rec ##urrent unit that updates its hidden state ( reduced query ) through time and layers . figure 1a depicts the sc ##hema ##tic structure of a q ##rn unit , and figure 1b demonstrates how layers are stacked . a q ##rn unit accepts two inputs ( local query vector q t ##\u2208 r d and sentence vector x t ##\u2208 r d ) , and two outputs ( reduced query vector h t ##\u2208 r d , which is similar to the hidden state in rn ##n , and the sentence vector x t from the input without modification ) . the local query vector is not necessarily identical to the original query ( question ) vector q . in order to compute the outputs , we use update gate function ##\u03b1 : intuitive ##ly , the update gate function measures the relevance between the sentence and the local query and is used to update the hidden state . the reduce function transforms the local query input to a candidate state which is a new reduced ( easier ) query given the sentence . the outputs are calculated with the following equations : where z t is the scala ##r update gate , h t is the candidate reduced query , and h t is the final reduced query at time step t , \u03c3 ( \u00b7 ) is si ##gm ##oid activation , tan ##h ( \u00b7 ) is hyper ##bo ##olic tangent activation ( applied element - wise ) , \u2022 is element - wise vector multiplication , and [ ; ] is vector con ##cate ##nation along the row . as a base case , h 0 = 0 . here we have explicitly defined ##\u03b1 and ##\u03c1 , but they can be any reasonable different ##iable functions . the update gate is similar to the global attention mechanism [ reference ] [ reference ] in that it measures the similarity between the sentence ( a memory slot ) and the query . however , a significant difference is that the update gate is computed using si ##gm ##oid ( \u03c3 ) function on the current memory slot only ( hence internally embedded within the unit ) , whereas the global attention is computed using soft ##max function over the entire memory ( hence globally defined ) . the update gate can be rather considered as local si ##gm ##oid attention . section : stack ##ing layers we just showed the single - layer case of q ##rn , but q ##rn with multiple layers is able to perform reasoning over multiple facts more effectively , as shown in the example of figure 1b . in order to stack several layers of q ##rn , the outputs of the current layer are used as the inputs to the next layer . that is , using super ##script k to denote the current layer ' s index ( assuming 1 - based index ##ing ) , we let q k + 1 t = h k t . note that x t is passed to the next layer without any modification , so we do not put a layer index on it . bi - direction . so far we have assumed that q ##rn only needs to look at past sentences , whereas often times , query answers can depend on future sentences . for instance , consider a sentence \" john dropped the football . \" at time t . then , even if there is no mention about the \" football \" in the past ( at time i < t ) , it can be implied that \" john \" has the \" football \" at the current time t . in order to incorporate the future dependency , we obtain ##\u2212 ##\u2192 h t and ##\u2190 ##\u2212 h t in both forward and backward directions , respectively , using equation 3 . we then add them together to get q t for the next layer . that is , [ reference ] are shared between the two directions . connecting input and output modules . figure 1 ##c depicts how q ##rn is connected with the input and output modules . in the first layer of q ##rn , q 1 t = q for all t , where q is obtained from the input module by processing the natural language question input q . x t is also obtained from x t by the same input module . the output at the last time step in the last layer is passed to the output module . that is , y = h k t where k represent the number of layers in the network . then the output module gives the predicted [UNK] in natural language . section : extensions here we introduce a few extensions of q ##rn , and later in our experiments , we test q ##rn ' s performance with and without each of these extensions . reset gate . inspired by gr ##u [ reference ] , we found that it is useful to allow the q ##rn unit to reset ( null ##ify ) the candidate reduced query ( i . e . , h t ) when necessary . for this we use a reset gate function ##\u03b2 : , which can be defined similarly to the update gate function : where w ( r ) \u2208 r 1 ##\u00d7 ##d is a weight matrix , and b ( r ) \u2208 r is a bias term . equation 3 is re ##written as note that we do not use the reset gate in the last layer . vector gates . as in l ##st ##m and gr ##u , update and reset gates can be vectors instead of scala ##r values for fine - controlled ga ##ting . for vector gates , we modify the row dimension of weights and bias ##es in equation 1 and 5 from 1 to d . then we obtain z t , r t ##\u2208 r d ( instead of z t , r t ##\u2208 r ) , and these can be element - wise multiplied ( \u2022 ) instead of being broadcast ##ed in equation 3 and 6 . section : parallel ##ization an important advantage of q ##rn is that the rec ##urrent updates in equation 3 and 5 can be computed in parallel across time . this is in contrast with most rn ##n - based models that can not be parallel ##ized , where computing the candidate hidden state at time t explicitly requires the previous hidden state . in q ##rn , the final reduced que ##ries ( h t ) can be deco ##mp ##osed into computing over candidate reduced que ##ries ( h t ) , without looking at the previous reduced query . here we primarily show that the query update in equation 3 can be parallel ##ized by re ##writing the equation with matrix operations . the extension to equation 5 is straightforward . the proof for q ##rn with vector gates is shown in appendix b . the rec ##urs ##ive definition of equation 3 can be explicitly written as let b i = log ( 1 ##\u2212 z i ) for br ##ev ##ity . then we can re ##write equation 7 as the following equation : [ reference ] figure 2 : the sc ##hema ##tics of q ##rn and the two state - of - the - art models , end - to - end memory networks ( n ##2 ##n ) and improved dynamic memory networks ( d ##m ##n + ) , simplified to emphasize the differences among the models . ag ##ru is a variant of gr ##u where the update gate is replaced with soft attention , proposed by [ reference ] . where l , l ##\u2208 r t ##\u00d7 ##t are lower and strictly lower triangular matrices of 1 ' s , respectively , \u2022 is element ##wise multiplication , and b is a matrix where t b ' s are tiled across the column , i . section : related work q ##rn is inspired by rn ##n - based models with ga ##ting mechanism , such as l ##st ##m [ reference ] and gr ##u [ reference ] . while gr ##u and l ##st ##m use the previous hidden state and the current input to obtain the candidate hidden state , q ##rn only uses the current two inputs to obtain the candidate reduced query ( equivalent to candidate hidden state ) . we conjecture that this not only gives computational advantage via parallel ##ization , but also makes training easier , i . e . , avoiding vanishing gradient ( which is critical for long - term dependency ) , over ##fi ##tting ( by sim ##plify ##ing the model ) , and con ##ver ##ging to local mini ##ma . the idea of structurally sim ##plify ##ing ( con ##stra ##ining ) rn ##ns for learning longer - term patterns has been explored in recent previous work , such as structurally constrained rec ##urrent network [ reference ] and strongly - typed rec ##urrent neural network ( st ##rn ##n ) [ reference ] . q ##rn is similar to st ##rn ##n in that both architecture ##s use ga ##ting mechanism , and the gates and the candidate hidden states do not depend on the previous hidden states , which sim ##pl ##ifies the rec ##urrent relation . however , q ##rn can be distinguished from st ##rn ##n in three ways . first , q ##rn ' s update gate simulate ##s attention mechanism , measuring the relevance between the input sentence and query . on the other hand , the gates in st ##rn ##n can be considered as the sim ##pl ##ification of l ##st ##m / gr ##u by removing their dependency on previous hidden state . second , q ##rn is an rn ##n that is native ##ly compatible with context - based q ##a tasks , where the q ##rn unit accepts two inputs , i . e . each context sentence and query . this is distinct from st ##rn ##n which has only one input . third , we show that q ##rn is time ##wise - parallel ##iza ##ble on gp ##us . our parallel ##ization algorithm is also applicable to st ##rn ##n . end - to - end memory network ( n ##2 ##n ) [ reference ] uses external memory with multi - layer attention mechanism to focus on sentences that are relevant to the question . there are two key differences between n ##2 ##n and our q ##rn . first , n ##2 ##n sum ##mar ##izes the entire memory in each layer to control the attention in the next layer ( circle nodes in figure 2 ##b ) . instead , q ##rn does not have any controller node ( figure 2a ) and is able to focus on relevant sentences through the update gate that is internally embodied within its unit . second , n ##2 ##n adds time - dependent train ##able weights to the sentence representations to model the time dependency of the sentences ( as discussed in section 1 ) . q ##rn does not need such additional weights as its inherent rn ##n architecture allows q ##rn to effectively model the time dependency . neural reason ##er [ reference ] and gate ##d end - toe ##nd memory network [ reference ] ) are variants of me ##m ##n ##2 ##n that share its fundamental characteristics . improved dynamic memory network ( d ##m ##n + ) [ reference ] uses the hybrid of the attention mechanism and the rn ##n architecture to model the sequence of sentences . it consists of two distinct gr ##us , one for the time axis ( rec ##tangle nodes in figure 2 ##c ) and one for the layer axis ( circle nodes in figure 2 ##c ) . note that the update gate of the gr ##u for the time axis is replaced with external soft ##max attention weights . d ##m ##n + uses the time - axis gr ##u to sum ##mar ##izes the entire memory in each layer , and then the layer - axis gr ##u controls the attention weights in each layer . in contrast , q ##rn is simply a single rec ##urrent unit without any controller node . section : experiments 5 . 1 data ba ##bi story - based q ##a data ##set ba ##bi story - based q ##a data ##set is composed of 20 different tasks ( appendix a ) , each of which has 1 , 000 ( 1 ##k ) synthetic ##ally - generated story - question pair . a story can be as short as two sentences and as long as 200 + sentences . a system is evaluated on the accuracy of getting the correct answers to the questions . the answers are single words or lists ( e . g . \" football , apple \" ) . answering questions in each task requires selecting a set of relevant sentences and applying different kinds of logical reasoning over them . the data ##set also includes 10 ##k training data ( for each task ) , which allows training more complex models . note that d ##m ##n + [ reference ] only reports on the 10 ##k data ##set . ba ##bi dial ##og data ##set ba ##bi dial ##og data ##set consists of 5 different tasks ( table 3 ) , each of which has 1 ##k synthetic ##ally - generated goal - oriented dial ##og ##s between a user and the system in the domain of restaurant reservation . each dial ##og is as long as 96 utter ##ances and comes with external knowledge base ( kb ) providing information of each restaurant . the authors also provide out - of - vocabulary ( o ##ov ) version of the data ##set , where many of the words and kb key ##words in test data are not seen during training . a system is evaluated on the accuracy of its response to each utter ##ance of the user , choosing from up to 2500 possible candidate responses . a system is required not only to understand the user ' s request but also refer to previous conversations in order to obtain the context information of the current conversation . transformed the second dial ##og state tracking challenge ( ds ##tc ##2 ) data ##set [ reference ] into the same format as the ba ##bi dial ##og data ##set , for the measurement of performance on a real data ##set . each dial ##og can be as long as 800 + utter ##ances , and a system needs to choose from 240 ##7 possible candidate responses for each utter ##ance of the user . note that the evaluation metric of the original ds ##tc ##2 is different from that of the transformed ds ##tc ##2 , so previous work on the original ds ##tc ##2 should not be directly compared to our work . we will refer to this transformed ds ##tc ##2 data ##set by \" task 6 \" of dial ##og data ##set . section : ds ##tc ##2 ( task 6 ) dial ##og data ##set section : model details input module . in the input module , we are given sentences ( previous conversations in dial ##og ) x t and a question ( most recent user utter ##ance ) q , and we want to obtain their vector representations , x t , q ##\u2208 r d . we use a train ##able em ##bed ##ding matrix a ##\u2208 r d ##\u00d7 ##v to en ##code the one - hot vector of each word x t ##j in each sentence x t into a d - dimensional vector x t ##j ##\u2208 r d . then the sentence representation x t is obtained by position en ##code ##r . the same en ##code ##r with the same em ##bed ##ding matrix is also used to obtain the question vector q from q . output module for story - based q ##a . in the output module , we are given the vector representation of the predicted [UNK] and we want to obtain the natural language form of the answer , [UNK] . we use a v - way single - layer soft ##max class ##ifier to [UNK] to a v - dimensional sparse vector , v = soft ##max w ( y ) [UNK] r v , where w ( y ) \u2208 r v ##\u00d7 ##d is a weight matrix . then the final [UNK] is simply the ar ##gm ##ax word in ##v . to handle questions with multiple - word answers , we consider each of them as a single word that contains pun ##ct ##uation ##s such as space and com ##ma , and put it in the vocabulary . output module for dial ##og . we use a fixed number single - layer soft ##max class ##ifiers , each of which is similar to that of the so ##try - based q ##a model , to sequential ##ly output each word of the system ' s response . while it is similar in spirit to the rn ##n deco ##der [ reference ] , our output module does not have a rec ##urrent hidden state or ga ##ting mechanism . instead , it solely uses the final ou ##pt ##ut of the q ##rn , [UNK] , and the current word output to influence the prediction of the next word among possible candidates . training . we with ##hold 10 % of the training for development . we use the hidden state size of 50 by deaf ##ult . batch sizes of 32 for ba ##bi story - based q ##a 1 ##k , ba ##bi dial ##og and ds ##tc ##2 dial ##og , and 128 for ba ##bi q ##a 10 ##k are used . the weights in the input and output modules are initial ##ized with zero mean and the standard deviation of 1 / \u221a d . weights in the q ##rn unit are initial ##ized using techniques by [ reference ] , and are tied across the layers . forget bias of 2 . 5 is used for update gates ( no bias for reset gates ) . l ##2 weight decay of 0 . 001 ( 0 . 000 ##5 for q ##a 10 ##k ) is used for all weights . the loss function is the cross entropy between ##v and the one - hot vector of the true answer . the loss is minimize ##d by st ##och ##astic gradient descent for maximal ##ly 500 epoch ##s , but training is early stopped if the loss on the development data does not decrease for 50 epoch ##s . the learning rate is controlled by ada ##grad [ reference ] with the initial learning rate of 0 . 5 ( 0 . 1 for q ##a 10 ##k ) . since the model is sensitive to the weight initial ##ization , we repeat each training procedure 10 times ( 50 times for 10 ##k ) with the new random initial ##ization of the weights and report the result on the test data with the lowest loss on the development data . section : results . we compare our model with baseline ##s and previous state - of - the - art models on story - based and dial ##og tasks ( table 1 ) . these include l ##st ##m [ reference ] , end - to - end memory networks ( n ##2 ##n ) [ reference ] , dynamic memory networks ( d ##m ##n + ) [ reference ] , gate ##d end - to - end memory networks ( gm ##em ##n ##2 ##n ) [ reference ] , and different ##iable neural computer ( d ##nc ) [ reference ] . story - based q ##a . table 1 ( top ) reports the summary of results of our model ( q ##rn ) and previous work on ba ##bi q ##a ( task - wise results are shown in table 2 in appendix ) . in 1 ##k data , q ##rn ' s ' 2 ##r ' ( 2 layers + reset gate + d = 50 ) out ##per ##forms all other models by a large margin ( 2 . 8 + % ) . in 10 ##k data ##set , the average accuracy of q ##rn ' s ' 6 ##r ##200 ' ( 6 layers + reset gate + d = 200 ) model out ##per ##forms all previous models by a large margin ( 2 . 5 + % ) , achieving a nearly perfect score of 99 . 7 % . dial ##og . table 1 ( bottom ) reports the summary of the results of our model ( q ##rn ) and previous work on ba ##bi dial ##og and task 6 dial ##og ( task - wise results are shown in table 3 in appendix ) . as done in previous work [ reference ] , we also report results when we use ' match ' for dial ##og ##s . ' match ' is the extension to the model which additionally takes as input whether each answer candidate matches with context ( more details on appendix ) . q ##rn out ##per ##forms previous work by a large margin ( 2 . 0 + % ) in every comparison . ab ##lation ##s . we test four types of ab ##lation ##s ( also discussed in section 2 . 2 ) : number of layers ( 1 , 2 , 3 , or 6 ) , reset gate ( r ) , and gate vector ##ization ( v ) and the dimension of the hidden vector ( 50 , 100 ) . we show a subset of combinations of the ab ##lation ##s for ba ##bi q ##a in table 1 and table 2 ; other combinations performed poorly and / or did not give interesting observations . according to the ab ##lation results , we in ##fer that : ( a ) when the number of layers is only one , the model lacks reasoning capability . in the case of 1 ##k data ##set , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult . in the case of 10 ##k data ##set , many layers ( 6 ) and hidden dimensions ( 200 ) parallel ##ization . we implement q ##rn with and without parallel ##ization in tensor ##flow [ reference ] ) on a single titan x gp ##u to qu ##nai ##tify the computational gain of the parallel ##ization . for q ##rn without parallel ##ization , we use the rn ##n library provided by tensor ##flow . q ##rn with parallel ##ization gives 6 . 2 times faster training and inference than q ##rn without parallel ##ization on average . we expect that the speed ##up can be even higher for data ##set ##s with larger context . interpretations . an advantage of q ##rn is that the intermediate query updates are interpret ##able . figure 1 shows intermediate local que ##ries ( q k t ) interpreted in natural language , such as \" where is sandra ? \" . in order to obtain these , we place a deco ##der on the input question em ##bed ##ding q and add its loss for recovering the question to the classification loss ( similarly to [ reference ] ) . we then use the same deco ##der to deco ##de the intermediate que ##ries . this helps us understand the flow of information in the networks . in figure 1 , the question where is apple ? is transformed into where is sandra ? at t = 1 . at t = 2 , as sandra dropped the apple , the apple is no more relevant to sandra . we obtain where is daniel ? at time t = 3 , and it is prop ##aga ##ted until t = 5 , where we observe a sentence ( fact ) that can be used to answer the query . visual ##ization . figure 3 shows viz ##ual ##ization of the ( scala ##r ) magnitude ##s of update and reset gates on story sentences and dial ##og utter ##ances . more visual ##izations are shown in app ##end ##ices : figure 4 and figure 5 . in figure 3 , we observe high values on facts that provide information to answer question ( the system ' s next utter ##ance for dial ##og ) . in q ##a task 2 example ( top left ) , we observe high update gate values in the first layer on facts that state who has the apple , and in the second layer , the high update gate values are on those that inform where that person went to . we also observe that the forward reset gate at t = 2 in the first layer ( \u2212 ##\u2192 r 1 2 ) is low , which is sign ##ifying that apple no more belongs to sandra . in dial ##og task 3 ( bottom left ) , the model is able to in ##fer that three restaurants are already recommended so that it can recommend another one . in dial ##og task 6 ( bottom ) , the model focuses on the sentences containing spanish , and does not concentrate much on other facts such as i do n ' t care . section : conclusion in this paper , we introduce query - reduction network ( q ##rn ) to answer context - based questions and carry out conversations with users that require multi - hop reasoning . we show the state - of - the ##art results in the three data ##set ##s of story - based q ##a and dial ##og . we model a story or a dial ##og as a sequence of state - changing triggers and compute the final answer to the question or the system ' s next utter ##ance by rec ##urrent ##ly up ##dating ( or reducing ) the query . q ##rn is situated between the attention mechanism and rn ##n , effectively handling time dependency and long - term dependency problems of each technique , respectively . it addresses the long - term dependency problem of most rn ##ns by sim ##plify ##ing the rec ##urrent update , in which the candidate hidden state ( reduced query ) does not depend on the previous state . moreover , q ##rn can be parallel ##ized and can address the well - known problem of rn ##n ' s vanishing gradient ##s . section : a task - wise results here we provide detailed per - task breakdown of our results in q ##a ( table 2 ) and dial ##og data ##set ##s ( table 3 ) . table 2 : ba ##bi q ##a data ##set error rates ( % ) of q ##rn and previous work : l ##st ##m , end - to - end memory networks ( n ##2 ##n ) [ reference ] , dynamic memory networks ( d ##m ##n + ) [ reference ] , gate ##d end - to - end memory networks ( gm ##em ##n ##2 ##n ) [ reference ] . results within each task of different ##iable neural computer ( d ##nc ) were not provided in its paper [ reference ] table 3 : ba ##bi dial ##og and ds ##tc ##2 dial ##og data ##set average error rates ( % ) of q ##rn and previous work : end - to - end memory networks ( n ##2 ##n ) and gate ##d end - to - end memory networks ( gm ##em ##n ##2 ##n [ reference ] ) . for q ##rn , a number in the front [ reference ] [ reference ] [ reference ] [ reference ] indicates the number of layers and a number in the back ( 100 ) indicates the dimension of hidden vector , while the default value is 50 . ' r ' indicates that the reset gate is used , ' v ' indicates that the gates were vector ##ized , and ' + ' indicates that ' match ' was used . section : b vector gate parallel ##ization for vector gates , we have z t ##\u2208 r d instead of z t ##\u2208 r . therefore the following equation replaces equation 7 : where z j k is the k - th column vector of z j . let b i ##j = log ( 1 ##\u2212 z i j ) for br ##ev ##ity . then , we can re ##write equation 8 as following : . . . where l , l ##\u2208 r t ##\u00d7 ##t are lower and strictly lower triangular matrices of 1 ' s are tiled across the column . section : c model details match . while similar in spirit , our ' match ' model is slightly different from previous work ( bo ##rdes and [ reference ] . we use answer candidate em ##bed ##ding matrix , and add 2 dimension of 0 - 1 matrix which expresses whether the answer candidate matches with any word in the paragraph and the question . in other words , the soft ##max is computed [UNK] are train ##able weight matrices , and m ( y ) \u2208 r v ##\u00d7 ##2 is the 0 - 1 match matrix . section : d visual ##izations visual ##ization of story - based q ##a . figure 4 shows visual ##ization of models for story - based q ##a tasks . in the task 3 ( left ) , the model focuses on the facts that contain ' football ' in the first layer , and found out where mary journey ##ed to before the bathroom in the second layer . in task 7 ( right ) , the model focuses on the facts that provide information about the location of sandra . can you make a restaurant reservation for eight in a cheap price range in madrid 0 . 00 1 . 00 0 . 93 1 . 00 i ' m on it . 0 . 00 1 . 00 0 . 74 0 . 00 any preference on a type of cuisine . 0 . 00 0 . 11 1 . 00 0 . 01 i love british food . 0 . 00 0 . 99 0 . 99 0 . 57 okay let me look into some options for you . here it is : rest ##o - paris - ex ##pen - spanish - 8 ##star ##s - address figure 5 : visual ##ization of update and reset gates in q ##rn ' 2 ##r ' model for on several tasks of ba ##bi dial ##og and ds ##tc ##2 dial ##og ( table 3 ) . we do not put reset gate in the last layer . note that we only show some of recent sentences here , even the dial ##og has more sentences . visual ##ization of dial ##og . figure 5 shows visual ##ization of models for dial ##og tasks . in the first dial ##og of task 1 , the model focuses on the user utter ##ance that mentions the user ' s desired cuisine and location , and the current query ( user ' s last utter ##ance ) informs the system of the number of people , so the system is able to learn that it now needs to ask the user about the desired price range . in the second dial ##og of task 1 , the model focuses on the facts that provide information about the requests of the user . in task 4 ( third ) , the model focuses on what restaurant a user is talking about and the information about the restaurant . section : section : ac ##k ##now ##led ##gm ##ents this research was supported by the ns ##f ( ii ##s 161 ##6 ##11 ##2 ) , allen institute for ai ( 66 - 91 ##75 ) , allen distinguished investigator award , google research faculty award , and samsung gr ##o award . we thank the anonymous reviewers for their helpful comments . section :",
        "pred_seq": "ba ##a [SEP] query networks [SEP] [SEP] question answering [SEP] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "babi qa"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "question answering"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "babi dialog dataset",
                        "babi storybased qa",
                        "babi dialog",
                        "babi qa"
                    ]
                ],
                "Method": [
                    [
                        "queryreduction networks",
                        "queryreduction network",
                        "qrn",
                        "queryreduction network 1"
                    ]
                ],
                "Metric": [
                    [
                        "error rates"
                    ]
                ],
                "Task": [
                    [
                        "question answering",
                        "qa",
                        "contextbased qa"
                    ]
                ]
            }
        ]
    },
    "38": {
        "doctext": "document : neural tree index ##ers for text understanding rec ##urrent neural networks ( rn ##ns ) process input text sequential ##ly and model the conditional transition between word token ##s . in contrast , the advantages of rec ##urs ##ive networks include that they explicitly model the composition ##ality and the rec ##urs ##ive structure of natural language . however , the current rec ##urs ##ive architecture is limited by its dependence on syn ##ta ##ctic tree . in this paper , we introduce a robust syn ##ta ##ctic par ##sing - independent tree structured model , neural tree index ##ers ( nt ##i ) that provides a middle ground between the sequential rn ##ns and the syn ##ta ##ctic tree - based rec ##urs ##ive models . nt ##i construct ##s a full n - ar ##y tree by processing the input text with its node function in a bottom - up fashion . attention mechanism can then be applied to both structure and node function . we implemented and evaluated a binary - tree model of nt ##i , showing the model achieved the state - of - the - art performance on three different nl ##p tasks : natural language inference , answer sentence selection , and sentence classification , out ##per ##form ##ing state - of - the - art rec ##urrent and rec ##urs ##ive neural networks . section : introduction rec ##urrent neural networks ( rn ##ns ) have been successful for modeling sequence data . rn ##ns equipped with gate ##d hidden units and internal short - term memories , such as long short - term memories ( l ##st ##m ) have achieved a notable success in several nl ##p tasks including named entity recognition , constituency par ##sing , textual en ##tail ##ment recognition , question answering , and machine translation . however , most l ##st ##m models explored so far are sequential . it en ##codes text sequential ##ly from left to right or vice versa and do not naturally support composition ##ality of language . sequential l ##st ##m models seem to learn syn ##ta ##ctic structure from the natural language however their general ##ization on unseen text is relatively poor comparing with models that exploit syn ##ta ##ctic tree structure . unlike sequential models , rec ##urs ##ive neural networks compose word phrases over syn ##ta ##ctic tree structure and have shown improved performance in sentiment analysis . however its dependence on a syn ##ta ##ctic tree architecture limits practical nl ##p applications . in this study , we introduce neural tree index ##ers ( nt ##i ) , a class of tree structured models for nl ##p tasks . nt ##i takes a sequence of token ##s and produces its representation by constructing a full n - ar ##y tree in a bottom - up fashion . each node in nt ##i is associated with one of the node transformation functions : leaf node mapping and non - leaf node composition functions . unlike previous rec ##urs ##ive models , the tree structure for nt ##i is relaxed , i . e . , nt ##i does not require the input sequences to be par ##sed syn ##ta ##ctic ##ally ; and therefore it is flexible and can be directly applied to a wide range of nl ##p tasks beyond sentence modeling . furthermore , we propose different variants of node composition function and attention over tree for our nt ##i models . when a sequential leaf node transform ##er such as l ##st ##m is chosen , the nt ##i network forms a sequence - tree hybrid model taking advantage of both conditional and composition ##al powers of sequential and rec ##urs ##ive models . figure shows a binary - tree model of nt ##i . although the model does not follow the syn ##ta ##ctic tree structure , we empirical ##ly show that it achieved the state - of - the - art performance on three different nl ##p applications : natural language inference , answer sentence selection , and sentence classification . section : related work sub ##section : rec ##urrent neural networks and attention mechanism rn ##ns model input text sequential ##ly by taking a single token at each time step and producing a corresponding hidden state . the hidden state is then passed along through the next time step to provide historical sequence information . although a great success in a variety of tasks , rn ##ns have limitations . among them , it is not efficient at memo ##riz ##ing long or distant sequence . this is frequently called as information flow bottle ##neck . approaches have therefore been developed to overcome the limitations . for example , to mit ##igate the information flow bottle ##neck , ba ##hd ##ana ##u : 15 extended rn ##ns with a soft attention mechanism in the context of neural machine translation , leading to improved the results in translating longer sentences . rn ##ns are linear chain - structured ; this limits its potential for natural language which can be represented by complex structures including syn ##ta ##ctic structure . in this study , we propose models to mit ##igate this limitation . sub ##section : rec ##urs ##ive neural networks unlike rn ##ns , rec ##urs ##ive neural networks explicitly model the composition ##ality and the rec ##urs ##ive structure of natural language over tree . the tree structure can be pre ##de ##fine ##d by a syn ##ta ##ctic par ##ser . each non - leaf tree node is associated with a node composition function which combines its children nodes and produces its own representation . the model is then trained by back - prop ##aga ##ting error through structures . the node composition function can be varied . a single layer network with non - linear ##ity was adopted in rec ##urs ##ive auto - associate memories and rec ##urs ##ive auto ##en ##code ##rs . soc ##her ##20 ##12 ##se ##man ##tic extended this network with an additional matrix representation for each node to aug ##ment the expressive power of the model . tensor networks have also been used as composition function for sentence - level sentiment analysis task . recently , zhu ##20 ##15 ##long introduced s - l ##st ##m which extends l ##st ##m units to compose tree nodes in a rec ##urs ##ive fashion . in this paper , we introduce a novel at ##ten ##tive node composition function that is based on s - l ##st ##m . our nt ##i model does not rely on either a par ##ser output or a fine - grain ##ed supervision of non - leaf nodes , both required in previous work . in nt ##i , the supervision from the target labels is provided at the root node . as such , our nt ##i model is robust and applicable to a wide range of nl ##p tasks . we introduce attention over tree in nt ##i to overcome the vanishing / explode gradient ##s challenges as shown in rn ##ns . section : methods our training set consists of examples , where the input is a sequence of word token ##s and the output can be either a single target or a sequence . each input word token is represented by its word em ##bed ##ding . nt ##i is a full n - ar ##y tree ( and the sub - trees can be overlap ##ped ) . it has two types of transformation function : non - leaf node function and leaf node function . compute ##s a ( possibly non - linear ) transformation of the input word em ##bed ##ding . is a function of its child nodes representation , where is the total number of child nodes of this non - leaf node . nt ##i can be implemented with different tree structures . in this study we implemented and evaluated a binary tree form of nt ##i : a non - leaf node can take in only two direct child nodes ( i . e . , ) . therefore , the function compose ##s its left child node and right child node . figure illustrates our nt ##i model that is applied to question answering ( a ) and natural language inference tasks ( b ) . note that the node and leaf node functions are neural networks and are the only training parameters in nt ##i . we explored two different approaches to compose node representations : an extended l ##st ##m and at ##ten ##tive node composition functions , to be described below . sub ##section : non - leaf node composition functions we define two different methods for non - leaf node function . l ##st ##m - based non - leaf node function ( s - l ##st ##m ) : we initiate with l ##st ##m . for non - leaf node , we adopt s - l ##st ##m zhu ##20 ##15 ##long , an extension of l ##st ##m to tree structures , to learn a node representation by its children nodes . let , , and be vector representations and cell states for the left and right children . an s - l ##st ##m compute ##s a parent node representation and a node cell state as where and bias ##es ( for br ##ev ##ity we eliminated the bias terms ) are the training parameters . and denote the element - wise function and the element - wise vector multiplication . extension of s - l ##st ##m non - leaf node function to compose more children is straightforward . however , the number of parameters increases quad ##ratic ##ally in s - l ##st ##m as we add more child nodes . at ##ten ##tive non - leaf node function ( an ##f ) : some nl ##p applications ( e . g . , q ##a and machine translation ) would benefit from a dynamic query dependent composition function . we introduce an ##f as a new non - leaf node function . unlike s - l ##st ##m , an ##f compose ##s the child nodes at ##ten ##tively in respect to another relevant input vector . the input vector can be a learn ##able representation from a sequence representation . given a matrix resulted by con ##cate ##nat ##ing the child node representations , and the third input vector , an ##f is defined as where is a learn ##able matrix , the attention score and the attention weight vector for each child . is an attention scoring function , which can be implemented as a multi - layer per ##ce ##pt ##ron ( ml ##p ) or a matrix - vector product . the matrices and and the vector are training parameters . is a vector of ones and the outer product . we use function for non - linear transformation . sub ##section : attention over tree comparing with sequential l ##st ##m models , nt ##i has less rec ##ur ##rence , which is defined by the tree depth , for binary tree where is the length of the input sequence . however , nt ##i still needs to com ##press all the input information into a single representation vector of the root . this impose ##s practical difficulties when processing long sequences . we address this issue with attention mechanism over tree . in addition , the attention mechanism can be used for matching trees ( described in section 4 as tree matching nt ##i ) that carry different sequence information . we first define a global attention and then introduce a tree attention which considers the parent - child dependency for calculation of the attention weights . global attention : an attention neural network for the global attention takes all node representations as input and produces an at ##ten ##tively blended vector for the whole tree . this neural net is similar to an ##f . particularly , given a matrix resulted by con ##cate ##nat ##ing the node representations , \u2026 , and the relevant input representation , the global attention is defined as where and are training parameters and the attention weight vector for each node . this attention mechanism is robust as it globally normal ##izes the attention score with to obtain the weights . however , it does not consider the tree structure when producing the final representation . tree attention : we modify the global attention network to the tree attention mechanism . the resulting tree attention network performs almost the same computation as an ##f for each node . it compares the parent and children nodes to produce a new representation assuming that all node representations are constructed . given a matrix resulted by con ##cate ##nat ##ing the parent node representation , the left child and the right child and the relevant input representation , every non - leaf node simply updates its own representation by using the following equation in a bottom - up manner . and this equation is similarity to the global attention . however , now each non - leaf node at ##ten ##tively collects its own and children representations and passes towards the root which finally construct ##s the at ##ten ##tively blended tree representation . note that unlike the global attention , the tree attention locally normal ##izes the attention scores with . section : experiments we describe in this section experiments on three different nl ##p tasks , natural language inference , question answering and sentence classification to demonstrate the flexibility and the effectiveness of nt ##i in the different settings . we trained nt ##i using adam with hyper ##para ##meter ##s selected on development set . the pre - trained 300 - d glove 840 ##b vectors were obtained for the word em ##bed ##ding ##s . the word em ##bed ##ding ##s are fixed during training . the em ##bed ##ding ##s for out - of - vocabulary words were set to zero vector . we pad the input sequence to form a full binary tree . a pad ##ding vector was inserted when pad ##ding . we analyzed the effects of the pad ##ding size and found out that it has no influence on the performance ( see appendix [ reference ] ) . the size of hidden units of the nt ##i modules were set to 300 . the models were regular ##ized by using drop ##outs and an weight decay . sub ##section : natural language inference we conducted experiments on the stanford natural language inference ( s ##nl ##i ) data ##set , which consists of 54 ##9 , 36 ##7 / 9 , 84 ##2 / 9 , 82 ##4 premise - hypothesis pairs for train / dev / test sets and target label indicating their relation . unless otherwise noted , we follow the setting in the previous work and use an ml ##p for classification which takes in nt ##i outputs and compute ##s the con ##cate ##nation , absolute difference and element ##wise product of the two sentence representations . the ml ##p has also an input layer with 102 ##4 units with activation and a output layer . we explored nine different task - oriented nt ##i models with varying complexity , to be described below . for each model , we set the batch size to 32 . the initial learning , the regular ##ization strength and the number of epoch to be trained are varied for each model . nt ##i - sl ##st ##m : this model does not rely on transform ##er but uses the s - l ##st ##m units for the non - leaf node function . we set the initial learning rate to 1 ##e - 3 and regular ##izer strength to 3 ##e - 5 , and train the model for 90 epoch ##s . the neural net was regular ##ized by 10 % input drop ##outs and the 20 % output drop ##outs . nt ##i - sl ##st ##m - l ##st ##m : we use l ##st ##m for the leaf node function . concrete ##ly , the l ##st ##m output vectors are given to nt ##i - sl ##st ##m and the memory cells of the lowest level s - l ##st ##m were initial ##ized with the l ##st ##m memory states . the hyper - parameters are the same as the previous model . nt ##i - sl ##st ##m node - by - node global attention : this model learns inter - sentence relation with the global attention over premise - indexed tree , which is similar to word - by - word attention model of rock ##tas ##chel : 16 in that it attends over the premise tree nodes at every time step of hypothesis encoding . we tie the weight parameters of the two nt ##i - sl ##st ##ms for premise and hypothesis and no transform ##er used . we set the initial learning rate to 3 ##e - 4 and regular ##izer strength to 1 ##e - 5 , and train the model for 40 epoch ##s . the neural net was regular ##ized by 15 % input drop ##outs and the 15 % output drop ##outs . nt ##i - sl ##st ##m node - by - node tree attention : this is a variation of the previous model with the tree attention . the hyper - parameters are the same as the previous model . nt ##i - sl ##st ##m - l ##st ##m node - by - node global attention : in this model we include l ##st ##m as the leaf node function . here we initial ##ize the memory cell of s - l ##st ##m with l ##st ##m memory and hidden / memory state of hypothesis l ##st ##m with premise l ##st ##m ( the later follows the work of ) . we set the initial learning rate to 3 ##e - 4 and regular ##izer strength to 1 ##e - 5 , and train the model for 10 epoch ##s . the neural net was regular ##ized by 10 % input drop ##outs and the 15 % output drop ##outs . nt ##i - sl ##st ##m - l ##st ##m node - by - node tree attention : this is a variation of the previous model with the tree attention . the hyper - parameters are the same as the previous model . tree matching nt ##i - sl ##st ##m - l ##st ##m global attention : this model first construct ##s the premise and hypothesis trees simultaneously with the nt ##i - sl ##st ##m - l ##st ##m model and then compute ##s their matching vector by using the global attention and an additional l ##st ##m . the attention vectors are produced at each hypothesis tree node and then are given to the l ##st ##m model sequential ##ly . the l ##st ##m model com ##press the attention vectors and outputs a single matching vector , which is passed to an ml ##p for classification . the ml ##p for this tree matching setting has an input layer with 102 ##4 units with activation and a output layer . unlike wang ##j ##15 ##b ' s matching l ##st ##m model which is specific to matching sequences , we use the standard l ##st ##m units and match trees . we set the initial learning rate to 3 ##e - 4 and regular ##izer strength to 3 ##e - 5 , and train the model for 20 epoch ##s . the neural net was regular ##ized by 20 % input drop ##outs and the 20 % output drop ##outs . tree matching nt ##i - sl ##st ##m - l ##st ##m tree attention : we replace the global attention with the tree attention . the hyper - parameters are the same as the previous model . full tree matching nt ##i - sl ##st ##m - l ##st ##m global attention : this model produces two sets of the attention vectors , one by attending over the premise tree regarding each hypothesis tree node and another by attending over the hypothesis tree regarding each premise tree node . each set of the attention vectors is given to a l ##st ##m model to achieve full tree matching . the last hidden states of the two l ##st ##m models ( i . e . one for each attention vector set ) are con ##cate ##nated for classification . the training weights are shared among the l ##st ##m models the hyper - parameters are the same as the previous model . table [ reference ] shows the results of our models . for comparison , we include the results from the published state - of - the - art systems . while most of the sentence en ##code ##r models rely solely on word em ##bed ##ding ##s , the dependency tree cnn and the spin ##n - pi models make use of sentence par ##ser output ; which present strong baseline systems . the last set of methods designs inter - sentence relation with soft attention . our best score on this task is 87 . 3 % accuracy obtained with the full tree matching nt ##i model . the previous best performing model on the task performs phrase matching by using the attention mechanism . our results show that nt ##i - sl ##st ##m improved the performance of the sequential l ##st ##m en ##code ##r by approximately 2 % . not surprisingly , using l ##st ##m as leaf node function helps in learning better representations . our nt ##i - sl ##st ##m - l ##st ##m is a hybrid model which en ##codes a sequence sequential ##ly through its leaf node function and then hierarchical ##ly compose ##s the output representations . the node - by - node attention models improve the performance , indicating that modeling inter - sentence interaction is an important element in nl ##i . ag ##gre ##gating matching vector between trees or sequences with a separate l ##st ##m model is effective . the global attention seems to be robust on this task . the tree attention were not helpful as it normal ##izes the attention scores locally in parent - child relationship . sub ##section : answer sentence selection for this task , a model is trained to identify the correct sentences that answer a factual question , from a set of candidate sentences . we experiment on wi ##ki ##qa data ##set constructed from wikipedia . the data ##set contains 20 , 360 / 2 , 73 ##3 / 6 , 165 q ##a pairs for train / dev / test sets . we used the same setup in the language inference task except that we replace the layer with a layer and model the following conditional probability distribution . where and are the question and the answer encoded vectors and denotes the output of the hidden layer of the ml ##p . for this task , we use nt ##i - sl ##st ##m - l ##st ##m to en ##code answer candidate sentences and nt ##i - an ##f - l ##st ##m to en ##code the question sentences . note that nt ##i - an ##f - l ##st ##m is relied on an ##f as the non - leaf node function . vector for nt ##i - an ##f - l ##st ##m is the answer representation produced by the answer encoding nt ##i - sl ##st ##m - l ##st ##m model . we set the batch size to 4 and the initial learning rate to 1 ##e - 3 , and train the model for 10 epoch ##s . we used 20 % input drop ##outs and no weight decay . following previous work , we adopt map and mr ##r as the evaluation metric ##s for this task . table [ reference ] presents the results of our model and the previous models for the task . the class ##ifier with hand ##craft ##ed features is a sv ##m model trained with a set of features . the big ##ram - cnn model is a simple con ##vo ##lu ##tion ##al neural net . the deep l ##st ##m and l ##st ##m attention models out ##per ##form the previous best result by a large margin , nearly 5 - 6 % . nas ##m improves the result further and sets a strong baseline by combining variation ##al auto - en ##code ##r with the soft attention . in nas ##m , they adopt a deep three - layer l ##st ##m and introduced a late ##nt st ##och ##astic attention mechanism over the answer sentence . our nt ##i model exceeds nas ##m by approximately 0 . 4 % on map for this task . sub ##section : sentence classification lastly , we evaluated nt ##i on the stanford sentiment tree ##bank ( ss ##t ) . this data ##set comes with standard train / dev / test sets and two sub ##tas ##ks : binary sentence classification or fine - grain ##ed classification of five classes . we trained our model on the text spans corresponding to labeled phrases in the training set and evaluated the model on the full sentences . we use nt ##i - sl ##st ##m and nt ##i - sl ##st ##m - l ##st ##m models to learn sentence representations for the task . the sentence representations were passed to a two - layer ml ##p for classification . we set the batch size to 64 , the initial learning rate to 1 ##e - 3 and regular ##izer strength to 3 ##e - 5 , and train each model for 10 epoch ##s . the nt ##i - sl ##st ##m model was regular ##ized by 10 % / 20 % of input / output and 20 % / 30 % of input / output drop ##outs and the nt ##i - sl ##st ##m - l ##st ##m model 20 % of input and 20 % / 30 % of input / output drop ##outs for binary and fine - grain ##ed settings . nt ##i - sl ##st ##m - l ##st ##m ( as shown in table [ reference ] ) set the state - of - the - art results on both sub ##tas ##ks . our nt ##i - sl ##st ##m model performed slightly worse than its constituency tree - based counter part , ct - l ##st ##m model . the ct - l ##st ##m model compose ##s phrases according to the output of a sentence par ##ser and uses a node composition function similar to s - l ##st ##m . after we transformed the input with the l ##st ##m leaf node function , we achieved the best performance on this task . section : qu ##ali ##tative analysis sub ##section : attention and composition ##ality to help analyzing the results , we output attention weights by our nt ##i - sl ##st ##m node - by - node global attention model . figure [ reference ] shows the attention heat ##ma ##ps for two sentences in the s ##nl ##i test set . it shows that our model semantic ##ally align ##s single or multi ##word expressions ( \" little child \" and \" todd ##ler \" ; \" rock wall \" and \" stone \" ) . in addition , our model is able to re - orient its attention over different parts of the hypothesis when the expression is more complex . for example , for ( c ) \" rock wall in autumn \" , nt ##i mostly focuses on the nodes in depth 1 , 2 and 3 representing contexts related to \" a stone \" , \" leaves . \" and \" a stone wall surrounded \" . surprisingly , attention degree for the single word expression like \" stone \" , \" wall \" and \" leaves \" is lower to compare with multi ##word phrases . sequence models lack this property as they have no explicit composition module to produce such mu ##ti ##word phrases . finally , the most interesting pattern is that the model attends over higher level ( low depth ) tree nodes with rich semantics when considering a ( c ) longer phrase or ( d ) full sentence . as shown in ( d ) , the nt ##i model align ##s the root node representing the whole hypothesis sentence to the higher level tree nodes covering larger sub - trees in the premise . it certainly ignores the lower level single word expressions and only starts to attend when the words are collectively to form rich semantics . sub ##section : learned representations of phrases and sentences using co ##sin ##e similarity between their representations produced by the nt ##i - sl ##st ##m model , we show that nt ##i is able to capture para ##ph ##rase ##s on s ##nl ##i test data . as shown in table [ reference ] , nt ##i seems to distinguish plural from singular forms ( similar phrases to \" a person \" ) . in addition , nt ##i captures non - surface knowledge . for example , the phrases similar to \" park for fun \" tend to align to the semantic content of fun and park , including \" people play fr ##is ##bee outdoors \" . the nt ##i model was able to relate \" santa claus \" to christmas and snow . interesting ##ly , the learned representations were also able to connect implicit semantics . for example , nt ##i found that \" sad , depressed , and hatred \" is close to the phrases like \" an obama supporter is upset \" . overall the nt ##i model is robust to the length of the phrases being matched . given a short phrase , nt ##i can retrieve longer yet semantic ##ally coherent sequences from the s ##nl ##i test set . in table [ reference ] , we show nearest - neighbor sentences from s ##nl ##i test set . note that the sentences listed in the first two columns sound semantic ##ally coherent but not the ones in the last column . the query sentence \" a dog sells a women a hat \" does not actually represent a common - sense knowledge and this sentence now seem to confuse the nt ##i model . as a result , the retrieved sentence are arbitrary and not coherent . sub ##section : effects of pad ##ding size we introduced a special pad ##ding character in order to construct full binary tree . does this pad ##ding character influence the performance of the nt ##i models ? in figure [ reference ] , we show relationship between the pad ##ding size and the accuracy on stanford sentiment analysis data . each sentence was padded to form a full binary tree . the x - axis represents the number of pad ##ding characters introduced . when the pad ##ding size is less ( up to 10 ) , the nt ##i - sl ##st ##m - l ##st ##m model performs better . however , this model tends to perform poorly or equally when the pad ##ding size is large . overall we do not observe any significant performance drop for both models as the pad ##ding size increases . this suggests that nt ##i learns to ignore the special pad ##ding character while processing padded sentences . the same scenario was also observed while analyzing attention weights . the attention over the padded nodes was nearly zero . section : discussion and conclusion we introduced neural tree index ##ers , a class of tree structured rec ##urs ##ive neural network . the nt ##i models achieved state - of - the - art performance on different nl ##p tasks . most of the nt ##i models form deep neural networks and we think this is one reason that nt ##i works well even if it lacks direct linguistic motivation ##s followed by other syn ##ta ##ctic - tree - structured rec ##urs ##ive models . cnn and nt ##i are topological ##ly related . both nt ##i and cnn ##s are hierarchical . however , current implementation of nt ##i only operates on non - overlapping sub - trees while cnn ##s can slide over the input to produce higher - level representations . nt ##i is flexible in selecting the node function and the attention mechanism . like cnn , the computation in the same tree - depth can be parallel ##ized effectively ; and therefore nt ##i is scala ##ble and suitable for large - scale sequence processing . note that nt ##i can be seen as a general ##ization of l ##st ##m . if we construct left - branching trees in a bottom - up fashion , the model acts just like sequential l ##st ##m . different branching factors for the underlying tree structure have yet to be explored . nt ##i can be extended so it learns to select and compose dynamic number of nodes for efficiency , essentially discovering intrinsic hierarchical structure in the input . section : ac ##k ##now ##led ##gm ##ents we would like to thank the anonymous reviewers for their insight ##ful comments and suggestions . this work was supported in part by the grant h ##l ##12 ##50 ##8 ##9 from the national institutes of health ( ni ##h ) . any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor . bibliography : references",
        "pred_seq": "[SEP] neural ##ers [SEP] [SEP] natural inference [SEP] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] natural inference inference inference inference inference inference inference inference inference inference inference inference inference inference inference inference [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "neural tree indexers"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "natural language inference"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "stanford natural language inference",
                        "snli"
                    ]
                ],
                "Method": [
                    [
                        "ntislstmlstm"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "natural language inference",
                        "nli",
                        "language inference task"
                    ]
                ]
            }
        ]
    },
    "39": {
        "doctext": "document : understanding humans in crowded scenes : deep nest ##ed ad ##vers ##aria ##l learning and a new bench ##mark for multi - human par ##sing despite the noticeable progress in per ##ce ##pt ##ual tasks like detection , instance segment ##ation and human par ##sing , computers still perform un ##sat ##is ##fa ##ctor ##ily on visually understanding humans in crowded scenes , such as group behavior analysis , person re - identification and autonomous driving , etc . to this end , models need to comprehensive ##ly perceive the semantic information and the differences between instances in a multi - human image , which is recently defined as the multi - human par ##sing task . in this paper , we present a new large - scale database \" m ul ##ti - h um ##an p ars ##ing ( m ##hp ) \" for algorithm development and evaluation , and advances the state - of - the - art in understanding humans in crowded scenes . m ##hp contains 25 , 403 elaborate ##ly ann ##ota ##ted images with 58 fine - grain ##ed semantic category labels , involving 2 - 26 persons per image and captured in real - world scenes from various viewpoint ##s , poses , o ##cc ##lusion , interactions and background . we further propose a novel deep n este ##d a d ##vers ##aria ##l n et ##work ( nan ) model for multi - human par ##sing . nan consists of three g en ##erative a d ##vers ##aria ##l n et ##work ( gan ) - like sub - nets , respectively performing semantic sal ##ien ##cy prediction , instance - ag ##nostic par ##sing and instance - aware cluster ##ing . these sub - nets form a nest ##ed structure and are carefully designed to learn jointly in an end - to - end way . nan consistently out ##per ##forms existing state - of - the - art solutions on our m ##hp and several other data ##set ##s , and serves as a strong baseline to drive the future research for multi - human par ##sing . section : introduction one of the primary goals of intelligent human - computer interaction is understanding the humans in visual scenes . it involves several per ##ce ##pt ##ual tasks including detection , i . e . local ##izing different persons at a coarse , bound ##ing box level ( fig . [ reference ] ( a ) ) , instance segment ##ation , i . e . label ##ling each pixel of each person uniquely ( fig . [ reference ] ( b ) ) , and human par ##sing , i . e . deco ##mp ##osing persons into their semantic categories ( fig . [ reference ] ( c ) ) . recently , deep learning based methods have achieved remarkable su ##ces ##s in these per ##ce ##pt ##ual tasks thanks to the availability of pl ##ent ##iful ann ##ota ##ted images for training and evaluation purposes . though exciting , current progress is still far from the ut ##imate goal of visually understanding humans . as fig . [ reference ] shows , previous efforts on understanding humans in visual scenes either only consider coarse information or are ag ##nostic to different instances . in the real - world scenarios , it is more likely that there sim ##uta ##neo ##usly exist multiple persons , with various human interactions , poses and o ##cc ##lusion . thus , it is more practically demanded to par ##se human body parts and fashion items at the instance level , which is recently defined as the multi - human par ##sing task . multi - human par ##sing enables more detailed understanding of humans in crowded scenes and align ##s better with many real - world applications , such as group behavior analysis , person re - identification , e - commerce , image editing , video surveillance , autonomous driving and virtual reality . however , the existing bench ##mark data ##set ##s are not suitable for such a new task . even though li et al . proposed a preliminary m ul ##ti - h um ##an p ars ##ing ( m ##hp v ##1 . 0 ) data ##set , it only contains 4 , 980 images ann ##ota ##ted with 18 semantic labels . in this work , we propose a new large - scale bench ##mark \" m ul ##ti - h um ##an p ars ##ing ( m ##hp v ##2 . 0 ) \" , aiming to push the frontiers of multi - human par ##sing research towards ho ##listic ##ally understanding humans in crowded scenes . the data in m ##hp v ##2 . 0 cover wide variability and complexity w . r . t . viewpoint ##s , poses , o ##cc ##lusion , human interactions and background . it in total includes 25 , 403 human images with pixel - wise ann ##ota ##tions of 58 semantic categories . we further propose a novel deep n este ##d a d ##vers ##aria ##l n et ##work ( nan ) model for solving the challenging multi - human par ##sing problem . unlike most existing methods which rely on separate stages of instance local ##ization , human par ##sing and result ref ##ine ##ment , the proposed nan par ##ses semantic categories and differentiate ##s different person instances simultaneously in an effective and time - efficient manner . nan consists of three g en ##erative a d ##vers ##aria ##l n et ##work ( gan ) - like sub - nets , respectively performing semantic sal ##ien ##cy prediction , instance - ag ##nostic par ##sing and instance - aware cluster ##ing . each sub - task is simpler than the original multi - human par ##sing task , and is more easily addressed by the corresponding sub - net . unlike many multi - task learning applications , in our method the sub - nets depend on each other , forming a causal nest by dynamic ##ally boost ##ing each other through an ad ##vers ##aria ##l strategy ( see fig . [ reference ] ) , which is hence called a \" nest ##ed ad ##vers ##aria ##l learning \" structure . such a structure enables effort ##less gradient b ac ##k ##pro p ag ##ation ( bp ) in nan such that it can be trained in a ho ##listic , end - to - end way , which is favorable to both accuracy and speed . we conduct qu ##ali ##tative and quantitative experiments on the m ##hp v ##2 . 0 data ##set proposed in this work , as well as the m ##hp v ##1 . 0 , pascal - person - part and buffy bench ##mark data ##set ##s . the results demonstrate the superiority of nan on multi - human par ##sing over the state - of - the - arts . our contributions are summarized as follows . we propose a new large - scale bench ##mark and evaluation server to advance understanding of humans in crowded scenes , which contains 25 , 403 images ann ##ota ##ted pixel - wise ##ly with 58 semantic category labels . we propose a novel deep n este ##d a d ##vers ##aria ##l n et ##work ( nan ) model for multi - human par ##sing , which serves as a strong baseline to inspire more future research efforts on this task . comprehensive evaluation ##s on the m ##hp v ##2 . 0 data ##set proposed in this work , as well as the m ##hp v ##1 . 0 , pascal - person - part and buffy bench ##mark data ##set ##s verify the superiority of nan on understanding humans in crowded scenes over the state - of - the - arts . section : related work paragraph : human par ##sing data ##set ##s the statistics of popular publicly available data ##set ##s for human par ##sing are summarized in tab . [ reference ] . the buffy data ##set was released in 2011 for human par ##sing and instance segment ##ation . it contains only 74 ##8 images ann ##ota ##ted with 13 semantic categories . the fashion ##ista data ##set was released in 2012 for human par ##sing , containing limited images ann ##ota ##ted with 56 fashion categories . the pascal - person - part data ##set was initially ann ##ota ##ted by chen et al . from the pascal - vo ##c - 2010 data ##set . chen et al . extended it for human par ##sing with 7 coarse body part labels . the at ##r data ##set was released in 2015 for human par ##sing with a large number of images ann ##ota ##ted with 18 semantic categories . the lip data ##set further extended at ##r by crop ##ping person instances from microsoft coco . it is a large - scale human par ##sing data ##set with densely pixel - wise ann ##ota ##tions of 20 semantic categories . but it has two limitations . 1 ) despite the large data size , it contains limited semantic category ann ##ota ##tions , which restrict ##s the fine - grain ##ed understanding of humans in visual scenes . 2 ) in lip , only a small proportion of images involve multiple persons with interactions . such an instance - ag ##nostic setting severely devi ##ates from reality . even in the m ##hp v ##1 . 0 data ##set proposed by li et al . for multi - human par ##sing , only 4 , 980 images are included and ann ##ota ##ted with 18 semantic labels . comparatively , our m ##hp v ##2 . 0 data ##set contains 25 , 403 elaborate ##ly ann ##ota ##ted images with 58 fine - grain ##ed semantic part labels . it is the largest and most comprehensive multi - human par ##sing data ##set to date , to our best knowledge . visual comparisons between lip , m ##hp v ##1 . 0 and our m ##hp v ##2 . 0 are provided in fig . [ reference ] . paragraph : human par ##sing approaches recently , many research efforts have been devoted to human par ##sing due to its wide range of potential applications . for example , liang et al . proposed a proposal - free network for instance segment ##ation by directly predicting the instance numbers of different categories and the pixel - level information . gong et al . proposed a self - supervised structure - sensitive learning approach , which impose ##s human pose structures to par ##sing results without resort ##ing to extra supervision . liu et al . proposed a single frame video par ##sing method which integrate ##s frame par ##sing , optical flow estimation and temporal fusion into a unified network . zhao et al . proposed a self - supervised neural aggregation network , which learns to aggregate the multi - scale features and incorporates a self - supervised joint loss to ensure the consistency between par ##sing and pose . he et al . proposed the mask r - cnn , which is extended from faster r - cnn by adding a branch for predicting an object mask in parallel with the existing branch for bound ##ing box recognition . bra ##band ##ere et al . proposed to tackle instance segment ##ation with a disc ##rim ##ina ##tive loss function , operating at the pixel level , which encourages a con ##vo ##lu ##tion ##al network to produce a representation of the image that can be easily clustered into instances with a simple post - processing step . however , these methods either only consider coarse semantic information or are ag ##nostic to different instances . to enable more detailed human - cent ##ric analysis , li et al . initially proposed the multi - human par ##sing task , which align ##s better with the realistic scenarios . they also proposed a novel m ##h - par ##ser model as a reference method which generates par ##sing maps and instance masks sim ##uta ##neo ##usly in a bottom - up fashion . jiang et al . proposed a new approach to segment human instances and label their body parts using region assembly . li et al . proposed a framework with a human detector and a category - level segment ##ation module to segment the parts of objects at the instance level . these methods involve mu ##tip ##le separate stages for instance local ##ization , human par ##sing and result ref ##ine ##ment . in comparison , the proposed nan produces accurate multi - human par ##sing results through a single forward - pass in a time - efficient manner without ted ##ious pre - or post - processing . section : multi - human par ##sing bench ##mark in this section , we introduce the \" m ul ##ti - h um ##an p ars ##ing ( m ##hp v ##2 . 0 ) \" , a new large - scale data ##set focusing on semantic understanding of humans in crowded scenes with several appealing properties . 1 ) it contains 25 , 403 elaborate ##ly ann ##ota ##ted images with 58 fine - grain ##ed labels on body parts , fashion items and one background label , which is larger and more comprehensive than previous similar attempts . 2 ) the images within m ##hp v ##2 . 0 are collected from real - world scenarios , involving humans with various viewpoint ##s , poses , o ##cc ##lusion , interactions and resolution . 3 ) the background of images in m ##hp v ##2 . 0 is more complex and diverse than previous data ##set ##s . some examples are showed in fig . [ reference ] . the m ##hp v ##2 . 0 data ##set is expected to provide a new bench ##mark suitable for multi - human par ##sing together with a standard evaluation server where the test set will be kept secret to avoid over ##fi ##tting . sub ##section : image collection and ann ##ota ##tion we manually specify some underlying relationships ( such as family , couple , team , etc . ) and possible scenes ( such as sports , conferences , banquet ##s , etc . ) to ensure the diversity of returned results . based on any one of these specifications , corresponding multi - human images are located by performing internet searches over creative commons licensed imagery . for each identified image , the contained human number and the corresponding ur ##l are stored in a spreads ##hee ##t . automated scrap ##ping software is used to download the multi - human imagery and stores all relevant information in a relational database . moreover , a pool of images containing clearly visible persons with interactions and rich fashion items is also constructed from the existing human - cent ##ric data ##set ##s to aug ##ment and complement internet scraping results . after cu ##rating the imagery , manual ann ##ota ##tion is conducted by professional data ann ##ota ##tors , which includes two distinct tasks . the first task is manually counting the number of fore ##ground persons and du ##pl ##ica ##ting each image to several copies according to the count number . each duplicate ##d image is marked with the image id , the contained person number and a self - index . the second is assign ##ing the fine - grain ##ed pixel - wise label to each semantic category for each person instance . we implement an ann ##ota ##tion tool and generate multi - scale super ##pi ##x ##els of images based on to speed up the ann ##ota ##tion . see fig . [ reference ] for an example . each multi - human image contains at least two instances . the ann ##ota ##tion for each instance is done in a left - to - right order , corresponding to the duplicate ##d image with the self - index from beginning to end . for each instance , 58 semantic categories are defined and ann ##ota ##ted , including cap / hat , helmet , face , hair , left - arm , right - arm , left - hand , right - hand , protector , bikini / bra , jacket / wind ##breaker / hood ##ie , t - shirt , polo - shirt , sweater , single ##t , torso - skin , pants , shorts / swim - shorts , skirt , stockings , socks , left - boot , right - boot , left - shoe , right - shoe , left - high ##hee ##l , right - high ##hee ##l , left - sand ##al , right - sand ##al , left - leg , right - leg , left - foot , right - foot , coat , dress , robe , jumps ##uit ##s , other - full - body - clothes , head ##wear , backpack , ball , bats , belt , bottle , carry ##bag , cases , sunglasses , eye ##wear , gloves , scarf , umbrella , wallet / purse , watch , wrist ##band , tie , other - access ##aries , other - upper - body - clothes and other - lower - body - clothes . each instance has a complete set of ann ##ota ##tions whenever the corresponding category appears in the current image . when ann ##ota ##ting one instance , others are regarded as background . thus , the resulting ann ##ota ##tion set for each image consists of instance - level par ##sing masks , where is the number of persons in the image . after ann ##ota ##tion , manual inspection is performed on all images and corresponding ann ##ota ##tions to verify the correct ##ness . in cases where ann ##ota ##tions are er ##rone ##ous , the information is manually rec ##ti ##fied by 5 well informed analysts . the whole work took around three months to accomplish by 25 professional data ann ##ota ##tors . sub ##section : data ##set splits and statistics in total , there are 25 , 403 images in the m ##hp v ##2 . 0 data ##set . each image contains 2 - 26 person instances , with 3 on average . the resolution of the images ranges from 85 100 to 4 , 51 ##1 6 , 91 ##9 , with 64 ##4 71 ##8 on average . we spit the images into training , validation and testing sets . following random selection , we arrive at a unique split consisting of 15 , 403 training and 5 , 000 validation images with publicly available ann ##ota ##tions , as well as 5 , 000 testing images with ann ##ota ##tions with ##held for bench ##mark ##ing purpose . the statistics w . r . t . data distribution on 59 semantic categories , the average semantic category number per image and the average instance number per image in the m ##hp v ##2 . 0 data ##set are illustrated in fig . [ reference ] ( a ) , ( b ) and ( c ) , respectively . in general , face , arms and legs are the most remarkable parts of a human body . however , understanding humans in crowded scenes needs to analyze fine - grain ##ed details of each person of interest , including different body parts , clothes and access ##aries . we therefore define 11 body parts , and 47 clothes and access ##aries . among these 11 body parts , we divide arms , hands , legs and feet into left and right side for more precise analysis , which also increases the difficulty of the task . we define hair , face and torso - skin as the remaining three body parts , which can be used as auxiliary guidance for more comprehensive instance - level analysis . as for clothing categories , we have common clothes like coat , jacket / wind ##breaker / hood ##ie , sweater , single ##t , pants , shorts / swim - shorts and shoes , confusing categories such as t - shirt v . s . polo - shirt , stockings v . s . socks , skirt v . s . dress and robe , and boots v . s . sandals and high ##hee ##ls , and in ##fr ##e ##quent categories such as cap / hat , helmet , protector , bikini / bra , jumps ##uit ##s , gloves and scarf . furthermore , access ##aries like sunglasses , belt , tie , watch and bags are also taken into account , which are common but hard to predict , especially for the small - scale ones . to sum ##mar ##ize , the pre - defined semantic categories of m ##hp v ##2 . 0 involve most body parts , clothes and access ##aries of different styles for men , women and children in all seasons . the images in the m ##hp v ##2 . 0 data ##set contain diverse instance numbers , viewpoint ##s , poses , o ##cc ##lusion , interactions and background complex ##ities . m ##hp v ##2 . 0 align ##s better with real - world scenarios and serves as a more realistic bench ##mark for human - cent ##ric analysis , which pushes the frontiers of fine - grain ##ed multi - human par ##sing research . section : deep nest ##ed ad ##vers ##aria ##l networks as shown in fig . [ reference ] , the proposed deep n este ##d a d ##vers ##aria ##l n et ##work ( nan ) model consists of three gan - like sub - nets that jointly perform semantic sal ##ien ##cy prediction , instance - ag ##nostic par ##sing and instance - aware cluster ##ing end - to - end . nan produces accurate multi - human par ##sing results through a single forward - pass in a time - efficient manner without ted ##ious pre - or post - processing . we now present each component in details . sub ##section : semantic sal ##ien ##cy prediction large mod ##ality and interaction variations are the main challenge to multi - human par ##sing and also the key obstacle to learning a well - performing human - cent ##ric analysis model . to address this problem , we propose to deco ##mp ##ose the original task into three gran ##ular ##ities and adaptive ##ly impose a prior on the specific process , each with the aid of a gan - based sub - net . this reduces the training complexity and leads to better empirical performance with limited data . the first sub - net estimates semantic sal ##ien ##cy maps to locate the most noticeable and eye - attracting human regions in images , which serves as a basic prior to facilitate further processing on humans , as illustrated in fig . [ reference ] left . we formula ##te semantic sal ##ien ##cy prediction as a binary pixel - wise label ##ling problem to segment out fore ##ground v . s . background . inspired by the recent success of f ul ##ly c on ##vo ##lu ##tion ##al n et ##work s ( fc ##ns ) based methods on image - to - image applications , we leverage an fc ##n backbone ( fc ##n - 8 ##s ) as the generator of nan for semantic sal ##ien ##cy prediction , where denotes the network parameters , and , , and denote the image height , width , channel number and semantic category ( i . e . , fore ##ground plus background ) number , rep ##ect ##ively . formally , let the input r ##gb image be denoted by and the semantic sal ##ien ##cy map be denoted by , then the key requirements for are that the semantic sal ##ien ##cy map should present ind ##ist ##ing ##uis ##hab ##le proper ##ities compared with a real one ( i . e . , ground truth ) in appearance while preserving the intrinsic context ##ually remarkable information . to this end , we propose to learn by mini ##mi ##zing a combination of two losses : where is the ad ##v er ##sari ##al loss for refining realism and all ##ev ##iating artifacts , is the s em ##ant ##ic s alien ##cy loss for pixel - wise image label ##ling , are weight ##ing parameters among different losses . is a pixel - wise cross - entropy loss calculated based on the binary pixel - wise ann ##ota ##tions to learn : is proposed to narrow the gap between the distributions of generated and real results . to facilitate this process , we leverage a c on ##vo ##lu ##tion ##al n eu ##ral n et ##work ( cnn ) backbone as the disc ##rim ##inator to be as simple as possible to avoid typical gan tricks . we alternatively opt ##imi ##ze and to learn and : where denotes the binary real v . s . fake indicator . sub ##section : instance - ag ##nostic par ##sing the second sub - net con ##cate ##nate ##s the information from the original r ##gb image with semantic sal ##ien ##cy prior as input and estimates a fine - grain ##ed instance - ag ##nostic par ##sing map , which further serves as stronger semantic guidance from the global perspective to facilitate instance - aware cluster ##ing , as illustrated in fig . [ reference ] middle . we formula ##te instance - ag ##nostic par ##sing as a multi - class dense classification problem to mask semantic ##ally consistent regions of body parts and fashion items . inspired by the leading performance of the skip - net on recognition tasks , we modify a skip - net ( w ##s - res ##net ) into an fc ##n - based architecture as the generator of nan to learn a highly non - linear transformation for instance - ag ##nostic par ##sing , where denotes the network parameters for the generator and denotes the semantic category number . the prediction is downs ##amp ##led by for accuracy v . s . speed trade - off . context ##ual information from global and local regions compensate ##s each other and naturally benefits human par ##sing . the hierarchical features within a skip - net are multi - scale in nature due to the increasing rec ##eptive field sizes , which are combined together via skip connections . such a combined representation comprehensive ##ly maintains the context ##ual information , which is crucial for generating smooth and accurate par ##sing results . formally , let the instance - ag ##nostic par ##sing map be denoted by , then similar to the first sub - net , we propose to learn by mini ##mi ##zing : where is the g lo ##bal p ars ##ing loss for semantic part label ##ling . is a standard pixel - wise cross - entropy loss calculated based on the multi - class pixel - wise ann ##ota ##tions to learn . is also slightly fine ##tu ##ned due to the hi ##nged gradient back ##pro ##pa ##gation route within the nest ##ed structure : is proposed to ensure the correct ##ness and realism of the current phase and also the previous one for information flow consistency . to facilitate this process , we leverage a same cnn backbone with as the disc ##rim ##inator , which are learned separately . we alternatively opt ##imi ##ze and to learn , and slightly fine ##tu ##ne : sub ##section : instance - aware cluster ##ing the third sub - net con ##cate ##nate ##s the information from the original r ##gb image with semantic sal ##ien ##cy and instance - ag ##nostic par ##sing prior ##s as input and estimates an instance - aware cluster ##ing map by ass ##oc ##iating each semantic par ##sing mask to one of the person instances in the scene , as illustrated in fig . [ reference ] right . inspired by the observation that a human glances at an image and instantly knows how many and where the objects are in the image , we formula ##te instance - aware cluster ##ing by parallel ##ly in ##fer ##ring the instance number and pixel - wise instance location , disc ##arding the requirement of time - consuming region proposal generation . we modify a same backbone architecture to incorporate two sibling branches as the generator of nan for location - sensitive learning , where denotes the network parameters for the generator and denotes the pre - defined instance location coordinate number . as multi - scale features integrating both global and local context ##ual information are crucial for increasing location prediction accuracy , we further aug ##ment the pixel - wise instance location prediction branch with a m ul ##ti - s cale f us ##ion u ni ##t ( ms ##fu ) to fuse shallow - , middle - and deep - level features , while using the feature maps downs ##amp ##led by con ##cate ##nated with feature maps from the first branch for instance number regression . formally , let the pixel - wise instance location map be denoted by and the instance number be denoted by , then we propose to learn by mini ##mi ##zing : where is the p ix ##el - wise i ns ##tance l o ##cation loss for pixel - wise instance location regression and is the i ns ##tance n um ##ber loss for instance number regression . is a standard smooth - loss calculated based on the fore ##ground pixel - wise instance location ann ##ota ##tions to learn . since a person instance can be identified by its top - left corner and bottom - right corner of the surrounding bound ##ing box , for each pixel belonging to the person instance , the pixel - wise instance location vector is defined as , where and are the width and height of the person instance for normal ##ization , respectively . is a standard loss calculated based on the instance number ann ##ota ##tions to learn . and are also slightly fine ##tu ##ned due to the chained sc ##hema within the nest : given these information , instance - aware cluster ##ing maps can be effortlessly generated with little computational overhead , which are denoted by . similar to , is proposed to ensure the correct ##ness and realism of all phases for the information flow consistency . to facilitate this process , we leverage a same cnn backbone with as the disc ##rim ##inator , which are learned separately . we alternatively opt ##imi ##ze and to learn , and slightly fine ##tu ##ne and : sub ##section : training and inference the goal of nan is to use sets of real targets to learn three gan - like sub - nets that mutually boost and jointly accomplish multi - human par ##sing . each separate loss serves as a deep supervision within the nest ##ed structure benefit ##ting network convergence . the overall objective function for nan is clearly , the nan is end - to - end train ##able and can be opt ##imi ##zed with the proposed nest ##ed ad ##vers ##aria ##l learning strategy and bp algorithm . during testing , we simply feed the input image into nan to get the instance - ag ##nostic par ##sing map from , pixel - wise instance location map and instance number from . then we employ an off - the - shelf cluster ##ing method to obtain the instance - aware cluster ##ing map . example results are visual ##ized in fig . [ reference ] . section : experiments we evaluate nan qu ##ali ##tative ##ly and quantitative ##ly under various settings and gran ##ular ##ities for understanding humans in crowded scenes . in particular , we evaluate multi - human par ##sing performance on the m ##hp v ##2 . 0 data ##set proposed in this work , as well as the m ##hp v ##1 . 0 and pascal - person - part bench ##mark data ##set ##s . we also evaluate instance - ag ##nostic par ##sing and instance - aware cluster ##ing results on the buffy bench ##mark data ##set , which are by ##pro ##du ##cts of nan . sub ##section : experimental settings sub ##su ##bs ##ection : implementation details throughout the experiments , the sizes of the r ##gb image , the semantic sal ##ien ##cy prediction , inputs to the disc ##rim ##inator and inputs to the generator are fixed as ; the sizes of the instance - ag ##nostic par ##sing prediction , instance - aware cluster ##ing prediction , inputs to the disc ##rim ##inator , inputs to the generator , inputs to the disc ##rim ##inator and instance location map are fixed as ; the channel number of the pixel - wise instance location map is fixed as , incorporating two corner points of the associated bound ##ing box ; the constraint factors are empirical ##ly fixed as and , respectively ; the generator is initial ##ized with fc ##n - 8 ##s by replacing the last layer with a new con ##vo ##lu ##tion ##al layer with kernel size , pre ##train ##ed on pascal - vo ##c - 2011 and fine ##tu ##ned on the target data ##set ; the generator is initial ##ized with w ##s - res ##net by eliminating the spatial pool ##ing layers , increasing the strides of the first con ##vo ##lu ##tion ##al layers up to 2 in b , eliminating the top - most global pool ##ing layer and the linear class ##ifier , and adding two new con ##vo ##lu ##tion ##al layers with kernel sizes and , pre ##train ##ed on image ##net and pascal - vo ##c - 2012 , and fine ##tu ##ned on the target data ##set ; the generator is initial ##ized with the same backbone architecture and pre - trained weights with ( which are learned separately ) , by further aug ##ment ##ing it with two sibling branches for pixel - wise instance location map prediction and instance number prediction , where the first branch utilizes a ms ##fu ( three con ##vo ##lu ##tion ##al layers with kern ##al sizes for specific scale adapt ##ion ) ended with a con ##vo ##lu ##tion ##al layer with kernel size for multi - scale feature aggregation and a final con ##vo ##lu ##tion ##al layer with kernel size for location regression and the second branch utilizes the feature maps downs ##amp ##led by 8 con ##cate ##nated with the feature maps from the first branch ended with a global pool ##ing layer , a hidden 512 - way fully - connected layer and a final 1 - way fully - connected layer for instance number regression ; the three disc ##rim ##inator ##s ( which are learned separately ) are all initial ##ized with a v ##gg - 16 by adding a new con ##vo ##lu ##tion ##al layer at the very begin ##ing with kernel size for input adapt ##ion , and replacing the last layer with a new 1 - way fully - connected layer activated by si ##gm ##oid , pre - trained on image ##net and fine ##tu ##ned on the target data ##set ; the newly added layers are randomly initial ##ized by drawing weights from a zero - mean ga ##uss ##ian distribution with standard deviation ; we employ an off - the - shelf cluster ##ing method to obtain the instance - aware cluster ##ing map ; the drop ##out ratio is empirical ##ly fixed as ; the weight decay and batch size are fixed as and , respectively ; we use an initial learning rate of for pre - trained layers , and for newly added layers in all our experiments ; we decrease the learning rate to of the previous one after 20 epoch ##s and train the network for roughly 60 epoch ##s one after the other ; the proposed network is implemented based on the publicly available tensor ##flow platform , which is trained using adam ( ) on four n ##vid ##ia ge ##force gt ##x titan x gp ##us with 12 g memory ; the same training setting is utilized for all our compared network variants ; we evaluate the testing time by averaging the running time for images on the target set on n ##vid ##ia ge ##force gt ##x titan x gp ##u and intel core i ##7 - 49 ##30 k cpu @ 3 . 40 ##gh ##z ; our nan can rapidly process one image in about 1 second , which compares much favorably to other state - of - the - art approaches , as the current state - of - the - art methods rely on region proposal prep ##ro ##ces ##sing and complex processing steps . sub ##su ##bs ##ection : evaluation metric ##s following , we use the a vera ##ge p rec ##ision based on p art ( ) and p er ##cent ##age of c orr ##ect ##ly par ##sed semantic p arts ( pc ##p ) metric ##s for multi - human par ##sing evaluation . different from the a vera ##ge p rec ##ision based on r e ##gio ##n ( ) used in instance segment ##ation , uses part - level pixel i nt ##ers ##ection o ve ##r u ni ##on ( io ##u ) of different semantic part categories within a person instance to determine if one instance is a true positive . we prefer over as we focus on human - cent ##ric analysis and we aim to investigate to how well a person instance as a whole is par ##sed . additionally , we also report the , which is the mean of the at io ##u threshold ##s ranging from to , in inc ##rem ##ents of 0 . 1 . as averages the io ##u of each semantic part category , it fails to reflect how many semantic parts are correctly par ##sed . we further incorporate the pc ##p , originally used in human pose estimation , to evaluate the par ##sing quality within person instances . for each true - positive person instance , we find all the semantic categories ( excluding background ) with pixel io ##u larger than a threshold , which are regarded as correctly par ##sed . the pc ##p of one person instance is the ratio between the correctly par ##sed semantic category number and the total semantic category number of that person . missed person instances are assigned with pc ##p . the overall pc ##p is the average pc ##p for all person instances . note that pc ##p is also a human - cent ##ric evaluation metric . sub ##section : evaluation ##s on the m ##hp v ##2 . 0 bench ##mark the m ##hp v ##2 . 0 data ##set proposed in this paper is the largest and most comprehensive multi - human par ##sing bench ##mark to date , which extends m ##hp v ##1 . 0 to push the frontiers of understanding humans in crowded scenes by containing 25 , 403 elaborate ##ly ann ##ota ##ted images with 58 fine - grain ##ed semantic category labels . ann ##ota ##tion examples are visual ##ized in fig . [ reference ] ( c ) . the data are randomly organized into 3 splits , consisting of 15 , 403 training and 5 , 000 validation images with publicly available ann ##ota ##tions , as well as 5 , 000 testing images with ann ##ota ##tions with ##held for bench ##mark ##ing purpose . evaluation systems report the and pc ##p over the validation and testing sets . sub ##su ##bs ##ection : component analysis we first investigate different architecture ##s and loss function combinations of nan to see their respective roles in multi - human par ##sing . we compare 16 variants from four aspects , i . e . , different baseline ##s ( mask r - cnn and m ##h - par ##ser ) , different network structures ( w / o , w / o con ##cate ##nated input ( r ##gb only ) , w / o con ##cate ##nated input ( r ##gb only ) , w / o , w / o , w / o con ##cate ##nated input , w / o , w / o con ##cate ##nated input , w / o ms ##fu ) , our proposed nan , and upper ##bound ##s ( : use the ground truth semantic sal ##ien ##cy maps instead of prediction while keeping other settings the same ; : use the ground truth instance - ag ##nostic par ##sing maps instead of prediction while keeping other settings the same ; : use the ground truth instance number instead of prediction while keeping other settings the same ; : use the ground truth pixel - wise instance location maps instead of prediction while keeping other settings the same ) . the performance comparison in terms of @ io ##u = 0 . 5 , and pc ##p @ io ##u = 0 . 5 on the m ##hp v ##2 . 0 validation set is reported in tab . [ reference ] . by coma ##ring the results from the v . s . panels , we observe that our proposed nan consistently out ##per ##forms the baseline ##s mask r - cnn and m ##h - par ##ser by a large margin , i . e . , and in terms of , and in terms of , and and in terms of pc ##p . mask r - cnn suffers difficulties to differentiate en ##tangled humans . m ##h - par ##ser involves multiple stages for instance local ##ization , human par ##sing and result ref ##ine ##ment with high complexity , yielding sub - optimal results , whereas nan par ##ses semantic categories , differentiate ##s different person instances and ref ##ines results simultaneously through deep nest ##ed ad ##vers ##aria ##l learning in an effective yet time - efficient manner . by coma ##ring the results from the v . s . panels , we observe that nan consistently out ##per ##forms the 9 variants in terms of network structure . in particular , w / o refers to tr ##un ##cating the semantic sal ##ien ##cy prediction sub - net from nan , leading to , and performance drop in terms of all metric ##s . this ve ##ri ##fies the necessity of semantic sal ##ien ##cy prediction that locate ##s the most noticeable human regions in images to serve as a basic prior to facilitate further human - cent ##ic processing . the superiority of incorporating adaptive prior information to specific process can be verified by comparing w / o con ##cate ##nated input with nan , i . e . , , and ; , and differences in terms of all metric ##s . the superiority of incorporating ad ##vers ##aria ##l learning to specific process can be verified by comparing w / o with nan , i . e . , , and ; , and ; , and decrease in terms of all metric ##s . nest ##ed ad ##vers ##aria ##l learning strategy ensures the correct ##ness and realism of all phases for information flow consistency , the superiority of which is verified by comparing w / o con ##cate ##nated input with nan , i . e . , , and ; , and decline in terms of all metric ##s . ms ##fu dynamic ##ally fuse ##s multi - scale features for enhancing instance - aware cluster ##ing accuracy , the superiority of which is verified by comparing w / o ms ##fu with nan , i . e . , , and drop in terms of all metric ##s . finally , we also evaluate the limitations of our current algorithm . by comparing with nan , only , and improvement in term of all metric ##s are obtained , which shows that the errors from semantic sal ##ien ##cy prediction are already small and have only little effect on the final results . a large gap between , and of and , and of nan shows that a better instance - ag ##nostic par ##sing network architecture can definitely help improve the performance of multi - human par ##sing under our nan framework . by comparing and with nan , , and ; , and improvement in term of all metric ##s are obtained , which shows that accurate instance - aware cluster ##ing results are critical for superior multi - human par ##sing . sub ##su ##bs ##ection : quantitative comparison the performance comparison of the proposed nan with two state - of - the - art methods in terms of @ io ##u = 0 . 5 , and pc ##p @ io ##u = 0 . 5 on the m ##hp v ##2 . 0 testing set is reported in tab . [ reference ] . following , we conduct experiments under three settings : all reports the evaluation over the whole testing set ; inter % 20 reports the evaluation over the sub - set containing the images with top 20 % interaction intensity ; inter % 10 reports the evaluation over the sub - set containing the images with top 10 % interaction intensity . our nan is significantly superior over other state - of - the - arts on setting - 1 . in particular , nan improves the - best by , and in terms of all metric ##s . for the more challenging scenarios with intensive interactions ( setting - 2 , 3 ) , nan also consistently achieve ##s the best performance . in particular , for inter % 20 and inter % 10 , nan improves the - best by , and ; , and in terms of all metric ##s . this ve ##ri ##fies the effectiveness of our nan for multi - human par ##sing and understanding humans in crowded scenes . moreover , nan can rapidly process one 512 512 image in about 1 second with acceptable resource consumption , which is attractive to real applications . this compares much favorably to m ##h - par ##ser ( 14 . 94 im ##g / s ) , which relies on separate and complex post - processing ( including cr ##f ) steps . sub ##su ##bs ##ection : qu ##ali ##tative comparison fig . [ reference ] visual ##izes the qu ##ali ##tative comparison of the proposed nan with two state - of - the - art methods and corresponding ground truths on the m ##hp v ##2 . 0 data ##set . note that mask r - cnn only offers silhouette ##s of different person instances , we only compare our instance - aware cluster ##ing results with it while comparing our ho ##listic results with m ##h - par ##ser . it can be observed that the proposed nan performs well in multi - human par ##sing with a wide range of viewpoint ##s , poses , o ##cc ##lusion , interactions and background complexity . the instance - ag ##nostic par ##sing and instance - aware cluster ##ing predictions of nan present high consistency with corresponding ground truths , thanks to the novel network structure and effective training strategy . in contrast , mask r - cnn suffers difficulties to differentiate en ##tangled humans , while m ##h - par ##ser struggles to generate fine - grain ##ed par ##sing results and clearly segment ##ed instance masks . this further des ##mons ##tra ##tes the effectiveness of the proposed nan . we also show some failure cases of our nan in fig . [ reference ] . as can be observed , humans in crowded scenes with heavy o ##cc ##lusion , extreme poses and intensive interactions are difficult to identify and segment . some small - scale semantic categories within person instances are difficult to par ##se . this confirms that m ##hp v ##2 . 0 align ##s with real - world situations and deserves more fur ##ture attention and research efforts . sub ##section : evaluation ##s on the m ##hp v ##1 . 0 bench ##mark the m ##hp v ##1 . 0 data ##set is the first multi - human par ##sing bench ##mark , originally proposed by li et al . , which contains 4 , 980 images ann ##ota ##ted with 18 semantic labels . ann ##ota ##tion examples are visual ##ized in fig . [ reference ] ( b ) . the data are randomly organized into 3 splits , consisting of 3 , 000 training , 1 , 000 validation and 1 , 000 testing images with publicly available ann ##ota ##tions . evaluation systems report the and pc ##p over the testing set . refer to for more details . the performance comparison of the proposed nan with three state - of - the - art methods in terms of @ io ##u = 0 . 5 , and pc ##p @ io ##u = 0 . 5 on the m ##hp v ##1 . 0 testing set is reported in tab . [ reference ] . with the nest ##ed ad ##vers ##aria ##l learning of semantic sal ##ien ##cy prediction , instance - ag ##nostic par ##sing and instance - aware cluster ##ing , our method out ##per ##forms the - best by for , for and for pc ##p . visual comparison of multi - human par ##sing results by nan and three state - of - the - art methods is provided in fig . [ reference ] , which further valid ##ates the advantages of our nan over existing solutions . sub ##section : evaluation ##s on the pascal - person - part bench ##mark the pascal - person - part data ##set is a set of additional ann ##ota ##tions for pascal - vo ##c - 2010 . it goes beyond the original pascal object detection task by providing pixel - wise labels for six human body parts , i . e . , head , torso , upper - / lower - arms , and upper - / lower - legs . the rest of each image is considered as background . there are 3 , 53 ##5 images in the pascal - person - part data ##set , which is split into separate training set containing 1 , 71 ##7 images and testing set containing 1 , 81 ##8 images . for fair comparison , we report the over the testing set for multi - human par ##sing . refer to for more details . the performance comparison of the proposed nan with two state - of - the - art methods in terms of @ io ##u = and on the pascal - person - part testing set is reported in tab . [ reference ] . our method dramatically sur ##pass ##es the - best by for and for . qu ##ali ##tative multi - human par ##sing results by nan are visual ##ized in fig . [ reference ] , which possess a high concord ##ance with corresponding ground truths . this again ve ##ri ##fies the effectiveness of our method for human - cent ##ric analysis . sub ##section : evaluation ##s on the buffy bench ##mark the buffy data ##set was released in 2011 for human par ##sing and instance segment ##ation , which contains 74 ##8 images ann ##ota ##ted with 12 semantic labels . the data are randomly organized into 2 splits , consisting of 45 ##2 training and 296 testing images with publicly available ann ##ota ##tions . for fair comparison , we report the f or ##ward ( f ) and b ac ##k ##ward ( b ) scores over the episode 4 , 5 and 6 for instance segment ##ation evaluation . refer to for more details . the performance comparison of the proposed nan with three state - of - the - art methods in terms of f and b scores on the buffy data ##set episode 4 , 5 and 6 is reported in tab . [ reference ] . our nan consistently achieve ##s the best performance for all metric ##s . in part ##ic ##ual ##r , nan significantly improves the - best by for f score and for b score , with an average boost of . qu ##ali ##tative instance - ag ##nostic par ##sing and instance - aware cluster ##ing results by nan are visual ##ized in fig . [ reference ] , which well shows the promising potential of our method for fine - grain ##ed understanding humans in crowded scenes . section : conclusions in this work , we presented \" m ul ##ti - h um ##an p ars ##ing ( m ##hp v ##2 . 0 ) \" , a large - scale multi - human par ##sing data ##set and a carefully designed bench ##mark to spark progress in understanding humans in crowded scenes . m ##hp v ##2 . 0 contains 25 , 403 images , which are richly labelled with 59 semantic categories . we also proposed a novel deep n este ##d a d ##vers ##aria ##l n et ##work ( nan ) model to address this challenging problem and performed detailed evaluation ##s of the proposed method with current state - of - the - arts on m ##hp v ##2 . 0 and several other data ##set ##s . we en ##vision the proposed m ##hp v ##2 . 0 data ##set and the baseline method would drive the human par ##sing research towards real - world application scenario with simultaneous presence of multiple persons and complex interactions among them . in future , we will continue to take efforts to construct a more comprehensive multi - human par ##sing bench ##mark data ##set with more images and more detailed semantic category ann ##ota ##tions to further push the frontiers of multi - human par ##sing research . section : acknowledge ##ment the work of jian zhao was partially supported by c hi ##na s cho ##lars ##hip c ou ##nc ##il ( cs ##c ) grant 2015 ##0 ##31 ##70 ##24 ##8 . the work of jia ##shi feng was partially supported by nu ##s startup r - 263 - 000 - c ##0 ##8 - 133 , moe tier - i r - 263 - 000 - c2 ##1 - 112 , nu ##s id ##s r - 263 - 000 - c ##6 ##7 - 64 ##6 and ec ##ra r - 263 - 000 - c ##8 ##7 - 133 . bibliography : references",
        "pred_seq": "m ##ing [SEP] m ##ing [SEP] [SEP] multi ##sing [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "m ultih uman p arsing"
                    ]
                ],
                "Method": [
                    [
                        "m ultih uman p arsing"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "multihuman parsing"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "mhp v10",
                        "mhp v10 benchmark"
                    ]
                ],
                "Method": [
                    [
                        "n ested dversarial n etwork",
                        "nan",
                        "deep nested adversarial networks",
                        "nested adversarial learning strategy"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "multihuman parsing",
                        "mhp",
                        "m ultih uman p arsing",
                        "lip"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "mhp v20",
                        "mhp v20 dataset",
                        "mhp v20 benchmark"
                    ]
                ],
                "Method": [
                    [
                        "n ested dversarial n etwork",
                        "nan",
                        "deep nested adversarial networks",
                        "nested adversarial learning strategy"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "multihuman parsing",
                        "mhp",
                        "m ultih uman p arsing",
                        "lip"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "pascalpersonpart",
                        "pascalpersonpart benchmark datasets"
                    ]
                ],
                "Method": [
                    [
                        "n ested dversarial n etwork",
                        "nan",
                        "deep nested adversarial networks",
                        "nested adversarial learning strategy"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "multihuman parsing",
                        "mhp",
                        "m ultih uman p arsing",
                        "lip"
                    ]
                ]
            }
        ]
    },
    "40": {
        "doctext": "document : deep fried con ##vn ##ets the fully - connected layers of deep con ##vo ##lu ##tion ##al neural networks typically contain over 90 % of the network parameters . reducing the number of parameters while preserving predict ##ive performance is critically important for training big models in distributed systems and for deployment in embedded devices . in this paper , we introduce a novel adaptive fast ##fo ##od transform to rep ##ara ##meter ##ize the matrix - vector multiplication of fully connected layers . rep ##ara ##meter ##izing a fully connected layer with inputs and outputs with the adaptive fast ##fo ##od transform reduces the storage and computational costs costs from to and respectively . using the adaptive fast ##fo ##od transform in con ##vo ##lu ##tion ##al networks results in what we call a deep fried con ##vn ##et . these con ##vn ##ets are end - to - end train ##able , and enable us to attain substantial reductions in the number of parameters without affecting prediction accuracy on the mn ##ist and image ##net data ##set ##s . section : introduction in recent years we have witnessed an explosion of applications of con ##vo ##lu ##tion ##al neural networks with millions and billions of parameters . reducing this vast number of parameters would improve the efficiency of training in distributed architecture ##s . it would also allow for the deployment of state - of - the - art con ##vo ##lu ##tion ##al neural networks on embedded mobile applications . these train and test time considerations are both of great importance . a standard con ##vo ##lu ##tion ##al network is composed of two types of layers , each with very different properties . con ##vo ##lu ##tion ##al layers , which contain a small fraction of the network parameters , represent most of the computational effort . in contrast , fully connected layers contain the vast majority of the parameters but are comparatively cheap to evaluate . this im ##balance between memory and computation suggests that the efficiency of these two types of layers should be addressed in different ways . and both describe methods for mini ##mi ##zing computational cost of evaluating a network at test time by approx ##imating the learned con ##vo ##lu ##tion ##al filters with sep ##arable approximation ##s . these approaches realize speed gains at test time but do not address the issue of training , since the approximation ##s are made after the network has been fully trained . additionally , neither approach achieve ##s a substantial reduction in the number of parameters , since they both work with approximation ##s of the con ##vo ##lu ##tion ##al layers , which represent only a small portion of the total number of parameters . many other works have addressed the computational efficiency of con ##vo ##lu ##tion ##al networks in more specialized settings . in contrast to the above approaches , demonstrates that there is significant red ##unda ##ncy in the parameter ##ization of several deep learning models , and exploits this to reduce the number of parameters . more specifically , their method represents the parameter matrix as a product of two low rank factors , and the training algorithm fix ##es one factor ( called static parameters ) and only updates the other factor ( called dynamic parameters ) . uses low - rank matrix factor ##ization to reduce the size of the fully connected layers at train time . they demonstrate large improvements in reducing the number of parameters of the output soft ##max layer , but only modest improvements for the hidden fully connected layers . implements low - rank factor ##izations using the sv ##d after training the full model . in contrast , the methods advanced in and this paper apply both at train and test time . in this paper we show how the number of parameters required to represent a deep con ##vo ##lu ##tion ##al neural network can be substantially reduced without sac ##ri ##fi ##cing predict ##ive performance . our approach works by replacing the fully connected layers of the network with an adaptive fast ##fo ##od transform , which is a general ##ization of the fast ##fo ##od transform for approx ##imating kernel ##s . con ##vo ##lu ##tion ##al neural networks with adaptive fast ##fo ##od transforms , which we refer to as deep fried con ##vn ##ets , are end - to - end train ##able and achieve the same predict ##ive performance as standard con ##vo ##lu ##tion ##al networks on image ##net using approximately half the number of parameters . several works have considered kernel methods in deep learning . the do ##ub ##ly st ##och ##astic gradient ##s method of showed that effective use of random ##ization can allow kernel methods to scale to extremely large data sets . however , the approach used fixed con ##vo ##lu ##tion ##al features , and can not jointly learn the kernel class ##ifier and con ##vo ##lu ##tion ##al filters . showed how to learn a kernel function in an un ##su ##per ##vis ##ed manner . there have been other attempts to replace the fully connected layers . the network in network architecture of achieve ##s state of the art results on several deep learning bench ##marks by replacing the fully connected layers with global average pool ##ing . a similar approach was used by to win the il ##s ##vr ##c 2014 object detection competition . although the global average pool ##ing approach achieve ##s impressive results , it has two significant draw ##backs . first , feature transfer is more difficult with this approach . it is very common in practice to take a con ##vo ##lu ##tion ##al network trained on image ##net and re - train the top layer on a different data set , re - using the features learned from image ##net for the new task ( potentially with fine - tuning ) , and this is difficult with global average pool ##ing . this deficiency is noted by , and mo ##tiv ##ates them to add an extra linear layer to the top of their network to enable them to more easily adapt and fine tune their network to other label sets . the second draw ##back of global average pool ##ing is computation . con ##vo ##lu ##tion ##al layers are much more expensive to evaluate than fully connected layers , so replacing fully connected layers with more con ##vo ##lu ##tions can decrease model size but comes at the cost of increased evaluation time . in parallel or after the first ( technical report ) version of this work , several researchers have attempted to create sparse networks by applying pr ##uni ##ng or spa ##rs ##ity regular ##izer ##s . these approaches however require training the original full model and , consequently , do not enjoy the efficient training time benefits of the techniques proposed in this paper . since then , hash ##ing methods have also been advanced to reduce the number of parameters . hash ##es have irregular memory access patterns and , consequently , good performance on large gp ##u - based platforms is yet to be demonstrated . finally , di ##sti ##llation also offers a way of com ##pressing neural networks , as a post - processing step . section : the adaptive fast ##fo ##od transform large dense matrices are the main building block of fully connected neural network layers . in prop ##aga ##ting the signal from the - th layer with activation ##s to the - th layer with activation ##s , we have to compute the storage and computational costs of this matrix multiplication step are both . the storage cost in particular can be prohibit ##ive for many applications . our proposed solution is to rep ##ara ##meter ##ize the matrix of parameters with an adaptive fast ##fo ##od transform , as follows in section [ reference ] , we will provide background and intuition ##s behind this design . for now it su ##ffi ##ces to state that the storage requirements of this rep ##ara ##meter ##ization are and the computational cost is . we will also show in the experimental section that these theoretical savings are mirrored in practice by significant reductions in the number of parameters without increased prediction errors . to understand these claims , we need to describe the component modules of the adaptive fast ##fo ##od transform . for simplicity of presentation , let us first assume that . adaptive fast ##fo ##od has three types of module : and are diagonal matrices of parameters . in the original non - adaptive fast ##fo ##od formulation they are random matrices , as described further in section [ reference ] . the computational and storage costs are trivial ##ly . is a random per ##mut ##ation matrix . it can be implemented as a look ##up table , so the storage and computational costs are also . denotes the walsh - had ##ama ##rd matrix , which is defined rec ##urs ##ively as the fast had ##ama ##rd transform , a variant of fast fourier transform , enables us to compute in time . in summary , the overall storage cost of the adaptive fast ##fo ##od transform is , while the computational cost is . these are substantial theoretical improvements over the costs of ordinary fully connected layers . when the number of output units is larger than the number of inputs , we can perform adaptive fast ##fo ##od transforms and stack them to attain the desired size . in doing so , the computational and storage costs become and respectively , as opposed to the more substantial costs for linear modules . the number of outputs can also be refined with pr ##uni ##ng . sub ##section : learning fast ##fo ##od by back ##pro ##pa ##gation the parameters of the adaptive fast ##fo ##od transform ( and ) can be learned by standard error derivative back ##pro ##pa ##gation . moreover , the backward pass can also be computed efficiently using the fast had ##ama ##rd transform . in particular , let us consider learning the - th layer of the network , . for simplicity , let us again assume that and that . using back ##pro ##pa ##gation , assume we already have , where is the objective function , then since is a diagonal matrix , we only need to calculate the derivative with respect to the diagonal entries and this step requires only operations . proceeding in this way , denote the partial products by then the gradient ##s with respect to different parameters in the fast ##fo ##od layer can be computed rec ##urs ##ively as follows : note that the operations in and are simply applications of the had ##ama ##rd transform , since , and consequently can be computed in time . the operation in is an application of a per ##mut ##ation ( the trans ##pose of per ##mut ##ation matrix is a per ##mut ##ation matrix ) and can be computed in time . all other operations are diagonal matrix multiplication ##s . section : intuition ##s behind adaptive fast ##fo ##od the proposed adaptive fast ##fo ##od transform may be understood either as a train ##able type of structured random projection or as an approximation to the feature space of a learned kernel . both views not only shed light on adaptive fast ##fo ##od and competing techniques , but also open up room to inn ##ova ##te new techniques to reduce computation and memory in neural networks . sub ##section : a view from structured random projections adaptive fast ##fo ##od is based on the fast ##fo ##od transform , in which the diagonal matrices , and have random entries . in the experiments , we will compare the performance of the existing random and proposed adaptive versions of fast ##fo ##od when used to replace fully connected layers in con ##vo ##lu ##tion ##al neural networks . the intriguing idea of constructing neural networks with random weights has been reasonably explored in the neural networks field . this idea is related to random projections , which have been deeply studied in theoretical computer science . in a random projection , the basic operation is of the form where is a random matrix , either ga ##uss ##ian or binary . importantly , the em ##bed ##ding ##s generated by these random projections approximately preserve metric information , as formal ##ized by many variants of the celebrated johnson - linden ##stra ##uss le ##mma . the one short ##coming of random projections is that the cost of storing the matrix is . using a sparse random matrix by itself to reduce this cost is often not a viable option because the variance of the estimates of can be very high for some inputs , for example when is also sparse . to see this , consider the extreme case of a very sparse input , then many of the products with will be zero and hence not help improve the estimates of metric properties of the em ##bed ##ding space . one popular option for reducing the storage and computational costs of random projections is to adopt random hash functions to replace the random matrix multiplication . for example , the count - sketch algorithm uses pair ##wise independent hash functions to carry this job very effectively in many applications . this technique is often referred to as the hash ##ing trick in the machine learning literature . hash ##es have irregular memory access patterns , so it is not clear how to get good performance on gp ##us when following this approach , as pointed out in . ai ##lon and cha ##zell ##e introduced an alternative approach that is not only very efficient , but also preserves most of the desirable theoretical properties of random projections . their idea was to replace the random matrix by a transform that mimic ##s the properties of random matrices , but which can be stored efficiently . in particular , they proposed the following phd transform : where is a sparse random matrix with ga ##uss ##ian entries , is a had ##ama ##rd matrix and is a diagonal matrix with entries drawn independently with probability . the inclusion of the had ##ama ##rd transform avoids the problems of using a sparse random matrix by itself , but it is still efficient to compute . we can think of the original fast ##fo ##od transform as an alternative to this . fast ##fo ##od reduces the computation and storage of random projections to and respectively . in the original formulation and are diagonal random matrices , which are computed once and then stored . in contrast , in our proposed adaptive fast ##fo ##od transform , the diagonal matrices are learned by back ##pro ##pa ##gation . by adapting , we are effectively implementing automatic relevance determination on features . the matrix controls the bandwidth of the kernel and its spectral inc ##oh ##erence . finally , represents different kernel types . for example , for the rb ##f kernel follows chi - squared distribution . by adapting , we learn the correct kernel type . while we have introduced fast ##fo ##od in this section , it was originally proposed as a fast way of computing random features to approximate kernel ##s . we expand on this perspective in the following section . sub ##section : a view from kernel ##s there is a nice dual ##ity between inner products of features and kernel ##s . this dual ##ity can be used to design neural network modules using kernel ##s and vice - versa . for computational reasons , we often want to determine the features associated with a kernel . working with features is prefer ##able when the kernel matrix is dense and large . ( storing this matrix requires space , and computing it takes operations , where is the number of data points and is the dimension . ) we might also want to design statistical methods using kernel ##s and then map these designs to features that can be used as modules in neural networks . unfortunately , one of the difficulties with this line of attack is that der ##iving features from kernel ##s is far from trivial in general . an important fact , noted in , is that infinite kernel expansion ##s can be approximate ##d in an un ##bia ##sed manner using randomly drawn features . for shift - invariant kernel ##s this relies on a classical result from harmonic analysis , known as bo ##chner ' s le ##mma , which states that a continuous shift - invariant kernel on is positive definite if and only if is the fourier transform of a non - negative measure . this measure , known as the spectral density , in turn implies the existence of a probability density such that where the imaginary part is dropped since both the kernel and distribution are real . we can apply monte carlo methods to approximate the above expectation , and hence approximate the kernel with an inner product of stacked co ##sin ##e and sin ##e features . specifically , suppose we sample vectors from and collect them in a matrix . the kernel can then be approximate ##d as the inner - product of the following random features : that is , is the neural network module , consisting of a linear layer and entry - wise nonlinear ##ities ( co ##sin ##e and sin ##e in the above equation ) , that corresponds to a particular implicit kernel function . approx ##imating a given kernel function with random features requires the specification of a sampling distribution . such distributions have been derived for many popular kernel ##s . for example , if we want the implicit kernel to be a squared exponential kernel , we know that the distribution must be ga ##uss ##ian : . in other words , if we draw the rows of from this ga ##uss ##ian distribution and use equation ( [ reference ] ) to implement a neural module , we are implicit ##ly approx ##imating a squared exponential kernel . as another example of the mapping between kernel ##s and random features , introduced the rotational ##ly invariant arc - co ##sin ##e kernel where is the angle between and . then by choosing to be a random ga ##uss ##ian matrix , they showed that this kernel can be approximate ##d with rec ##ti ##fied linear unit ( re ##lu ) features : the fast ##fo ##od transform was introduced to replace in equation [ reference ] with , thus decreasing the computational and storage costs . section : deep fried con ##vo ##lu ##tion ##al networks we propose to greatly reduce the number of parameters of the fully connected layers by replacing them with an adaptive fast ##fo ##od transform followed by a nonlinear ##ity . we call this new architecture a deep fried con ##vo ##lu ##tion ##al network . an illustration of this architecture is shown in figure [ reference ] . in principle , we could also apply the adaptive fast ##fo ##od transform to the soft ##max class ##ifier . however , reducing the memory cost of this layer is already well studied ; for example , show that low - rank matrix factor ##ization can be applied during training to reduce the size of the soft ##max layer substantially . importantly , they also show that training a low rank factor ##ization for the internal layers performs poorly , which agrees with the results of . for this reason , we focus our attention on reducing the size of the internal layers . section : mn ##ist experiment the first problem we study is the classical mn ##ist optical character recognition task . this simple task serves as an easy proof of concept for our method , and contrasting the results in this section with our later experiments gives insights into the behavior of the adaptive fast ##fo ##od transform at different scales . as a reference model we use the caf ##fe implementation of the len ##et con ##vo ##lu ##tion ##al network . it achieve ##s an error rate of on the mn ##ist data ##set . we jointly train all layers of the deep fried network ( including con ##vo ##lu ##tion ##al layers ) from scratch . we compare both the adaptive and non - adaptive fast ##fo ##od transforms using 102 ##4 and 204 ##8 features . for the non - adaptive transforms we report the best performance achieved by varying the standard deviation of the random ga ##uss ##ian matrix over the set , and for the adaptive variant we learn these parameters by back ##pro ##pa ##gation as described in section [ reference ] . the results of the mn ##ist experiment are shown in table [ reference ] . because the width of the deep fried network is substantially larger than the reference model , we also experimented with adding drop ##out in the model , which increased performance in the deep fried case . deep fried networks are able to obtain high accuracy using only a small fraction of of parameters of the original network ( 11 times reduction in the best case ) . interesting ##ly , we see no benefit from adaptation in this experiment , with the more powerful adaptive models performing equivalent ##ly or worse than their non - adaptive counterparts ; however , this should be contrasted with the image ##net results reported in the following sections . section : image ##net experiments we now examine how deep fried networks behave in a more realistic setting with a much larger data ##set and many more classes . specifically , we use the image ##net il ##s ##vr ##c - 2012 data ##set which has 1 . 2 m training examples and 50 k validation examples distributed across 1000 classes . we use the the caf ##fe image ##net model as the reference model in these experiments . this model is a modified version of alex ##net , and achieve ##s top - 1 error on the il ##s ##vr ##c - 2012 validation set . the initial layers of this model are a cascade of con ##vo ##lu ##tion and pool ##ing layers with interspersed normal ##ization . the last several layers of the network take the form of an ml ##p and follow a 92 ##16 - 40 ##9 ##6 - 40 ##9 ##6 - 1000 architecture . the final layer is a log ##istic regression layer with 1000 output classes . all layers of this network use the re ##lu nonlinear ##ity , and drop ##out is used in the fully connected layers to prevent over ##fi ##tting . there are total of 58 , 64 ##9 , 184 parameters in the reference model , of which 58 , 62 ##1 , 95 ##2 are in the fully connected layers and only 27 , 232 are in the con ##vo ##lu ##tion ##al layers . the parameters of fully connected layer take up of the total number of parameters . we show that the adaptive fast ##fo ##od transform can be used to substantially reduce the number of parameters in this model . sub ##section : fixed feature extract ##or previous work on applying kernel methods to image ##net has focused on building models on features extracted from the con ##vo ##lu ##tion ##al layers of a pre - trained network . this setting is less general than training a network from scratch but does mirror the common use case where a con ##vo ##lu ##tion ##al network is first trained on image ##net and used as a feature extract ##or for a different task . in order to compare our adaptive fast ##fo ##od transform directly to this previous work , we extract features from the final con ##vo ##lu ##tion ##al layer of a pre - trained reference model and train an adaptive fast ##fo ##od transform class ##ifier using these features . although the reference model uses two fully connected layers , we investigate replacing these with only a single fast ##fo ##od transform . we experiment with two sizes for this transform : fast ##fo ##od 16 and fast ##fo ##od 32 using 16 , 38 ##4 and 32 , 76 ##8 fast ##fo ##od features respectively . since the fast ##fo ##od transform is a composite module , we can apply drop ##out between any of its layers . in the experiments reported here , we applied drop ##out after the matrix and after the matrix . we also applied drop ##out to the last con ##vo ##lu ##tion ##al layer ( that is , before the matrix ) . we also train an ml ##p with the same structure as the top layers of the reference model for comparison . in this setting it is important to compare against the re - trained ml ##p rather than the jointly trained reference model , as training on features extracted from fixed con ##vo ##lu ##tion ##al layers typically leads to lower performance than joint training . the results of the fixed feature experiment are shown in table [ reference ] . following and we observe that training on image ##net activation ##s produces significantly lower performance than of the original , jointly trained network . nonetheless , deep fried networks are able to out ##per ##form both the re - trained ml ##p model as well as the results in while using fewer parameters . in contrast with our mn ##ist experiment , here we find that the adaptive fast ##fo ##od transform provides a significant performance boost over the non - adaptive version , improving top - 1 performance by 4 . 5 - 6 . 5 % . sub ##section : jointly trained model finally , we train a deep fried network from scratch on image ##net . with 16 , 38 ##4 features in the fast ##fo ##od layer we lose less than 0 . 3 % top - 1 validation performance , but the number of parameters in the network is reduced from 58 . 7 m to 16 . 4 m which corresponds to a factor of 3 . 6 ##x . by further increasing the number of features to 32 , 76 ##8 , we are able to perform 0 . 6 % better than the reference model while using approximately half as many parameters . results from this experiment are shown in table [ reference ] . nearly all of the parameters of the deep fried network reside in the final soft ##max regression layer , which still uses a dense linear transformation , and accounts for more than 99 % of the parameters of the network . this is a side effect of the large number of classes in image ##net . for a data set with fewer classes the advantage of deep fried con ##vo ##lu ##tion ##al networks would be even greater . moreover , as shown by , the last layer often contains considerable red ##unda ##ncy . we also note that any of the techniques from could be applied to the final layer of a deep fried network to further reduce memory consumption at test time . we illustrate this with low - rank matrix factor ##ization in the following section . section : comparison with post processing in this section we provide a comparison to some existing works on reducing the number of parameters in a con ##vo ##lu ##tion ##al neural network . the techniques we compare against here are post - processing techniques , which start from a full trained model and attempt to com ##press it , whereas our method trains the compressed network from scratch . matrix factor ##ization is the most common method for com ##pressing neural networks , and has proven to be very effective . given the weight matrix of fully connected layers , we factor ##ize it as where and and is a diagonal matrix . in order to reduce the parameters , we tr ##un ##cate all but the largest singular values , leading to the approximation where and and has been absorbed into the other two factors . if is sufficiently small then storing and is less expensive than storing directly , and this parameter ##ization is still learn ##able . it has been shown that training a factor ##ized representation directly leads to poor performance ( although it does work when applied only to the final log ##istic regression layer ) . however , first training a full model , then pre ##form ##ing an sv ##d of the weight matrices followed by a fine tuning phase preserves much of the performance of the original model . we compare our deep fried approach to sv ##d followed by fine tuning , and show that our approach achieve ##s better performance per parameter in spite of training a compressed parameter ##ization from scratch . we also compare against a post - processed version of our model , where we train a deep fried con ##vn ##et and then apply sv ##d plus fine - tuning to the final soft ##max layer , which further reduces the number of parameters . results of these post - processing experiments are shown in table [ reference ] . for the sv ##d decomposition of each of the three fully connected layers in the reference model we set in sv ##d - half and in sv ##d - quarter . sv ##d - half - f and sv ##d - quarter - f mean that the model has been fine tuned after the decomposition . there is 1 % drop in accuracy for sv ##d - half and 3 . 5 % drop for sv ##d - quarter . even though the increase in the error for the sv ##d can be mit ##igate ##d by fine ##tu ##ning ( the drop decreases to 0 . 1 % for sv ##d - half - f and 1 . 3 % for sv ##d - quarter - f ) , deep fried con ##vn ##ets still perform better both in terms of the accuracy and the number of parameters . applying a rank 600 sv ##d followed by fine tuning to the final soft ##max layer of the adaptive fast ##fo ##od 32 model removes an additional 12 . 5 m parameters at the expense of 0 . 7 % top - 1 error . for reference , we also include the results of collins and ko ##hli , who pre - train a full network and use a spa ##rs ##ity regular ##izer during fine - tuning to encourage connections in the fully connected layers to be zero . they are able to achieve a significant reduction in the number of parameters this way , however the performance of their compressed network suffers when compared to the reference model . another draw ##back of this method is that using sparse weight matrices requires additional overhead to store the index ##es of the non - zero values . the index storage takes up space and using sparse representation is better than using a dense matrix only when number of non ##zer ##o entries is small . section : conclusion many methods have been advanced to reduce the size of con ##vo ##lu ##tion ##al networks at test time . in contrast to this trend , the adaptive fast ##fo ##od transform introduced in this paper is end - to - end different ##iable and hence it enables us to attain reductions in the number of parameters even at train time . deep fried con ##vn ##ets capital ##ize on the proposed adaptive fast ##fo ##od transform to achieve a substantial reduction in the number of parameters without sac ##ri ##fi ##cing predict ##ive performance on mn ##ist and image ##net . they also compare favorably against simple test - time low - rank matrix factor ##ization schemes . our experiments have also cast some light on the issue of random versus adaptive weights . the structured random transformations developed in the kernel literature perform very well on mn ##ist without any learning ; however , when moving to image ##net , the benefit of adaptation becomes clear , as it allows us to achieve substantially better performance . this is an important point which illustrates the importance of learning which would not have been visible from experiments only on small data sets . the fast ##fo ##od transform allows for a theoretical reduction in computation from to . however , the computation in con ##vo ##lu ##tion ##al neural networks is dominated by the con ##vo ##lu ##tions , and hence deep fried con ##vn ##ets are not necessarily faster in practice . it is clear looking at out results on image ##net in table 2 that the remaining parameters are mostly in the output soft ##max layer . the comparative experiment in section 7 showed that the matrix of parameters in the soft ##max can be easily compressed using the sv ##d , but many other methods could be used to achieve this . one avenue for future research involves replacing the soft ##max matrix , at train and test times , using the abundant set of techniques that have been proposed to solve this problem , including low - rank decomposition , adaptive fast ##fo ##od , and pr ##uni ##ng . the development of gp ##u opt ##imi ##zed fast ##fo ##od transforms that can be used to replace linear layers in arbitrary neural models would also be of great value to the entire research community given the u ##bi ##qui ##ty of fully connected layers layers . bibliography : references",
        "pred_seq": "mn ##ist [SEP] prediction accuracy [SEP] prediction accuracy [SEP] prediction accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy [CLS]",
        "pred_templates": [],
        "gold_templates": [
            {
                "Material": [
                    [
                        "mnist",
                        "imagenet ilsvrc2012 dataset"
                    ]
                ],
                "Method": [
                    [
                        "deep fried convnets",
                        "deep fried convnet",
                        "convnets",
                        "deep fried convolutional networks",
                        "deep fried convolutional network",
                        "deep fried network",
                        "deep fried networks"
                    ]
                ],
                "Metric": [
                    [
                        "prediction errors",
                        "error rate",
                        "top1 error",
                        "error"
                    ]
                ],
                "Task": []
            }
        ]
    },
    "41": {
        "doctext": "strong baseline ##s for neural semi - supervised learning under domain shift section : abstract novel neural models have been proposed in recent years for learning under domain shift . most models , however , only evaluate on a single task , on proprietary data ##set ##s , or compare to weak baseline ##s , which makes comparison of models difficult . in this paper , we re - evaluate classic general - purpose boots ##tra ##pping approaches in the context of neural networks under domain shifts vs . recent neural approaches and propose a novel multi - task tri - training method that reduces the time and space complexity of classic tri - training . extensive experiments on two bench ##marks are negative : while our novel method establishes a new state - of - the - art for sentiment analysis , it does not fare consistently the best . more importantly , we arrive at the somewhat surprising conclusion that classic tri - training , with some additions , out ##per ##forms the state of the art . we conclude that classic approaches constitute an important and strong baseline . section : introduction deep neural networks ( d ##nn ##s ) excel at learning from labeled data and have achieved state of the art in a wide array of supervised nl ##p tasks such as dependency par ##sing [ reference ] , named entity recognition [ reference ] , and semantic role labeling [ reference ] . in contrast , learning from un ##lab ##ele ##d data , especially under domain shift , remains a challenge . this is common in many real - world applications where the distribution of the training and test data differs . many state - of - the - art domain adaptation approaches leverage task - specific characteristics such as sentiment words [ reference ] [ reference ] or distribution ##al features [ reference ] [ reference ] which do not general ##ize to other tasks . other approaches that are in theory more general only evaluate on proprietary data ##set ##s [ reference ] or on a single bench ##mark [ reference ] , which carries the risk of over ##fi ##tting to the task . in addition , most models only compare against weak baseline ##s and , striking ##ly , almost none considers evaluating against approaches from the extensive semi - supervised learning ( ss ##l ) literature [ reference ] . in this work , we make the argument that such algorithms make strong baseline ##s for any task in line with recent efforts highlighting the useful ##ness of classic approaches [ reference ] [ reference ] . we re - evaluate boots ##tra ##pping algorithms in the context of d ##nn ##s . these are general - purpose semi - supervised algorithms that treat the model as a black box and can thus be used easily - with a few additions - with the current generation of nl ##p models . many of these methods , though , were originally developed with in - domain performance in mind , so their effectiveness in a domain adaptation setting remains une ##x ##pl ##ored . in particular , we re - evaluate three traditional boots ##tra ##pping methods , self - training [ reference ] , tri - training [ reference ] , and tri ##train ##ing with disagreement [ reference ] for neural network - based approaches on two nl ##p tasks with different characteristics , namely , a sequence prediction and a classification task ( po ##s tag ##ging and sentiment analysis ) . we evaluate the methods across multiple domains on two welles ##tablished bench ##marks , without taking any further task - specific measures , and compare to the best results published in the literature . we make the somewhat surprising observation that classic tri - training out ##per ##forms task - ag ##nostic state - of - the - art semi - supervised learning [ reference ] and recent neural adaptation approaches [ reference ] [ reference ] . in addition , we propose multi - task tri - training , which reduces the main deficiency of tri - training , namely its time and space complexity . it establishes a new state of the art on un ##su ##per ##vis ##ed domain adaptation for sentiment analysis but it is out ##per ##formed by classic tri - training for po ##s tag ##ging . contributions our contributions are : a ) we propose a novel multi - task tri - training method . b ) we show that tri - training can serve as a strong and robust semi - supervised learning baseline for the current generation of nl ##p models . c ) we perform an extensive evaluation of boots ##tra ##pping 1 algorithms compared to state - of - the - art approaches on two bench ##mark data ##set ##s . d ) we shed light on the task and data characteristics that yield the best performance for each model . section : neural boots ##tra ##pping methods we first introduce three classic boots ##tra ##pping methods , self - training , tri - training , and tri - training with disagreement and detail how they can be used with neural networks . for in - depth details we refer the reader to [ reference ] [ reference ] [ reference ] ) . we introduce our novel multi ##tas ##k tri - training method in \u00a7 2 . 3 . section : self - training self - training [ reference ] [ reference ] ) is one of the earliest and simplest boots ##tra ##pping approaches . in essence , it leverage ##s the model ' s own predictions on un ##lab ##ele ##d data to obtain additional information that can be used during training . typically the most confident predictions are taken at face value , as detailed next . self - training trains a model m on a labeled training set l and an un ##lab ##ele ##d data set u . at each iteration , the model provides predictions m ( x ) in the form of a probability distribution over classes for all un ##lab ##ele ##d examples x in u . if the probability assigned to the most likely class is higher than a pre ##de ##ter ##mined threshold ##\u03c4 , x is added to the labeled examples with p ( x ) = ar ##g max m ( x ) as pseudo - label . this instant ##iation is the most widely used and shown in algorithm 1 . cal ##ib ##ration it is well - known that output pro ##ba ##bilities in neural networks are poorly cal ##ib ##rated [ reference ] . using a fixed threshold ##\u03c4 is thus algorithm 1 self - training [ reference ] l ##\u2190 l ##\u222a { ( x , p ( x ) ) } 6 : until no more predictions are confident not the best choice . while the absolute confidence value is inaccurate , we can expect that the relative order of confidence ##s is more robust . for this reason , we select the top n un ##lab ##ele ##d examples that have been predicted with the highest confidence after every epoch and add them to the labeled data . this is one of the many variants for self - training , called th ##rot ##tling [ reference ] . we empirical ##ly confirm that this out ##per ##forms the classic selection in our experiments . online learning in contrast to many classic algorithms , d ##nn ##s are trained online by default . we compare training setup ##s and find that training until convergence on labeled data and then training until convergence using self - training performs best . classic self - training has shown mixed success . in par ##sing it proved successful only with small data ##set ##s [ reference ] or when a genera ##tive component is used together with a re ##rank ##er in high - data conditions [ reference ] [ reference ] . some success was achieved with careful task - specific data selection [ reference ] , while others report limited success on a variety of nl ##p tasks [ reference ] [ reference ] [ reference ] . its main downs ##ide is that the model is not able to correct its own mistakes and errors are amplified , an effect that is increased under domain shift . section : tri - training tri - training [ reference ] ) is a classic method that reduces the bias of predictions on un ##lab ##ele ##d data by utilizing the agreement of three independently trained models . tri - training ( cf . algorithm 2 ) first trains three models m 1 , m 2 , and m 3 on boots ##tra ##p samples of the labeled data l . an un ##lab ##ele ##d data point is added to the training set of a model m i if the other two models m j and m k agree on its label . training stops when the class ##ifiers do not change anymore . tri - training with disagreement [ reference ] algorithm 2 tri - training [ reference ] 1 : for i ##\u2208 { 1 . . 3 } do 2 : for i ##\u2208 { 1 . . 3 } do 6 : for x ##\u2208 u do 8 : 10 : until none of m i changes 11 : apply majority vote over m i is based on the intuition that a model should only be strengthened in its weak points and that the labeled data should not be sk ##ew ##ed by easy data points . in order to achieve this , it adds a simple modification to the original algorithm ( altering line 8 in algorithm 2 ) , requiring that for an un ##lab ##ele ##d data point on which m j and m k agree , the other model m i disagree ##s on the prediction . tri - training with disagreement is more data - efficient than tri ##train ##ing and has achieved competitive results on part - of - speech tag ##ging [ reference ] . sampling un ##lab ##ele ##d data both tri - training and tri - training with disagreement can be very expensive in their original formulation as they require to produce predictions for each of the three models on all un ##lab ##ele ##d data samples , which can be in the millions in realistic applications . we thus propose to sample a number of un ##lab ##ele ##d examples at every epoch . for all traditional boots ##tra ##pping approaches we sample 10 ##k candidate instances in each epoch . for the neural approaches we use a linear ##ly growing candidate sampling scheme proposed by [ reference ] , increasing the candidate pool size as the models become more accurate . confidence threshold ##ing similar to self ##train ##ing , we can introduce an additional requirement that pseudo - labeled examples are only added if the probability of the prediction of at least one model is higher than some threshold ##\u03c4 . we did not find this to out ##per ##form prediction without threshold for traditional tri - training , but threshold ##ing proved essential for our method ( \u00a7 2 . 3 ) . the most important condition for tri - training and tri - training with disagreement is that the models are diverse . typically , boots ##tra ##p samples are used to create this diversity [ reference ] [ reference ] . however , training separate models on boots ##tra ##p samples of a potentially large amount of training data is expensive and takes a lot of time . this draw ##back mo ##tiv ##ates our approach . section : multi - task tri - training in order to reduce both the time and space complexity of tri - training , we propose multi - task tri ##train ##ing ( mt - tri ) . mt - tri leverage ##s insights from multi - task learning ( mt ##l ) [ reference ] to share knowledge across models and accelerate training . rather than storing and training each model separately , we propose to share the parameters of the models and train them jointly using mt ##l . 2 all models thus collaborate on learning a joint representation , which improves convergence . the output soft ##max layers are model - specific and are only updated for the input of the respective model . we show the model in figure 1 ( as instant ##iated for po ##s tag ##ging ) . as the models leverage a joint representation , we need to ensure that the features used for prediction in the soft ##max layers of the different models are as diverse as possible , so that the models can still learn from each other ' s predictions . in contrast , if the parameters in all output soft ##max layers were the same , the method would de ##gen ##erate to self - training . to guarantee diversity , we introduce an orthogonal ##ity constraint [ reference ] as an additional loss term , which we define as follows : where | \u00b7 2 f is the squared fr ##obe ##nius norm and w m 1 and w m 2 are the soft ##max output parameters of the two source and pseudo - labeled output layers m 1 and m 2 , respectively . the orthogonal ##ity constraint encourages the models not to rely on the same features for prediction . as enforcing pair ##wise orthogonal ##ity between three matrices is not possible , we only enforce orthogonal ##ity between the soft ##max output layers of m 1 and m 2 , 3 while m 3 is gradually trained to be more target - specific . we parameter ##ize l or ##th by ##\u03b3 = 0 . 01 following . we do not further tune ##\u03b3 . more formally , let us illustrate the model by taking the sequence prediction task ( figure 1 ) as illustration . given an utter ##ance with labels y 1 , . . , y n , our multi - task tri - training loss consists of three task - specific ( m 1 , m 2 , m 3 ) tag ##ging loss functions ( where h is the upper ##most bi - l ##st ##m encoding ) : in contrast to classic tri - training , we can train the multi - task model with its three model - specific outputs jointly and without boots ##tra ##p sampling on the labeled source domain data until convergence , as the orthogonal ##ity constraint enforce ##s different representations between models m 1 and m 2 . from this point , we can leverage the pair - wise agreement of two output layers to add pseudo - labeled examples as training data to the third model . we train the third output layer m 3 only on pseudo - labeled target instances in order to make tri - training more robust to a domain shift . for the final prediction , majority voting of all three output layers is used , which resulted in the best instant ##iation , together with confidence threshold ##ing ( \u03c4 = 0 . 9 , except for high ##res ##our ##ce po ##s where ##\u03c4 = 0 . 8 performed slightly better ) . we also experimented with using a domain ##ad ##vers ##aria ##l loss [ reference ] on the jointly learned representation , but found this not to help . the full pseudo - code is given in algorithm 3 . computational complexity the motivation for mt - tri was to reduce the space and time complexity of tri - training . we thus give an estimate of its efficiency gains . mt - tri is ~ 3 ##\u00d7 more space ##ef ##fi ##cie ##nt than regular tri - training ; tri - training stores one set of parameters for each of the three models , while mt - tri only stores one set of parameters ( we use three output layers , but these make up a comparatively small part of the total parameter budget ) . in terms of time efficiency , tri - training first [ reference ] we also tried enforcing orthogonal ##ity on a hidden layer rather than the output layer , but this did not help . 10 : until end condition is met 11 : apply majority vote over m i requires to train each of the models from scratch . the actual tri - training takes about the same time as training from scratch and requires a separate forward pass for each model , effectively training three independent models simultaneously . in contrast , mt - tri only nec ##ess ##itate ##s one forward pass as well as the evaluation of the two additional output layers ( which takes a ne ##gli ##gible amount of time ) and requires about as many epoch ##s as tri - training until convergence ( see table 3 , second column ) while adding fewer un ##lab ##ele ##d examples per epoch ( see section 3 . 4 ) . in our experiments , mt - tri trained about 5 - 6 ##\u00d7 faster than traditional tri - training . mt - tri can be seen as a self - en ##se ##mbling technique , where different variations of a model are used to create a stronger ensemble prediction . recent approaches in this line are snaps ##hot en ##se ##mbling ) that ensembles models converge ##d to different mini ##ma during a training run , as ##ym ##metric tri - training [ reference ] ) ( as ##ym ) that leverage ##s agreement on two models as information for the third , and temporal en ##se ##mbling [ reference ] , which ensembles predictions of a model at different epoch ##s . we tried to compare to temporal en ##se ##mbling in our experiments , but were not able to obtain consistent results . [ reference ] we compare to the closest most recent method , as ##ym ##metric tri ##train ##ing [ reference ] . it differs from ours in two aspects : a ) as ##ym leverage ##s only pseudo ##lab ##els from data points on which m 1 and m 2 agree , and b ) it uses only one task ( m 3 ) as final predict ##or . in essence , our formulation of mt - tri is closer to the original tri - training formulation ( agreements on two provide pseudo - labels to the third ) thereby incorporating more diversity . [ reference ] for po ##s tag ##ging ( above ) and the amazon reviews data ##set [ reference ] for sentiment analysis ( below ) . section : experiments in order to as ##cer ##tain which methods are robust across different domains , we evaluate on two widely used un ##su ##per ##vis ##ed domain adaptation data ##set ##s for two tasks , a sequence labeling and a classification task , cf . table 1 for data statistics . section : po ##s tag ##ging for po ##s tag ##ging we use the san ##cl 2012 shared task data ##set [ reference ] and compare to the top results in both low and high - data conditions [ reference ] [ reference ] . both are strong baseline ##s , as the fl ##ors tag ##ger has been developed for this challenging data ##set and it is based on context ##ual distribution ##al features ( excluding the word ' s identity ) , and hand - crafted suffix and shape features ( including some languages ##pe ##ci ##fi ##c morphological features ) . we want to gauge to what extent we can adopt a nowadays fairly standard ( but more lexi ##cal ##ized ) general neural tag ##ger . our po ##s tag ##ging model is a state - of - the - art bi - l ##st ##m tag ##ger [ reference ] with word and 100 - dim character em ##bed ##ding ##s . word em ##bed ##ding ##s are initial ##ized with the 100 - dim glove em ##bed ##ding ##s [ reference ] . the bi ##ls ##tm has one hidden layer with 100 dimensions . the base po ##s model is trained on w ##s ##j with early stopping on the w ##s ##j development set , using patience 2 , ga ##uss ##ian noise with ##\u03c3 = 0 . 2 and word drop ##out with p = 0 . 25 [ reference ] . regarding data , the source domain is the onto ##notes 4 . 0 release of the penn tree ##bank wall street journal ( w ##s ##j ) ann ##ota ##ted for 48 fine - grain ##ed po ##s tags . this amounts to 30 , 06 ##0 labeled sentences . we use 100 , 000 w ##s ##j sentences from 1988 as un ##lab ##ele ##d data , following [ reference ] . [ reference ] as target data , we use the five san ##cl domains ( answers , emails , news ##group ##s , reviews , web ##log ##s ) . we restrict the amount of un ##lab ##ele ##d data for each san ##cl domain to the first 100 ##k sentences , and do not do any pre - processing . we consider the development set of answers as our only target dev set to set hyper ##para ##meter ##s . this may result in sub ##op ##ti ##mal per - domain settings but better resembles an un ##su ##per ##vis ##ed adaptation scenario . section : sentiment analysis for sentiment analysis , we evaluate on the amazon reviews data ##set [ reference ] . reviews with 1 to 3 stars are ranked as negative , while reviews with 4 or 5 stars are ranked as positive . the data ##set consists of four domains , yielding 12 adaptation scenarios . we use the same pre - processing and architecture as used in [ reference ] [ reference ] : 5 , 000 - dimensional t ##f - idf weighted un ##ig ##ram and big ##ram features as input ; 2 ##k labeled source samples and 2 ##k un ##lab ##ele ##d target samples for training , 200 labeled target samples for validation , and between 3 ##k - 6 ##k samples for testing . the model is an ml ##p with one hidden layer with 50 dimensions , si ##gm ##oid activation ##s , and a soft ##max output . we compare against the variation ##al fair auto ##en ##code ##r ( v ##fa ##e ) [ reference ] model and domain - ad ##vers ##aria ##l neural networks ( dan ##n ) [ reference ] . section : baseline ##s besides comparing to the top results published on both data ##set ##s , we include the following baseline ##s : a ) the task model trained on the source domain ; b ) self - training ( self ) ; c ) tri - training ( tri ) ; d ) tri - training with disagreement ( tri - d ) ; and e ) as ##ym ##metric tri - training [ reference ] ) . our proposed model is multi - task tri - training ( mt ##tri ) . we implement our models in d ##yne ##t . reporting single evaluation scores might result in biased results [ reference ] . throughout the paper , we report mean accuracy and standard deviation over five runs for po ##s tag ##ging and over ten runs for sentiment analysis . significance is computed using boots ##tra ##p test . the code for all experiments is released at : https : / / gi ##th ##ub . com / bp ##lan ##k / semi - supervised - baseline ##s . section : results section : sentiment analysis we show results for sentiment analysis for all 12 domain adaptation scenarios in figure 2 . for clarity , we also show the accuracy scores averaged across each target domain as well as a global macro average in table 2 self - training achieve ##s surprisingly good results but is not able to compete with tri - training . tri ##train ##ing with disagreement is only slightly better than self - training , showing that the disagreement component might not be useful when there is a strong domain shift . tri - training achieve ##s the best average results on two target domains and clearly out ##per ##forms the state of the art on average . mt - tri finally out ##per ##forms the state of the art on 3 / 4 domains , and even slightly traditional tri ##train ##ing , resulting in the overall best method . this improvement is mainly due to the b - > e and d - > e scenarios , on which tri - training struggles . these domain pairs are among those with the highest adi ##stan ##ce [ reference ] , which highlights that tri - training has difficulty dealing with a strong shift in domain . our method is able to mit ##igate this deficiency by training one of the three output layers only on pseudo - labeled target domain examples . in addition , mt - tri is more efficient as it adds a smaller number of pseudo - labeled examples than tri - training at every epoch . for sentiment analysis , tri - training adds around 1800 - 1950 / 2000 un ##lab ##ele ##d examples at every epoch , while mt - tri only adds around 100 - 300 in early epoch ##s . this shows that the orthogonal ##ity constraint is useful for inducing diversity . in addition , adding fewer examples poses a smaller risk of swamp ##ing the learned representations with useless signals and is more akin to fine - tuning , the standard method for supervised domain adaptation [ reference ] . we observe an as ##ym ##metry in the results between some of the domain pairs , e . g . b - > d and d - > b . we h ##yp ##oth ##es ##ize that the as ##ym ##metry may be due to properties of the data and that the domains are relatively far apart e . g . , in terms of a - distance . in fact , as ##ym ##metry in these domains is already reflected table 4 : accuracy for po ##s tag ##ging on the dev and test sets of the san ##cl domains , models trained on full source data setup . values for methods with * are from [ reference ] . in the results of [ reference ] and is co ##rro ##bor ##ated in the results for as ##ym ##metric tri - training [ reference ] and our method . we note a weakness of this data ##set is high variance . existing approaches only report the mean , which makes an objective comparison difficult . for this reason , we believe it is essential to evaluate proposed approaches also on other tasks . po ##s tag ##ging results for tag ##ging in the low - data regime ( 10 % of w ##s ##j ) are given in table 3 . self - training does not work for the sequence prediction task . we report only the best instant ##iation ( th ##rot ##tling with n = 800 ) . our results contribute to negative findings regarding self - training [ reference ] [ reference ] . in the low - data setup , tri - training with disagreement works best , reaching an overall average accuracy of 89 . 70 , closely followed by classic tri ##train ##ing , and significantly out ##per ##form ##ing the baseline on 4 / 5 domains . the exception is news ##group ##s , a difficult domain with high o ##ov rate where none of the app ##ro ##ches beats the baseline ( see \u00a7 3 . 4 ) . our proposed mt - tri is better than as ##ym ##metric tri ##train ##ing , but falls below classic tri - training . it beats table 5 : accuracy scores on dev sets for o ##ov and unknown word - tag ( u ##wt ) token ##s . the baseline significantly on only 2 / 5 domains ( answers and emails ) . the fl ##ors tag ##ger [ reference ] fares better . its context ##ual distribution ##al features are particularly helpful on unknown word - tag combinations ( see \u00a7 3 . 4 ) , which is a limitation of the lexi ##cal ##ized generic bi - l ##st ##m tag ##ger . for the high - data setup ( table 4 ) results are similar . disagreement , however , is only favorable in the low - data setup ##s ; the effect of avoiding easy points no longer holds in the full data setup . classic tri ##train ##ing is the best method . in particular , traditional tri - training is complementary to word em ##bed ##ding initial ##ization , pushing the non - pre - trained baseline to the level of sr ##c with glove in ##ital ##ization . tri ##train ##ing pushes performance even further and results in the best model , significantly out ##per ##form ##ing the baseline again in 4 / 5 cases , and reaching fl ##ors performance on web ##log ##s . multi - task tri ##train ##ing is often slightly more effective than as ##ym ##metric tri - training [ reference ] ; however , improvements for both are not robust across domains , sometimes performance even drops . the model likely is too sim ##pl ##istic for such a high - data po ##s setup , and exploring shared - private models might prove more fruit ##ful . on the test sets , tri - training performs consistently the best . section : po ##s analysis we analyze po ##s tag ##ging accuracy with respect to word frequency 6 and unseen word - tag combinations ( u ##wt ) on the dev sets . known tags , o ##ov ##s and unknown word - tag ( u ##wt ) rate . the san ##cl data ##set is overall very challenging : o ##ov rates are high ( 6 . 8 - 11 % compared to 2 . 3 % in w ##s ##j ) , so is the unknown word - tag ( u ##wt ) rate ( answers and emails contain 2 . 91 % and 3 . 47 % u ##wt compared to 0 . 61 % on w ##s ##j ) and almost all target domains even contain unknown tags [ reference ] ) ( unknown tags : add , g ##w , n ##fp , xx ) , except for web ##log ##s . email is the domain with the highest o ##ov rate and highest unknown - tag - for - known - words rate . we plot accuracy with respect to word frequency on email in figure 3 , analyzing how the three methods fare in comparison to the baseline on this difficult domain . regarding o ##ov ##s , the results in table 5 ( second part ) show that classic tri - training out ##per ##forms the source model ( trained on only source data ) on 3 / 5 domains in terms of o ##ov accuracy , except on two domains with high o ##ov rate ( news ##group ##s and web ##log ##s ) . in general , we note that tri - training works best on o ##ov ##s and on low - frequency token ##s , which is also shown in figure 3 ( left ##most bin ##s ) . both other methods fall typically below the baseline in terms of o ##ov accuracy , but mt - tri still out ##per ##forms as ##ym in 4 / 5 cases . table 5 ( last part ) also shows that no boots ##tra ##pping method works well on unknown word - tag combinations . u ##wt token ##s are very difficult to predict correctly using an un ##su ##per ##vis ##ed approach ; the less lexi ##cal ##ized and more context - driven approach taken by fl ##ors is clearly superior for these cases , resulting in higher u ##wt acc ##ura ##cies for 4 / 5 domains . section : related work learning under domain shift there is a large body of work on domain adaptation . studies on un ##su ##per ##vis ##ed domain adaptation include early work on boots ##tra ##pping [ reference ] [ reference ] , shared feature representations [ reference ] [ reference ] and instance weight ##ing [ reference ] . recent approaches include ad ##vers ##aria ##l learning [ reference ] and fine - tuning [ reference ] . there is almost no work on boots ##tra ##pping approaches for recent neural nl ##p , in particular under domain shift . tri - training is less studied , and only recently re - emerged in the vision community [ reference ] , albeit is not compared to classic tri - training . neural network en ##se ##mbling related work on self - en ##se ##mbling approaches includes snaps ##hot en ##se ##mbling or temporal en ##se ##mbling [ reference ] . in general , the line between \" explicit \" and \" implicit \" en ##se ##mbling , like drop ##out [ reference ] or temporal en ##se ##mbling [ reference ] , is more fuzzy . as we noted earlier our multi - task learning setup can be seen as a form of self - en ##se ##mbling . multi - task learning in nl ##p neural networks are particularly well - suited for mt ##l allowing for parameter sharing [ reference ] . recent nl ##p conferences witnessed a \" tsunami \" of deep learning papers [ reference ] , followed by what we call a multi - task learning \" wave \" : mt ##l has been successfully applied to a wide range of nl ##p tasks [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . related to it is the pioneering work on ad ##vers ##aria ##l learning ( dan ##n ) [ reference ] . for sentiment analysis we found tri - training and our mt - tri model to out ##per ##form dan ##n . our mt - tri model lend ##s itself well to shared - private models such as those proposed recently [ reference ] , which extend upon [ reference ] by having separate source and target - specific en ##code ##rs . section : conclusions we re - evaluate a range of traditional general ##pur ##pose boots ##tra ##pping algorithms in the context of neural network approaches to semi - supervised learning under domain shift . for the two examined nl ##p tasks classic tri - training works the best and even out ##per ##forms a recent state - of - the - art method . the draw ##back of tri - training it its time and space complexity . we therefore propose a more efficient multi - task tri - training model , which out ##per ##forms both traditional tri - training and recent alternatives in the case of sentiment analysis . for po ##s tag ##ging , classic tri - training is superior , performing especially well on o ##ov ##s and low frequency token ##s , which suggests it is less affected by error propagation . overall we emphasize the importance of comparing neural approaches to strong baseline ##s and reporting results across several runs . section : section : ac ##k ##now ##led ##gm ##ents we thank the anonymous reviewers for their valuable feedback . sebastian is supported by irish research council grant number e ##b ##pp ##g / 2014 / 30 and science foundation ireland grant number sf ##i / 12 / rc / 228 ##9 . barbara is supported by n ##vid ##ia corporation and thanks the computing center of the university of groningen for hp ##c support . section :",
        "pred_seq": "[SEP] [SEP] [SEP] sentiment analysis [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "sentiment analysis"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "sancl dataset"
                    ]
                ],
                "Method": [
                    [
                        "multitask tritraining method",
                        "multitask model",
                        "multitask learning"
                    ]
                ],
                "Metric": [
                    [
                        "global macro average",
                        "overall average accuracy"
                    ]
                ],
                "Task": [
                    [
                        "sentiment analysis"
                    ]
                ]
            }
        ]
    },
    "42": {
        "doctext": "document : pc ##l : proposal cluster learning for weakly supervised object detection weakly supervised object detection ( w ##so ##d ) , using only image - level ann ##ota ##tions to train object detectors , is of growing importance in object recognition . in this paper , we propose a novel deep network for w ##so ##d . unlike previous networks that transfer the object detection problem to an image classification problem using multiple instance learning ( mil ) , our strategy generates proposal clusters to learn refined instance class ##ifiers by an it ##erative process . the proposals in the same cluster are spatial ##ly adjacent and associated with the same object . this prevents the network from concentrating too much on parts of objects instead of whole objects . we first show that instances can be assigned object or background labels directly based on proposal clusters for instance class ##ifier ref ##ine ##ment , and then show that treating each cluster as a small new bag yields fewer am ##bi ##gui ##ties than the directly assign ##ing label method . the it ##erative instance class ##ifier ref ##ine ##ment is implemented online using multiple streams in con ##vo ##lu ##tion ##al neural networks , where the first is an mil network and the others are for instance class ##ifier ref ##ine ##ment supervised by the preceding one . experiments are conducted on the pascal vo ##c , image ##net detection , and ms - coco bench ##marks for w ##so ##d . results show that our method out ##per ##forms the previous state of the art significantly . object detection , weakly supervised learning , con ##vo ##lu ##tion ##al neural network , multiple instance learning , proposal cluster . section : introduction object detection is one of the most important problems in computer vision with many applications . recently , due to the development of con ##vo ##lu ##tion ##al neural network ( cnn ) and the availability of large scale data ##set ##s with detailed bound ##ing ##box - level ann ##ota ##tions , there have been great leap forwards in object detection . however , it is very labor - intensive and time - consuming to collect detailed ann ##ota ##tions , whereas acquiring images with only image - level ann ##ota ##tions ( i . e . , image tags ) indicating whether an object class exists in an image or not is much easier . for example , we can use image search que ##ries to search on the internet ( e . g . , google and flick ##r ) to obtain a mass of images with such image - level ann ##ota ##tions . this fact inspire ##s us to explore methods for the weakly supervised object detection ( w ##so ##d ) problem , i . e . , training object detectors with only image tag supervision ##s . many previous methods follow the multiple instance learning ( mil ) pipeline for w ##so ##d . they treat images as bags and proposals as instances ; then instance class ##ifiers ( object detectors ) are trained under mil constraints ( i . e . , a positive bag contains at least one positive instance and all instances in negative bags are negative ) . in addition , inspired by the great success of cnn , recent efforts often combine mil and cnn to obtain better w ##so ##d performance . some researches have shown that treating cnn ##s pre - trained on large scale data ##set ##s as off - the - shelf proposal feature extract ##ors can obtain much better performance than traditional hand - designed features . moreover , many recent works have achieved even better results for w ##so ##d by an mil network using standard end - to - end training or a variant of end - to - end training . see section [ reference ] for this variant of end - to - end and how it differs from the standard one . we use the same strategy of training a variant of end - to - end mil network inspired by . although some promising results have been obtained by mil networks for w ##so ##d , they do not perform as well as fully supervised ones . as shown in fig . [ reference ] ( a ) , previous mil networks integrate the mil constraints into the network training by transferring the instance classification ( object detection ) problem to a bag classification ( image classification ) problem , where the final image scores are the aggregation of the proposal scores . however , there is a big gap between image classification and object detection . for classification , even parts of objects can contribute to correct results ( e . g . , the red boxes in fig . [ reference ] ) , because important parts include many characteristics of the objects . many proposals only cover parts of objects , and \" seeing \" proposals only of parts may be enough to roughly local ##ize the objects . but this may not local ##ize objects well enough considering the performance requirement of high intersection - over - union ( io ##u ) between the resulting boxes and ground ##tr ##uth bound ##ing ##box ##es : the top ranking proposals may only local ##ize parts of objects instead of whole objects . recall that for detection , the resulting boxes should not only give correct classification , but also local ##ize objects and have enough overlap with ground ##tr ##uth bound ##ing ##box ##es ( e . g . , the green boxes in fig . [ reference ] ) . before presenting our solution of the problem referred above , we first introduce the concept of proposal cluster . object detection requires algorithms to generate multiple overlapping proposals closely surrounding objects to ensure high proposal recall ( e . g . , for each object , there are tens of proposals on average from selective search which have io ##u 0 . 5 with the ground ##tr ##uth bound ##ing ##box on the pascal vo ##c data ##set ) . object proposals in an image can be grouped into different spatial clusters . except for one cluster for background proposals , each object cluster is associated with a single object and proposals in each cluster are spatial ##ly adjacent , as shown in fig . [ reference ] . for fully supervised object detection ( i . e . , training object detectors using bound ##ing ##box - level ann ##ota ##tions ) , proposal clusters can be generated by treating the ground ##tr ##uth bound ##ing ##box ##es as cluster centers . then object detectors are trained according to the proposal clusters ( e . g . , assign ##ing all proposals the label of the corresponding object class for each cluster ) . this alleviate ##s the problem that detectors may only focus on parts . but in the weakly supervised scenario , it is difficult to generate proposal clusters because ground ##tr ##uth bound ##ing ##box ##es that can be used as cluster centers are not provided . to cope with this difficulty , we suggest to find proposal clusters as follows . first we generate proposal cluster centers from those proposals which have high classification scores during training , because these top ranking proposals can always detect at least parts of objects . that is , for each image , after obtaining proposal scores , we select some proposals with high scores as cluster centers , and then proposal clusters are generated based on spatial overlap ##s with the cluster centers . then the problem reduces to how to select proposals as centers , because many high scoring proposals may correspond to the same object . the most straightforward way is to choose the proposal with the highest score for each positive object class ( i . e . , the object class exists in the image ) as the center . but such a method ignores the fact that there may exist more than one object with the same object category in natural images ( e . g . , the two motor ##bi ##kes in fig . [ reference ] ) . therefore , we propose a graph - based method to find cluster centers . more specifically , we build a graph of top ranking proposals according to the spatial similarity for each positive object class . in the graph , two proposals are connected if they have enough spatial overlap ##s . then we greed ##ily and it ##erative ##ly choose the proposals which have most connections with others to estimate the centers . although a cluster center proposal may only capture an object partially , its adjacent proposals ( i . e . , other proposals in the cluster ) can cover the whole object , or at worst contain larger parts of the object . based on these proposal clusters , we propose two methods to ref ##ine instance class ##ifiers ( object detectors ) during training . we first propose to assign proposals object labels directly . that is , for each cluster , we assign its proposals the label of its corresponding object class , as in fig . [ reference ] ( b ) . compared with the conventional mil network in fig . [ reference ] ( a ) , this strategy forces network to \" see \" larger parts of objects by assign ##ing object labels to proposals that cover larger parts of objects directly , which fills the gap between classification and detection to some extent . while effective , this strategy still has potential am ##bi ##gui ##ties , because assign ##ing the same object label to proposals that cover different parts of objects simultaneously may confuse the network and will hurt the disc ##rim ##ina ##tive power of the detector . to address this problem , we propose to treat each proposal cluster as a small new bag to train refined instance class ##ifiers , as in fig . [ reference ] ( c ) . most of the proposals in these new bags should have relatively high classification scores because the cluster centers covers at least parts of objects and proposals in the same cluster are spatial ##ly adjacent ( except for the background cluster ) . in the same time , not all proposals in the bags should have high classification scores . thus compared with the directly assign ##ing label strategy , this strategy is more flexible and can reduce the am ##bi ##gui ##ties to some extent . we name our method proposal cluster learning ( pc ##l ) because it learns refined instance class ##ifiers based on proposal clusters . to implement our idea effectively and efficiently , we further propose an online training approach . our network has multiple output streams as in fig . [ reference ] . the first stream is a basic mil network which aggregate ##s proposal scores into final image scores to train basic instance class ##ifiers , and the other streams ref ##ine the instance class ##ifiers it ##erative ##ly . during the forward process of training , proposal classification scores are obtained and proposal clusters are generated consequently for each stream . then based on these proposal clusters , supervision ##s are generated to compute losses for the next stream . according to the losses , these refined class ##ifiers are trained during back - propagation . except for the first stream that is supervised by image labels , the other streams are supervised by the image labels as well as outputs from their preceding streams . as our method forces the network to \" see \" larger parts of objects , the detector can discover the whole object instead of parts gradually by performing ref ##ine ##ment multiple times ( i . e . , multiple output streams ) . but at the start of training , all class ##ifiers are almost un ##train ##ed , which will result in very noisy proposal clusters , and so the training will devi ##ate from the correct solutions a lot . thus we design a weighted loss further by ass ##oc ##iating different proposals with different weights in different training iteration ##s . after that , all training procedures can thus be integrated into a single end - to - end network . this can improve the performance benefit ##ing from our pc ##l - based class ##ifier ref ##ine ##ment procedure . it is also very computational efficient in both training and testing . in addition , performance can be improved by sharing proposal features among different output streams . we elaborate ##ly conduct many experiments on the challenging pascal vo ##c , image ##net detection , and ms - coco data ##set ##s to confirm the effectiveness of our method . our method achieve ##s map and co ##rl ##oc on vo ##c 2007 which is more than absolute improvement compared with previous best performed methods . this paper is an extended version of our previous work . in particular , we give more analyses of our method and en ##rich literature ##s of most recent related works , making the manuscript more complete . in addition , we make two method ##ological improvements : the first one is to generate proposal clusters using graphs of top ranking proposals instead of using the highest scoring proposal , and the second one is to treat each proposal cluster as a small new bag . in addition , we provide more discussions of experimental results , and show the effectiveness of our method on the challenging image ##net detection and ms - coco data ##set ##s . the rest of our paper is organized as follows . in section [ reference ] , some related works are introduced . in section [ reference ] , the details of our method are described . elaborate experiments and analyses are conducted in section [ reference ] . finally , conclusions and future directions are presented in section [ reference ] . section : related work sub ##section : multiple instance learning mil , first proposed for drug activity prediction , is a classical weakly supervised learning problem . many variants have been proposed for mil . in mil , a set of bags are given , and each bag is associated with a collection of instances . it is natural to treat w ##so ##d as an mil problem . then the problem turns into finding instance class ##ifiers only given bag labels . our method also follows the mil strategy and makes several improvements to w ##so ##d . in particular , we learn refined instance class ##ifiers based on proposal clusters according to both instance scores and spatial relations in an online manner . mil has many applications to computer vision , such as image classification , weakly supervised semantic segment ##ation , object detection , object tracking , etc . the strategy of treating proposal clusters as bags was partly inspired by , where proposes to train mil for patches around ground ##tr ##uth locations and proposes to train mil for patches around predicted object locations . however , they require ground ##tr ##uth locations for either all training samples or the beginning time frames , whereas w ##so ##d does not have such ann ##ota ##tions . therefore , it is much harder to generate proposal clusters only guided by image - level supervision ##s for w ##so ##d . in addition , we incorporate the strategy of treating proposal clusters as bags into the network training whereas do not . o ##qua ##b et al . also train a cnn network using the max po ##oin ##g mil strategy to local ##ize objects . but their methods can only coarse ##ly local ##ize objects regardless of their sizes and aspect ratios , whereas our method can detect objects more accurately . sub ##section : weakly supervised object detection w ##so ##d has attracted great interests nowadays because the amount of data with image - level ann ##ota ##tions is much bigger and is growing much faster than that with bound ##ing ##box - level ann ##ota ##tions . many methods are emerging for the w ##so ##d problem . for example , chu ##m and z ##isse ##rman first initial ##ize object locations by disc ##rim ##ina ##tive visual words and then introduce an ex ##em ##pl ##ar model to measure similarity between image pairs for up ##dating locations . des ##ela ##ers et al . propose to initial ##ize boxes by object ##ness and use a cr ##f - based model to it ##erative ##ly local ##ize objects . pan ##de ##y and la ##ze ##bn ##ik train a d ##pm model under weak supervision ##s for w ##so ##d . shi et al . use bay ##esian late ##nt topic models to jointly model different object classes and background . song et al . develop a technology to discover frequent disc ##rim ##ina ##tive configurations of visual patterns for robust w ##so ##d . ci ##nb ##is et al . it ##erative ##ly train a multi - fold mil to avoid the detector being locked onto inaccurate local opt ##ima . wang et al . relax the mil constraints into a der ##iva ##ble loss function to train detectors more efficient . recently , with the revolution of cnn ##s in computer vision , many works also try to combine the w ##so ##d with cnn ##s . early works treat cnn models pre - trained on image ##net as off - the - shelf feature extract ##ors . they extract cnn features for each candidate regions , and then train their own detectors on top of these features . these methods have shown that cnn des ##cript ##ors can boost performance against traditional hand - designed features . more recent efforts tend to train end - to - end networks for w ##so ##d . they integrate the mil constraints into the network training by ag ##gre ##gating proposal classification scores into final image classification scores , and then image - level supervision can be directly added to image classification scores . for example , tang et al . propose to use max pool ##ing for aggregation . bile ##n and ve ##dal ##di develop a weighted sum po ##oin ##g strategy . building on , kant ##oro ##v et al . argue that context information can improve the performance . di ##ba et al . show that weakly supervised segment ##ation map can be used as guidance to filter proposals , and jointly train the weakly supervised segment ##ation network and w ##so ##d end - to - end . our method is built on these networks and any of them can be chosen as our basic network . our strategy proposes to learn refined instance class ##ifiers based on proposal clusters , and propose a novel online approach to train our network effectively and efficiently . experimental results show our strategies can boost the results significantly . in addition to the weighted sum po ##oin ##g , also proposes a \" spatial regular ##iser \" that forces features of the highest scoring proposal and its spatial ##ly adjacent proposals to be the same . unlike this , we show that finding proposal cluster centers using graph and treating proposal clusters as bags are more effective . the contemporary work uses a graph model to generate seed proposals . their network training has many steps : first , an mil network is trained ; second , seed proposals are generated using the graph ; third , based on these seed proposals , a fast r - cnn like detector is trained . our method differs from in many aspects : first , we propose to generate proposal clusters for each training iteration and thus our network is trained end - to - end instead of step - by - step , which is more efficient and can benefit from sharing proposal features among different streams ; second , we propose to treat proposal clusters as bags for training better class ##ifiers . as evidenced by experiments , our method obtain ##s much better and more robust results . sub ##section : end - to - end and its variants in standard end - to - end training , the update requires opt ##imi ##zing losses w . r . t . all functions of network parameters . for example , the fast r - cnn opt ##imi ##zes their classification loss and bound ##ing ##box regression loss w . r . t . proposal classification and feature extraction for fully supervised object detection . the mil networks in opt ##imi ##ze their mil loss w . r . t . proposal classification and feature extraction for w ##so ##d . unlike the standard end - to - end training , there exists a variant of end - to - end training . the variant contains functions which depend on network parameters , but losses are not opt ##imi ##zed w . r . t . all these functions . as we described in section [ reference ] , the \" spatial regular ##iser \" in forces features of the highest scoring proposal and its spatial ##ly adjacent proposals to be the same . they use a function of network parameters to compute the highest scoring proposal , and do not opt ##imi ##ze their losses w . r . t . this function . di ##ba et al . filter out background proposals using a function of network parameters and use these filtered proposals in their latter network computation ##s . they also do not opt ##imi ##ze their losses w . r . t . this function . inspired by , we use this variant of end - to - end training . more precisely , we do not opt ##imi ##ze our losses w . r . t . the generated supervision ##s for instance class ##ifier ref ##ine ##ment . sub ##section : others there are many other important related works that do not focus on weakly supervised learning but should be discussed . similar to other end - to - end mil networks , our method is built on top of the region of interest ( roi ) pool ##ing layer or spatial pyramid pool ##ing ( spp ) layer to share con ##vo ##lu ##tion ##al computation ##s among different proposals for model acceleration . but both and require bound ##ing ##box - level ann ##ota ##tions to train their detectors . the sharing proposal feature strategy in our network is similar to multi - task learning . unlike the multi - task learning that each output stream has their own relatively independent external supervision ##s for different tasks , in our method , all streams have the same task and supervision ##s of later streams depend on the outputs from their preceding streams . section : method the overall architecture of our method is shown in fig . [ reference ] . given an image , about object proposals from selective search or edge ##box are generated . during the forward process of training , the image and these proposals are fed into some con ##vo ##lu ##tion ##al ( con ##v ) layers with an spp layer to produce a fixed - size con ##v feature map per - proposal . after that , proposal feature maps are fed into two fully connected ( fc ) layers to produce proposal features . these features are branched into different streams : the first one is an mil network to train basic instance class ##ifiers and the others ref ##ine the class ##ifiers it ##erative ##ly . for each stream , proposal classification scores are obtained and proposal clusters are generated consequently . then based on these proposal clusters , supervision ##s are generated to compute losses for the next stream . during the back - propagation process of training , the network losses are opt ##imi ##zed to train proposal features and class ##ifiers . as shown in the figure , supervision ##s of the - st refined class ##ifier depend on the output from the basic class ##ifier , and supervision ##s of - th refined class ##ifier depend on outputs from - th refined class ##ifier . in this section , we will introduce our method of learning refined instance class ##ifiers based on proposal clusters in detail . sub ##section : notation ##s before presenting our method , we first introduce some of the mostly used notation ##s as follows . we have proposals with boxes for an given image and proposal features , where is the - th proposal box . the number of refined instance class ##ifiers is ( i . e . , we ref ##ine instance class ##ifier times ) , and thus there are streams . the number of object classes is . and are the parameters of the basic instance class ##ifier and the - th refined instance class ##ifier , respectively . and are the predicted score matrices of the basic instance class ##ifier and the - th refined instance class ##ifier , respectively , where indicates the object classes and background class . we use later for sim ##pl ##ification , dropping the dependence on . is the predicted score of the - th proposal for class from the - th instance class ##ifier . is the image label vector , where or indicates the image with or without object class . is the supervision of the - th instance class ##ifier , where is the image label vector . is the loss function to train the - th instance class ##ifier . we compute proposal cluster centers for the - th ref ##ine ##ment . the - th cluster center consists of a proposal box , an object label ( indicates the - th object class ) , and a confidence score indicating the confidence that covers at least part of an object of class . we have proposal clusters according to ( for background and others for objects ) . for object clusters , the - th cluster consists of proposal boxes , an object label that is the same as the cluster center label , and a confidence score that is the same as the cluster center score , where indicates the confidence that corresponds to an object of class . unlike object clusters , the background cluster consists of proposals and a label indicating the background . the - th proposal consists of a proposal box and a confidence score indicating the confidence that is the background . sub ##section : basic mil network it is necessary to generate proposal scores and clusters to supervise refined instance class ##ifiers . more specifically , the first refined class ##ifier requires basic instance class ##ifiers to generate proposal scores and clusters . therefore , we first introduce our basic mil network as the basic instance class ##ifier . our overall network is independent of the specific mil methods , and thus any method that can be trained end - to - end could be used . there are many possible choices . here we choose the method by bile ##n and ve ##dal ##di which proposes a weighted sum pool ##ing strategy to obtain the instance class ##ifier , because of its effectiveness and implementation convenience . to make our paper self - contained , we briefly introduce as follows . given an input image and its proposal boxes , a set of proposal features are first generated by the network . then as shown in the \" basic mil network \" block of fig . [ reference ] , there are two branches which process the proposal features to produce two matrices ( we use later for sim ##pl ##ification , dropping the dependence on ) of an input image by two fc layers , where and denote the parameters of the fc layer for and the parameters of the fc layer for , respectively . then the two matrices are passed through two soft ##max layer along different directions : and . let us denote by . the proposal scores are generated by element - wise product . finally , the image score of the - th class is obtained by the sum over all proposals : . a simple interpretation of the two branches framework is as follows . is the probability of the - th proposal belonging to class . is the normal ##ized weight that indicates the contribution of the - th proposal to image being classified to class . so is obtained by weighted sum pool ##ing and falls in the range of . given the image label vector . we train the basic instance class ##ifier by opt ##imi ##zing the multi - class cross entropy loss e ##q . ( [ reference ] ) w . r . t . . sub ##section : the overall training strategy to ref ##ine instance class ##ifiers it ##erative ##ly , we add multiple output streams in our network where each stream corresponds to a refined class ##ifier , as shown in fig . [ reference ] . we integrate the basic mil network and the class ##ifier ref ##ine ##ment into an end - to - end network to learn the refined class ##ifier online . unlike the basic instance class ##ifier , for an input image the output score matrix of the - th refined class ##ifier is a matrix and is obtained by passing the proposal features through a single fc layer ( with parameters ) as well as a soft ##max over - classes layer , i . e . , , as in the \" instance class ##ifier ref ##ine ##ment \" blocks of fig . [ reference ] . notice that we use the same proposal features for all class ##ifiers . we use later for sim ##pl ##ification , dropping the dependence on . as we stated before , supervision ##s to train the - th instance class ##ifier are generated based on proposal scores and image label . thus we denote the supervision ##s by . then we train our overall network by opt ##imi ##zing the loss e ##q . ( [ reference ] ) w . r . t . . we do not opt ##imi ##ze the loss w . r . t . , which means that the supervision ##s are only computed in the forward process and we do not compute their gradient ##s to train our network . the loss for the - th refined instance class ##ifier is defined in later e ##q . ( [ reference ] ) / ( [ reference ] ) / ( [ reference ] ) which are loss functions with supervision ##s provided by . we will give details about how to get supervision ##s and loss functions in section [ reference ] . [ t ] the overall training procedure ( one iteration ) [ 1 ] an image , its proposal boxes , and its image label vector ; ref ##ine ##ment times . an updated network . feed the image and into the network to produce proposal score matrices ( simplified as later ) . compute loss by e ##q . ( [ reference ] ) , see section [ reference ] . generate supervision ##s , see section [ reference ] . compute loss by e ##q . ( [ reference ] ) / ( [ reference ] ) / ( [ reference ] ) , see section [ reference ] . opt ##imi ##ze , i . e . , e ##q . ( [ reference ] ) , w . r . t . ( not w . r . t . ) . during the forward process of each st ##och ##astic gradient descent ( sg ##d ) training iteration , we obtain a set of proposal scores of an input image . accordingly , we generate the supervision ##s for the iteration to compute the loss e ##q . ( [ reference ] ) . during the back - propagation process of each sg ##d training iteration , we opt ##imi ##ze the loss e ##q . ( [ reference ] ) w . r . t . proposal features and class ##ifiers . we sum ##mar ##ize this procedure in algorithm [ reference ] . note that we do not use an alternating training strategy , i . e . , fixing supervision ##s and training a complete model , fixing the model and up ##dating supervision ##s . the reasons are that : 1 ) it is very time - consuming because it requires training models multiple times ; 2 ) training different models in different ref ##ine ##ment steps separately may harm the performance because it hind ##ers the process to benefit from the shared proposal features ( i . e . , ) . sub ##section : proposal cluster learning here we will introduce our methods to learn refined instance class ##ifiers based on proposal clusters ( i . e . , proposal cluster learning ) . recall from section [ reference ] that we have a set of proposals with boxes . for the - th ref ##ine ##ment , our goal is to generate supervision ##s for the loss functions using the proposal scores and image label in each training iteration . we use later for sim ##pl ##ification , dropping the dependence on . we do this in three steps . 1 ) we find proposal cluster centers which are proposals corresponding to different objects . 2 ) we group the remaining proposals into different clusters , where each cluster is associated with a cluster center or corresponds to the background . 3 ) we generate the supervision ##s for the loss functions , enabling us to train the refined instance class ##ifiers . for the first step , we compute proposal cluster centers based on and . the - th cluster center is defined in section [ reference ] . we propose two algorithms to find in section [ reference ] ( 1 ) and ( 2 ) ( also algorithm [ reference ] and algorithm [ reference ] ) , where the first one was proposed in the conference version paper and the second one is proposed in this paper . for the second step , according to the proposal cluster centers , proposal clusters are generated ( for background and others for objects ) . the - th object cluster and the background cluster are defined in section [ reference ] . we use the different notation for the background cluster because background proposals are scattered in each image , and thus it is hard to determine a cluster center and accordingly a cluster score . the method to generate was proposed in the conference version paper and is described in section [ reference ] ( also algorithm [ reference ] ) . for the third step , supervision ##s to train the - th refined instance class ##ifier are generated based on the proposal clusters . we use two strategies where are either proposal - level labels indicating whether a proposal belongs to an object class , or cluster - level labels that treats each proposal cluster as a bag . subsequently these are used to compute the loss functions . we propose two approaches to do this as described in section [ reference ] ( 1 ) and ( 2 ) , where the first one was proposed in the conference version paper and the second one is proposed in this paper . sub ##su ##bs ##ection : finding proposal cluster centers in the following we introduce two algorithms to find proposal cluster centers . [ t ] finding proposal cluster centers using the highest scoring proposal [ 1 ] proposal boxes ; image label vector ; proposal score matrix . proposal cluster centers . initial ##ize . choose the - th proposal by e ##q . ( [ reference ] ) . . ( 1 ) finding proposal cluster centers using the highest scoring proposal . a solution for finding proposal cluster centers is to choose the highest scoring proposal , as in our conference version paper . as in algorithm [ reference ] , suppose an image has object class label ( i . e . , ) . for the - th ref ##ine ##ment , we first select the - th proposal which has the highest score by e ##q . ( [ reference ] ) , where is the predicted score of the - th proposal , as defined in section [ reference ] . then this proposal is chosen as the cluster center , i . e . , , where is the box of the - th proposal . is chosen as the confidence score that the - th proposal covers at least part of an object of class , because is the predicted score of the - th proposal been categorized to class . therefore , the highest scoring proposal can probably cover at least part of the object and thus be chosen as the cluster center . there is a potential problem that one proposal may be chosen as the cluster centers for multiple object classes . to avoid this problem , if one proposal corresponds to the cluster centers for multiple object classes , this proposal would be chosen as the cluster center only by the class with the highest predicted score and we re - choose cluster centers for other classes . ( 2 ) finding proposal cluster centers using graphs of top ranking proposals . as stated in section [ reference ] , although we can find good proposal cluster centers using the highest scoring proposal , this ignores that in natural images there are often more than one object for each category . therefore , we propose a new method to find cluster centers using graphs of top ranking proposals . [ t ] finding proposal cluster centers using graphs of top ranking proposals [ 1 ] proposal boxes ; image label vector ; proposal score matrix . proposal cluster centers . initial ##ize . select top ranking proposals with index ##es . build a graph using the top ranking proposals . set . set . . remove the - th proposal box from , or . is empty . more specifically , suppose an image has object class label . we first select the top ranking proposals with index ##es for the - th ref ##ine ##ment . then we build an und ##ire ##cted un ##weight ##ed graph of these proposals based on spatial similarity , where vertex ##es correspond to these top ranking proposals , and edges correspond to the connections between the vertex ##es . is determined according to the spatial similarity between two vertex ##es ( i . e . , proposals ) as in e ##q . ( [ reference ] ) , where is the io ##u between the - th and - th proposals and is a threshold ( e . g . , ) . therefore , two vertex ##es are connected if they are spatial ##ly adjacent . after that , we greed ##ily generate some cluster centers for class using this graph . that is , we it ##erative ##ly select vertex ##es which have most connections to be the cluster centers , as in algorithm [ reference ] . the number of cluster centers ( i . e . , ) changes for each image in each training iteration because the top ranking proposals change . see section [ reference ] for some typical values of . we use the same method as in section [ reference ] ( 1 ) to avoid one proposal been chosen as the cluster centers for multiple object classes . the reasons for this strategy are as follows . first , according to our observation , the top ranking proposals can always cover at least parts of objects , thus generating centers from these proposals encourages the selected centers to meet our requirements . second , because these proposals cover objects well , better proposals ( covering more parts of objects ) should have more spatial ##ly overlap ##ped proposals ( i . e . , have more connections ) . third , these centers are spatial ##ly far apart , and thus different centers can correspond to different objects . this method also has the attractive characteristic that it can generate adaptive number of proposals for each object class , which is desirable because in natural images there are arbitrary number of objects per - class . we set the score of the - th proposal cluster center by ( see the - th line in algorithm [ reference ] ) because if the adjacent proposals of a center proposal have high confidence to cover at least part of an object ( i . e . , have high classification scores ) the center proposal should also have such high confidence . there is an important issue for the graph - based method : how to select the top ranking proposals ? a simple method is to select proposals whose scores exceed a threshold . but in our case , proposal scores change in each training iteration , and thus it is hard to determine a threshold . instead , for each positive object class , we use the - means algorithm to divide proposal scores of an image into some clusters , and choose proposals in the cluster which has the highest score center to form the top ranking proposals . this method ensures that we can select the top ranking proposals although proposal scores change during training . other choices are possible , but this method works well in experiments . [ t ] generating proposal clusters [ 1 ] proposal boxes ; proposal cluster centers . proposal clusters . initial ##ize . set of to of , . initial ##ize and set . compute io ##us . choose the most spatial ##ly adjacent center . . . sub ##su ##bs ##ection : generating proposal clusters after the cluster centers are found , we generate the proposal clusters as in our conference version paper . except for the cluster for background , good proposal clusters require that proposals in the same cluster are associated with the same object , and thus proposals in the same cluster should be spatial ##ly adjacent . specially , given the - th proposal , we compute a set of io ##us , where is the io ##u between the - th proposal and the box of the - th cluster center . then we assign the - th proposal to the - th object cluster if is larger than a threshold ( e . g . , ) and to the background cluster otherwise , where is the index of the most spatial ##ly adjacent cluster center as e ##q . ( [ reference ] ) . the overall procedures to generate proposal clusters are summarized in algorithm [ reference ] . we set the proposal scores for the background cluster to the scores of their most spatial ##ly adjacent centers as the 10 - the line in algorithm [ reference ] , because if the cluster center has confidence that it covers an object , the proposal far away from should have confidence to be background . sub ##su ##bs ##ection : learning refined instance class ##ifiers to get supervision ##s and loss functions to learn the - th refined instance class ##ifier , we design two approaches as follows . ( 1 ) assign ##ing proposals object labels . the most straightforward way to ref ##ine class ##ifiers is to directly assign object labels to all proposals in object clusters because these proposals potentially correspond to whole objects , as in our conference version paper . as the cluster centers covers at least parts of objects , their adjacent proposals ( i . e . , proposals in the cluster ) can contain larger parts of objects . accordingly , we can assign the cluster label to all proposals in the - th cluster . more specifically , the supervision ##s are proposal - level labels , i . e . , . is the label vector of the - th proposal for the - th ref ##ine ##ment , where and if the - th proposal belongs to the - th clusters . consequently , we use the standard soft ##max loss function to train the refined class ##ifiers as in e ##q . ( [ reference ] ) , where is the predicted score of the - th proposal as defined in section [ reference ] . through it ##erative instance class ##ifier ref ##ine ##ment ( i . e . , multiple times of ref ##ine ##ment as increase ) , the detector detect ##s larger parts of objects gradually by forcing the network to \" see \" larger parts of objects . actually , the so learnt supervision ##s are very noisy , especially in the beginning of training . this results in unstable solutions . to solve this problem , we change the loss in e ##q . ( [ reference ] ) to a weighted version , as in e ##q . ( [ reference ] ) . is the loss weight that is the same as the cluster confidence score for object clusters or proposal confidence score for the background cluster if the - th proposal belongs to the - th cluster . from algorithm [ reference ] , we can observe that is the same as the cluster center confidence score . the reasons for this strategy are as follows . in the beginning of training , although we can not obtain good proposal clusters , each is small , hence each is small and the loss is also small . as a consequence , the performance of the network will not decrease a lot . during the training , the top ranking proposals will cover objects well , and thus we can generate good proposal clusters . then we can train satisfactory instance class ##ifiers . ( 2 ) treating clusters as bags . as we stressed before , although directly assign ##ing proposals object labels can boost the results , it may confuse the network because we simultaneously assign the same label to different parts of objects . focusing on this , we further propose to treat each proposal cluster as a small new bag and use the cluster label as the bag label . thus the supervision ##s for the - th ref ##ine ##ment are bag - level ( cluster - level ) labels , i . e . , . is the label of the - th bag , i . e . , the label of the - th proposal cluster , as defined in section [ reference ] . specially , for object clusters , we choose average mil pool ##ing , because these proposals should cover at least parts of objects and thus should have relatively high prediction scores . for the background cluster , we assign the background label to all proposals in the cluster according to the mil constraints ( all instances in negative bags are negative ) . then the loss function for ref ##ine ##ment will be e ##q . ( [ reference ] ) . , , and are the cluster confidence score of the - th object cluster , the number of proposals in the - th cluster , and the predicted score of the - th proposal , respectively , as defined in section [ reference ] . and indicate that the - th proposal belongs to the - th object cluster and the background cluster respectively . compared with the directly assign ##ing label approach , this method tolerate ##s some proposals to have low scores , which can reduce the am ##bi ##gui ##ties to some extent . sub ##section : testing during testing , the proposal scores of refined instance class ##ifiers are used as the final detection scores , as the blue arrows in fig . [ reference ] . here the mean output of all refined class ##ifiers is chosen . the non - maxim ##a suppression ( nm ##s ) is used to filter out redundant detection ##s . section : experiments in this section , we first introduce our experimental setup including data ##set ##s , evaluation metric ##s , and implementation details . then we conduct elaborate experiments to discuss the influence of different settings . next , we compare our results with others to show the effectiveness of our method . after that , we show some qu ##ali ##tative results for further analyses . finally , we give some run ##time analyses of our method . codes for rep ##rod ##uc ##ing our results are available at . sub ##section : experimental setup sub ##su ##bs ##ection : data ##set ##s and evaluation metric ##s we evaluate our method on four challenging data ##set ##s : the pascal vo ##c 2007 and 2012 data ##set ##s , the image ##net detection data ##set , and the ms - coco data ##set . only image - level ann ##ota ##tions are used to train our models . the pascal vo ##c 2007 and 2012 data ##set ##s have and images respectively for object classes . these two data ##set ##s are divided into train , val , and test sets . here we choose the train ##val set ( images for 2007 and images for 2012 ) to train our network . for testing , there are two metric ##s for evaluation : map and co ##rl ##oc . following the standard pascal vo ##c protocol , average precision ( ap ) and the mean of ap ( map ) is the evaluation metric to test our model on the testing set . correct local ##ization ( co ##rl ##oc ) is to test our model on the training set measuring the local ##ization accuracy . all these two metric ##s are based on the pascal criterion , i . e . , io ##u 0 . 5 between ground ##tr ##uth bound ##ing ##box ##es and predicted boxes . the image ##net detection data ##set has hundreds of thousands of images with object classes . it is also divided into train , val , and test sets . following , we split the val set into val ##1 and val ##2 , and randomly choose at most k images in the train set for each object class ( we call it train ) . we train our model on the mixture of train and val ##1 sets , and test it on the val ##2 set , which will lead to images for training and images for testing . we also use the map for evaluation on the image ##net . the ms - coco data ##set has object classes and is divided into train , val , and test sets . since the ground ##tr ##uth ##s on the test set are not released , we train our model on the ms - coco 2014 train set ( about k images ) and test it on the val set ( about k images ) . for evaluation , we use two metric ##s map @ 0 . 5 and map @ [ . 5 , . 95 ] which are the standard pascal criterion ( i . e . , io ##u 0 . 5 ) and the standard ms - coco criterion ( i . e . , computing the average of map for io ##u [ 0 . 5 : 0 . 05 : 0 . 95 ] ) respectively . sub ##su ##bs ##ection : implementation details our method is built on two pre - trained image ##net networks v ##gg m and v ##gg ##16 , each of which has some con ##v layers with max - pool ##ing layers and three fc layers . we replace the last max - pool ##ing layer by the spp layer , and the last fc layer as well as the soft ##max loss layer by the layers described in section [ reference ] . to increase the feature map size from the last con ##v layer , we replace the penultimate max - pool ##ing layer and its subsequent con ##v layers by the dil ##ated con ##v layers . the newly added layers are initial ##ized using ga ##uss ##ian distributions with - mean and standard deviation ##s . bias ##es are initial ##ized to . during training , the mini - batch size for sg ##d is set to be , , and for pascal vo ##c , image ##net , and ms - coco , respectively . the learning rate is set to for the first k , k , k , and k iteration ##s for the pascal vo ##c 2007 , pascal vo ##c 2012 , image ##net , and ms - coco data ##set ##s , respectively . then we decrease the learning rate to in the following k , k , k , and k iteration ##s for the pascal vo ##c 2007 , pascal vo ##c 2012 , image ##net , and ms - coco data ##set ##s , respectively . the momentum and weight decay are set to be and respectively . selective search , edge ##box , and mc ##g are adopted to generate about proposals per - image for the pascal vo ##c , image ##net , and ms - coco data ##set ##s , respectively . for data aug ##ment ##ation , we use five image scales ( res ##ize the shortest side to one of these scales ) with horizontal flip ##s for both training and testing . if not specified , the instance class ##ifiers are refined three times , i . e . , in section [ reference ] , so there are four output streams ; the io ##u threshold in section [ reference ] ( 2 ) ( also e ##q . ( [ reference ] ) ) is set to ; the number of - means clusters in the last paragraph of section [ reference ] ( 2 ) is set to ; in section [ reference ] ( also the - th line of algorithm [ reference ] ) is set to . similar to other works , we train a supervised object detector through choosing the top - scoring proposals given by our method as pseudo ground ##tr ##uth ##s to further improve our results . here we train a fast r - cnn ( fr ##c ##nn ) using the v ##gg ##16 model and the same five image scales ( horizontal flip ##s only in training ) . the same proposals are chosen to train and test the fr ##c ##nn . nm ##s ( with io ##u threshold ) is applied to compute ap . our experiments are implemented based on the caf ##fe deep learning framework , using python and c + + . the - means algorithm to produce top ranking proposals is implemented by sci ##kit - learn . all of our experiments are running on an n ##vid ##ia gt ##x titan ##x pascal gp ##u and intel ( r ) i ##7 - 68 ##50 k cpu ( 3 . 60 ##gh ##z ) . sub ##section : discussions we first conduct some experiments to discuss the influence of different components of our method ( including instance class ##ifier ref ##ine ##ment , different proposal generation methods , different ref ##ine ##ment strategies , and weighted loss ) and different parameter settings ( including the io ##u threshold defined in section [ reference ] ( 2 ) , the number of - means clusters described in section [ reference ] ( 2 ) , the io ##u threshold defined in section [ reference ] , and multi - scale training and testing . ) we also discuss the number of proposal cluster centers . without loss of general ##ity , we only perform experiments on the vo ##c 2007 data ##set and use the v ##gg m model . sub ##su ##bs ##ection : the influence of instance class ##ifier ref ##ine ##ment as the five curves in fig . [ reference ] show , we observe that compared with the basic mil network , for both ref ##ine ##ment methods , even refining instance class ##ifier a single time boost ##s the performance a lot . this confirms the necessity of ref ##ine ##ment . if we ref ##ine the class ##ifier multiple times , the results are improved further . but when ref ##ine ##ment is implemented too many times , the performance gets saturated ( there are no obvious improvements from times to times ) . this is because the network tends to converge so that the supervision of the - th time is similar to the - rd time . in the rest of this paper we only ref ##ine class ##ifiers times . notice that in fig . [ reference ] , the \" 0 time \" is similar to the w ##sd ##d ##n using selective search as proposals . sub ##su ##bs ##ection : the influence of different proposal cluster generation methods we discuss the influence of different proposal cluster generation methods . as shown in the fig . [ reference ] ( green and purple solid curves for the highest scoring proposal based method , blue and red solid curves for the graph - based method ) , for all ref ##ine ##ment times , the graph - based method obtain ##s better performance , because it can generate better cluster centers . thus we choose the graph - based method in the rest of our paper . sub ##su ##bs ##ection : the influence of different ref ##ine ##ment strategies we then show the influence of different ref ##ine ##ment strategies . the directly assign ##ing label method is replaced by treating clusters as bags ( blue and green solid curves ) . from fig . [ reference ] , it is obvious that the results by treating clusters as bags are better . in addition , compared with the alternating training strategy ( blue dashed curve ) , our online training boost ##s the performance consistently and significantly , which confirms the necessity of sharing proposal features . online training also reduces the training time a lot , because it only requires training a single model instead of training models for times ref ##ine ##ment in the alternating strategy . in the rest of our paper , we only report results by the \" pc ##l - ob - g \" method in fig . [ reference ] because it achieve ##s the best performance . sub ##su ##bs ##ection : the influence of weighted loss we also study the influence of our weighted loss in e ##q . ( [ reference ] ) . note that e ##q . ( [ reference ] ) can be easily changed to the un ##weight ##ed version by simply setting and to be . here we train a network using the un ##weight ##ed loss . the results of the un ##weight ##ed loss are map and co ##rl ##oc . we see that if we use the un ##weight ##ed loss , the improvement from ref ##ine ##ment is very scan ##t and the performance is even worse than the alternating strategy . using the weighted loss achieve ##s much better performance ( map and co ##rl ##oc ) , which confirms our theory in section [ reference ] . sub ##su ##bs ##ection : the influence of the io ##u threshold here we discuss the influence of the io ##u threshold defined in section [ reference ] ( 2 ) and e ##q . ( [ reference ] ) . from fig . [ reference ] , we see that setting to obtain ##s the best performance . therefore , we set to for the other experiments . sub ##su ##bs ##ection : the influence of the number of - means clusters in previous experiments we set the number of - means clusters described in the last paragraph of section [ reference ] ( 2 ) to be . here we set it to other numbers to explore its influence . the results from other numbers of - means clusters are map and co ##rl ##oc for clusters , and map and co ##rl ##oc for clusters , which are a little worse than the results from cluster . therefore , we set the number of - means clusters to for the other experiments . sub ##su ##bs ##ection : the influence of the io ##u threshold we also anal ##yse the influence of defined in section [ reference ] and the - th line of algorithm [ reference ] . as shown in fig . [ reference ] , out ##per ##forms other choices . therefore , we set to for the other experiments . sub ##su ##bs ##ection : the influence of multi - scale training and testing previously our experiments are conducted based on five image scales for training and testing . here we show the influence of this multi - scale setting . we train and test our method using a single image scale as the default scale setting of fr ##c ##nn . the single - scale results are map and co ##rl ##oc which are much worse than our multi - scale results ( map and co ##rl ##oc ) . therefore , we use five image scales as many w ##so ##d networks . sub ##su ##bs ##ection : the number of proposal cluster centers as we stated in section [ reference ] ( 2 ) , the number of proposal cluster centers ( i . e . , ) changes for each image in each training iteration . here we give some typical values of . in the beginning of training , the proposal scores are very noisy and thus the selected top ranking proposals to form graphs are scattered in images , which results in dozens of proposal cluster centers for each image . after some ( about 3 k ) training iteration ##s , the proposal scores are more reliable and our method finds 1 3 proposal cluster centers for each positive object class . to make the training more stable in the beginning , for each positive object class we empirical ##ly select at most five proposal cluster centers which have higher scores , and the number of selected proposal cluster centers does not influence the performance much . sub ##section : comparison with other methods here we compare our best performed strategy pc ##l - ob - g , i . e . , using graph - based method and treating clusters as bags to train the network online , with other methods . we first report our results for each class on vo ##c 2007 and 2012 in table [ reference ] , table [ reference ] , table [ reference ] , and table [ reference ] . it is obvious that our method out ##per ##forms other methods using single model v ##gg m or v ##gg ##16 ( pc ##l - ob - g + v ##gg m and pc ##l - ob - g + v ##gg ##16 in tables . ) our single model results even better than others by combining multiple different models ( e . g . , ensemble of models ) . specially , our method obtain ##s much better results compared with other two methods also using the same basic mil network . importantly , also e ##qui ##ps the weighted sum pool ##ing with object ##ness measure of edge ##box and the spatial regular ##iser , and adds context information into the network , both of which are more complicated than our basic mil network . we believe that our performance can be improved by choosing better basic mil networks , like the complete network in and using context information . as rei ##mple ##ment ##ing their method completely is non - trivial , here we only choose the simplest architecture in . even in this simplified case , our method achieve ##s very promising results . our results can also be improved by comb ##ing multiple models . as shown in the tables , there are little improvements from the ensemble of the v ##gg m and v ##gg ##16 models ( pc ##l - ob - g - en ##s . in tables ) . here we do the ensemble by sum ##ming up the scores produced by the two models . also , as mentioned in section [ reference ] , similar to , we train a fr ##c ##nn detector using top - scoring proposals produced by pc ##l - ob - g - en ##s . as ground ##tr ##uth ##s ( pc ##l - ob - g - en ##s . + fr ##c ##nn in tables ) . as we can see , the performance is improved further . we then show results of our method on the large scale image ##net detection data ##set in table [ reference ] . we observe similar phenomenon that our method out ##per ##forms other methods by a large margin . we finally report results of our method on ms - coco in table [ reference ] . our method obtain ##s better performance than the recent work . in particular , ge et al . use the method proposed in our conference version paper as a basic component . we can expect to obtain better detection performance through replacing our conference version method in by our newly proposed method here , which we would like to explore in the future . sub ##section : qu ##ali ##tative results we first show some proposal clusters generated by our method in fig . [ reference ] . as we can see , the cluster centers contain at least parts of objects and are able to cover adaptive number of objects for each class . we then show qu ##ali ##tative comparisons among the w ##sd ##d ##n , the w ##sd ##d ##n + context , and our pc ##l method , both of which use the same basic mil network . as shown in fig . [ reference ] , we can observe that for classes such as bike , car , cat , etc . , our method tends to provide more accurate detection ##s , whereas other two methods sometimes fails by producing boxes that are over ##lar ##ge or only contain parts of objects ( the first four rows in fig . [ reference ] ) . but for some classes such as person , our method sometimes fails by only detecting parts of objects such as the head of person ( the fifth row in fig . [ reference ] ) . exploit ##ing context information sometimes help the detection ( as in w ##sd ##d ##n + context ) , we believe our method can be further improved by incorporating context information into our framework . all these three methods ( actually almost all weakly supervised object detection methods ) suffers from two problems : producing boxes that not only contain the target object but also include their adjacent similar objects , or only detecting parts of object for objects with deformation ( the last row in fig . [ reference ] ) . we finally visual ##ize some success and failure detection results on vo ##c 2007 train ##val by pc ##l - en ##s . + fr ##c ##nn , as in fig . [ reference ] . we observe similar phenomena as in fig . [ reference ] . our method is robust to the size and aspect of objects , especially for rigid objects . the main failures for these rigid objects are always due to over ##lar ##ge boxes that not only contain objects , but also include adjacent similar objects . for non - rigid objects like \" cat \" , \" dog \" , and \" person \" , they often have great deformation ##s , but their parts ( e . g . , head of person ) have much less deformation , so our detector is still inclined to find these parts . an ideal solution is yet wanted because there is still room for improvement . sub ##section : run ##time the run ##time comparisons between our method and our basic mil network are shown in table [ reference ] , where the run ##time of proposal generation is not considered . as we can see , although our method has more components than our basic mil network , our method takes almost the same testing time as it . this is because all our output streams share the same proposal feature computation ##s . the small extra training computation ##s of our method mainly come from the procedures to find proposal cluster centers and generate proposal clusters . although with small extra training computation ##s , our method obtain ##s much better detection results than the basic mil network . section : conclusion in this paper , we propose to generate proposal clusters to learn refined instance class ##ifiers for weakly supervised object detection . we propose two strategies for proposal cluster generation and class ##ifier ref ##ine ##ment , both of which can boost the performance significantly . the class ##ifier ref ##ine ##ment is implemented by multiple output streams corresponding to some instance class ##ifiers in multiple instance learning networks . an online training algorithm is introduced to train the proposed network end - to - end for effectiveness and efficiency . experiments show substantial and consistent improvements by our method . we observe that the most common failure cases of our algorithm are connected with the deformation of non - rigid objects . in the future , we will concentrate on this problem . in addition , we believe our learning algorithm has the potential to be applied in other weakly supervised visual learning tasks such as weakly supervised semantic segment ##ation . we will also explore how to apply our method to these related applications . section : acknowledge ##ments this work was supported by ns ##fc ( no . 61 ##7 ##33 ##00 ##7 , no . 61 ##57 ##22 ##0 ##7 , no . 61 ##8 ##7 ##6 ##21 ##2 , no . 61 ##6 ##7 ##23 ##36 , no . 61 ##57 ##31 ##60 ) , on ##r with grant n ##00 ##01 ##4 - 15 - 1 - 235 ##6 , hub ##ei scientific and technical innovation key project , and the program for hu ##st academic frontier youth team . the corresponding author of this paper is xi ##ng ##gang wang . bibliography : references [ ] peng tang received the b . s . degree in electronics and information engineering from hua ##zh ##ong university of science and technology ( hu ##st ) in 2014 . he is currently pursuing the ph . d . degree in the school of electronic information and communications at hu ##st , and visiting the department of computer science at johns hopkins university . he was an intern at microsoft research asia in 2017 . his research interests include image classification and object detection in images / videos . [ ] xi ##ng ##gang wang is an assistant professor of school of electronics information and communications of hua ##zh ##ong university of science and technology ( hu ##st ) . he received his bachelor degree in communication and information system and ph . d . degree in computer vision both from hu ##st . from may 2010 to july 2011 , he was with the department of computer and information science , temple university , philadelphia , pa . , as a visiting scholar . from february 2013 to september 2013 , he was with the university of california , los angeles ( ucla ) , as a visiting graduate researcher . he is a reviewer of ieee trans on pam ##i , ieee trans on image processing , ieee trans . on cyber ##net ##ics , pattern recognition , computer vision and image understanding , ne ##uro ##com ##put ##ing , ni ##ps , ic ##ml , cv ##pr , icc ##v and ec ##c ##v etc . his research interests include computer vision and machine learning , especially object recognition . [ ] song bai received the b . s . and ph . d . degree in electronics and information engineering from hua ##zh ##ong university of science and technology ( hu ##st ) , wu ##han , china in 2013 and 2018 , respectively . he was with university of texas at san antonio ( ut ##sa ) and johns hopkins university ( j ##hu ) as a research scholar . his research interests include image retrieval and classification , 3d shape recognition , person re - identification , semantic segment ##ation and deep learning . more information can be found in his home ##page : . [ ] wei shen received his b . s . and ph . d . degree both in electronics and information engineering from the hua ##zh ##ong university of science and technology ( hu ##st ) , wu ##han , china , in 2007 and in 2012 . from april 2011 to november 2011 , he worked in microsoft research asia as an intern . in 2012 , he joined school of communication and information engineering , shanghai university as an assistant professor . from 2017 , he became an associate professor . he is currently visiting department of computer science , johns hopkins university . his current research interests include random forests , deep learning , object detection and segment ##ation . [ ] xiang bai received his b . s . , m . s . , and ph . d . degrees from the hua ##zh ##ong university of science and technology ( hu ##st ) , wu ##han , china , in 2003 , 2005 , and 2009 , respectively , all in electronics and information engineering . he is currently a professor with the school of electronic information and communications , hu ##st . he is also the vice - director of the national center of anti - counter ##feit ##ing technology , hu ##st . his research interests include object recognition , shape analysis , scene text recognition and intelligent systems . he serves as an associate editor for pattern recognition , pattern recognition letters , ne ##uro ##com ##put ##ing and frontiers of computer science . [ ] wen ##yu liu received the b . s . degree in computer science from ts ##ing ##hua university , beijing , china , in 1986 , and the m . s . and ph . d . degrees , both in electronics and information engineering , from hua ##zh ##ong university of science and technology ( hu ##st ) , wu ##han , china , in 1991 and 2001 , respectively . he is now a professor and associate dean of the school of electronic information and communications , hu ##st . his current research areas include computer vision , multimedia , and machine learning . he is a senior member of ieee . [ ] alan yu ##ille received the b . a . degree in mathematics from the university of cambridge in 1976 , and the ph . d . degree in theoretical physics from cambridge in 1980 . he then held a post - doctoral position with the physics department , university of texas , austin , and the institute for theoretical physics , santa barbara . he then became a research scientists with the artificial intelligence laboratory , mit , from 1982 to 1986 , and followed this with a faculty position in the division of applied sciences , harvard , from 1986 to 1995 , rising to the position of an associate professor . from 1995 to 2002 , he was a senior scientist with the smith - kettle ##well eye research institute in san francisco . from 2002 to 2016 , he was a full professor with the department of statistics , ucla , with joint appointments in psychology , computer science , and psychiatry . in 2016 , he became a bloomberg distinguished professor of cognitive science and computer science with johns hopkins university . he received the mar ##r prize and the helm ##hol ##tz prize .",
        "pred_seq": "pascal ##c [SEP] proposal learning [SEP] [SEP] weakly detection [SEP] [unused0] ms ##marks [SEP] proposal learning [SEP] [SEP] weakly detection [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "pascal voc"
                    ]
                ],
                "Method": [
                    [
                        "proposal cluster learning"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "weakly supervised object detection"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "mscoco benchmarks"
                    ]
                ],
                "Method": [
                    [
                        "proposal cluster learning"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "weakly supervised object detection"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "imagenet",
                        "image"
                    ]
                ],
                "Method": [
                    [
                        "pclobgens"
                    ]
                ],
                "Metric": [
                    [
                        "map",
                        "multiclass cross entropy loss eq",
                        "mean of ap",
                        "map05",
                        "map 5"
                    ]
                ],
                "Task": [
                    [
                        "weakly supervised object detection",
                        "wsod",
                        "object detection",
                        "detection",
                        "weakly supervised scenario",
                        "wsod problem",
                        "robust wsod"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "pascal voc",
                        "voc 2007",
                        "pascal voc 2007"
                    ]
                ],
                "Method": [
                    [
                        "pclobgens"
                    ]
                ],
                "Metric": [
                    [
                        "map",
                        "multiclass cross entropy loss eq",
                        "mean of ap",
                        "map05",
                        "map 5"
                    ]
                ],
                "Task": [
                    [
                        "weakly supervised object detection",
                        "wsod",
                        "object detection",
                        "detection",
                        "weakly supervised scenario",
                        "wsod problem",
                        "robust wsod"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "pascal voc",
                        "2012 datasets",
                        "pascal voc 2012",
                        "2012"
                    ]
                ],
                "Method": [
                    [
                        "pclobgens"
                    ]
                ],
                "Metric": [
                    [
                        "map",
                        "multiclass cross entropy loss eq",
                        "mean of ap",
                        "map05",
                        "map 5"
                    ]
                ],
                "Task": [
                    [
                        "weakly supervised object detection",
                        "wsod",
                        "object detection",
                        "detection",
                        "weakly supervised scenario",
                        "wsod problem",
                        "robust wsod"
                    ]
                ]
            }
        ]
    },
    "43": {
        "doctext": "document : aggregate channel features for multi - view face detection face detection has drawn much attention in recent decades since the seminal work by viola and jones . while many sub ##se ##que ##nce ##s have improved the work with more powerful learning algorithms , the feature representation used for face detection still ca n ' t meet the demand for effectively and efficiently handling faces with large appearance variance in the wild . to solve this bottle ##neck , we borrow the concept of channel features to the face detection domain , which extends the image channel to diverse types like gradient magnitude and oriented gradient his ##to ##gram ##s and therefore en ##codes rich information in a simple form . we adopt a novel variant called aggregate channel features , make a full exploration of feature design , and discover a multi - scale version of features with better performance . to deal with poses of faces in the wild , we propose a multi - view detection approach featuring score re - ranking and detection adjustment . following the learning pipeline ##s in viola - jones framework , the multi - view face detector using aggregate channel features shows competitive performance against state - of - the - art algorithms on af ##w and f ##dd ##b tests ##ets , while runs at 42 f ##ps on v ##ga images . section : introduction human face detection have long been one of the most fundamental problems in computer vision and human - computer interaction . in the past decade , the most influential work should be the face detection framework proposed by viola and jones . the viola - jones ( abbreviated as v ##j below ) framework uses rectangular ha ##ar - like features and learns the hypothesis using ada ##bo ##ost algorithm . combined with the attention ##al cascade structure , the v ##j detector achieved real - time face detection at that time . despite the great success of the v ##j detector , the performance is still far from satisfactory due to the large appearance variance of faces in un ##con ##stra ##ined settings . to handle faces in the wild , many sub ##se ##que ##nce ##s of v ##j framework merged . these methods mainly get the performance gains in two aspects , more complicated features and ( or ) more powerful learning algorithms . as the combination of boost ##ing and cascade has been proven to be quite effective in face detection , the bottle ##neck lies in the feature representation since complicated features adopted in the above literature ##s bring about limited performance gains at the cost of large computation cost . lately in another domain of pedestrian detection , a family of channel features has achieved record performances . channel features compute registered maps of the original images like gradient ##s and his ##to ##gram ##s of oriented gradient ##s and then extract features on these extended channels . the class ##ifier learning process follows the v ##j framework pipeline . in this paper , we adopt a variant of channel features called aggregate channel features , which are extracted directly as pixel values on sub ##sam ##pled channels . channel extension offers rich representation capacity , while simple feature form guarantees fast computation . with these two superior ##ities , the aggregate channel features break through the bottle ##neck in v ##j framework and have the potential to make great advance in face detection . as we mainly concentrate our efforts to the feature representation rather than learning algorithms in this paper , we not only just adopt the aggregate channel features in face detection , but also try to explore the full potential of this novel representation . to do so , we make a deep and all - round investigation into the specific feature parameters concerning channel types , feature pool size , sub ##sam ##pling method , feature scale and so on , which gives insights into the feature design and hopefully provides helpful guidelines for practitioners . through the deep exploration , we find that : 1 ) multi - scaling the feature representation further en ##rich ##es the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in lu ##v space , plus gradient magnitude channel and gradient his ##to ##gram ##s channels in r ##gb space show best result ; 3 ) multi - view detection is proven to be a good match with aggregate channel features as the representation naturally en ##codes the facial structure ( figure [ reference ] ) . although multi - view detection could effectively deal with diverse poses , additional issues come up as how to merge detection ##s output by separately trained sub ##view detectors , and how to deal with the offset ##s of location and scale between output detection ##s and ground - truth . we solve these problems by carefully designed post - processing including score re - ranking , detection merging and bound ##ing box adjustment . the detailed experimental exploration of aggregate channel features , along with our improvements on multi - view detection , leads to large performance gain in face detection in the wild . on two challenging face databases , af ##w and f ##dd ##b , the proposed multi - view face detector shows competitive performance against state - of - the - art detectors in both detection accuracy and speed . the remaining parts of this paper are organized as follows . section 2 rev ##isi ##ts related work in face detection . section 3 describes how we build the face detector using aggregate channel features . section 4 addresses problems concerning multi - view face detection . experimental results on af ##w and f ##dd ##b are shown in section 5 and we conclude the paper in section 6 . section : related work face detection has drawn much attention since the early time of computer vision . although many solutions had been put forward , it was not until viola and jones proposed their milestone work that face detection saw surprising progress in the past decades . the v ##j face detector features in three aspects : fast feature computation via integral image representation , class ##ifier learning using ada ##bo ##ost , and the attention ##al cascade structure . one main draw ##back of the v ##j framework is that the features have limited representation capacity , while the feature pool size is quite large to compensate for that . typically , in a detection window , the number of ha ##ar - like features is 160 , 000 . to address the problem , efforts are made in two directions . some focus on more complicated features like hog , surf . some aim to speed up the feature selection in a he ##uri ##stic way . however , the problem has n ' t been solved perfectly . in this paper , we mainly focus on the feature representation part and make a deep exploration into it , which is complementary to existing work on the learning algorithm and class ##ifier structure in the v ##j framework . recently channel features have been proposed and shown record performance in pedestrian detection . due to the channel extension to diverse types like gradient ##s and local his ##to ##gram ##s , the features show richer representation capacity for classification . however , the features are extracted as rectangular sums at various locations and scales which we believe leads to a redundant feature pool . during preparation of this paper , mathias independently discover the effectiveness of integral channel features in face detection domain . in this paper , we adopt a novel variant of channel features called aggregate channel features , which extract features directly as pixel values in extended channels without computing rectangular sums at various locations and scales . the feature has powerful representation capacity and the feature pool size is only several thousands . through careful design in section 3 and implementation of multi - view detection in section 4 , the aggregate channel features based detector achieve ##s state - of - the - art performance on challenging databases . section : proposed face detector in this section , we make a full exploration of the aggregate channel features in the context of face detection . we first give a brief introduction of the feature itself , including its computation , properties and advantages over traditional ha ##ar - like features used in v ##j framework . then the detailed experimental investigation is described in two parts , feature design and training design . before that , some guidelines concerning how we conduct the investigation are demonstrated . each design part is divided into several separate experiments ended with a summary explaining the specific parameters used in our proposed face detector . note that each experiment focuses on only one parameter and the others remain constant . through the well - designed experiments , the proposed face detector based on aggregate channel features is built step by step . issues concerning the implementation of multi - view face detection which further improves the performance are discussed in the next section . sub ##section : feature description channel extension : the basic structure of the aggregate channel features is channel . the application of channel has a long history since digital images were invented . the most common type of channel should be the color channels of the image , with gray - scale and r ##gb being typical ones . besides color channels , many different channel types have been invented to en ##code different types of information for more difficult problems . generally , channels can be defined as a registered map of the original image , whose pixels are computed from corresponding patches of original pixels . different channels can be computed with linear or non - linear transformation of the original image . to allow for sliding window detection , the transformations are constrained to be translation ##ally invariant . feature computation : based on the definition of channels , the computation of aggregate channel features is quite simple . as shown in figure [ reference ] , given a color image , all defined channels are computed and sub ##sam ##pled by a pre - set factor . the aggregate pixels in all sub ##sam ##pled channels are then vector ##ized into a pixel look - up table . note that an optional smoothing procedure can be done on each channel with a bin ##omi ##al filter both before computation and after sub ##sam ##pling . class ##ifier learning : the learning process is quite simple . two changes are made compared with v ##j framework . first is that weak class ##ifier is changed from decision stump to depth - 2 decision tree . the more complex weak class ##ifier shows stronger ability in seeking the disc ##rim ##ina ##nt intra and inter channel correlation ##s for classification . second difference is that soft - cascade structure is used . unlike the attention ##al cascade structure in v ##j framework which has several cascade stages , a single - stage class ##ifier is trained on the whole training data and a threshold is then set after each weak class ##ifier picked by ada ##bo ##ost . these two changes lead to more efficient training and detection . overall superiority : compared with traditional ha ##ar - like features used in v ##j framework , aggregate channel features have the following differences and advantages : 1 ) the image channels are extended to more types in order to en ##code diverse information like color , gradient ##s , local his ##to ##gram ##s and so on , therefore possess richer representation capacity . 2 ) features are extracted directly as pixel values on downs ##amp ##led channels rather than computing rectangular sums with various locations and scales using integral images , leading to a faster feature computation and smaller feature pool size for boost ##ing learning . with the help of cascade structure , detection speed is accelerated more . 3 ) due to its structure consist ##ence with the overall image , when coupled with boost ##ing method , the boosted class ##ifier naturally en ##codes structured pattern information from large training data ( see figure [ reference ] for an illustration ) , which gives more accurate local ##ization of faces in the image . sub ##section : investigation guidelines all investigations are trained on the afl ##w face database and tested on the ann ##ota ##ted faces in the wild ( af ##w ) tests ##et . to make it clear , there are in total positive samples and negative samples selected from afl ##w which are kept constant in all investigations . tests ##et contains natural images with faces that vary a lot in pose , appearance and illumination . to alleviate the ground - truth offset caused by different ann ##ota ##tion styles ( figure [ reference ] ) in training and testing set and make the evaluation more comparable , a lower ja ##cca ##rd index with threshold is adopted in comparative evaluation . practically the lower threshold wo n ' t cause errors being mistakenly corrected . note that in final evaluation of the proposed face detector ( section 5 ) , the af ##w tests ##et , together with another face bench ##mark f ##dd ##b database , are used as test ##bed and the evaluation metric follows the database protocol . sub ##section : feature design to fully exploit the power of aggregate channel features in face detection domain , a deep investigation into the design of the feature is done mainly on channel types , window size , sub ##sam ##pling method and feature scale . results of comparative experiments are shown in figure [ reference ] . channel types : three types of channels are used , which are color channel ( gray - scale , r ##gb , hs ##v and lu ##v ) , gradient magnitude , and gradient his ##to ##gram ##s . the computation of the latter two channel types could be seen as a generalized version of hog features . specifically , gradient magnitude is the biggest response on all three color channels , and oriented gradient his ##to ##gram ##s follow the idea of hog in that : 1 ) rectangular cell size in hog equals the sub ##sam ##pling factor in aggregate ##d channel features ; 2 ) each orientation bin results in one feature channel ( 6 orientation bin ##s are used in this paper ) . figure [ reference ] ( a ) ~ ( c ) show how much each of these three types alone contributes to the performance of face detection . it can be seen that the gradient his ##to ##gram ##s contribute most to the performance among all three channel types . figure [ reference ] ( d ) shows the performances of combinations of these three types computed on different color channels . detection window size : detection window size is the scale to which we res ##ize all face and non - face samples and then train our detector . larger window size includes more pixels in feature pool and thus may improve the face detection performance . on the other hand , too large window will miss some small faces and dim ##ini ##sh the detection efficiency . figure [ reference ] ( e ) shows comparison of window size ranging from to with a stride of pixels . sub ##sam ##pling : the factor for sub ##sam ##pling can be regarded as the per ##ceptive scale for that it controls the scale at which the aggregation is done . changing the factor from large to small leads to the feature representation shifting from coarse to fine and the feature pool size getting bigger . experiments on different sub ##sam ##pling factors are shown in figure [ reference ] ( f ) . in original aggregate channel features , the way to do sub ##sam ##pling is average pool ##ing . following the idea in con ##vo ##lu ##tion ##al neural networks , another two ways of sub ##sam ##pling , max pool ##ing and st ##och ##astic pool ##ing are tested in figure [ reference ] ( g ) . smoothing : as described in feature description , both pre and post smoothing is done in default setting of aggregate channel features . a bin ##omi ##al filter with a radius of is used for smoothing . the smoothing procedure also has a great influence on the scale of the feature representation . concrete ##ly , pre - smoothing determines how far the local neighborhood is in which local correlation ##s are encoded before channel computation , while post - smoothing determines the neighborhood size in which the computed channel features are integrated with each other . in , the former corresponds to the ' local scale ' of the feature , while the latter represents the ' integration scale ' . we vary the filter radius used in pre and post smoothing and find that both using a radius of gets the best results . figure [ reference ] ( h ) ~ ( i ) present the comparative results . multi - scale : in aggregate channel features , although hidden information at different scale could be extracted at a cost of more weak class ##ifiers , it would be better to make the integrated channel features multi - scaled and thus make themselves more disc ##rim ##ina ##nt . therefore the same or better classification performance can be achieved with fewer weak class ##ifiers . in this part , we implement three multi - scale version of aggregate channel features in the aforementioned three kinds of scale , per ##ceptive scale ( sub ##sam ##pling ) , local scale ( pre - smoothing ) and integration scale ( post - smoothing ) and compare their perform ##ace ##s . see results in figure [ reference ] ( j ) ~ ( l ) . summary : the color channel , gradient magnitude and gradient his ##to ##gram ##s prove themselves a good match in aggregate channel features . however , different choices of color channel used and on which gradient ##s are computed have a great impact on performance . according to the experiments , lu ##v channel and gradient magnitude and 6 - bin his ##to ##gram ##s computed on r ##gb color space ( in total 10 channels ) are the best choice for face detection . larger detection window size generally gets better performance , but will miss many small faces in testing and lead to in ##ef ##fi ##cie ##nt detection . in this work , we set the size to as its optimal performance . a sub ##sam ##pling factor of is most reasonable according to the experiments , while different pool ##ing methods show small differences . however , max pool ##ing and st ##och ##astic pool ##ing are much slower than average pool ##ing , therefore the average pool ##ing becomes the best match for the sake of efficiency . in this way , the resulting feature pool size of our face detector is , considerably smaller than that in v ##j framework . as for multi - scale version of aggregate channel features , multi - local - scale with an additional scale of radius shows the best performance . the probable reason is that pre - smoothing controls the local scale of the neighborhood feature correlation ##s and therefore matches the intuition inside multi - scale best . compared with other fine - tuning , the multi - scale version has a notable performance gain for that it makes up for the scale uniform ##ity caused by sub ##sam ##pling to some extent . one main draw ##back is that it doubles the feature pool size and as a result slow ##s down the detection speed somewhat . based on the trade - off , we implement two face detectors with different scale settings , one is single - scaled with faster speed and the other is multi - scaled with better accuracy . we evaluate and discuss the performances of these two versions in detail in section 5 . sub ##section : training design besides careful design of the aggregate channel features , experiments on the training process which is similar to that in v ##j framework are also carried out . the differences are that the weak class ##ifier is changed into depth - 2 decision tree and soft - cascade structure is used . details of the training design are as follows . number of weak class ##ifiers : given a feature pool size of , we vary the number of weak class ##ifiers contained in the soft - cascade . in figure [ reference ] performances of various numbers of weak class ##ifiers ranging from to are displayed , which shows that apparently more class ##ifiers generate better performance , and when the number gets larger the performance begins to sat ##ura ##te . since more class ##ifiers slow down the detection speed , there ' s a trade - off between accuracy and speed . searching for the sat ##ura ##te point as the optimal is significant during training in such framework . training data : empirical ##ly , more training data will get better performance given powerful representation capacity . in this case , afl ##w database is used as the only positive training data . however , as images in afl ##w database are very sal ##ient and the background has very less variance , negative samples crop ##ped from the afl ##w database ca n ' t represent the real world scenario well , which limits the face detection performance in the wild . in this part , we further use pascal vo ##c database and randomly crop windows from images without person as the new negative samples . experiments show that the new training data containing cl ##uttered background significantly improve the performance with . summary : based on observations above , we choose as the number of weak class ##ifiers contained in the soft cascade . as each weak class ##ifier is a depth - 2 decision tree , it takes only two comparing operations to apply a weak class ##ifier , which is quite fast . during training , as negative data is large , we adopt a standard boots ##tra ##p procedure to sample hard negative samples from pascal vo ##c in the implementation of the proposed face detector . section : multi - view detection human faces in real world usually have highly varied poses . in afl ##w database , the human pose is divided into three aspects : 1 in - plane rotation ' roll ' and 2 out - of - plane rotation ##s ' ya ##w ' and ' pitch ' . because of this large variance in face pose , it is difficult to train a single view face detector to handle all the poses effectively . a multi - view detection is further examined in this part . due to the adoption of soft - cascade structure , a multi - view version of face detector wo n ' t cause too much computation burden . typically , we divide the out - of - plane rotation \u00a1 \u00b0 ya ##w \u00a1 \u00b1 into different views and let the class ##ifier itself tolerate the pose variance in the other two types of rotation ##s . adopting multi - view detection also brings about many troubles ##ome issues . if handled improper ##ly , the performance will differ greatly . first , detectors of different view will each produce a set of candidate positive windows followed with a set of confidence scores . for application purpose , we need to merge these detection ##s from different views and also remove duplicate ##d windows . a typical approach is non - maximum suppression ( nm ##s ) . an issue rises on how to compare confidence scores from different class ##ifiers and how to do window merging in the trade - off between high precision rate and high detection rate . second , as for detection evaluation , usually the overlap of bound ##ing boxes is used as the criterion . however , ann ##ota ##tions in different data sets may not have a consistent style ( figure [ reference ] ( a ) ) . this diversity suffers more in profile faces . since our face detector is trained and tested on different data sets , this issue impacts the performance a lot . third , detectors of different views need to be trained with different samples separately . how to divide the views therefore becomes another concerning problem . in this section , we address the above three issues successfully by careful designs and therefore fully exploit the advantage of multi - view detection . sub ##section : view partition in the scenario of detecting faces in the wild , pose variation caused by ya ##w is usually severe ##r than pitch and roll . therefore we divide the faces in afl ##w database according to ya ##w angle . we have sub ##view ##s which are horizontally symmetric ( see figure . [ reference ] ( b ) ) because we flip each image in the training set . specifically , there are , , , , , images in views from to . benefit ##ting from the symmetry of our model , we can only train three sub ##view detectors of the right side for simplicity , and use these trained right - side detectors to generate the left - side detectors . detection ##s of all six detectors are then merged to get the final detection ##s . though multi - view detection significantly improves the detection performance ( especially the recall rate ) , the post - processing of detection ##s from different detectors becomes a trouble . if handled improper ##ly , the performance de ##grade ##s a lot . sub ##section : post - processing difficulties in the post - processing of multi - view detection mainly reflect on the following aspects : 1 ) different score distributions and ; 2 ) different bound ##ing box styles . concrete ##ly , as each sub ##view detector is trained separately , their output confidence scores usually have different distributions . what ' s more , due to the ann ##ota ##tion rule in the afl ##w database that the face ' s nose is approximately at the center location of the bound ##ing box ground - truth , as the sub ##view changes , the bound ##ing box shifts . this bound ##ing box offset causes difficulty both in detection merging and final evaluation using ja ##cca ##rd index metric . to solve these annoying issues and make the best use of multi - view detection , we introduce the following methods for post - processing . score re - ranking : we propose the following three kinds of score re - ranking : 1 ) normal ##izing scores of different views to [ 0 , 1 ] ; 2 ) defining a new score that has uniform distribution and ; 3 ) taking overlapping detection ##s into consideration . : after training a class ##ifier , calculate the output range of the class ##ifier and use the range to do normal ##ization later so that output score has a range of [ 0 , 1 ] . : originally , each weak class ##ifier in the soft - cascade owns a score and final score is the sum of all scores . instead , we use the number of weak class ##ifier that the image patch passed positively as the new score . therefore the upper limit of the new score is in our case . : given an image , multiple detection ##s from multi - view detectors exist each with a score . for each detection , we first calculate the number of overlap ##ped detection it has ( overlap threshold is ) and then multi ##ply score of each detection with a factor of its overlapping number ranking . : instead of using overlapping as a multi ##ply factor , here we use the sum of overlap ##ped detection ##s ' scores as the current detection ' s new score . detection merging : apart from the version of non maximum suppression , we also use the detection combination introduced in . it averages the locations of overlap ##ped detection ##s rather than suppress ##es them . detection adjustment : as shown in figure [ reference ] ( a ) , different databases have different ann ##ota ##tion styles of ground - truth . specifically , afl ##w has square ann ##ota ##tions with nose located approximately at the center . af ##w uses tight rectangular bound ##ing boxes as ann ##ota ##tions with the eye - brow being the approximate upper bound . f ##dd ##b uses elliptical ann ##ota ##tions bound ##ing the whole head . as our detector is trained on afl ##w and tested on af ##w and f ##dd ##b , there exist offset ##s in both detection position and scale . according to observations , the offset ##s vary as face pose changes . therefore we adopt a view - specific detection adjustment to alleviate the offset ##s . note that the adjustment is constant for all images and faces in the same database , see figure [ reference ] ( b ) for details . summary : according to experimental results ( table [ reference ] ) , seems to be the best score re - ranking method . the underlying reason may be that true positive ##s usually have many overlap ##ped detection ##s , while the false positive ##s would only get a few responses . therefore lever ##aging this overlapping information in score re - ranking can reduce many false positive ##s . however , in practice , overlap related methods and detection combination both cost much time to process , which is in ##fe ##asi ##ble in a large majority of applications . we finally adopt score re - ranking combined with non maximum suppression for the sake of detection speed . section : experiments in this section , we compare our method with state - of - the - art methods on af ##w and f ##dd ##b databases which contain challenging faces in the wild . in af ##w , we compare with three commercial systems ( google pic ##asa , face . com and face + + ) and five academic methods ( shen , zhu , d ##pm , multi ##hog and kala ##l ) . in f ##dd ##b , we compare with one commercial system ( ol ##aw ##or ##ks ) and six academic methods ( yan , boosted ex ##em ##pl ##ar , surf multi ##view , pep - adapt , x ##z ##j ##y and zhu ) listed on f ##dd ##b results page . sub ##section : evaluation on bench ##mark face database as shown in figure [ reference ] , in af ##w , our multi - scale detector achieve ##s an ap value of , out ##per ##form ##ing other academic methods by a large margin . when it comes to commercial systems , ours is better than face . com and almost equal to face + + and google pic ##asa . note that most of our false positive ##s on af ##w database are faces that have n ' t been ann ##ota ##ted ( small , seriously o ##cc ##lu ##ded or artificial faces like mask and cartoon character ) . when evaluated on f ##dd ##b database , we follow the evaluation protocol in and report the average discrete and continuous roc of the ten sub ##fold ##ers . for equality , we fix the number of false positive ##s to ( equivalent to an average of false positive per image ) and compare the true positive rate . in discrete score where evaluation metric is the same as in af ##w , our detector achieve ##s , which is a little better than yan . note that the ground - truth in f ##dd ##b are elliptical faces , therefore the evaluation metric of an overlap ratio bigger than can not reveal the true performance of the proposed detector well . when using continuous score which takes the overlap ratio as the score , our method gets true positive rate at f ##pp ##i for multi - scale version , surpassing other methods which output rectangular detection ##s by a notable margin ( the yan detector outputs the same elliptical detection ##s as the ground - truth , therefore having advantages with this metric ) . our detector using single - scale features performs a little worse with the benefit of faster detection speed . sub ##section : discussion training efficiency : we implement the method with pi ##ot ##r ' s mat ##lab tool ##box on a pc with intel core i ##7 - 37 ##70 cpu and 16 gb ram . with positive images and negative images in total 6 views , the training process takes about min ##s for a single - scale sub ##view detector containing weak class ##ifiers and min ##s for multi - scale version . note that we use much fewer training data than surf multi ##view whilst still out ##per ##form ##ing their performance . comparative results : when inspecting detection ##s of the proposed face detector and other algorithms on the tests ##ets , some patterns can be found to explain why our detector out ##per ##forms others . one evident strength lies in detecting faces with extreme poses . because we adopt multi - view detection and train each sub ##view detector separately , our detector handles pose variations very well . second is the outstanding illumination in ##var ##iance of our detector , which is mainly owing to the extension of channel types to lu ##v color space and gradient - related channels . detection speed : due to the simple form of aggregate channel features and fast computation of feature pyramid , detection is quite efficient . for full ya ##w pose face detection in v ##ga image , the proposed detector using single - scale features runs at f ##ps on a single thread and f ##ps if threads are used . if only frontal faces are concerned , the detector runs at f ##ps and f ##ps after parallel ##ization . when it comes to the proposed detector using multi - scale features , the above four indices reduce to , , and f ##ps . considering the large performance gain and similar speed , the proposed method can replace viola - jones detector for face detection in the wild . section : conclusion a novel feature representation called aggregate channel features possesses the merits of fast feature extraction and powerful representation capacity . in this paper , we successfully apply the feature representation to face detection domain through a deep investigation into the feature design , and propose a multi - scale version of feature which further en ##rich ##es the representation capacity . combined with our efforts into solving issues concerning multi - view detection , the proposed multi - view face detector shows state - of - the - art performance in both effectiveness and efficiency on faces in the wild . the proposed method appeals to real world application demands and has the potential to be embedded into low power devices . section : acknowledge ##ment this work was supported by the chinese national natural science foundation projects # 61 ##10 ##50 ##23 , # 61 ##10 ##31 ##56 , # 61 ##10 ##50 ##37 , # 61 ##20 ##32 ##6 ##7 , # 61 ##37 ##50 ##37 , national science and technology support program project # 2013 ##ba ##k ##0 ##2 ##b ##01 , chinese academy of sciences project no . kg ##zd - e ##w - 102 - 2 , and au ##the ##n ##metric r & d funds . bibliography : references",
        "pred_seq": "af ##w [SEP] [SEP] [SEP] multi detection [SEP] [unused0] af ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w ##w [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "afw"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "multiview face detection"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "afw"
                    ]
                ],
                "Metric": [
                    [
                        "representation capacity",
                        "feature pool size",
                        "classification performance",
                        "false positives"
                    ]
                ],
                "Task": [
                    [
                        "multiview face detection"
                    ]
                ]
            }
        ]
    },
    "44": {
        "doctext": "document : training region - based object detectors with online hard example mining the field of object detection has made significant advances riding on the wave of region - based con ##vn ##ets , but their training procedure still includes many he ##uri ##stic ##s and hyper ##para ##meter ##s that are costly to tune . we present a simple yet surprisingly effective online hard example mining ( oh ##em ) algorithm for training region - based con ##vn ##et detectors . our motivation is the same as it has always been - detection data ##set ##s contain an overwhelming number of easy examples and a small number of hard examples . automatic selection of these hard examples can make training more effective and efficient . oh ##em is a simple and intuitive algorithm that eliminate ##s several he ##uri ##stic ##s and hyper ##para ##meter ##s in common use . but more importantly , it yields consistent and significant boost ##s in detection performance on bench ##marks like pascal vo ##c 2007 and 2012 . its effectiveness increases as data ##set ##s become larger and more difficult , as demonstrated by the results on the ms coco data ##set . moreover , combined with complementary advances in the field , oh ##em leads to state - of - the - art results of 78 . 9 % and 76 . 3 % map on pascal vo ##c 2007 and 2012 respectively . section : introduction image classification and object detection are two fundamental computer vision tasks . object detectors are often trained through a reduction that converts object detection into an image classification problem . this reduction introduces a new challenge that is not found in natural image classification tasks : the training set is distinguished by a large im ##balance between the number of ann ##ota ##ted objects and the number of background examples ( image regions not belonging to any object class of interest ) . in the case of sliding - window object detectors , such as the def ##or ##mable parts model ( d ##pm ) , this im ##balance may be as extreme as 100 , 000 background examples to every one object . the recent trend towards object - proposal - based detectors mit ##igate ##s this issue to an extent , but the im ##balance ratio may still be high ( , 70 : 1 ) . this challenge opens space for learning techniques that cope with im ##balance and yield faster training , higher accuracy , or both . un ##sur ##pr ##ising ##ly , this is not a new challenge and a standard solution , originally called boots ##tra ##pping ( and now often called hard negative mining ) , has existed for at least 20 years . boots ##tra ##pping was introduced in the work of sung and po ##ggio in the mid - 1990 ' s ( if not earlier ) for training face detection models . their key idea was to gradually grow , or boots ##tra ##p , the set of background examples by selecting those examples for which the detector triggers a false alarm . this strategy leads to an it ##erative training algorithm that alternate ##s between up ##dating the detection model given the current set of examples , and then using the updated model to find new false positive ##s to add to the boots ##tra ##pped training set . the process typically commence ##s with a training set consisting of all object examples and a small , random set of background examples . boots ##tra ##pping has seen widespread use in the intervening decades of object detection research . dal ##al and tri ##ggs used it when training sv ##ms for pedestrian detection . fe ##lz ##ens ##z ##wal ##b later proved that a form of boots ##tra ##pping for sv ##ms converge ##s to the global optimal solution defined on the entire data ##set . their algorithm is often referred to as hard negative mining and is frequently used when training sv ##ms for object detection . boots ##tra ##pping was also successfully applied to a variety of other learning models , including shallow neural networks and boosted decision trees . even modern detection methods based on deep con ##vo ##lu ##tion ##al neural networks ( con ##vn ##ets ) , such as r - cnn and spp ##net , still employ sv ##ms trained with hard negative mining . it may seem odd then that the current state - of - the - art object detectors , embodied by fast r - cnn and its descendants , do not use boots ##tra ##pping . the underlying reason is a technical difficulty brought on by the shift towards purely online learning algorithms , particularly in the context of deep con ##vn ##ets trained with st ##och ##astic gradient descent ( sg ##d ) on millions of examples . boots ##tra ##pping , and its variants in the literature , rely on the aforementioned alter ##nation template : ( a ) for some period of time a fixed model is used to find new examples to add to the active training set ; ( b ) then , for some period of time the model is trained on the fixed active training set . training deep con ##vn ##et detectors with sg ##d typically requires hundreds of thousands of sg ##d steps and freezing the model for even a few iteration ##s at a time would dramatically slow progress . what is needed , instead , is a purely online form of hard example selection . in this paper , we propose a novel boots ##tra ##pping technique called online hard example mining ( oh ##em ) for training state - of - the - art detection models based on deep con ##vn ##ets . the algorithm is a simple modification to sg ##d in which training examples are sampled according to a non - uniform , non - stationary distribution that depends on the current loss of each example under consideration . the method takes advantage of detection - specific problem structure in which each sg ##d mini - batch consists of only one or two images , but thousands of candidate examples . the candidate examples are sub ##sam ##pled according to a distribution that favors diverse , high loss instances . gradient computation ( back ##pro ##pa ##gation ) is still efficient because it only uses a small subset of all candidates . we apply oh ##em to the standard fast r - cnn detection method and show three benefits compared to the baseline training algorithm : it removes the need for several he ##uri ##stic ##s and hyper ##para ##meter ##s commonly used in region - based con ##vn ##ets . it yields a consistent and significant boost ##s in mean average precision . its effectiveness increases as the training set becomes larger and more difficult , as demonstrated by results on the ms coco data ##set . moreover , the gains from oh ##em are complementary to recent improvements in object detection , such as multi - scale testing and it ##erative bound ##ing - box regression . combined with these tricks , oh ##em gives state - of - the - art results of 78 . 9 % and 76 . 3 % map on pascal vo ##c 2007 and 2012 , respectively . section : related work object detection is one of the oldest and most fundamental problems in computer vision . the idea of data ##set boots ##tra ##pping , typically called hard negative mining in recent work , appears in the training of most successful object detectors . many of these approaches use sv ##ms as the detection scoring function , even after training a deep con ##vo ##lu ##tion ##al neural network ( con ##vn ##et ) for feature extraction . one notable exception is the fast r - cnn detector and its descendants , such as faster r - cnn . since these models do not use sv ##ms , and are trained purely online with sg ##d , existing hard example mining techniques can not be immediately applied . this work addresses that problem by introducing an online hard example mining algorithm that improves optimization and detection accuracy . we briefly review hard example mining , modern con ##vn ##et - based object detection , and relationships to concurrent works using hard example selection for training deep networks . paragraph : hard example mining . there are two hard example mining algorithms in common use . the first is used when opt ##imi ##zing sv ##ms . in this case , the training algorithm maintains a working set of examples and alternate ##s between training an sv ##m to convergence on the working set , and up ##dating the working set by removing some examples and adding others according to a specific rule . the rule removes examples that are \" easy \" in the sense that they are correctly classified beyond the current model ' s margin . conversely , the rule adds new examples that are hard in the sense that they violate the current model ' s margin . applying this rule leads to the global sv ##m solution . importantly , the working set is usually a small subset of the entire training set . the second method is used for non - sv ##ms and has been applied to a variety of models including shallow neural networks and boosted decision trees . this algorithm usually starts with a data ##set of positive examples and a random set of negative examples . the machine learning model is then trained to convergence on that data ##set and subsequently applied to a larger data ##set to harvest false positive ##s . the false positive ##s are then added to the training set and then the model is trained again . this process is usually it ##erated only once and does not have any convergence proof ##s . paragraph : con ##vn ##et - based object detection . in the last three years significant gains have been made in object detection . these improvements were made possible by the successful application of deep con ##vn ##ets to image ##net classification . the r - cnn and over ##fe ##at detectors lead this wave with impressive results on pascal vo ##c and image ##net detection . over ##fe ##at is based on the sliding - window detection method , which is perhaps the most intuitive and oldest search method for detection . r - cnn , in contrast , uses region proposals , a method that was made popular by the selective search algorithm . since r - cnn , there has been rapid progress in region - based con ##vn ##ets , including spp ##net , mr - cnn , and fast r - cnn , which our work builds on . paragraph : hard example selection in deep learning . there is recent work concurrent to our own that selects hard examples for training deep networks . similar to our approach , all these methods base their selection on the current loss for each data ##point . independently selects hard positive and negative example from a larger set of random examples based on their loss to learn image des ##cript ##ors . given a positive pair of patches , finds hard negative patches from a large set using triple ##t loss . akin to our approach , investigates online selection of hard examples for mini - batch sg ##d methods . their selection is also based on loss , but the focus is on con ##vn ##ets for image classification . complementary to , we focus on online hard example selection strategy for region - based object detectors . section : overview of fast r - cnn we first sum ##mar ##ize the fast r - cnn ( fr ##c ##n ) framework . fr ##c ##n takes as input an image and a set of object proposal regions of interest ( roi ##s ) . the fr ##c ##n network itself can be divided into two sequential parts : a con ##vo ##lu ##tion ##al ( con ##v ) network with several con ##vo ##lu ##tion and max - pool ##ing layers ( figure [ reference ] , \" con ##vo ##lu ##tion ##al network \" ) ; and an roi network with an roi - pool ##ing layer , several fully - connected ( fc ) layers and two loss layers ( figure [ reference ] , \" roi network \" ) . during inference , the con ##v network is applied to the given image to produce a con ##v feature map , size of which depends on the input image dimensions . then , for each object proposal , the roi - pool ##ing layer projects the proposal onto the con ##v feature map and extracts a fixed - length feature vector . each feature vector is fed into the fc layers , which finally give two outputs : ( 1 ) a soft ##max probability distribution over the object classes and background ; and ( 2 ) reg ##ressed coordinates for bound ##ing - box re ##lo ##cal ##ization . there are several reasons for choosing fr ##c ##n as our base object detector , apart from it being a fast end - to - end system . firstly , the basic two network setup ( con ##v and roi ) is also used by other recent detectors like spp ##net and mr - cnn ; therefore , our proposed algorithm is more broadly applicable . secondly , though the basic setup is similar , fr ##c ##n also allows for training the entire con ##v network , as opposed to both spp ##net and mr - cnn which keep the con ##v network fixed . and finally , both spp ##net and mr - cnn require features from the roi network to be cache ##d for training a separate sv ##m class ##ifier ( using hard negative mining ) . fr ##c ##n uses the roi network itself to train the desired class ##ifiers . in fact , shows that in the unified system using the sv ##m class ##ifiers at later stages was unnecessary . sub ##section : training like most deep networks , fr ##c ##n is trained using st ##och ##astic gradient descent ( sg ##d ) . the loss per example roi is the sum of a classification log loss that encourages predicting the correct object ( or background ) label and a local ##ization loss that encourages predicting an accurate bound ##ing box ( see for details ) . to share con ##v network computation between roi ##s , sg ##d mini - batch ##es are created hierarchical ##ly . for each mini - batch , images are first sampled from the data ##set , and then roi ##s are sampled from each image . setting and works well in practice . the roi sampling procedure uses several he ##uri ##stic ##s , which we describe briefly below . one contribution of this paper is to eliminate some of these he ##uri ##stic ##s and their hyper ##para ##meter ##s . paragraph : fore ##ground roi ##s . for an example roi to be labeled as fore ##ground ( f ##g ) , its intersection over union ( io ##u ) overlap with a ground - truth bound ##ing box should be at least . this is a fairly standard design choice , in part inspired by the evaluation protocol of the pascal vo ##c object detection bench ##mark . the same criterion is used in the sv ##m hard mining procedures of r - cnn , spp ##net , and mr - cnn . we use the same setting . paragraph : background roi ##s . a region is labeled background ( b ##g ) if its maximum io ##u with ground truth is in the interval b ##g _ lo , 0 . 5 ) . a lower threshold of b ##g _ lo is used by both fr ##c ##n and spp ##net , and is h ##yp ##oth ##es ##ized in to crude ##ly approximate hard negative mining ; the assumption is that regions with some overlap with the ground truth are more likely to be the confusing or hard ones . we show in section [ reference ] that although this he ##uri ##stic helps convergence and detection accuracy , it is sub ##op ##ti ##mal because it ignores some in ##fr ##e ##quent , but important , difficult background regions . our method removes the b ##g _ lo threshold . paragraph : balancing f ##g - b ##g roi ##s : to handle the data im ##balance described in section [ reference ] , designed he ##uri ##stic ##s to re ##balance the fore ##ground - to - background ratio in each mini - batch to a target of by under ##sam ##pling the background patches at random , thus ensuring that of a mini - batch is f ##g roi ##s . we found that this is an important design decision for the training fr ##c ##n . removing this ratio ( randomly sampling roi ##s ) , or increasing it , decreases accuracy by points map . with our proposed method , we can remove this ratio hyper ##para ##meter with no ill effect . section : our approach we propose a simple yet effective online hard example mining algorithm for training fast r - cnn ( or any fast r - cnn style object detector ) . we argue that the current way of creating mini - batch ##es for sg ##d ( section [ reference ] ) is in ##ef ##fi ##cie ##nt and sub ##op ##ti ##mal , and we demonstrate that our approach leads to better training ( lower training loss ) and higher testing performance ( map ) . sub ##section : online hard example mining recall the alternating steps that define a hard example mining algorithm : ( a ) for some period of time a fixed model is used to find new examples to add to the active training set ; ( b ) then , for some period of time the model is trained on the fixed active training set . in the context of sv ##m - based object detectors , such as the sv ##ms trained in r - cnn or spp ##net , step ( a ) inspect ##s a variable number of images ( often 10 ' s or 100 ' s ) until the active training set reaches a threshold size , and then in step ( b ) the sv ##m is trained to convergence on the active training set . this process repeats until the active training set contains all support vectors . applying an analogous strategy to fr ##c ##n con ##vn ##et training slow ##s learning because no model updates are made while selecting examples from the 10 ' s or 100 ' s of images . our main observation is that these alternating steps can be combined with how fr ##c ##n is trained using online sg ##d . the key is that although each sg ##d iteration samples only a small number of images , each image contains thousands of example roi ##s from which we can select the hard examples rather than a he ##uri ##stic ##ally sampled subset . this strategy fits the alter ##nation template to sg ##d by \" freezing \" the model for only one mini - batch . thus the model is updated exactly as frequently as with the baseline sg ##d approach and therefore learning is not delayed . more specifically , the online hard example mining algorithm ( oh ##em ) proceeds as follows . for an input image at sg ##d iteration , we first compute a con ##v feature map using the con ##v network . then the roi network uses this feature map and the all the input roi ##s , instead of a sampled mini - batch , to do a forward pass . recall that this step only involves roi pool ##ing , a few fc layers , and loss computation for each roi . the loss represents how well the current network performs on each roi . hard examples are selected by sorting the input roi ##s by loss and taking the examples for which the current network performs worst . most of the forward computation is shared between roi ##s via the con ##v feature map , so the extra computation needed to forward all roi ##s is relatively small . moreover , because only a small number of roi ##s are selected for up ##dating the model , the backward pass is no more expensive than before . however , there is a small cave ##at : co - located roi ##s with high overlap are likely to have correlated losses . moreover , these overlapping roi ##s can project onto the same region in the con ##v feature map , because of resolution di ##spar ##ity , thus leading to loss double counting . to deal with these redundant and correlated regions , we use standard non - maximum suppression ( nm ##s ) to perform de ##du ##plication ( the implementation from ) . given a list of roi ##s and their losses , nm ##s works by it ##erative ##ly selecting the roi with the highest loss , and then removing all lower loss roi ##s that have high overlap with the selected region . we use a relaxed io ##u threshold of to suppress only highly overlapping roi ##s . we note that the procedure described above does not need a f ##g - b ##g ratio for data balancing . if any class were neglected , its loss would increase until it has a high probability of being sampled . there can be images where the f ##g roi ##s are easy ( canonical view of a car ) , so the network is free to use only b ##g regions in a mini - batch ; and vice - versa when b ##g is trivial ( sky , grass ) , the mini - batch can be entirely f ##g regions . sub ##section : implementation details there are many ways to implement oh ##em in the fr ##c ##n detector , each with different trade - offs . an obvious way is to modify the loss layers to do the hard example selection . the loss layer can compute loss for all roi ##s , sort them based on this loss to select hard roi ##s , and finally set the loss of all non - hard roi ##s to . though straightforward , this implementation is in ##ef ##fi ##cie ##nt as the roi network still all ##oca ##tes memory and performs backward pass for all roi ##s , even though most roi ##s have loss and hence no gradient updates ( a limitation of current deep learning tool ##box ##es ) . to overcome this , we propose the architecture presented in figure [ reference ] . our implementation maintains two copies of the roi network , one of which is read ##on ##ly . this implies that the read ##on ##ly roi network ( figure [ reference ] ( a ) ) all ##oca ##tes memory only for forward pass of all roi ##s as opposed to the standard roi network , which all ##oca ##tes memory for both forward and backward passes . for an sg ##d iteration , given the con ##v feature map , the read ##on ##ly roi network performs a forward pass and compute ##s loss for all input roi ##s ( figure [ reference ] , green arrows ) . then the hard roi sampling module uses the procedure described in section [ reference ] to select hard examples , which are input to the regular roi network ( figure [ reference ] ( b ) , red arrows ) ) . this network compute ##s forward and backward passes only for , accumulate ##s the gradient ##s and passes them to the con ##v network . in practice , we use all roi ##s from all images as , therefore the effective batch size for the read ##on ##ly roi network is and for the regular roi network is the standard from section [ reference ] . we implement both options described above using the caf ##fe framework ( see ) . our implementation uses gradient accumulation with forward - backward passes of single image mini - batch ##es . following fr ##c ##n , we use ( which results in ) and . under these settings , the proposed architecture ( figure [ reference ] ) has similar memory footprint as the first option , but is faster . unless specified otherwise , the architecture and settings described above will be used throughout this paper . section : analyzing online hard example mining this section compares fr ##c ##n training with online hard example mining ( oh ##em ) to the baseline he ##uri ##stic sampling approach . we also compare fr ##c ##n with oh ##em to a less efficient approach that uses all available example roi ##s in each mini - batch , not just the hardest examples . sub ##section : experimental setup we conduct experiments with two standard con ##vn ##et architecture ##s : v ##gg _ cnn _ m _ 102 ##4 ( v ##gg ##m , for short ) from , which is a wider version of alex ##net , and v ##gg ##16 from . all experiments in this section are performed on the pascal vo ##c ##0 ##7 data ##set . training is done on the train ##val set and testing on the test set . unless specified otherwise , we will use the default settings from fr ##c ##n . we train all methods with sg ##d for 80 ##k mini - batch iteration ##s , with an initial learning rate of 0 . 001 and we decay the learning rate by 0 . 1 every 30 ##k iteration ##s . the baseline numbers reported in table [ reference ] ( row 1 - 2 ) were reproduced using our training schedule and are slightly higher than the ones reported in . sub ##section : oh ##em vs . he ##uri ##stic sampling standard fr ##c ##n , reported in table [ reference ] ( rows ) , uses as a he ##uri ##stic for hard mining ( section [ reference ] ) . to test the importance of this he ##uri ##stic , we ran fr ##c ##n with . table [ reference ] ( rows ) shows that for v ##gg ##m , map drops by points , whereas for v ##gg ##16 it remains roughly the same . now compare this to training fr ##c ##n with oh ##em ( rows ) . oh ##em improves map by points compared to fr ##c ##n with the he ##uri ##stic for v ##gg ##m , and points without the he ##uri ##stic . this result demonstrates the sub - optimal ##ity of these he ##uri ##stic ##s and the effectiveness of our hard mining approach . sub ##section : robust gradient estimates one concern over using only images per batch is that it may cause unstable gradient ##s and slow convergence because roi ##s from an image may be highly correlated . fr ##c ##n reports that this was not a practical issue for their training . but this detail might raise concerns over our training procedure because we use examples with high loss from the same image and as a result they may be more highly correlated . to address this concern , we experiment with in order to increase correlation in an effort to break our method . as seen in table [ reference ] ( rows ) , performance of the original fr ##c ##n drops by point with , but when using our training procedure , map remains approximately the same . this shows that oh ##em is robust in case one needs fewer images per batch in order to reduce gp ##u memory usage . sub ##section : why just hard examples , when you can use all ? online hard example mining is based on the hypothesis that it is important to consider all roi ##s in an image and then select hard examples for training . but what if we train with all the roi ##s , not just the hard ones ? the easy examples will have low loss , and wo n ' t contribute much to the gradient ; training will automatically focus on the hard examples . to compare this option , we ran standard fr ##c ##n training with a large mini - batch size of , using , and with other hyper ##para ##meter ##s fixed . because this experiment uses a large mini - batch , it ' s important to tune the learning rate to adjust for this change . we found optimal results by increasing it to for v ##gg ##16 and for v ##gg ##m . the outcomes are reported in table [ reference ] ( rows ) . using these settings , map of both v ##gg ##16 and v ##gg ##m increased by point compared to , but the improvement from our approach is still points over using all roi ##s . moreover , because we compute gradient ##s with a smaller mini - batch size training is faster . removing hard mining he ##uri ##stic ( section [ reference ] ) fewer images per batch ( section [ reference ] ) bigger batch , high l ##r ( section [ reference ] ) our approach sub ##section : better optimization finally , we analyze the training loss for the various fr ##c ##n training methods discussed above . it ' s important to measure training loss in a way that does not depend on the sampling procedure and thus results in a valid comparison between methods . to achieve this goal , we take model snaps ##hot ##s from each method every 20 ##k steps of optimization and run them over the entire vo ##c ##0 ##7 train ##val set to compute the average loss over all roi ##s . this measures the training set loss in a way that does not depend on the example sampling scheme . figure [ reference ] shows the average loss per roi for v ##gg ##16 with the various hyper ##para ##meter settings discussed above and presented in table [ reference ] . we see that results in the highest training loss , while using the he ##uri ##stic results in a much lower training loss . increasing the mini - batch size to and increasing the learning rate lowers the training loss below the he ##uri ##stic . our proposed online hard example mining method achieve ##s the lowest training loss of all methods , valid ##ating our claims that oh ##em leads to better training for fr ##c ##n . * : uses gradient accumulation over two forward / backward passes sub ##section : computational cost oh ##em adds reasonable computational and memory overhead , as reported in table [ reference ] . oh ##em costs 0 . 09 ##s per training iteration for v ##gg ##m network ( 0 . 43 ##s for v ##gg ##16 ) and requires 1 g more memory ( 2 . 3 g for v ##gg ##16 ) . given that fr ##c ##n is a fast detector to train , the increase in training time is likely acceptable to most users . , 2 ##ht ##tp : / / host . robots . ox . ac . uk : 80 ##80 / anonymous / h ##49 ##pt ##t . html , 3 ##ht ##tp : / / host . robots . ox . ac . uk : 80 ##80 / anonymous / l ##sant ##b . html , 4 ##ht ##tp : / / host . robots . ox . ac . uk : 80 ##80 / anonymous / r ##7 ##ea ##mx . html section : pascal vo ##c and ms coco results in this section , we evaluate our method on vo ##c 2012 as well as the more challenging ms coco data ##set . we demonstrate consistent and significant improvement in fr ##c ##n performance when using the proposed oh ##em approach . per - class results are also presented on vo ##c 2007 for comparison with prior work . paragraph : experimental setup . we use v ##gg ##16 for all experiments . when training on vo ##c ##0 ##7 train ##val , we use the sg ##d parameters as in section [ reference ] and when using extra data ( 07 + 12 and 07 + + 12 , see table [ reference ] and [ reference ] ) , we use 200 ##k mini - batch iteration ##s , with an initial learning rate of 0 . 001 and decay step size of 40 ##k . when training on ms coco , we use 240 ##k mini - batch iteration ##s , with an initial learning rate of 0 . 001 and decay step size of 160 ##k , owing to a larger epoch size . sub ##section : vo ##c 2007 and 2012 results table [ reference ] shows that on vo ##c ##0 ##7 , oh ##em improves the map of fr ##c ##n from 67 . 2 % to 69 . 9 % ( and 70 . 0 % to 74 . 6 % with extra data ) . on vo ##c ##12 , oh ##em leads to an improvement of 4 . 1 points in map ( from 65 . 7 % to 69 . 8 % ) . with extra data , we achieve an map of 71 . 9 % as compared to 68 . 4 % map of fr ##c ##n , an improvement of 3 . 5 points . interesting ##ly the improvements are not uniform across categories . bottle , chair , and tv ##mon ##itor show larger improvements that are consistent across the different pascal splits . why these classes benefit the most is an interesting and open question . sub ##section : ms coco results to test the benefit of using oh ##em on a larger and more challenging data ##set , we conduct experiments on ms coco and report numbers from test - dev 2015 evaluation server ( table [ reference ] ) . on the standard coco evaluation metric , fr ##c ##n scores 19 . 7 % ap , and oh ##em improves it to 22 . 6 % ap . using the vo ##c overlap metric of , oh ##em gives a 6 . 6 points boost in ap . it is also interesting to note that oh ##em helps improve the ap of medium sized objects by 4 . 9 points on the strict coco ap evaluation metric , which indicates that the proposed hard example mining approach is helpful when dealing with smaller sized objects . note that fr ##c ##n with and without oh ##em were trained on ms coco train set . section : adding bells and whistle ##s we ' ve demonstrated consistent gains in detection accuracy by applying oh ##em to fr ##c ##n training . in this section , we show that these improvements are orthogonal to recent bells and whistle ##s that enhance object detection accuracy . oh ##em with the following two additions yields state - of - the - art results on vo ##c and competitive results on ms coco . paragraph : multi - scale ( m ) . we adopt the multi - scale strategy from spp ##net ( and used by both fr ##c ##n and mr - cnn ) . scale is defined as the size of the shortest side ( ) of an image . during training , one scale is chosen at random , whereas at test time inference is run on all scales . for v ##gg ##16 networks , we use for training , and during testing , with the max dimension capped at 1000 . the scales and caps were chosen because of gp ##u memory constraints . paragraph : it ##erative bound ##ing - box regression ( b ) . we adopt the it ##erative local ##ization and bound ##ing - box ( bb ##ox ) voting scheme from . the network evaluate ##s each proposal roi to get scores and re ##lo ##cal ##ized boxes . high - scoring boxes are the res ##core ##d and re ##lo ##cal ##ized , yielding boxes . union of and is used as the final set for post - processing , where is obtained using nm ##s on with an io ##u threshold of 0 . 3 and weighted voting is performed on each box in using boxes in with an io ##u of 0 . 5 with ( see for details ) . from the leader ##board , * trained on train ##val set sub ##section : vo ##c 2007 and 2012 results we report the results on vo ##c bench ##marks in table [ reference ] and [ reference ] . on vo ##c ##0 ##7 , fr ##c ##n with the above mentioned additions achieve ##s 72 . 4 % map and oh ##em improves it to 75 . 1 % , which is currently the highest reported score under this setting ( 07 data ) . when using extra data ( 07 + 12 ) , oh ##em achieve ##s 78 . 9 % map , surpassing the current state - of - the - art mr - cnn ( 78 . 2 % map ) . we note that mr - cnn uses selective search and edge boxes during training , whereas we only use selective search boxes . our multi - scale implementation is also different , using fewer scales than mr - cnn . on vo ##c ##12 ( table [ reference ] ) , we consistently perform better than mr - cnn . when using extra data , we achieve state - of - the - art map of 76 . 3 % ( 73 . 9 % map of mr - cnn ) . paragraph : ab ##lation analysis . we now study in detail the impact of these two additions and whether oh ##em is complementary to them , and report the analysis in table [ reference ] . baseline fr ##c ##n map improves from 67 . 2 % to 68 . 6 % when using multi - scale during both training and testing ( we refer to this as m ) . however , note that there is only a marginal benefit of using it at training time . it ##erative bb ##ox regression ( b ) further improves the fr ##c ##n map to 72 . 4 % . but more importantly , using oh ##em improves it to 75 . 1 % map , which is state - of - the - art for methods trained on vo ##c ##0 ##7 data ( see table [ reference ] ) . in fact , using oh ##em consistently results in higher map for all variants of these two additions ( see table [ reference ] ) . it ##erative bb ##ox reg . ( b ) sub ##section : ms coco results ms coco test - dev 2015 evaluation server results are reported in table [ reference ] . using multi - scale improves the performance of our method to 24 . 4 % ap on the standard coco metric and to 44 . 4 % ap on the vo ##c metric . this again shows the complementary nature of using multi - scale and oh ##em . finally , we train our method using the entire ms coco train ##val set , which further improves performance to 25 . 5 % ap ( and 45 . 9 % ap ) . in the 2015 ms coco detection challenge , a variant of this approach finished place overall . section : conclusion we presented an online hard example mining ( oh ##em ) algorithm , a simple and effective method to train region - based con ##vn ##et detectors . oh ##em eliminate ##s several he ##uri ##stic ##s and hyper ##para ##meter ##s in common use by automatically selecting hard examples , thus sim ##plify ##ing training . we conducted extensive experimental analysis to demonstrate the effectiveness of the proposed algorithm , which leads to better training convergence and consistent improvements in detection accuracy on standard bench ##marks . we also reported state - of - the - art results on pascal vo ##c 2007 and 2012 when using oh ##em with other orthogonal additions . though we used fast r - cnn throughout this paper , oh ##em can be used for training any region - based con ##vn ##et detector . our experimental analysis was based on the overall detection accuracy , however it will be an interesting future direction to study the impact of various training method ##ologies on individual category performance . paragraph : ac ##k ##now ##led ##gm ##ent . this project started as an intern project at microsoft research and continued at cm ##u . we thank larry z ##it ##nick , is ##han mis ##ra and sean bell for many helpful discussions . as was supported by the microsoft research phd fellowship . this work was also partially supported by on ##r mu ##ri n ##00 ##01 ##41 ##6 ##12 ##00 ##7 . we thank n ##vid ##ia for dona ##ting gp ##us . bibliography : references",
        "pred_seq": "ms ##set [SEP] online mining [SEP] map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map [CLS]",
        "pred_templates": [],
        "gold_templates": [
            {
                "Material": [
                    [
                        "pascal voc 2007",
                        "pascal voc",
                        "pascal voc07 dataset",
                        "voc 2007",
                        "voc07 trainval",
                        "voc07",
                        "voc",
                        "voc07 data"
                    ]
                ],
                "Method": [
                    [
                        "online hard example mining",
                        "ohem",
                        "online hard example selection strategy",
                        "iterative localization and boundingbox bbox voting scheme"
                    ]
                ],
                "Metric": [
                    [
                        "map",
                        "mean average precision",
                        "points map",
                        "baseline frcn map"
                    ]
                ],
                "Task": [
                    [
                        "object detection",
                        "object detection research",
                        "convnetbased object detection",
                        "imagenet detection",
                        "detection",
                        "regionbased object detectors"
                    ]
                ]
            }
        ]
    },
    "45": {
        "doctext": "document : the microsoft 2016 conversation ##al speech recognition system we describe microsoft ' s conversation ##al speech recognition system , in which we combine recent developments in neural - network - based acoustic and language modeling to advance the state of the art on the switch ##board recognition task . inspired by machine learning ensemble techniques , the system uses a range of con ##vo ##lu ##tion ##al and rec ##urrent neural networks . i - vector modeling and lattice - free mm ##i training provide significant gains for all acoustic model architecture ##s . language model res ##cor ##ing with multiple forward and backward running rn ##nl ##ms , and word posterior - based system combination provide a 20 % boost . the best single system uses a res ##net architecture acoustic model with rn ##nl ##m res ##cor ##ing , and achieve ##s a word error rate of 6 . 9 % on the ni ##st 2000 switch ##board task . the combined system has an error rate of 6 . 2 % , representing an improvement over previously reported results on this bench ##mark task . w . xi ##ong , j . drop ##po , x . huang , f . se ##ide , m . se ##lt ##zer , a . st ##ol ##cke , d . yuan ##d ##g . z ##weig microsoft ##res ##ear ##ch conversation ##al speech recognition , con ##vo ##lu ##tion ##al neural networks , rec ##urrent neural networks , v ##gg , res ##net , lace , b ##ls ##tm . section : introduction recent years have seen a rapid reduction in speech recognition error rates as a result of careful engineering and optimization of con ##vo ##lu ##tion ##al and rec ##urrent neural networks . while the basic structures have been well known for a long period , it is only recently that they have dominated the field as the best models for speech recognition . surprisingly , this is the case for both acoustic modeling and language modeling . in comparison to standard feed - forward ml ##ps or d ##nn ##s , these acoustic models have the ability to model a large amount of acoustic context with temporal in ##var ##iance , and in the case of con ##vo ##lu ##tion ##al models , with frequency in ##var ##iance as well . in language modeling , rec ##urrent models appear to improve over classical n - gram models through the general ##ization ability of continuous word representations . in the meantime , ensemble learning has become commonly used in several neural models , to improve robust ##ness by reducing bias and variance . in this paper , we use ensembles of models extensively , as well as improvements to individual component models , to to advance the state - of - the - art in conversation ##al telephone speech recognition ( ct ##s ) , which has been a bench ##mark speech recognition task since the 1990s . the main features of this system are : an ensemble of two fundamental acoustic model architecture ##s , con ##vo ##lu ##tion ##al neural nets ( cnn ##s ) and long - short - term memory nets ( l ##st ##ms ) , with multiple variants of each an attention mechanism in the lace cnn which differential ##ly weights distant context lattice - free mm ##i training the use of i - vector based adaptation in all models language model ( l ##m ) res ##cor ##ing with multiple , rec ##urrent neural net l ##ms running in both forward and reverse direction confusion network system combination coupled with search for best system subset , as necessitated by the large number of candidate systems . the remainder of this paper describes our system in detail . section [ reference ] describes the cnn and l ##st ##m models . section [ reference ] describes our implementation of i - vector adaptation . section [ reference ] presents out lattice - free mm ##i training process . language model res ##cor ##ing is a significant part of our system , and described in section [ reference ] . experimental results are presented in section [ reference ] , followed by a discussion of related work and conclusions . section : con ##vo ##lu ##tion ##al and l ##st ##m neural networks we use three cnn variants . the first is the v ##gg architecture of . compared to the networks used previously in image recognition , this network uses small ( 3 ##x ##3 ) filters , is deeper , and applies up to five con ##vo ##lu ##tion ##al layers before pool ##ing . the second network is modeled on the res ##net architecture , which adds highway connections , i . e . a linear transform of each layer ' s input to the layer ' s output . the only difference is that we move the batch normal ##ization node to the place right before each re ##lu activation . the last cnn variant is the lace ( layer - wise context expansion with attention ) model . lace is a td ##nn variant in which each higher layer is a weighted sum of nonlinear transformations of a window of lower layer frames . in other words , each higher layer exploits broader context than lower layers . lower layers focus on extract ##ing simple local patterns while higher layers extract complex patterns that cover broader contexts . since not all frames in a window carry the same importance , an attention mask is applied . the lace model differs from the earlier td ##nn models e . g . in the use of a learned attention mask and res ##net like linear pass - through . as illustrated in detail in figure [ reference ] , the model is composed of 4 blocks , each with the same architecture . each block starts with a con ##vo ##lu ##tion layer with stride 2 which sub - samples the input and increases the number of channels . this layer is followed by 4 re ##lu - con ##vo ##lu ##tion layers with jump links similar to those used in res ##net . table [ reference ] compares the layer structure and parameters of the three cnn architecture ##s . while our best performing models are con ##vo ##lu ##tion ##al , the use of long short - term memory networks is a close second . we use a bid ##ire ##ction ##al architecture without frame - skipping . the core model structure is the l ##st ##m defined in . we found that using networks with more than six layers did not improve the word error rate on the development set , and chose 512 hidden units , per direction , per layer , as that provided a reasonable trade - off between training time and final model accuracy . network parameters for different configurations of the l ##st ##m architecture are summarized in table [ reference ] . section : speaker adaptive modeling speaker adaptive modeling in our system is based on conditioning the network on an i - vector characterization of each speaker . a 100 - dimensional i - vector is generated for each conversation side . for the l ##st ##m system , the conversation - side i - vector is app ##ended to each frame of input . for con ##vo ##lu ##tion ##al networks , this approach is inappropriate because we do not expect to see spatial ##ly contiguous patterns in the input . instead , for the cnn ##s , we add a learn ##able weight matrix to each layer , and add to the activation of the layer before the nonlinear ##ity . thus , in the cnn , the i - vector essentially serves as an additional bias to each layer . note that the i - vectors are estimated using m ##fc ##c features ; by using them subsequently in systems based on log - filter ##bank features , we may benefit from a form of feature combination . section : lattice - free sequence training after standard cross - entropy training , we opt ##imi ##ze the model parameters using the maximum mutual information ( mm ##i ) objective function . den ##oting a word sequence by and its corresponding acoustic realization by , the training criterion is as noted in , the necessary gradient for use in back ##pro ##pa ##gation is a simple function of the posterior probability of a particular acoustic model state at a given time , as computed by sum ##ming over all possible word sequences in an un ##con ##stra ##ined manner . as first done in , and more recently in , this can be accomplished with a straightforward alpha - beta computation over the finite state accept ##or representing the deco ##ding search space . in , the search space is taken to be an accept ##or representing the composition for a un ##ig ##ram language model on words . in , a language model on phone ##mes is used instead . in our implementation , we use a mixed - history acoustic unit language model . in this model , the probability of transition ##ing into a new context - dependent phonetic state ( sen ##one ) is conditioned both the sen ##one and phone history . we found this model to perform better than either purely word - based or phone - based models . based on a set of initial experiments , we developed the following procedure : perform a forced alignment of the training data to select lexi ##cal variants and determine frame - aligned sen ##one sequences . com ##press consecutive frame ##wise occurrences of a single sen ##one into a single occurrence . estimate an un ##smo ##oth ##ed , variable - length n - gram language model from this data , where the history state consists of the previous phone and previous sen ##ones within the current phone . to illustrate this , consider the sample sen ##one sequence { s _ s ##2 . 128 ##8 , s _ s ##3 . 106 ##1 , s _ s ##4 . 109 ##6 } , { eh _ s ##2 . 52 ##7 , eh _ s ##3 . 128 , eh _ s ##4 . 66 } , { t _ s ##2 . 72 ##9 , t _ s ##3 . 57 ##2 , t _ s ##4 . 74 ##8 } . when predicting the state following eh _ s ##4 . 66 the history consists of ( s , eh _ s ##2 . 52 ##7 , eh _ s ##3 . 128 , eh _ s ##4 . 66 ) , and following t _ s ##2 . 72 ##9 , the history is ( eh , t _ s ##2 . 72 ##9 ) . we construct the den ##omi ##nator graph from this language model , and hmm transition pro ##ba ##bilities as determined by transition - counting in the sen ##one sequences found in the training data . our approach not only largely reduces the complexity of building up the language model but also provides very reliable training performance . we have found it convenient to do the full computation , without pr ##uni ##ng , in a series of matrix - vector operations on the gp ##u . the underlying accept ##or is represented with a sparse matrix , and we maintain a dense likelihood vector for each time frame . the alpha and beta rec ##urs ##ions are implemented with cu ##spar ##se level - 2 routines : sparse - matrix , dense vector multi ##pl ##ies . run time is about 100 times faster than real time . as in , we use cross - entropy regular ##ization . in all the lattice - free mm ##i ( l ##fm ##mi ) experiments mentioned below we use a tri ##gram language model . most of the gain is usually obtained after processing 24 to 48 hours of data . section : l ##m res ##cor ##ing and system combination an initial deco ##ding is done with a w ##fs ##t deco ##der , using the architecture described in . we use an n - gram language model trained and pr ##une ##d with the sri ##lm tool ##kit . the first - pass l ##m has approximately 15 . 9 million big ##ram ##s , tri ##gram ##s , and 4 ##gram ##s , and a vocabulary of 30 , 500 words . it gives a per ##plex ##ity of 69 on the 1997 ct ##s evaluation transcript ##s . the initial deco ##ding produces a lattice with the pronunciation variants marked , from which 500 - best lists are generated for res ##cor ##ing purposes . subsequent n - best res ##cor ##ing uses an un ##pr ##une ##d l ##m comprising 145 million n - grams . all n - gram l ##ms were estimated by a maximum entropy criterion as described in . sub ##section : rn ##nl ##m setup the n - best h ##yp ##oth ##eses are then res ##core ##d using a combination of the large n - gram l ##m and several rn ##nl ##ms , trained and evaluated using the cue ##d - rn ##nl ##m tool ##kit . our rn ##nl ##m configuration has several distinctive features , as described below . 1 ) we trained both standard , forward - predicting rn ##nl ##ms and backward rn ##nl ##ms that predict words in reverse temporal order . the log pro ##ba ##bilities from both models are added . 2 ) as is customary , the rn ##nl ##m probability estimates are inter ##pol ##ated at the word - level with corresponding n - gram l ##m pro ##ba ##bilities ( separately for the forward and backward models ) . in addition , we trained a second rn ##nl ##m for each direction , obtained by starting with different random initial weights . the two rn ##nl ##ms and the n - gram l ##m for each direction are inter ##pol ##ated with weights of ( 0 . 375 , 0 . 375 , 0 . 25 ) . 3 ) in order to make use of l ##m training data that is not fully matched to the target conversation ##al speech domain , we start rn ##nl ##m training with the union of in - domain ( here , ct ##s ) and out - of - domain ( e . g . , web ) data . upon convergence , the network undergoes a second training phase using the in - domain data only . both training phases use in - domain validation data to regulate the learning rate schedule and termination . because the size of the out - of - domain data is a multiple of the in - domain data , a standard training on a simple union of the data would not yield a well - matched model , and have poor per ##plex ##ity in the target domain . 4 ) we found best results with an rn ##nl ##m configuration that had a second , non - rec ##urrent hidden layer . this produced lower per ##plex ##ity and word error than the standard , single - hidden - layer rn ##nl ##m architecture . the overall network architecture thus had two hidden layers with 1000 units each , using re ##lu nonlinear ##ities . training used noise - contrast ##ive estimation ( nc ##e ) . 5 ) the rn ##nl ##m output vocabulary consists of all words occurring more than once in the in - domain training set . while the rn ##nl ##m estimates a probability for unknown words , we take a different approach in res ##cor ##ing : the number of out - of - set words is recorded for each hypothesis and a penalty for them is estimated for them when opt ##imi ##zing the relative weights for all model scores ( acoustic , l ##m , pronunciation ) , using the sri ##lm n ##bes ##t - opt ##imi ##ze tool . sub ##section : training data the 4 - gram language model for deco ##ding was trained on the available ct ##s transcript ##s from the dar ##pa ears program : switch ##board ( 3 m words ) , bb ##n switch ##board - 2 transcript ##s ( 850 ##k ) , fisher ( 21 m ) , english call ##hom ##e ( 200 ##k ) , and the university of washington conversation ##al web corpus ( 191 m ) . a separate model was trained from each source and inter ##pol ##ated with weights opt ##imi ##zed on rt - 03 transcript ##s . for the un ##pr ##une ##d large res ##cor ##ing 4 - gram , an additional l ##m component was added , trained on 133 m word of ld ##c broadcast news texts . the n - gram l ##m configuration is modeled after that described in , except that max ##ent smoothing was used . the rn ##nl ##ms were trained on switch ##board and fisher transcript ##s as in - domain data ( 20 m words for gradient computation , 3 m for validation ) . to this we added 62 m words of u ##w web data as out - of - domain data , for use in the two - phase training procedure described above . sub ##section : rn ##nl ##m performance table [ reference ] gives per ##plex ##ity and word error performance for various rn ##nl ##m setup ##s , from simple to more complex . the acoustic model used was the res ##net cnn . as can be seen , each of the measures described earlier adds inc ##rem ##ental gains , which , while small individually , add up to a 9 % relative error reduction over a plain rn ##nl ##m . sub ##section : system combination the l ##m res ##cor ##ing is carried out separately for each acoustic model . the res ##core ##d n - best lists from each sub ##sy ##ste ##m are then aligned into a single confusion network using the sri ##lm n ##bes ##t - rover tool . however , the number of potential candidate systems is too large to allow an all - out combination , both for practical reasons and due to over ##fi ##tting issues . instead , we perform a greedy search , starting with the single best system , and successively adding additional systems , to find a small set of systems that are maximal ##ly complementary . the rt - 02 switch ##board set was used for this search procedure . the relative weight ##ing ( for confusion - network mediated voting ) of the different systems is opt ##imi ##zed using an em algorithm , using the same data , and smoothed hierarchical ##ly by inter ##pol ##ating each set of system weights with the preceding one in the search . section : experimental setup and results sub ##section : speech corp ##ora we train with the commonly used english ct ##s ( switch ##board and fisher ) corp ##ora . evaluation is carried out on the ni ##st 2000 ct ##s test set , which comprises both switch ##board ( sw ##b ) and call ##hom ##e ( ch ) subset ##s . the switch ##board - 1 portion of the ni ##st 2002 ct ##s test set was used for tuning and development . the acoustic training data is comprised by ld ##c corp ##ora 97 ##s ##6 ##2 , 2004 ##s ##13 , 2005 ##s ##13 , 2004 ##s ##11 and 2004 ##s ##0 ##9 ; see for a full description . sub ##section : 1 - bit sg ##d training all presented models are costly to train . to make training feasible , we parallel ##ize training with our previously proposed 1 - bit sg ##d parallel ##ization technique . this data - parallel method distribute ##s mini ##bat ##ches over multiple worker nodes , and then aggregate ##s the sub - gradient ##s . while the necessary communication time would otherwise be prohibit ##ive , the 1 - bit sg ##d method eliminate ##s the bottle ##neck by two techniques : 1 - bit quan ##ti ##zation of gradient ##s and automatic mini ##bat ##ch - size scaling . in , we showed that gradient values can be quan ##tized to just a single bit , if one carries over the quan ##ti ##zation error from one mini ##bat ##ch to the next . each time a sub - gradient is quan ##tized , the quan ##ti ##zation error is computed and remembered , and then added to the next mini ##bat ##ch ' s sub - gradient . this reduces the required bandwidth 32 - fold with minimal loss in accuracy . secondly , automatic mini ##bat ##ch - size scaling progressively decreases the frequency of model updates . at regular intervals ( e . g . every 72 ##h of training data ) , the trainer tries larger mini ##bat ##ch sizes on a small subset of data and picks the largest that maintains training loss . sub ##section : acoustic model details forty - dimensional log - filter ##bank features were extracted every 10 mill ##ise ##con ##ds , using a 25 - mill ##ise ##con ##d analysis window . the cnn models used window sizes as indicated in table [ reference ] , and the l ##st ##ms processed one frame of input at a time . the bulk of our models use three state left - to - right trip ##hone models with 900 ##0 tied states . additionally , we have trained several models with 27 ##k tied states . the phonetic inventory includes special models for noise , vocal ##ized - noise , laughter and silence . we use a 30 ##k - vocabulary derived from the most common words in the switch ##board and fisher corp ##ora . the deco ##der uses a static ##ally compiled un ##ig ##ram graph , and dynamic ##ally applies the language model score . the un ##ig ##ram graph has about 300 ##k states and 500 ##k arcs . all acoustic models were trained using the open - source computational network tool ##kit ( cn ##t ##k ) . table [ reference ] shows the result of i - vector adaptation and l ##fm ##mi training on several of our systems . we achieve a 5 - 8 % relative improvement from i - vectors , including on cnn systems . the last row of table [ reference ] shows the effect of l ##fm ##mi training on the different models . we see a consistent 7 - 10 % further relative reduction in error rate for all models . considering the great increase in procedural simplicity of l ##fm ##mi over the previous practice of writing lattice ##s and post - processing them , we consider l ##fm ##mi to be a significant advance in technology . sub ##section : comparative system performance model performance for our individual models as well as relevant comparisons from the literature are shown in table [ reference ] . out of the 15 models built , only models given non - zero weight in the final system combination are shown . section : relation to prior work compared to earlier applications of cnn ##s to speech recognition , our networks are much deeper , and use linear bypass connections across con ##vo ##lu ##tion ##al layers . they are similar in spirit to those studied more recently by . we improve on these architecture ##s with the lace model , which it ##erative ##ly expands the effective window size , layer - by - layer , and adds an attention mask to differential ##ly weight distant context . our use of lattice - free mm ##i is distinctive , and extends previous work by proposing the use of a mixed trip ##hone / phone ##me history in the language model . on the language modeling side , we achieve a performance boost by combining multiple rn ##nl ##ms in both forward and backward directions , and by using a two - phase training regime ##n to get best results from out - of - domain data . for our best cnn system , rn ##nl ##m res ##cor ##ing yields a relative word error reduction of 20 % , and a 16 % relative gain for the combined recognition system . ( elsewhere we report further improvements , using l ##st ##m - based l ##ms . ) section : conclusions we have described microsoft ' s conversation ##al speech recognition system for 2016 . the use of cnn ##s in the acoustic model has proved singular ##ly effective , as has the use of rn ##n language models . our best single system achieve ##s an error rate of 6 . 9 % on the ni ##st 2000 switch ##board set . we believe this is the best performance reported to date for a recognition system not based on system combination . an ensemble of acoustic models advances the state of the art to 6 . 2 % on the switch ##board test data . ac ##k ##now ##led ##gm ##ents . we thank x . chen from cue ##d for valuable assistance with the cue ##d - rn ##nl ##m tool ##kit , and ic ##si for compute and data resources . bibliography : references",
        "pred_seq": "microsoft system [SEP] [SEP] word rate [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "microsoft 2016 conversational speech recognition system"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "word error rate"
                    ]
                ],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "switchboard"
                    ]
                ],
                "Method": [
                    [
                        "microsofts conversational speech recognition system",
                        "neuralnetworkbased acoustic and language modeling",
                        "forward and backward running rnnlms",
                        "rnnlm rescoring",
                        "recurrent neural net lms",
                        "unsmoothed variablelength ngram language model",
                        "rnnlm setup",
                        "cuedrnnlm toolkit",
                        "rnnlm configuration",
                        "forwardpredicting rnnlms",
                        "backward rnnlms",
                        "rnnlm probability estimates",
                        "rnnlm",
                        "rnnlm training",
                        "singlehiddenlayer rnnlm architecture",
                        "rnnlm setups"
                    ]
                ],
                "Metric": [
                    [
                        "word error rate",
                        "error rate",
                        "word error",
                        "word error performance",
                        "relative error reduction"
                    ]
                ],
                "Task": [
                    [
                        "microsoftresearch conversational speech recognition",
                        "speech recognition",
                        "conversational telephone speech recognition",
                        "speech recognition task",
                        "image recognition"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "switchboard"
                    ]
                ],
                "Method": [
                    [
                        "ngram models",
                        "tdnn models",
                        "unigram language model",
                        "trigram language model",
                        "ngram language model",
                        "decoding",
                        "training",
                        "lm rescoring",
                        "cnn models",
                        "decoder",
                        "language model score"
                    ]
                ],
                "Metric": [
                    [
                        "word error rate",
                        "error rate",
                        "word error",
                        "word error performance",
                        "relative error reduction"
                    ]
                ],
                "Task": [
                    [
                        "microsoftresearch conversational speech recognition",
                        "speech recognition",
                        "conversational telephone speech recognition",
                        "speech recognition task",
                        "image recognition"
                    ]
                ]
            }
        ]
    },
    "46": {
        "doctext": "document : pyramid scene par ##sing network scene par ##sing is challenging for unrest ##ricted open vocabulary and diverse scenes . in this paper , we exploit the capability of global context information by different - region - based context aggregation through our pyramid pool ##ing module together with the proposed pyramid scene par ##sing network ( ps ##p ##net ) . our global prior representation is effective to produce good quality results on the scene par ##sing task , while ps ##p ##net provides a superior framework for pixel - level prediction . the proposed approach achieve ##s state - of - the - art performance on various data ##set ##s . it came first in image ##net scene par ##sing challenge 2016 , pascal vo ##c 2012 bench ##mark and city ##sca ##pes bench ##mark . a single ps ##p ##net yields the new record of mi ##ou accuracy 85 . 4 % on pascal vo ##c 2012 and accuracy 80 . 2 % on city ##sca ##pes . section : introduction scene par ##sing , based on semantic segment ##ation , is a fundamental topic in computer vision . the goal is to assign each pixel in the image a category label . scene par ##sing provides complete understanding of the scene . it predict ##s the label , location , as well as shape for each element . this topic is of broad interest for potential applications of automatic driving , robot sensing , to name a few . difficulty of scene par ##sing is closely related to scene and label variety . the pioneer scene par ##sing task is to classify 33 scenes for 2 , 68 ##8 images on l ##mo data ##set . more recent pascal vo ##c semantic segment ##ation and pascal context data ##set ##s include more labels with similar context , such as chair and sofa , horse and cow , etc . the new ad ##e ##20 k data ##set is the most challenging one with a large and unrest ##ricted open vocabulary and more scene classes . a few representative images are shown in fig . [ reference ] . to develop an effective algorithm for these data ##set ##s needs to conquer a few difficulties . state - of - the - art scene par ##sing framework ##s are mostly based on the fully con ##vo ##lu ##tion ##al network ( fc ##n ) . the deep con ##vo ##lu ##tion ##al neural network ( cnn ) based methods boost dynamic object understanding , and yet still face challenges considering diverse scenes and unrest ##ricted vocabulary . one example is shown in the first row of fig . [ reference ] , where a boat is mistaken as a car . these errors are due to similar appearance of objects . but when viewing the image regarding the context prior that the scene is described as boat ##house near a river , correct prediction should be yielded . towards accurate scene perception , the knowledge graph relies on prior information of scene context . we found that the major issue for current fc ##n based models is lack of suitable strategy to utilize global scene category clues . for typical complex scene understanding , previously to get a global image - level feature , spatial pyramid pool ##ing was widely employed where spatial statistics provide a good des ##cript ##or for overall scene interpretation . spatial pyramid pool ##ing network further enhance ##s the ability . different from these methods , to incorporate suitable global features , we propose pyramid scene par ##sing network ( ps ##p ##net ) . in addition to traditional dil ##ated fc ##n for pixel prediction , we extend the pixel - level feature to the specially designed global pyramid pool ##ing one . the local and global clues together make the final prediction more reliable . we also propose an optimization strategy with deeply supervised loss . we give all implementation details , which are key to our decent performance in this paper , and make the code and trained models publicly available . our approach achieve ##s state - of - the - art performance on all available data ##set ##s . it is the champion of image ##net scene par ##sing challenge 2016 , and arrived the 1st place on pascal vo ##c 2012 semantic segment ##ation bench ##mark , and the 1st place on urban scene city ##sca ##pes data . they manifest that ps ##p ##net gives a promising direction for pixel - level prediction tasks , which may even benefit cnn - based stereo matching , optical flow , depth estimation , etc . in follow - up work . our main contributions are three ##fold . we propose a pyramid scene par ##sing network to em ##bed difficult scenery context features in an fc ##n based pixel prediction framework . we develop an effective optimization strategy for deep res ##net based on deeply supervised loss . we build a practical system for state - of - the - art scene par ##sing and semantic segment ##ation where all crucial implementation details are included . section : related work in the following , we review recent advances in scene par ##sing and semantic segment ##ation tasks . driven by powerful deep neural networks , pixel - level prediction tasks like scene par ##sing and semantic segment ##ation achieve great progress inspired by replacing the fully - connected layer in classification with the con ##vo ##lu ##tion layer . to en ##lar ##ge the rec ##eptive field of neural networks , methods of used dil ##ated con ##vo ##lu ##tion . no ##h proposed a coarse - to - fine structure with deco ##n ##vo ##lu ##tion network to learn the segment ##ation mask . our baseline network is fc ##n and dil ##ated network . other work mainly proceeds in two directions . one line is with multi - scale feature en ##se ##mbling . since in deep networks , higher - layer feature contains more semantic meaning and less location information . combining multi - scale features can improve the performance . the other direction is based on structure prediction . the pioneer work used conditional random field ( cr ##f ) as post processing to ref ##ine the segment ##ation result . following methods refined networks via end - to - end modeling . both of the two directions am ##eli ##ora ##te the local ##ization ability of scene par ##sing where predicted semantic boundary fits objects . yet there is still much room to exploit necessary information in complex scenes . to make good use of global image - level prior ##s for diverse scene understanding , methods of extracted global context information with traditional features not from deep neural networks . similar improvement was made under object detection framework ##s . liu proved that global average pool ##ing with fc ##n can improve semantic segment ##ation results . however , our experiments show that these global des ##cript ##ors are not representative enough for the challenging ad ##e ##20 k data . therefore , different from global pool ##ing in , we exploit the capability of global context information by different - region - based context aggregation via our pyramid scene par ##sing network . section : pyramid scene par ##sing network we start with our observation and analysis of representative failure cases when applying fc ##n methods to scene par ##sing . they mo ##tiv ##ate proposal of our pyramid pool ##ing module as the effective global context prior . our pyramid scene par ##sing network ( ps ##p ##net ) illustrated in fig . [ reference ] is then described to improve performance for open - vocabulary object and stuff identification in complex scene par ##sing . sub ##section : important observations the new ad ##e ##20 k data ##set contains 150 stuff / object category labels ( , wall , sky , and tree ) and 1 , 03 ##8 image - level scene des ##cript ##ors ( , airport _ terminal , bedroom , and street ) . so a large amount of labels and vast distributions of scenes come into existence . inspecting the prediction results of the fc ##n baseline provided in , we sum ##mar ##ize several common issues for complex - scene par ##sing . paragraph : mis ##mat ##ched relationship context relationship is universal and important especially for complex scene understanding . there exist co - occur ##rent visual patterns . for example , an airplane is likely to be in runway or fly in sky while not over a road . for the first - row example in fig . [ reference ] , fc ##n predict ##s the boat in the yellow box as a \" car \" based on its appearance . but the common knowledge is that a car is seldom over a river . lack of the ability to collect context ##ual information increases the chance of mis ##class ##ification . paragraph : confusion categories there are many class label pairs in the ad ##e ##20 k data ##set that are confusing in classification . examples are field and earth ; mountain and hill ; wall , house , building and skyscraper . they are with similar appearance . the expert ann ##ota ##tor who labeled the entire data ##set , still makes 17 . 60 % pixel error as described in . in the second row of fig . [ reference ] , fc ##n predict ##s the object in the box as part of skyscraper and part of building . these results should be excluded so that the whole object is either skyscraper or building , but not both . this problem can be re ##med ##ied by utilizing the relationship between categories . paragraph : inc ##ons ##pic ##uous classes scene contains objects / stuff of arbitrary size . several small - size things , like street ##light and sign ##board , are hard to find while they may be of great importance . contra ##rily , big objects or stuff may exceed the rec ##eptive field of fc ##n and thus cause disco ##nti ##nu ##ous prediction . as shown in the third row of fig . [ reference ] , the pillow has similar appearance with the sheet . overlooking the global scene category may fail to par ##se the pillow . to improve performance for remarkably small or large objects , one should pay much attention to different sub - regions that contain inc ##ons ##pic ##uous - category stuff . to sum ##mar ##ize these observations , many errors are partially or completely related to context ##ual relationship and global information for different rec ##eptive fields . thus a deep network with a suitable global - scene - level prior can much improve the performance of scene par ##sing . sub ##section : pyramid pool ##ing module with above analysis , in what follows , we introduce the pyramid pool ##ing module , which empirical ##ly proves to be an effective global context ##ual prior . in a deep neural network , the size of rec ##eptive field can roughly indicates how much we use context information . although theoretically the rec ##eptive field of res ##net is already larger than the input image , it is shown by zhou that the empirical rec ##eptive field of cnn is much smaller than the theoretical one especially on high - level layers . this makes many networks not sufficiently incorporate the moment ##ous global scenery prior . we address this issue by proposing an effective global prior representation . global average pool ##ing is a good baseline model as the global context ##ual prior , which is commonly used in image classification tasks . in , it was successfully applied to semantic segment ##ation . but regarding the complex - scene images in ad ##e ##20 k , this strategy is not enough to cover necessary information . pixels in these scene images are ann ##ota ##ted regarding many stuff and objects . directly fu ##sing them to form a single vector may lose the spatial relation and cause ambiguity . global context information along with sub - region context is helpful in this regard to distinguish among various categories . a more powerful representation could be fused information from different sub - regions with these rec ##eptive fields . similar conclusion was drawn in classical work of scene / image classification . in , feature maps in different levels generated by pyramid pool ##ing were finally flattened and con ##cate ##nated to be fed into a fully connected layer for classification . this global prior is designed to remove the fixed - size constraint of cnn for image classification . to further reduce context information loss between different sub - regions , we propose a hierarchical global prior , containing information with different scales and varying among different sub - regions . we call it pyramid pool ##ing module for global scene prior construction upon the final - layer - feature - map of the deep neural network , as illustrated in part ( c ) of fig . [ reference ] . the pyramid pool ##ing module fuse ##s features under four different pyramid scales . the coarse ##st level highlighted in red is global pool ##ing to generate a single bin output . the following pyramid level separates the feature map into different sub - regions and forms poole ##d representation for different locations . the output of different levels in the pyramid pool ##ing module contains the feature map with varied sizes . to maintain the weight of global feature , we use con ##vo ##lu ##tion layer after each pyramid level to reduce the dimension of context representation to of the original one if the level size of pyramid is . then we directly ups ##amp ##le the low - dimension feature maps to get the same size feature as the original feature map via bi ##line ##ar inter ##pol ##ation . finally , different levels of features are con ##cate ##nated as the final pyramid pool ##ing global feature . noted that the number of pyramid levels and size of each level can be modified . they are related to the size of feature map that is fed into the pyramid pool ##ing layer . the structure abstracts different sub - regions by adopting varying - size pool ##ing kernel ##s in a few strides . thus the multi - stage kernel ##s should maintain a reasonable gap in representation . our pyramid pool ##ing module is a four - level one with bin sizes of , , and respectively . for the type of pool ##ing operation between max and average , we perform extensive experiments to show the difference in section [ reference ] . sub ##section : network architecture with the pyramid pool ##ing module , we propose our pyramid scene par ##sing network ( ps ##p ##net ) as illustrated in fig . [ reference ] . given an input image in fig . [ reference ] ( a ) , we use a pre ##train ##ed res ##net model with the dil ##ated network strategy to extract the feature map . the final feature map size is of the input image , as shown in fig . [ reference ] ( b ) . on top of the map , we use the pyramid pool ##ing module shown in ( c ) to gather context information . using our 4 - level pyramid , the pool ##ing kernel ##s cover the whole , half of , and small portions of the image . they are fused as the global prior . then we con ##cate ##nate the prior with the original feature map in the final part of ( c ) . it is followed by a con ##vo ##lu ##tion layer to generate the final prediction map in ( d ) . to explain our structure , ps ##p ##net provides an effective global context ##ual prior for pixel - level scene par ##sing . the pyramid pool ##ing module can collect levels of information , more representative than global pool ##ing . in terms of computational cost , our ps ##p ##net does not much increase it compared to the original dil ##ated fc ##n network . in end - to - end learning , the global pyramid pool ##ing module and the local fc ##n feature can be opt ##imi ##zed simultaneously . section : deep supervision for res ##net - based fc ##n deep pre ##train ##ed networks lead to good performance . however , increasing depth of the network may introduce additional optimization difficulty as shown in for image classification . res ##net solve ##s this problem with skip connection in each block . latter layers of deep res ##net mainly learn residues based on previous ones . we contra ##rily propose generating initial results by supervision with an additional loss , and learning the residue afterwards with the final loss . thus , optimization of the deep network is deco ##mp ##osed into two , each is simpler to solve . an example of our deeply supervised res ##net ##10 ##1 model is illustrated in fig . [ reference ] . apart from the main branch using soft ##max loss to train the final class ##ifier , another class ##ifier is applied after the fourth stage , i . e . , the res ##4 ##b ##22 residue block . different from relay back ##pro ##pa ##gation that blocks the backward auxiliary loss to several shallow layers , we let the two loss functions pass through all previous layers . the auxiliary loss helps opt ##imi ##ze the learning process , while the master branch loss takes the most responsibility . we add weight to balance the auxiliary loss . in the testing phase , we abandon this auxiliary branch and only use the well opt ##imi ##zed master branch for final prediction . this kind of deeply supervised training strategy for res ##net - based fc ##n is broadly useful under different experimental settings and works with the pre - trained res ##net model . this manifest ##s the general ##ity of such a learning strategy . more details are provided in section [ reference ] . section : experiments our proposed method is successful on scene par ##sing and semantic segment ##ation challenges . we evaluate it in this section on three different data ##set ##s , including image ##net scene par ##sing challenge 2016 , pascal vo ##c 2012 semantic segment ##ation and urban scene understanding data ##set city ##sca ##pes . sub ##section : implementation details for a practical deep learning system , devil is always in the details . our implementation is based on the public platform caf ##fe . inspired by , we use the \" poly \" learning rate policy where current learning rate equals to the base one multi ##ply ##ing . we set base learning rate to 0 . 01 and power to 0 . 9 . the performance can be improved by increasing the iteration number , which is set to 150 k for image ##net experiment , 30 k for pascal vo ##c and 90 k for city ##sca ##pes . momentum and weight decay are set to 0 . 9 and 0 . 000 ##1 respectively . for data aug ##ment ##ation , we adopt random mirror and random res ##ize between 0 . 5 and 2 for all data ##set ##s , and additionally add random rotation between - 10 and 10 degrees , and random ga ##uss ##ian blur for image ##net and pascal vo ##c . this comprehensive data aug ##ment ##ation scheme makes the network resist over ##fi ##tting . our network contains dil ##ated con ##vo ##lu ##tion following . during the course of experiments , we notice that an appropriately large \" crops ##ize \" can yield good performance and \" batch ##si ##ze \" in the batch normal ##ization layer is of great importance . due to limited physical memory on gp ##u cards , we set the \" batch ##si ##ze \" to 16 during training . to achieve this , we modify caf ##fe from together with branch and make it support batch normal ##ization on data gathered from multiple gp ##us based on open ##mp ##i . for the auxiliary loss , we set the weight to 0 . 4 in experiments . sub ##section : image ##net scene par ##sing challenge 2016 paragraph : data ##set and evaluation metric ##s the ad ##e ##20 k data ##set is used in image ##net scene par ##sing challenge 2016 . different from other data ##set ##s , ad ##e ##20 k is more challenging for the up to 150 classes and diverse scenes with a total of 1 , 03 ##8 image - level labels . the challenge data is divided into 20 ##k / 2 ##k / 3 k images for training , validation and testing . also , it needs to par ##se both objects and stuff in the scene , which makes it more difficult than other data ##set ##s . for evaluation , both pixel - wise accuracy ( pixel acc . ) and mean of class - wise intersection over union ( mean io ##u ) are used . paragraph : ab ##lation study for ps ##p ##net to evaluate ps ##p ##net , we conduct experiments with several settings , including pool ##ing types of max and average , pool ##ing with just one global feature or four - level features , with and without dimension reduction after the pool ##ing operation and before con ##cate ##nation . as listed in table [ reference ] , average pool ##ing works better than max pool ##ing in all settings . pool ##ing with pyramid par ##sing out ##per ##forms that using global pool ##ing . with dimension reduction , the performance is further enhanced . with our proposed ps ##p ##net , the best setting yields results 41 . 68 / 80 . 04 in terms of mean io ##u and pixel acc . ( % ) , exceeding global average pool ##ing of 40 . 07 / 79 . 52 as idea in liu by 1 . 61 / 0 . 52 . and compared to the baseline , ps ##p ##net out ##per ##form ##ing it by 4 . 45 / 2 . 03 in terms of absolute improvement and 11 . 95 / 2 . 60 in terms of relative difference . paragraph : ab ##lation study for auxiliary loss the introduced auxiliary loss helps opt ##imi ##ze the learning process while not influencing learning in the master branch . we experiment with setting the auxiliary loss weight between 0 and 1 and show the results in table [ reference ] . the baseline uses res ##net ##50 - based fc ##n with dil ##ated network , with the master branch ' s soft ##max loss for optimization . adding the auxiliary loss branch , = 0 . 4 yields the best performance . it out ##per ##forms the baseline with an improvement of 1 . 41 / 0 . 94 in terms of mean io ##u and pixel acc . ( % ) . we believe deeper networks will benefit more given the new augmented auxiliary loss . paragraph : ab ##lation study for pre - trained model deeper neural networks have been shown in previous work to be beneficial to large scale data classification . to further analyze ps ##p ##net , we conduct experiments for different depths of pre - trained res ##net . we test four depths of { 50 , 101 , 152 , 269 } . as shown in fig . [ reference ] , with the same setting , increasing the depth of res ##net from 50 to 269 can improve the score of ( mean io ##u + pixel acc . ) / 2 ( % ) from 60 . 86 to 62 . 35 , with 1 . 49 absolute improvement . detailed scores of ps ##p ##net pre - trained from different depth res ##net models are listed in table [ reference ] . paragraph : more detailed performance analysis we show our more detailed analysis on the validation set of ad ##e ##20 k in table [ reference ] . all our results except the last - row one use single - scale test . \" res ##net ##26 ##9 + da + al + ps ##p + ms \" uses multi - scale testing . our baseline is adapted from res ##net ##50 with dil ##ated network , which yields mean ##io ##u 34 . 28 and pixel acc . 76 . 35 . it already out ##per ##forms other prior systems possibly due to the powerful res ##net . our proposed architecture makes further improvement compared to the baseline . using data aug ##ment ##ation , our result exceeds the baseline by 1 . 54 / 0 . 72 and reaches 35 . 82 / 77 . 07 . using the auxiliary loss can further improve it by 1 . 41 / 0 . 94 and reaches 37 . 23 / 78 . 01 . with ps ##p ##net , we notice relatively more significant progress for improvement of 4 . 45 / 2 . 03 . the result reaches 41 . 68 / 80 . 04 . the difference from the baseline result is 7 . 40 / 3 . 69 in terms of absolute improvement and 21 . 59 / 4 . 83 ( % ) in terms of relativity . a deeper network of res ##net ##26 ##9 yields even higher performance up to 43 . 81 / 80 . 88 . finally , the multi - scale testing scheme moves the scores to 44 . 94 / 81 . 69 . paragraph : results in challenge using the proposed architecture , our team came in the 1st place in image ##net scene par ##sing challenge 2016 . table [ reference ] shows a few results in this competition . our ensemble submission achieve ##s score 57 . 21 % on the testing set . our single - model yields score 55 . 38 % , which is even higher than a few other multi - model ensemble submissions . this score is lower than that on the validation set possibly due to the difference of data distributions between validation and testing sets . as shown in column ( d ) of fig . [ reference ] , ps ##p ##net solve ##s the common problems in fc ##n . fig . [ reference ] shows another few par ##sing results on validation set of ad ##e ##20 ##k . our results contain more accurate and detailed structures compared to the baseline . sub ##section : pascal vo ##c 2012 our ps ##p ##net also works satisfying ##ly on semantic segment ##ation . we carry out experiments on the pascal vo ##c 2012 segment ##ation data ##set , which contains 20 object categories and one background class . following the procedure of , we use augmented data with the ann ##ota ##tion of resulting 10 , 58 ##2 , 1 , 44 ##9 and 1 , 45 ##6 images for training , validation and testing . results are shown in table [ reference ] , we compare ps ##p ##net with previous best - performing methods on the testing set based on two settings , i . e . , with or without pre - training on ms - coco data ##set . methods pre - trained with ms - coco are marked by ' \u2020 ' . for fair comparison with current res ##net based framework ##s in scene par ##sing / semantic segment ##ation task , we build our architecture based on res ##net ##10 ##1 while without post - processing like cr ##f . we evaluate ps ##p ##net with several - scale input and use the average results following . as shown in table [ reference ] , ps ##p ##net out ##per ##forms prior methods on both settings . trained with only vo ##c 2012 data , we achieve 82 . 6 % accuracy - we get the highest accuracy on all 20 classes . when ps ##p ##net is pre - trained with ms - coco data ##set , it reaches 85 . 4 % accuracy where 19 out of the 20 classes receive the highest accuracy . intriguing ##ly , our ps ##p ##net trained with only vo ##c 2012 data out ##per ##forms existing methods trained with the ms - coco pre - trained model . one may argue that our based classification model is more powerful than several prior methods since res ##net was recently proposed . to exhibit our unique contribution , we show that our method also out ##per ##forms state - of - the - art framework ##s that use the same model , including fc ##rn ##s , l ##rr , and deep ##lab . in this process , we even do not employ time - consuming but effective post - processing , such as cr ##f , as that in . several examples are shown in fig . [ reference ] . for \" cows \" in row one , our baseline model treats it as \" horse \" and \" dog \" while ps ##p ##net correct ##s these errors . for \" aero ##plane \" and \" table \" in the second and third rows , ps ##p ##net finds missing parts . for \" person \" , \" bottle \" and \" plant \" in following rows , ps ##p ##net performs well on these small - size - object classes in the images compared to the baseline model . more visual comparisons between ps ##p ##net and other methods are included in fig . [ reference ] . sub ##section : city ##sca ##pes city ##sca ##pes is a recently released data ##set for semantic urban scene understanding . it contains 5 , 000 high quality pixel - level finely ann ##ota ##ted images collected from 50 cities in different seasons . the images are divided into sets with numbers 2 , 97 ##5 , 500 , and 1 , 525 for training , validation and testing . it defines 19 categories containing both stuff and objects . also , 20 , 000 coarse ##ly ann ##ota ##ted images are provided for two settings in comparison , i . e . , training with only fine data or with both the fine and coarse data . methods trained using both fine and coarse data are marked with ' ' . detailed results are listed in table [ reference ] . our base model is res ##net ##10 ##1 as in deep ##lab for fair comparison and the testing procedure follows section [ reference ] . statistics in table [ reference ] show that ps ##p ##net out ##per ##forms other methods with notable advantage . using both fine and coarse data for training makes our method yield 80 . 2 accuracy . several examples are shown in fig . [ reference ] . detailed per - class results on testing set are shown in table [ reference ] . section : concluding remarks we have proposed an effective pyramid scene par ##sing network for complex scene understanding . the global pyramid pool ##ing feature provides additional context ##ual information . we have also provided a deeply supervised optimization strategy for res ##net - based fc ##n network . we hope the implementation details publicly available can help the community adopt these useful strategies for scene par ##sing and semantic segment ##ation and advance related techniques . section : acknowledge ##ments we would like to thank gang sun and tong xiao for their help in training the basic classification models , qu ##n lu ##o for technical support . this work is supported by a grant from the research grants council of the hong kong sar ( project no . 215 ##0 ##7 ##60 ) . bibliography : references",
        "pred_seq": "city ##mark [SEP] pyramid ##sing [SEP] mi ##ou [SEP] mi ##ou [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "cityscapes benchmark"
                    ]
                ],
                "Method": [
                    [
                        "pyramid scene parsing"
                    ]
                ],
                "Metric": [
                    [
                        "miou"
                    ]
                ],
                "Task": [
                    [
                        "miou"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "ade20 k dataset",
                        "ade20 k",
                        "ade20k"
                    ]
                ],
                "Method": [
                    [
                        "pyramid scene parsing network",
                        "pspnet"
                    ]
                ],
                "Metric": [
                    [
                        "score"
                    ]
                ],
                "Task": [
                    [
                        "semantic segmentation",
                        "segmentation",
                        "data augmentation"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "cityscapes benchmark",
                        "cityscapes"
                    ]
                ],
                "Method": [
                    [
                        "pyramid scene parsing network",
                        "pspnet"
                    ]
                ],
                "Metric": [
                    [
                        "mean iou"
                    ]
                ],
                "Task": [
                    [
                        "semantic segmentation",
                        "segmentation",
                        "data augmentation"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "cityscapes benchmark",
                        "cityscapes"
                    ]
                ],
                "Method": [
                    [
                        "pyramid scene parsing network",
                        "pspnet"
                    ]
                ],
                "Metric": [],
                "Task": []
            },
            {
                "Material": [
                    [
                        "cityscapes benchmark",
                        "cityscapes"
                    ]
                ],
                "Method": [
                    [
                        "pyramid scene parsing network",
                        "pspnet"
                    ]
                ],
                "Metric": [
                    [
                        "miou accuracy",
                        "meaniou 3428"
                    ]
                ],
                "Task": []
            },
            {
                "Material": [
                    [
                        "pascal voc 2012 benchmark",
                        "pascal voc 2012",
                        "pascal context datasets",
                        "pascal voc 2012 semantic segmentation",
                        "pascal voc",
                        "pascal voc 2012 segmentation dataset",
                        "voc 2012 data"
                    ]
                ],
                "Method": [
                    [
                        "pyramid scene parsing network",
                        "pspnet"
                    ]
                ],
                "Metric": [
                    [
                        "mean iou"
                    ]
                ],
                "Task": [
                    [
                        "semantic segmentation",
                        "segmentation",
                        "data augmentation"
                    ]
                ]
            }
        ]
    },
    "47": {
        "doctext": "document : art ##tra ##ck : articulated multi - person tracking in the wild in this paper we propose an approach for articulated tracking of multiple people in un ##con ##stra ##ined videos . our starting point is a model that resembles existing architecture ##s for single - frame pose estimation but is substantially faster . we achieve this in two ways : ( 1 ) by sim ##plify ##ing and spa ##rs ##ifying the body - part relationship graph and lever ##aging recent methods for faster inference , and ( 2 ) by off ##loading a substantial share of computation onto a feed - forward con ##vo ##lu ##tion ##al architecture that is able to detect and associate body joints of the same person even in cl ##utter . we use this model to generate proposals for body joint locations and formula ##te articulated tracking as spat ##io - temporal grouping of such proposals . this allows to jointly solve the association problem for all people in the scene by prop ##aga ##ting evidence from strong detection ##s through time and enforcing constraints that each proposal can be assigned to one person only . we report results on a public \" mp ##ii human pose \" bench ##mark and on a new \" mp ##ii video pose \" data ##set of image sequences with multiple people . we demonstrate that our model achieve ##s state - of - the - art results while using only a fraction of time and is able to leverage temporal information to improve state - of - the - art for crowded scenes . section : introduction this paper addresses the task of articulated human pose tracking in mono ##cular video . we focus on scenes of realistic complexity that often include fast motions , large variability in appearance and clothing , and person - person o ##cc ##lusion ##s . a successful approach must thus identify the number of people in each video frame , determine locations of the joints of each person and associate the joints over time . one of the key challenges in such scenes is that people might overlap and only a subset of joints of the person might be visible in each frame either due to person - person o ##cc ##lusion or tr ##un ##cation by image boundaries ( . fig . [ reference ] ) . arguably , resolving such cases correctly requires reasoning beyond purely geometric information on the arrangement of body joints in the image , and requires incorporation of a variety of image cues and joint modeling of several persons . the design of our model is motivated by two factors . we would like to leverage bottom - up end - to - end learning to directly capture image information . at the same time we aim to address a complex multi - person articulated tracking problem that does not naturally lend itself to an end - to - end prediction task and for which training data is not available in the amounts usually required for end - to - end learning . to leverage the available image information we learn a model for ass ##oc ##iating a body joint to a specific person in an end - to - end fashion relying on a con ##vo ##lu ##tion ##al network . we then incorporate these part - to - person association responses into a framework for jointly reasoning about assignment of body joints within the image and over time . to that end we use the graph partition ##ing formulation that has been used for people tracking and pose estimation in the past , but has not been shown to enable articulated people tracking . to facilitate efficient inference in video we resort to fast inference methods based on local comb ##inator ##ial optimization and aim for a sparse model that keeps the number of connections between variables to a minimum . as we demonstrate , in combination with feed - forward reasoning for joint - to - person association this allows us to achieve substantial speed - ups compared to state - of - the - art while maintaining the same level of accuracy . the main contribution of this work is a new articulated tracking model that operates by bottom - up assembly of part detection ##s within each frame and over time . in contrast to this model is suitable for scenes with an unknown number of subjects and reasons jointly across multiple people incorporating inter - person exclusion constraints and prop ##aga ##ting strong observations to neighboring frames . our second contribution is a formulation for single - frame pose estimation that relies on a sparse graph between body parts and a mechanism for generating body - part proposals conditioned on a person ' s location . this is in contrast to state - of - the - art approaches that perform expensive inference in a full graph and rely on generic bottom - up proposals . we demonstrate that a sparse model with a few spatial edges performs competitive ##ly with a fully - connected model while being much more efficient . notably , a simple model that operates in top - down / bottom - up fashion exceeds the performance of a fully - connected model while being x faster at inference time ( cf . tab . [ reference ] ) . this is due to off ##loading of a large share of the reasoning about body - part association onto a feed - forward con ##vo ##lu ##tion ##al architecture . finally , we contribute a new challenging data ##set for evaluation of articulated body joint tracking in crowded realistic environments with multiple overlapping people . related work . con ##vo ##lu ##tion ##al networks have emerged as an effective approach to local ##izing body joints of people in images and have also been extended for joint estimation of body configurations over time , and 3d pose estimation in outdoor environments in multi - camera setting . current approaches are increasingly effective for est ##imating body configurations of single people achieving high acc ##ura ##cies on this task , but are still failing on fast moving and articulated limbs . more complex recent models jointly reason about entire scenes , but are too complex and in ##ef ##fi ##cie ##nt to directly general ##ize to image sequences . recent feed - forward models are able to jointly in ##fer body joints of the same person and even operate over time but consider isolated persons only and do not general ##ize to the case of multiple overlapping people . similarly , consider a simplified task of tracking upper body poses of isolated upright individuals . we build on recent cnn detectors that are effective in local ##izing body joints in cl ##uttered scenes and explore different mechanisms for ass ##em ##bling the joints into multiple person configurations . to that end we rely on a graph partition ##ing approach closely related to . in contrast to who focus on pedestrian tracking , and who perform single frame multi - person pose estimation , we solve a more complex problem of articulated multi - person pose tracking . earlier approaches to articulated pose tracking in mono ##cular videos rely on hand - crafted image representations and focus on simplified tasks , such as tracking upper body poses of frontal isolated people , or tracking walking pedestrians with little degree of art ##ic ##ulation . in contrast , we address a harder problem of multi - person articulated pose tracking and do not make assumptions about the type of body motions or activities of people . our approach is closely related to who propose a similar formulation based on graph partition ##ing . our approach differs from primarily in the type of body - part proposals and the structure of the spat ##io - temporal graph . in our approach we introduce a person - conditioned model that is trained to associate body parts of a specific person already at the detection stage . this is in contrast to the approach of that relies on the generic body - part detectors . overview . our model consists of the two components : ( 1 ) a con ##vo ##lu ##tion ##al network for generating body part proposals and ( 2 ) an approach to group the proposals into spat ##io - temporal clusters . in sec . [ reference ] we introduce a general formulation for multi - target tracking that follows and allows us to define pose estimation and articulated tracking in a unified framework . we then describe the details of our articulated tracking approach in sec . [ reference ] , and introduce two variants of our formulation : bottom - up ( bu ) and top - down / bottom - up ( td / bu ) . we present experimental results in sec . [ reference ] . section : tracking by spat ##io - temporal grouping our body part detector generates a set of proposals for each frame of the video . each proposal is given by , where denotes the index of the video frame , is the spatial location of the proposal in image coordinates , is the probability of correct detection , and is the type of the body joint ( ankle or shoulder ) . let be a graph whose nodes are the joint detection ##s in a video and whose edges connect pairs of detection ##s that hypothetical ##ly correspond to the same target . the output of the tracking algorithm is a sub ##graph of , where is a subset of nodes after filtering redundant and er ##rone ##ous detection ##s and are edges linking nodes corresponding to the same target . we specify via binary variables and that define subset ##s of edges and nodes included in . in particular each track will correspond to a connected component in . as a general way to introduce constraints on edge configurations that correspond to a valid tracking solution we introduce a set and define a combination of edge and node indicator variables to be feasible if and only if . an example of a constraint encoded through is that end ##point nodes of an edge included by must also be included by . note that the variables and are coupled though . moreover , assuming that we are free to set components of and independently to maximize the tracking objective . given image observations we compute a set of features for each node and edge in the graph . we denote such node and edge features as and respectively . assuming independence of the feature vectors the conditional probability of indicator functions of nodes and of edges given features and and given a feasible set is given by where assigns a constant non - zero probability to every feasible solution and is equal to zero otherwise . mini ##mi ##zing the negative log - likelihood of e ##q . [ reference ] is equivalent to solving the following integer - linear program : where is the cost of retaining as part of the solution , and is the cost of assign ##ing the detection ##s linked by an edge to the same track . we define the set of constraints as in : jointly with the objective in e ##q . [ reference ] the constraints ( [ reference ] ) - ( [ reference ] ) define an instance of the minimum cost sub ##graph multi ##cut problem . the constraints ( [ reference ] ) and ( [ reference ] ) ensure that assignment of node and edge variables is consistent . the constraint ( [ reference ] ) ensures that for every two nodes either all or none of the paths between these nodes in graph are contained in one of the connected components of sub ##graph . this constraint is necessary to una ##mb ##igo ##usly assign person identity to a body part proposal based on its membership in a specific con ##nne ##cted component of . section : articulated multi - person tracking in sec . [ reference ] we introduced a general framework for multi - object tracking by solving an instance of the sub ##graph multi ##cut problem . the sub ##graph multi ##cut problem is np - hard , but recent work has shown that efficient approximate inference is possible with local search methods . the framework allows for a variety of graphs and connectivity patterns . simpler connectivity allows for faster and more efficient processing at the cost of ignoring some of the potentially inform ##ative depend ##encies between model variables . our goal is to design a model that is efficient , with as few edges as possible , yet effective in crowded scenes , and that allows us to model temporal continuity and inter - person exclusion . our articulated tracking approach proceeds by constructing a graph that couples body part proposals within the same frame and across neighboring frames . in general the graph will have three types of edges : ( 1 ) cross - type edges shown in fig . [ reference ] ( a ) and fig . [ reference ] ( b ) that connect two parts of different types , ( 2 ) same - type edges shown in fig . [ reference ] ( b ) that connect two nodes of the same type in the same image , and ( 3 ) temporal edges shown in fig . [ reference ] ( c ) that connect nodes in the neighboring frames . we now define two variants of our model that we denote as bottom - up ( bu ) and top - down / bottom - up ( td / bu ) . in the bu model the body part proposals are generated with our publicly available con ##vo ##lu ##tion ##al part detector . in the td / bu model we substitute these generic part detectors with a new con ##vo ##lu ##tion ##al body - part detector that is trained to output consistent body configurations conditioned on the person location . this al ##ows to further reduce the complexity of the model graph since the task of ass ##oc ##iating body parts is addressed within the proposal mechanism . as we show in sec . [ reference ] this leads to considerable gains in performance and allows for faster inference . note that the bu and td / bu models have identical same - type and temporal pair ##wise terms , but differ in the form of cross - type pair ##wise terms , and the connectivity of the nodes in . for both models we rely on the solve ##r from for inference . sub ##section : bottom - up model ( bu ) . for each body part proposal the detector outputs image location , probability of detection , and a label that indicates the type of the detected part ( shoulder or ankle ) . we directly use the probability of detection to derive the una ##ry costs in e ##q . [ reference ] as . image features in this case correspond to the image representation generated by the con ##vo ##lu ##tion ##al network . we consider two connectivity patterns for nodes in the graph . we either define edges for every pair of proposals which results in a fully connected graph in each image . alternatively we obtain a sparse version of the model by defining edges for a subset of part types only as is shown in fig . [ reference ] ( a ) . the rational ##e behind the sparse version is to obtain a simpler and faster version of the model by om ##itt ##ing edges between parts that carry little information about each other ' s image location ( left ankle and right arm ) . edge costs . in our bottom - up model the cost of the edges connecting two body part detection ##s and is defined as a function of the detection types and . following we thus train for each pair of part types a regression function that predict ##s relative image location of the parts in the pair . the cost is given by the output of the log ##istic regression given the features computed from offset and angle of the predicted and actual location of the other joint in the pair . we refer to for more details on these pair ##wise terms . note that our model general ##izes in that the edge cost depends on the type of nodes linked by the edge . it also general ##izes by allowing to be sparse . this is achieved by reform ##ulating the model with a more general type of cycle constraint ( [ reference ] ) , in contrast to simple triangle in ##e ##qual ##ities used in . sub ##section : top - down / bottom - up model ( td / bu ) we now introduce a version of our model that operates by first generating body part proposals conditioned on the locations of people in the image and then performing joint reasoning to group these proposals into spat ##io - temporal clusters corresponding to different people . we follow the intuition that it is considerably easier to identify and detect individual people ( e . g . by detecting their heads ) compared to correctly ass ##oc ##iating body parts such as ankles and wrists to each person . we select person ' s head as a root part that is responsible for representing the person location , and delegate the task of identifying body parts of the person corresponding to a head location to a con ##vo ##lu ##tion ##al network . the structure of td / bu model is illustrated in fig . [ reference ] ( b ) for the simplified case of two distinct head detection ##s . let us denote the set of all root part detection ##s as . for each pair of the root nodes we explicitly set the corresponding edge indicator variables . this implements a \" must - not - link \" constraint between these nodes , and in combination with the cycle inequality ( [ reference ] ) implies that each proposal can be connected to one of the \" person nodes \" only . the cost for an edge connecting detection proposal and a \" person node \" is based on the conditional distribution generated by the con ##vo ##lu ##tion ##al network . the output of such network is a set of conditional distributions , one for each node type . we aug ##ment the graph with attractive / rep ##ulsive and temporal terms as described in sec . [ reference ] and sec . [ reference ] and set the una ##ry costs for all indicator variables to a constant . any proposal not connected to any of the root nodes is excluded from the final solution . we use the solve ##r from for consistency , but a simpler k ##l - based solve ##r as in could be used as well since the td / bu model effectively ignores the una ##ry variables . the processing stages of td / bu model are shown in fig . [ reference ] . note that the body - part heat ##ma ##ps change depending on the person - identity signal provided by the person ' s neck , and that the bottom - up step was able to correct the predictions on the forearms of the front person . implementation details . for head detection , we use a version of our model that contains the two head parts ( neck and head top ) . this makes our td / bu model related to the hierarchical model defined in that also uses easier - to - detect parts to guide the rest of the inference process . however here we replace all the stages in the hierarchical inference except the first one with a con ##vo ##lu ##tion ##al network . the structure of the con ##vo ##lu ##tion ##al network used to generate person - conditioned proposals is shown on fig . [ reference ] . the network uses the res ##net - 101 from that we modify to bring the stride of the network down to 8 pixels . the network generates predictions for all body parts after the con ##v ##4 _ 4 block . we use the cross - entropy binary classification loss at this stage to predict the part heat ##ma ##ps . at each training iteration we forward pass an image with multiple people potentially in close proximity to each other . we select a single person from the image and condition the network on the person ' s neck location by zero ##ing out the heat ##ma ##p of the neck joint outside the ground - truth region . we then pass the neck heat ##ma ##p through a con ##vo ##lu ##tion ##al layer to match the dimensional ##ity of the feature channels and add them to the main stream of the res ##net . we finally add a joint prediction layer at the end of the network with a loss that considers predictions to be correct only if they correspond to the body joints of the selected person . spatial propagation ( sp ) . in our network the person identity signal is provided by the location of the head . in principle the rec ##eptive field size of the network is large enough to prop ##aga ##te this signal to all body parts . however we found that it is useful to introduce an additional mechanism to prop ##aga ##te the person identity signal . to that end we in ##ject intermediate supervision layers for individual body parts arranged in the order of kin ##ema ##tic proximity to the root joint ( fig . [ reference ] ) . we place prediction layers for shoulders at con ##v ##4 _ 8 , for elbows and hips at con ##v ##4 _ 14 and for knees at con ##v ##4 _ 18 . we empirical ##ly found that such an explicit form of spatial propagation significantly improves performance on joints such as ankles , that are typically far from the head in the image space ( see tab . [ reference ] for details ) . training . we use caf ##fe ' s res ##net implementation and initial ##ize from the image ##net - pre - trained models . networks are trained on the mp ##ii human pose data ##set with sg ##d for 1 m iteration ##s with step ##wise learning rate ( l ##r = 0 . 00 ##2 for 400 ##k , l ##r = 0 . 000 ##2 for 300 ##k and l ##r = 0 . 000 ##1 for 300 ##k ) . sub ##section : attractive / rep ##ulsive edges attractive / rep ##ulsive edges are defined between two proposals of the same type within the same image . the costs of these edges is inverse ##ly - proportional to distance . the decision to group two nodes is made based on the evidence from the entire image , which is in contrast to typical non - maximum suppression based on the state of just two detection ##s . inverse ##ly , these edges prevent grouping of multiple distant hypothesis of the same type , prevent merging two heads of different people . sub ##section : temporal model regardless of the type of within frame model ( bu or td / bu ) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames . we derive the costs for such temporal edges via log ##istic regression . given the feature vector the probability that the two proposals and in adjacent frames correspond to the same body part is given by : , where , and - , is euclidean distance between the si ##ft des ##cript ##ors computed at and , and and measure the agreement with the dense motion field computed with the deep ##mat ##ching approach of . for si ##ft features we specify the location of the detection proposal , but rely on si ##ft to identify the local orientation . in cases with multiple local maxim ##a in orientation estimation we compute si ##ft des ##cript ##or for each orientation and set to the minimal distance among all pairs of des ##cript ##ors . we found that this makes the si ##ft distance more robust in the presence of rotation ##s of the body limbs . we define the features and as in . let be an squared image region centered on the part proposal . we define as a ratio of the number of point correspondence ##s between the regions and and the total number of point correspondence ##s in either of them . specifically , let be a set of point correspondence ##s between the two images computed with deep ##mat ##ching , where and and denote the corresponding points in the first and second image respectively . using this notation we define : the rational ##e behind computing by ag ##gre ##gating across multiple correspondence ##s is to make the feature robust to out ##lier ##s and to ina ##cc ##ura ##cies in body part detection . is defined analogous ##ly , but using the deep ##mat ##ching correspondence ##s obtained by in ##vert ##ing the order of images . discussion . as we demonstrate in sec . [ reference ] , we found the set of features described above to be complementary to each other . euclidean distance between proposals is inform ##ative for finding correspondence ##s for slow motions , but fails for faster motions and in the presence of multiple people . deep ##mat ##ching is usually effective in finding corresponding regions between the two images , but occasionally fails in the case of sudden background changes due to fast motion or large changes in body limb orientation . in these cases si ##ft is often still able to provide a meaningful measure of similarity due to its rotation in ##var ##iance . section : experiments sub ##section : data ##set ##s and evaluation measure single frame . we evaluate our single frame models on the mp ##ii multi - person data ##set . we report all intermediate results on a validation set of images sampled uniformly at random ( mp ##ii multi - person val ) , while major results and comparison to the state of the art are reported on the test set . video . in order to evaluate video - based models we introduce a novel \" mp ##ii video pose \" data ##set . to this end we manually selected challenging key ##frame ##s from mp ##ii multi - person data ##set . selected key ##frame ##s represent crowded scenes with highly articulated people engaging in various dynamic activities . in addition to each key ##frame , we include + / - neighboring frames from the corresponding publicly available video sequences , and ann ##ota ##te every second frame . each body pose was ann ##ota ##ted following the standard ann ##ota ##tion procedure , while maintaining person identity throughout the sequence . in contrast to mp ##ii multi - person where some frames may contain non - ann ##ota ##ted people , we ann ##ota ##te all people participating in the activity captured in the video , and add ignore regions for areas that contain dense crowds ( e . g . static spectators in the dancing sequences ) . in total , our data ##set consists of sequences with over ann ##ota ##ted poses . evaluation details . the average precision ( ap ) measure is used for evaluation of pose estimation accuracy . for each algorithm we also report run time of the proposal generation and of the graph partition ##ing stages . all time measurements were conducted on a single core intel x ##eon ghz . finally we also evaluate tracking per ##fo ##man ##ce using standard mo ##ta metric . evaluation on our \" mp ##ii video pose \" data ##set is performed on the full frames using the publicly available evaluation kit of . on mp ##ii multi - person we follow the official evaluation protocol and evaluate on groups using the provided rough group location and scale . sub ##section : single - frame models we compare the performance of different variants of our bottom - up ( bu ) and top - down / bottom - up ( td / bu ) models introduced in sec . [ reference ] and sec . [ reference ] . for bu we consider a model that ( 1 ) uses a fully - connected graph with up to detection proposals and jointly performs partition ##ing and body - part labeling similar to ( bu - full , label ) ; ( 2 ) is same as ( 1 ) , but labeling of detection proposals is done based on detection score ( bu - full ) ; ( 3 ) is same as ( 2 ) , but uses a sparsely - connected graph ( bu - sparse ) . the results are shown in tab . [ reference ] . bu - full , label achieve ##s % ap with a median inference run - time of s / f . bu - full achieve ##s run - time reduction ( vs . s / f ) : pre - labeling detection candidates based on detection score significantly reduces the number of variables in the problem graph . interesting ##ly , pre - labeling also improves the performance ( vs . % ap ) : some of the low - scoring detection ##s may com ##pl ##icate the search for an optimal labeling . bu - sparse further reduces run - time ( vs . s / f ) , as it reduces the complexity of the initial problem by spa ##rs ##ifying the graph , at a price of a drop in performance ( vs . % ap ) . in tab . [ reference ] we compare the variants of the td / bu model . our td approach achieve ##s % ap , performing on par with a more complex bu - full . explicit spatial propagation ( td + sp ) further improves the results ( vs . % ap ) . the largest improvement is observed for ankles : progressive prediction that conditions on the close - by parts in the tree hierarchy reduces the distance between the conditioning signal and the location of the predicted body part and sim ##pl ##ifies the prediction task . performing inference ( td / bu + sp ) improves the performance to % ap , due to more optimal assignment of part detection candidates to corresponding persons . graph sim ##pl ##ification in td / bu allows to further reduce the inference time for graph partition ##ing ( vs . for bu - sparse ) . single frame tracking single frame tracking comparison to the state of the art . we compare the proposed single - frame approaches to the state of the art on mp ##ii multi - person test and wa ##f data ##set ##s . comparison on mp ##ii is shown in tab . [ reference ] . both bu - full and td / bu improve over the best published result of deeper ##cut , achieving and % ap respectively vs . % ap by deeper ##cut . for the td / bu the improvements on articulated parts ( elbows , wrists , ankles , knees ) are particularly pronounced . we argue that this is due to using the network that is directly trained to di ##sam ##bi ##gua ##te body parts of different people , instead of using explicit geometric pair ##wise terms that only serve as a proxy to person ' s identity . overall , the performance of our best td / bu method is noticeably higher ( vs . % ap ) . remarkably , its run - time of graph partition ##ing stage is orders of magnitude faster compared to deeper ##cut . this speed - up is due to two factors . first , td / bu relies on a faster solve ##r that tackles the graph - partition ##ing problem via local search , in contrast to the exact solve ##r used in . second , in the case of td / bu model the graph is sparse and a large portion of the computation is performed by the feed - forward cnn introduced in sec . [ reference ] . on wa ##f data ##set td / bu substantially improves over the best published result ( vs . % ap by ) . we refer to supplemental material for details . sub ##section : multi - frame models comparison of video - based models . performance of the proposed video - based models is compared in tab . [ reference ] . video - based models out ##per ##form single - frame models in each case . bu - full + temporal slightly out ##per ##forms bu - full , where improvements are noticeable for ankle , knee and head . bu - sparse + temporal noticeably improves over bu - sparse ( vs . % ap ) . we observe significant improvements on the most difficult parts such as ankles ( % ap ) and wrists ( % ap ) . interesting ##ly , bu - sparse + temporal out ##per ##forms : longer - range connections such as , , head to ankle , may introduce additional confusion when information is prop ##aga ##ted over time . finally , td / bu + temporal improves over td / bu ( % ap ) . similarly to bu - sparse + temporal , improvement is most prominent on ankles ( % ap ) and wrists ( % ap ) . note that even the single - frame td / bu out ##per ##forms the best temporal bu model . we show examples of articulated tracking on \" mp ##ii video pose \" in fig . [ reference ] . temporal reasoning helps in cases when image information is ambiguous due to close proximity of multiple people . for example the video - based approach succeeds in correctly local ##izing legs of the person in fig . [ reference ] ( d ) and ( h ) . temporal features . we perform an ab ##lative experiment on the \" mp ##ii video pose \" data ##set to evaluate the individual contribution of the temporal features introduced in sec . [ reference ] . the euclidean distance alone achieve ##s ap , adding deep ##mat ##ching features improves the res ##uls to ap , whereas the combination of all features achieve ##s the best result of ap ( details in supplemental material ) . tracking evaluation . in tab . [ reference ] we present results of the evaluation of multi - person articulated body tracking . we treat each body joint of each person as a tracking target and measure tracking performance using a standard multiple object tracking accuracy ( mo ##ta ) metric that incorporates identity switches , false positive ##s and false negative ##s . we experimental ##ly compare to a baseline model that first tracks people across frames and then performs per - frame pose estimation . to track a person we use a reduced version of our algorithm that operates on the two head joints only . this allows to achieve near perfect person tracking results in most cases . our tracker still fails when the person head is o ##cc ##lu ##ded for multiple frames as it does not incorporate long - range connectivity between target hypothesis . we leave handling of long - term o ##cc ##lusion ##s for the future work . for full - body tracking we use the same in ##ital head tracks and add them to the set of body part proposals , while also adding must - link and must - cut constraints for the temporal edges corresponding to the head parts detection ##s . the rest of the graph remains unchanged so that at inference time the body parts can be freely assigned to different person tracks . for the bu - sparse the full body tracking improves performance by and mo ##ta on wrists and ankles , and by and mo ##ta on elbows and knees respectively . td / bu benefits from adding temporal connections between body parts as well , but to a lesser extent than bu - sparse . the most significant improvement is for ankles ( mo ##ta ) . bu - sparse also achieve ##s the best overall score of compared to by td / bu . this is surprising since td / bu out ##per ##formed bu - sparse on the pose estimation task ( see tab . [ reference ] and [ reference ] ) . we h ##yp ##oth ##es ##ize that limited improvement of td / bu could be due to balancing issues between the temporal and spatial pair ##wise terms that are estimated independently of each other . section : conclusion in this paper we introduced an efficient and effective approach to articulated body tracking in mono ##cular video . our approach defines a model that jointly groups body part proposals within each video frame and across time . grouping is formulated as a graph partition ##ing problem that lend ##s itself to efficient inference with recent local search techniques . our approach improves over state - of - the - art while being substantially faster compared to other related work . acknowledge ##ments . this work has been supported by the max planck center for visual computing and communication . the authors thank var ##vara ob ##olo ##nch ##yk ##ova and baha ##r tara ##kam ##eh for their help in creating the video data ##set . section : additional results on the mp ##ii multi - person data ##set we perform qu ##ali ##tative comparison of the proposed single - frame based td / bu and bu - full methods on challenging scenes containing highly articulated and strongly overlapping individuals . results are shown in fig . [ reference ] and figure [ reference ] . the bu - full works well when persons are sufficiently separated ( images 11 and 12 ) . however , it fails on images where people significantly overlap ( images 1 - 3 , 5 - 10 ) or exhibit high degree of art ##ic ##ulation ( image 4 ) . this is due to the fact that geometric image - conditioned pair ##wise may get confused in the presence of multiple overlapping individuals and thus mis ##lea ##d post - cnn bottom - up ass ##em ##bling of body poses . in contrast , td / bu performs explicit modeling of person identity via top - do ##p bottom - up reasoning while off ##loading the larger share of the reasoning about body - part association onto feed - forward con ##vo ##lu ##tion ##al architecture , and thus is able to resolve such challenging cases . interesting ##ly , td / bu is able to correctly predict lower limbs of people in the back through partial o ##cc ##lusion ( image 3 , 5 , 7 , 10 ) . td / bu model occasionally incorrectly assemble ##s body parts in kin ##ema ##tically imp ##laus ##ible manner ( image 12 ) , as it does not explicitly model geometric body part relations . finally , both models fail in pre ##sen ##se of high variations in scale ( image 13 ) . we en ##vision that reasoning over multiple scales is likely to improve the results . bu td / bu bu td / bu bu td / bu bu td / bu section : results on the we are family data ##set we compare our proposed td / bu model to the state - of - the - art methods on the \" we are family \" ( wa ##f ) data ##set and present results in table [ reference ] . we use evaluation protocol from and report the ap evaluation measure . td / bu model out ##per ##forms the best published results across all body parts ( vs % ap ) as well improves on articulated parts such as wrists ( % ap ) and elbows ( % ap ) . we attribute that to the ability of top - down model to better learn part associations compared to explicit modeling geometric pair ##wise relations as in . section : evaluation of temporal features . we evaluate the importance of combining temporal features introduced in sec . 3 . 4 of the paper on our multi - person video data ##set . to that end , we consider bu - sparse + temporal model and compare results to bu - sparse in tab . [ reference ] . single - frame bu - sparse achieve ##s % ap . it can be seen that using geometry based det - distance features slightly improves the results to % ap , as it enables the propagation of information from neighboring frames . using deep ##mat ##ch features slightly improves the performance further as it helps to link the same body part of the same person over time based on the body part appearance . it is especially helpful in the case of fast motion where det - distance may fail . the combination of both geometry and appearance based features further improves the performance to % , which shows their complement ##ari ##ty . finally , adding the si ##ft - distance feature improves the results to % , since it cope ##s better with the sudden changes in background and body part orientation ##s . overall , using a combination of temporal features in bu - sparse + temporal results in a % ap improvement over the single - frame bu - sparse . this demonstrates the advantages of the proposed approach to improve pose estimation performance using temporal information . bibliography : references",
        "pred_seq": "mp ##mark [SEP] art ##ck [SEP] [SEP] single estimation [SEP] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "mpii human pose benchmark"
                    ]
                ],
                "Method": [
                    [
                        "arttrack"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "singleframe pose estimation"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "mpii human pose dataset",
                        "mpii multiperson test",
                        "mpii",
                        "multiperson video dataset"
                    ]
                ],
                "Method": [
                    [
                        "arttrack",
                        "articulated multiperson tracking",
                        "articulated tracking model",
                        "articulated tracking approach"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy",
                        "tracking perfomance",
                        "ap",
                        "false positives"
                    ]
                ],
                "Task": [
                    [
                        "singleframe pose estimation",
                        "3d pose estimation",
                        "single frame multiperson pose estimation",
                        "pose estimation",
                        "perframe pose estimation",
                        "mota",
                        "pose estimation task"
                    ]
                ]
            }
        ]
    },
    "48": {
        "doctext": "question answering with sub ##graph em ##bed ##ding ##s section : abstract . this paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand ##craft ##ed features . our model learns low - dimensional em ##bed ##ding ##s of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers . training our system using pairs of questions and structured representations of their answers , and pairs of question para ##ph ##rase ##s , yields competitive results on a recent bench ##mark of the literature . section : introduction teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been a long standing goal in artificial intelligence . with the rise of large scale structured knowledge bases ( kb ##s ) , this problem , known as open - domain question answering ( or open q ##a ) , boil ##s down to being able to query efficiently such databases with natural language . these kb ##s , such as free ##base [ reference ] encompass huge ever growing amounts of information and ease open q ##a by organizing a great variety of answers in a structured format . however , the scale and the difficulty for machines to interpret natural language still makes this task a challenging problem . the state - of - the - art techniques in open q ##a can be classified into two main classes , namely , information retrieval based and semantic par ##sing based . information retrieval systems first retrieve a broad set of candidate answers by query ##ing the search api of kb ##s with a transformation of the question into a valid query and then use fine - grain ##ed detection he ##uri ##stic ##s to identify the exact answer [ reference ] [ reference ] [ reference ] . on the other hand , semantic par ##sing methods focus on the correct interpretation of the meaning of a question by a semantic par ##sing system . a correct interpretation converts a question into the exact database query that returns the correct answer . interesting ##ly , recent works [ reference ] [ reference ] [ reference ] [ reference ] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large - scale regimes , while bypass ##ing most of the ann ##ota ##tion costs . yet , even if both kinds of system have shown the ability to handle large ##sca ##le kb ##s , they still require experts to hand - craft lexi ##con ##s , grammar ##s , and kb sc ##hema to be effective . this non - ne ##gli ##gible human intervention might not be generic enough to convenient ##ly scale up to new databases with other sc ##hema , broader vo ##ca ##bular ##ies or languages other than english . in contrast , [ reference ] proposed a framework for open q ##a requiring almost no human ann ##ota ##tion . despite being an interesting approach , this method is out ##per ##formed by other competing methods . [ reference ] introduced an em ##bed ##ding model , which learns low - dimensional vector representations of words and symbols ( such as kb ##s constituents ) and can be trained with even less supervision than the system of [ reference ] while being able to achieve better prediction performance . however , this approach is only compared with [ reference ] which operates in a simplified setting and has not been applied in more realistic conditions nor evaluated against the best performing methods . in this paper , we improve the model of [ reference ] by providing the ability to answer more complicated questions . st ##he main contributions of the paper are : ( 1 ) a more sophisticated inference procedure that is both efficient and can consider longer paths ( [ reference ] considered only answers directly connected to the question in the graph ) ; and ( 2 ) a richer representation of the answers which en ##codes the question - answer path and surrounding sub ##graph of the kb . our approach is competitive with the current state - of - the - art on the recent bench ##mark web ##quest ##ions [ reference ] without using any lexi ##con , rules or additional system for part ##of - speech tag ##ging , syn ##ta ##ctic or dependency par ##sing during training as most other systems do . section : task definition our main motivation is to provide a system for open q ##a able to be trained as long as it has access to : ( 1 ) a training set of questions paired with answers and ( 2 ) a kb providing a structure among answers . we suppose that all potential answers are entities in the kb and that questions are sequences of words that include one identified kb entity . when this entity is not given , plain string matching is used to perform entity resolution . smarter methods could be used but this is not our focus . we use web ##quest ##ions [ 1 ] as our evaluation be ##mc ##hma ##rk . since it contains few training samples , it is impossible to learn on it alone , and this section describes the various data sources that were used for training . these are similar to those used in [ reference ] . web ##quest ##ions this data ##set is built using free ##base as the kb and contains 5 , 81 ##0 question - answer pairs . it was created by crawling questions through the google suggest api , and then obtaining answers using amazon mechanical turk . we used the original split ( 3 , 77 ##8 examples for training and 2 , 03 ##2 for testing ) , and isolated 1 ##k questions from the training set for validation . web ##quest ##ions is built on free ##base since all answers are defined as free ##base entities . in each question , we identified one free ##base entity using string matching between words of the question and entity names in free ##base . when the same string matches multiple entities , only the entity appearing in most triple ##s , i . e . the most popular in free ##base , was kept . example questions ( answers ) in the data ##set include \" where did edgar allan poe died ? \" ( baltimore ) or \" what degrees did barack obama get ? \" ( bachelor of arts , juris doctor ) . free ##base free ##base [ reference ] is a huge and freely available database of general facts ; data is organized as triple ##ts ( subject , type ##1 . type ##2 . pre ##dicate , object ) , where two entities subject and object ( identified by mid ##s ) are connected by the relation type type ##1 . type ##2 . pre ##dicate . we used a subset , created by only keeping triple ##s where one of the entities was appearing in either the web ##quest ##ions training / validation set or in clue ##we ##b extraction ##s . we also removed all entities appearing less than 5 times and finally obtained a free ##base set containing 14 m triple ##s made of 2 . 2 m entities and 7 ##k relation types . section : web ##quest ##ions 1 since the format of triple ##s does not correspond to any structure one could find in language , we decided to transform them into automatically generated questions . hence , all triple ##s were converted into questions \" what is the pre ##dicate of the type ##2 subject ? \" ( using the mid of the subject ) with the answer being object . an example is \" what is the nationality of the person barack obama ? \" ( united states ) . more examples and details are given in a longer version of this paper [ reference ] . section : clue ##we ##b extraction ##s free ##base data allows to train our model on 14 m questions but these have a fixed lexi ##con and vocabulary , which is not realistic . following [ reference ] , we also created questions using clue ##we ##b extraction ##s provided by [ reference ] . using string matching , we ended up with 2 m extraction ##s structured as ( subject , \" text string \" , object ) with both subject and object linked to free ##base . we also converted these triple ##s into questions by using simple patterns and free ##base types . an example of generated question is \" where barack obama was allegedly bear in ? \" ( hawaii ) . para ##ph ##rase ##s the automatically generated questions that are useful to connect free ##base triple ##s and natural language , do not provide a satisfactory modeling of natural language because of their semi - automatic word ##ing and rigid syntax . to overcome this issue , we follow [ reference ] table 2 . examples of questions , answer paths and para ##ph ##rase ##s used in this paper . as rep ##hra ##sing ##s of each other : [ reference ] harvested a set of 2 m distinct questions from wi ##kian ##sw ##ers , which were grouped into 350 ##k para ##ph ##rase clusters . section : em ##bed ##ding questions and answers inspired by [ reference ] , our model works by learning low - dimensional vector em ##bed ##ding ##s of words appearing in questions and of entities and relation types of free ##base , so that representations of questions and of their corresponding answers are close to each other in the joint em ##bed ##ding space . let q denote a question and a a candidate answer . learning em ##bed ##ding ##s is achieved by learning a scoring function s ( q , a ) , so that s generates a high score if a is the correct answer to the question q , and a low score otherwise . note that both q and a are represented as a combination of the em ##bed ##ding ##s of their individual words and / or symbols ; hence , learning s essentially involves learning these em ##bed ##ding ##s . in our model , the form of the scoring function is : let w be a matrix of r k ##\u00d7 ##n , where k is the dimension of the em ##bed ##ding space which is fixed a - prior ##i , and n is the dictionary of em ##bed ##ding ##s to be learned . let n w denote the total number of words and n s the total number of entities and relation types . with n = n w + n s , the i - th column of w is the em ##bed ##ding of the i - th element ( word , entity or relation type ) in the dictionary . the function f ( . ) , which maps the questions into the em ##bed ##ding space r k is defined as f ( q ) = w ##\u03c6 ( q ) , where ##\u03c6 ( q ) \u2208 n n , is a sparse vector indicating the number of times each word appears in the question q ( usually 0 or 1 ) . likewise the function g ( . ) which maps the answer into the same em ##bed ##ding space r k as the questions , is given by g ( a ) = w ##\u03c8 ( a ) . here ##\u03c8 ( a ) \u2208 n n is a sparse vector representation of the answer a , which we now detail . section : em ##bed ##ding model free ##base sub ##graph section : binary encoding of the sub ##graph ##\u03c8 ( a ) section : em ##bed ##ding of the sub ##graph g ( a ) binary encoding of the que ##s ##0 ##on ##\u03c6 ( q ) section : em ##bed ##ding of the que ##s ##0 ##on f ( q ) que ##s ##0 ##on q sub ##graph of a candidate answer a ( here k . preston ) section : score s ( q , a ) how the candidate answer fits the que ##s ##0 ##on section : dot product em ##bed ##ding matrix w fig . 1 . illustration of the sub ##graph em ##bed ##ding model scoring a candidate answer : ( i ) locate entity in the question ; ( ii ) compute path from entity to answer ; ( iii ) represent answer as path plus all connected entities to the answer ( the sub ##graph ) ; ( iv ) em ##bed both the question and the answer sub ##graph separately using the learnt em ##bed ##ding vectors , and score the match via their dot product . section : representing candidate answers we now describe possible feature representations for a single candidate answer . ( when there are multiple correct answers , we average these representations , see section 3 . 4 . ) we consider three different types of representation , corresponding to different sub ##graphs of free ##base around it . ( i ) single entity . the answer is represented as a single entity from free ##base : \u03c8 ( a ) is a 1 - of - n s coded vector with 1 corresponding to the entity of the answer , and 0 elsewhere . ( ii ) path representation . the answer is represented as a path from the entity mentioned in the question to the answer entity . in our experiments , we considered 1 - or 2 - hop ##s paths ( i . e . with either 1 or 2 edges to traverse ) : ( barack obama , people . person . place of birth , honolulu ) is a 1 - hop path and ( barack obama , people . person . place of birth , location . location . contained ##by , hawaii ) a 2 - hop ##s path . this results in a ##\u03c8 ( a ) which is a 3 - of - n s or 4 - of - n s coded vector , expressing the start and end entities of the path and the relation types ( but not entities ) in - between . ( iii ) sub ##graph representation . we en ##code both the path representation from ( ii ) , and the entire sub ##graph of entities connected to the candidate answer entity . that is , for each entity connected to the answer we include both the relation type and the entity itself in the representation ##\u03c8 ( a ) . in order to represent the answer path differently to the surrounding sub ##graph ( so the model can differentiate them ) , we double the dictionary size for entities , and use one em ##bed ##ding representation if they are in the path and another if they are in the sub ##graph . thus we now learn a parameter matrix r k ##\u00d7 ##n where n = n w + 2 ##n s ( n s is the total number of entities and relation types ) . if there are c connected entities with d relation types to the candidate answer , its representation is a 3 + c + d or 4 + c + d - of - n s coded vector , depending on the path length . our hypothesis is that including more information about the answer in its representation will lead to improved results . while it is possible that all required information could be encoded in the k dimensional em ##bed ##ding of the single entity ( i ) , it is unclear what dimension k should be to make this possible . for example the em ##bed ##ding of a country entity encoding all of its citizens seems un ##real ##istic . similarly , only having access to the path ignores all the other information we have about the answer entity , unless it is encoded in the em ##bed ##ding ##s of either the entity of the question , the answer or the relations linking them , which might be quite complicated as well . we thus adopt the sub ##graph approach . figure 1 illustrates our model . section : training and loss function as in [ reference ] , we train our model using a margin - based ranking loss function . let d = { ( q i , a i ) : i = 1 , . . . , | d | } be the training set of questions q i paired with their correct answer a i . the loss function we minimize is where m is the margin ( fixed to 0 . 1 ) . mini ##mi ##zing e ##q . ( 2 ) learns the em ##bed ##ding matrix w so that the score of a question paired with a correct answer is greater than with any incorrect [UNK] by at least m . [UNK] is sampled from a set of incorrect [UNK] . this is achieved by sampling 50 % of the time from the set of entities connected to the entity of the question ( i . e . other candidate paths ) , and by replacing the answer entity by a random one otherwise . optimization is accomplished using st ##och ##astic gradient descent , multi - threaded with hog ##wil ##d ! [ reference ] , with the constraint that the columns w i of w remain within the unit - ball , i . e . , [UNK] i , | | w i | | 2 ##\u2264 1 . section : multi ##tas ##k training of em ##bed ##ding ##s since a large number of questions in our training data ##set ##s are synthetic ##ally generated , they do not adequately cover the range of syntax used in natural language . hence , we also multi - task the training of our model with the task of para ##ph ##rase prediction . we do so by alternating the training of s with that of a scoring function s pr ##p ( q 1 , q 2 ) = f ( q 1 ) f ( q 2 ) , which uses the same em ##bed ##ding matrix w and makes the em ##bed ##ding ##s of a pair of questions ( q 1 , q 2 ) similar to each other if they are para ##ph ##rase ##s ( i . e . if they belong to the same para ##ph ##rase cluster ) , and make them different otherwise . training s pr ##p is similar to that of s except that negative samples are obtained by sampling a question from another para ##ph ##rase cluster . we also multi ##tas ##k the training of the em ##bed ##ding ##s with the mapping of the mid ##s of free ##base entities to the actual words of their names , so that the model learns that the em ##bed ##ding of the mid of an entity should be similar to the em ##bed ##ding of the word ( s ) that compose its name ( s ) . section : inference once w is trained , at test time , for a given question q the model predict ##s the answer with : [UNK] = ar ##gm ##ax a ##\u2208 ##a ( q ) s ( q , a ) where a ( q ) is the candidate answer set . this candidate set could be the whole kb but this has both speed and potentially precision issues . instead , we create a candidate set a ( q ) for each question . we recall that each question contains one identified free ##base entity . a ( q ) is first populated with all triple ##s from free ##base involving this entity . this allows to answer simple factual questions whose answers are directly connected to them ( i . e . 1 - hop paths ) . this strategy is denoted c 1 . since a system able to answer only such questions would be limited , we supplement a ( q ) with examples situated in the kb graph at 2 - hop ##s from the entity of the question . we do not add all such quad ##rup ##lets since this would lead to very large candidate sets . instead , we consider the following general approach : given that we are predicting a path , we can predict its elements in turn using a beam search , and hence avoid scoring all candidates . specifically , our model first ranks relation types using e ##q . ( 1 ) , i . e . selects which relation types are the most likely to be expressed in q . we keep the top 10 types ( 10 was selected on the validation set ) and only add 2 - hop ##s candidates to a ( q ) when these relations appear in their path . scores of 1 - hop triple ##s are weighted by 1 . 5 since they have one less element than 2 - hop ##s quad ##rup ##lets . this strategy , denoted c 2 , is used by default . a prediction a can commonly actually be a set of candidate answers , not just one answer , for example for questions like \" who are david beck ##ham ' s children ? \" . this is achieved by considering a prediction to be all the entities that lie on the same 1 - hop or 2 - hop ##s path from the entity found in the question . hence , all answers to the above question are connected to david beck ##ham via the same path ( david beck ##ham , people . person . children , * ) . the feature representation of the prediction is then the average over each candidate entity ' s features ( see section 3 . 1 ) , i . e . \u03c8 all ( a ) = 1 | a | a j : a ##\u03c8 ( a j ) where a j are the individual entities in the overall prediction a . in the results , we compare to a baseline method that can only predict single candidates , which understand ##ly performs poorly . section : experiments we compare our system in terms of f1 score as computed by the official evaluation script 2 ( f1 ( be ##rant ) ) but also with a slightly different f1 definition , termed f1 ( yao ) which was used in [ reference ] ( the difference being the way that questions with no answers are dealt with ) , and precision @ 1 ( p @ 1 ) of the first candidate entity ( even when there are a set of correct answers ) , comparing to recently published systems . 3 the upper part of table 3 indicates that our approach out ##per ##forms [ reference ] , [ reference ] and [ reference ] , and performs similarly as [ reference ] . the lower part of table 3 compares various versions of our model . our default approach uses the sub ##graph representation for answers and c 2 as the candidate answers set . replacing c 2 by c 1 induce ##s a large drop in performance because many questions do not have answers that ##are directly connected to their in ##lu ##ded entity ( not in c 1 ) . however , using all 2 - hop ##s connections as a candidate set is also detrimental , because the larger number of candidates confuse ##s ( and slow ##s a lot ) our ranking based inference . our results also verify our hypothesis of section 3 . 1 , that a richer representation for answers ( using the local sub ##graph ) can store more per ##tine ##nt information . finally , we demonstrate that we greatly improve upon the model of [ reference ] , which actually corresponds to a setting with the path representation and c 1 as candidate set . we also considered an ensemble of our approach and that of [ reference ] . as we only had access to their test predictions we used the following combination method . our approach gives a score s ( q , a ) for the answer it predict ##s . we chose a threshold such that our approach predict ##s 50 % of the time ( when s ( q , a ) is above its value ) , and the other 50 % of the time we use the prediction of [ reference ] instead . we aimed for a 50 / 50 ratio because both methods perform similarly . the ensemble improves the state - of - the - art , and indicates that our models are significantly different in their design . section : conclusion this paper presented an em ##bed ##ding model that learns to perform open q ##a using training data made of questions paired with their answers and of a kb to provide a structure among answers , and can achieve promising performance on the competitive bench ##mark web ##quest ##ions . section :",
        "pred_seq": "[SEP] open answering [SEP] [SEP] question answering [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "opendomain question answering"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "question answering"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "webquestions"
                    ]
                ],
                "Method": [
                    [
                        "subgraph embeddings",
                        "embedding model",
                        "subgraph embedding model",
                        "s"
                    ]
                ],
                "Metric": [
                    [
                        "f1 score",
                        "f1"
                    ]
                ],
                "Task": [
                    [
                        "question answering",
                        "opendomain question answering",
                        "open qa"
                    ]
                ]
            }
        ]
    },
    "49": {
        "doctext": "document : [ comb ##inator ##ial features are essential for the success of many commercial models . manually craft ##ing these features usually comes with high cost due to the variety , volume and velocity of raw data in web - scale systems . factor ##ization based models , which measure interactions in terms of vector product , can learn patterns of comb ##inator ##ial features automatically and general ##ize to unseen features as well . with the great success of deep neural networks ( d ##nn ##s ) in various fields , recently researchers have proposed several d ##nn - based factor ##ization model to learn both low - and high - order feature interactions . despite the powerful ability of learning an arbitrary function from data , plain d ##nn ##s generate feature interactions implicit ##ly and at the bit - wise level . in this paper , we propose a novel compressed interaction network ( ci ##n ) , which aims to generate feature interactions in an explicit fashion and at the vector - wise level . we show that the ci ##n share some functional ##ities with con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) and rec ##urrent neural networks ( rn ##ns ) . we further combine a ci ##n and a classical d ##nn into one unified model , and named this new model extreme deep factor ##ization machine ( x ##dee ##pf ##m ) . on one hand , the x ##dee ##pf ##m is able to learn certain bounded - degree feature interactions explicitly ; on the other hand , it can learn arbitrary low - and high - order feature interactions implicit ##ly . we conduct comprehensive experiments on three real - world data ##set ##s . our results demonstrate that x ##dee ##pf ##m out ##per ##forms state - of - the - art models . we have released the source code of x ##dee ##pf ##m at . combining explicit and implicit feature interactions for recommend ##er systems ] x ##dee ##pf ##m : combining explicit and implicit feature interactions for recommend ##er systems j . lia ##n ] jian ##x ##un lia ##n x . zhou ] xiao ##hua ##n zhou f . zhang ] fu ##zh ##eng zhang z . chen ] z ##hong ##xia chen x . xi ##e ] xi ##ng xi ##e g . sun ] gu ##ang ##zh ##ong sun \u00a1 cc ##s ##20 ##12 \u00bf \u00a1 concept \u00bf \u00a1 / concept _ id \u00bf \u00a1 concept _ des ##c \u00bf information systems personal ##ization \u00a1 / concept _ des ##c \u00bf \u00a1 concept _ significance \u00bf 500 \u00a1 / concept _ significance \u00bf \u00a1 / concept \u00bf \u00a1 concept \u00bf \u00a1 / concept _ id \u00bf \u00a1 concept _ des ##c \u00bf computing method ##ologies neural networks \u00a1 / concept _ des ##c \u00bf \u00a1 concept _ significance \u00bf 500 \u00a1 / concept _ significance \u00bf \u00a1 / concept \u00bf \u00a1 concept \u00bf \u00a1 / concept _ id \u00bf \u00a1 concept _ des ##c \u00bf computing method ##ologies factor ##ization methods \u00a1 / concept _ des ##c \u00bf \u00a1 concept _ significance \u00bf 500 \u00a1 / concept _ significance \u00bf \u00a1 / concept \u00bf \u00a1 / cc ##s ##20 ##12 \u00bf [ 500 ] information systems personal ##ization [ 500 ] computing method ##ologies neural networks [ 500 ] computing method ##ologies factor ##ization methods 2018 2018 ac ##mc ##op ##yr ##ight [ k ##dd ' 18 ] the 24th ac ##m si ##g ##k ##dd international conference on knowledge discovery & data mining ##au ##gus ##t 19 - 23 , 2018 ##lon ##don , united kingdom k ##dd ' 18 : the 24th ac ##m si ##g ##k ##dd international conference on knowledge discovery & data mining , august 19 - 23 , 2018 , london , united kingdom 15 . 00 10 . 114 ##5 / 321 ##9 ##8 ##19 . 322 ##00 ##23 978 - 1 - 450 ##3 - 555 ##2 - 0 / 18 / 08 section : introduction features play a central role in the success of many predict ##ive systems . because using raw features can rarely lead to optimal results , data scientists usually spend a lot of work on the transformation of raw features in order to generate best predict ##ive systems or to win data mining games . one major type of feature transformation is the cross - product transformation over cat ##egorical features . these features are called cross features or multi - way features , they measure the interactions of multiple raw features . for instance , a 3 - way feature and ( user _ organization = ms ##ra , item _ category = deep ##lea ##rn ##ing , time = monday ) has value 1 if the user works at microsoft research asia and is shown a technical article about deep learning on a monday . there are three major downs ##ides for traditional cross feature engineering . first , obtaining high - quality features comes with a high cost . because right features are usually task - specific , data scientists need spend a lot of time exploring the potential patterns from the product data before they become domain experts and extract meaningful cross features . second , in large - scale predict ##ive systems such as web - scale recommend ##er systems , the huge number of raw features makes it in ##fe ##asi ##ble to extract all cross features manually . third , hand - crafted cross features do not general ##ize to unseen interactions in the training data . therefore , learning to interact features without manual engineering is a meaningful task . factor ##ization machines ( fm ) em ##bed each feature to a late ##nt factor vector , and pair ##wise feature interactions are modeled as the inner product of late ##nt vectors : . in this paper we use the term bit to denote a element ( such as ) in late ##nt vectors . the classical fm can be extended to arbitrary higher - order feature interactions , but one major downs ##ide is that , proposes to model all feature interactions , including both useful and useless combinations . as revealed in , the interactions with useless features may introduce noises and de ##grade the performance . in recent years , deep neural networks ( d ##nn ##s ) have become successful in computer vision , speech recognition , and natural language processing with their great power of feature representation learning . it is promising to exploit d ##nn ##s to learn sophisticated and selective feature interactions . proposes a factor ##isation - machine supported neural network ( f ##nn ) to learn high - order feature interactions . it uses the pre - trained factor ##ization machines for field em ##bed ##ding before applying d ##nn . further proposes a product - based neural network ( p ##nn ) , which introduces a product layer between em ##bed ##ding layer and d ##nn layer , and does not rely on pre - trained fm . the major downs ##ide of f ##nn and p ##nn is that they focus more on high - order feature interactions while capture little low - order interactions . the wide & deep and deep ##fm models overcome this problem by introducing hybrid architecture ##s , which contain a shallow component and a deep component with the purpose of learning both memo ##rization and general ##ization . therefore they can jointly learn low - order and high - order feature interactions . all the above ##ment ##ioned models leverage d ##nn ##s for learning high - order feature interactions . however , d ##nn ##s model high - order feature interactions in an implicit fashion . the final function learned by d ##nn ##s can be arbitrary , and there is no theoretical conclusion on what the maximum degree of feature interactions is . in addition , d ##nn ##s model feature interactions at the bit - wise level , which is different from the traditional fm framework which models feature interactions at the vector - wise level . thus , in the field of recommend ##er systems , whether d ##nn ##s are indeed the most effective model in representing high - order feature interactions remains an open question . in this paper , we propose a neural network - based model to learn feature interactions in an explicit , vector - wise fashion . our approach is based on the deep & cross network ( dc ##n ) , which aims to efficiently capture feature interactions of bounded degrees . however , we will argue in section [ reference ] that dc ##n will lead to a special format of interactions . we thus design a novel compressed interaction network ( ci ##n ) to replace the cross network in the dc ##n . ci ##n learns feature interactions explicitly , and the degree of interactions grows with the depth of the network . following the spirit of the wide & deep and deep ##fm models , we combine the explicit high - order interaction module with implicit interaction module and traditional fm module , and name the joint model extreme deep factor ##ization machine ( x ##dee ##pf ##m ) . the new model requires no manual feature engineering and release data scientists from ted ##ious feature searching work . to sum ##mar ##ize , we make the following contributions : we propose a novel model , named extreme deep factor ##ization machine ( x ##dee ##pf ##m ) , that jointly learns explicit and implicit high - order feature interactions effectively and requires no manual feature engineering . we design a compressed interaction network ( ci ##n ) in x ##dee ##pf ##m that learns high - order feature interactions explicitly . we show that the degree of feature interactions increases at each layer , and features interact at the vector - wise level rather than the bit - wise level . we conduct extensive experiments on three real - world data ##set , and the results demonstrate that our x ##dee ##pf ##m out ##per ##forms several state - of - the - art models significantly . the rest of this paper is organized as follows . section [ reference ] provides some preliminary knowledge which is necessary for understanding deep learning - based recommend ##er systems . section [ reference ] introduces our proposed ci ##n and x ##dee ##pf ##m model in detail . we will present experimental exploration ##s on multiple data ##set ##s in section [ reference ] . related works are discussed in section [ reference ] . section [ reference ] concludes this paper . section : pre ##lim ##ina ##ries sub ##section : em ##bed ##ding layer in computer vision or natural language understanding , the input data are usually images or textual signals , which are known to be spatial ##ly and / or temporal ##ly correlated , so d ##nn ##s can be applied directly on the raw feature with dense structures . however , in web - scale recommend ##er systems , the input features are sparse , of huge dimension , and present no clear spatial or temporal correlation . therefore , multi - field cat ##egorical form is widely used by related works . for example , one input instance [ user _ id = s ##0 ##2 , gender = male , organization = ms ##ra , interests = comedy & rock ] is normally transformed into a high - dimensional sparse features via field - aware one - hot encoding : an em ##bed ##ding layer is applied upon the raw feature input to com ##press it to a low dimensional , dense real - value vector . if the field is un ##iva ##lent , the feature em ##bed ##ding is used as the field em ##bed ##ding . take the above instance as an example , the em ##bed ##ding of feature male is taken as the em ##bed ##ding of field gender . if the field is multi ##valent , the sum of feature em ##bed ##ding is used as the field em ##bed ##ding . the em ##bed ##ding layer is illustrated in figure [ reference ] . the result of em ##bed ##ding layer is a wide con ##cate ##nated vector : where denotes the number of fields , and denotes the em ##bed ##ding of one field . although the feature lengths of instances can be various , their em ##bed ##ding ##s are of the same length , where is the dimension of field em ##bed ##ding . sub ##section : implicit high - order interactions f ##nn , deep crossing , and the deep part in wide & deep exploit a feed - forward neural network on the field em ##bed ##ding vector to learn high - order feature interactions . the forward process is : where is the layer depth , is an activation function , and is the output of the - th layer . the visual structure is very similar to what is shown in figure [ reference ] , except that they do not include the fm or product layer . this architecture models the interaction in a bit - wise fashion . that is to say , even the elements within the same field em ##bed ##ding vector will influence each other . p ##nn and deep ##fm modify the above architecture slightly . besides applying d ##nn ##s on the em ##bed ##ding vector , they add a two - way interaction layer in the architecture . therefore , both bit - wise and vector - wise interaction is included in their model . the major difference between p ##nn and deep ##fm , is that p ##nn connects the outputs of product layer to the d ##nn ##s , whereas deep ##fm connects the fm layer directly to the output unit ( refer to figure [ reference ] ) . sub ##section : explicit high - order interactions proposes the cross network ( cross ##net ) whose architecture is shown in figure [ reference ] . it aims to explicitly model the high - order feature interactions . unlike the classical fully - connected feed - forward network , the hidden layers are calculated by the following cross operation : where are weights , bias and output of the - th layer , respectively . we argue that the cross ##net learns a special type of high - order feature interactions , where each hidden layer in the cross ##net is a scala ##r multiple of . consider a - layer cross network with the ( i + 1 ) - th layer defined as . then , the output of the cross network is a scala ##r multiple of . when = 1 , according to the ass ##oc ##ia ##tive law and di ##st ##ri ##bu ##tive law for matrix multiplication , we have : where the scala ##r is actually a linear regression of . thus , is a scala ##r multiple of . suppose the scala ##r multiple statement holds for = . for = , we have : where , is a scala ##r . thus is still a scala ##r multiple of . by induction hypothesis , the output of cross network is a scala ##r multiple of . note that the scala ##r multiple does not mean is linear with . the coefficient is sensitive with . the cross ##net can learn feature interactions very efficiently ( the complexity is ne ##gli ##gible compared with a d ##nn model ) , however the downs ##ides are : ( 1 ) the output of cross ##net is limited in a special form , with each hidden layer is a scala ##r multiple of ; ( 2 ) interactions come in a bit - wise fashion . section : our proposed model . 33 . 32 . 32 sub ##section : compressed interaction network we design a new cross network , named compressed interaction network ( ci ##n ) , with the following considerations : ( 1 ) interactions are applied at vector - wise level , not at bit - wise level ; ( 2 ) high - order feature interactions is measured explicitly ; ( 3 ) the complexity of network will not grow exponential ##ly with the degree of interactions . since an em ##bed ##ding vector is regarded as a unit for vector - wise interactions , here ##af ##ter we formula ##te the output of field em ##bed ##ding as a matrix , where the - th row in is the em ##bed ##ding vector of the - th field : , and is the dimension of the field em ##bed ##ding . the output of the - th layer in ci ##n is also a matrix , where denotes the number of ( em ##bed ##ding ) feature vectors in the - th layer and we let . for each layer , are calculated via : where , is the parameter matrix for the - th feature vector , and denotes the had ##ama ##rd product , for example , . note that is derived via the interactions between and , thus feature interactions are measured explicitly and the degree of interactions increases with the layer depth . the structure of ci ##n is very similar to the rec ##urrent neural network ( rn ##n ) , where the outputs of the next hidden layer are dependent on the last hidden layer and an additional input . we hold the structure of em ##bed ##ding vectors at all layers , thus the interactions are applied at the vector - wise level . it is interesting to point out that equation [ reference ] has strong connections with the well - known con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) in computer vision . as shown in figure [ reference ] , we introduce an intermediate tensor , which is the outer products ( along each em ##bed ##ding dimension ) of hidden layer and original feature matrix . then can be regarded as a special type of image and is a filter . we slide the filter across along the em ##bed ##ding dimension ( d ) as shown in figure [ reference ] , and get an hidden vector , which is usually called a feature map in computer vision . therefore , is a collection of different feature maps . the term \" compressed \" in the name of ci ##n indicates that the - th hidden layer com ##press the potential space of vectors down to vectors . figure [ reference ] provides an overview of the architecture of ci ##n . let t denotes the depth of the network . every hidden layer has a connection with output units . we first apply sum pool ##ing on each feature map of the hidden layer : for . thus , we have a pool ##ing vector with length for the - th hidden layer . all pool ##ing vectors from hidden layers are con ##cate ##nated before connected to output units : . if we use ci ##n directly for binary classification , the output unit is a si ##gm ##oid node on : where are the regression parameters . sub ##section : ci ##n analysis we analyze the proposed ci ##n to study the model complexity and the potential effectiveness . sub ##su ##bs ##ection : space complexity the - th feature map at the - th layer contains parameters , which is exactly the size of . thus , there are parameters at the - th layer . considering the last regression layer for the output unit , which has parameters , the total number of parameters for ci ##n is . note that ci ##n is independent of the em ##bed ##ding dimension . in contrast , a plain - layers d ##nn contains parameters , and the number of parameters will increase with the em ##bed ##ding dimension . usually and will not be very large , so the scale of is acceptable . when necessary , we can exploit a - order decomposition and replace with two smaller matrices and : where and . here ##af ##ter we assume that each hidden layer has the same number ( which is ) of feature maps for simplicity . through the - order decomposition , the space complexity of ci ##n is reduced from to . in contrast , the space complexity of the plain d ##nn is , which is sensitive to the dimension ( d ) of field em ##bed ##ding . sub ##su ##bs ##ection : time complexity the cost of computing tensor ( as shown in figure [ reference ] ) is time . because we have feature maps in one hidden layer , computing a - layers ci ##n takes time . a - layers plain d ##nn , by contrast , takes time . therefore , the major downs ##ide of ci ##n lies in the time complexity . sub ##su ##bs ##ection : polynomial approximation next we examine the high - order interaction properties of ci ##n . for simplicity , we assume that numbers of feature maps at hidden layers are all equal to the number of fields . let denote the set of positive integers that are less than or equal to . the - th feature map at the first layer , denoted as , is calculated via : therefore , each feature map at the first layer models pair - wise interactions with coefficients . similarly , the - th feature map at the second layer is : note that all calculations related to the sub ##script and is already finished at the previous hidden layer . we expand the factors in equation [ reference ] just for clarity . we can observe that each feature map at the second layer models 3 - way interactions with new parameters . a classical - order polynomial has coefficients . we show that ci ##n approximate this class of polynomial with only parameters in terms of a chain of feature maps . by induction hypothesis , we can prove that the - th feature map at the - th layer is : for better illustration , here we borrow the notation ##s from . let denote a multi - index , and . we om ##it the original super ##script from , and use to denote it since we only we the feature maps from the - th layer ( which is exactly the field em ##bed ##ding ##s ) for the final expanded expression ( refer to e ##q . [ reference ] ) . now a super ##script is used to denote the vector operation , such as . let denote a multi - vector polynomial of degree : each vector poly ##ln ##omi ##al in this class has coefficients . then , our ci ##n approaches the coefficient with : where , is a multi - index , and is the set of all the per ##mut ##ations of the indices . sub ##section : combination with implicit networks as discussed in section [ reference ] , plain d ##nn ##s learn implicit high - order feature interactions . since ci ##n and plain d ##nn ##s can complement each other , an intuitive way to make the model stronger is to combine these two structures . the resulting model is very similar to the wide & deep or deep ##fm model . the architecture is shown in figure [ reference ] . we name the new model extreme deep factor ##ization machine ( x ##dee ##pf ##m ) , considering that on one hand , it includes both low - order and high - order feature interactions ; on the other hand , it includes both implicit feature interactions and explicit feature interactions . its resulting output unit becomes : where is the si ##gm ##oid function , is the raw features . are the outputs of the plain d ##nn and ci ##n , respectively . and are learn ##able parameters . for binary classifications , the loss function is the log loss : where is the total number of training instances . the optimization process is to minimize the following objective function : where denotes the regular ##ization term and denotes the set of parameters , including these in linear part , ci ##n part , and d ##nn part . sub ##su ##bs ##ection : relationship with fm and deep ##fm suppose all fields are un ##iva ##lent . it ' s not hard to observe from figure [ reference ] that , when the depth and feature maps of the ci ##n part are both set to 1 , x ##dee ##pf ##m is a general ##ization of deep ##fm by learning the linear regression weights for the fm layer ( note that in deep ##fm , units of fm layer are directly linked to the output unit without any coefficients ) . when we further remove the d ##nn part , and at the same time use a constant sum filter ( which simply takes the sum of inputs without any parameter learning ) for the feature map , then x ##dee ##pf ##m is down ##grade ##d to the traditional fm model . section : experiments in this section , we conduct extensive experiments to answer the following questions : ( q ##1 ) how does our proposed ci ##n perform in high - order feature interactions learning ? ( q ##2 ) is it necessary to combine explicit and implicit high - order feature interactions for recommend ##er systems ? ( q ##3 ) how does the settings of networks influence the performance of x ##dee ##pf ##m ? we will answer these questions after presenting some fundamental experimental settings . sub ##section : experiment setup sub ##su ##bs ##ection : data ##set ##s . we evaluate our proposed models on the following three data ##set ##s : 1 . cr ##ite ##o data ##set . it is a famous industry bench ##mark ##ing data ##set for developing models predicting ad click - through rate , and is publicly accessible . given a user and the page he is visiting , the goal is to predict the probability that he will cl ##ik on a given ad . 2 . dia ##np ##ing data ##set . dia ##np ##ing . com is the largest consumer review site in china . it provides diverse functions such as reviews , check - ins , and shops ' meta information ( including geographical messages and shop attributes ) . we collect 6 months ' users check - in activities for restaurant recommendation experiments . given a user ' s profile , a restaurant ' s attributes and the user ' s last three visited po ##is ( point of interest ) , we want to predict the probability that he will visit the restaurant . for each restaurant in a user ' s check - in instance , we sample four restaurants which are within 3 kilometers as negative instances by po ##i popularity . 3 . bing news data ##set . bing news is part of microsoft ' s bing search engine . in order to evaluate the performance of our model in a real commercial data ##set , we collect five consecutive days ' impression logs on news reading service . we use the first three days ' data for training and validation , and the next two days for testing . for the cr ##ite ##o data ##set and the dia ##np ##ing data ##set , we randomly split instances by 8 : 1 : 1 for training , validation and test . the characteristics of the three data ##set ##s are summarized in table [ reference ] . sub ##su ##bs ##ection : evaluation metric ##s . we use two metric ##s for model evaluation : au ##c ( area under the roc curve ) and log ##los ##s ( cross entropy ) . these two metric ##s evaluate the performance from two different angels : au ##c measures the probability that a positive instance will be ranked higher than a randomly chosen negative one . it only takes into account the order of predicted instances and is ins ##ens ##itive to class im ##balance problem . log ##los ##s , in contrast , measures the distance between the predicted score and the true label for each instance . sometimes we rely more on log ##los ##s because we need to use the predicted probability to estimate the benefit of a ranking strategy ( which is usually adjusted as ct ##r bid ) . sub ##su ##bs ##ection : baseline ##s . we compare our x ##dee ##pf ##m with l ##r ( log ##istic regression ) , fm , d ##nn ( plain deep neural network ) , p ##nn ( choose the better one from ip ##nn and op ##nn ) , wide & deep , dc ##n ( deep & cross network ) and deep ##fm . as introduced and discussed in section [ reference ] , these models are highly related to our x ##dee ##pf ##m and some of them are state - of - the - art models for recommend ##er systems . note that the focus of this paper is to learn feature interactions automatically , so we do not include any hand - crafted cross features . sub ##su ##bs ##ection : rep ##rod ##uc ##ibility we implement our method using tensor ##flow . hyper - parameters of each model are tuned by grid - searching on the validation set , and the best settings for each model will be shown in corresponding sections . learning rate is set to 0 . 001 . for optimization method , we use the adam with a mini - batch size of 40 ##9 ##6 . we use a l ##2 regular ##ization with for d ##nn , dc ##n , wide & deep , deep ##fm and x ##dee ##pf ##m , and use drop ##out 0 . 5 for p ##nn . the default setting for number of neurons per layer is : ( 1 ) 400 for d ##nn layers ; ( 2 ) 200 for ci ##n layers on cr ##ite ##o data ##set , and 100 for ci ##n layers on dia ##np ##ing and bing news data ##set ##s . since we focus on neural networks structures in this paper , we make the dimension of field em ##bed ##ding for all models be a fixed value of 10 . we conduct experiments of different settings in parallel with 5 tesla k ##80 gp ##us . the source code is available at . sub ##section : performance comparison among individual neural components ( q ##1 ) we want to know how ci ##n performs individually . note that fm measures 2 - order feature interactions explicitly , d ##nn model high - order feature interactions implicit ##ly , cross ##net tries to model high - order feature interactions with a small number of parameters ( which is proven not effective in section [ reference ] ) , and ci ##n models high - order feature interactions explicitly . there is no theo ##ret ##ic guarantee of the superiority of one individual model over the others , due to that it really depends on the data ##set . for example , if the practical data ##set does not require high - order feature interactions , fm may be the best individual model . thus we do not have any expectation for which model will perform the best in this experiment . table [ reference ] shows the results of individual models on the three practical data ##set ##s . surprisingly , our ci ##n out ##per ##form the other models consistently . on one hand , the results indicate that for practical data ##set ##s , higher - order interactions over sparse features are necessary , and this can be verified through the fact that d ##nn , cross ##net and ci ##n out ##per ##form fm significantly on all the three data ##set ##s . on the other hand , ci ##n is the best individual model , which demonstrates the effectiveness of ci ##n on modeling explicit high - order feature interactions . note that a - layer ci ##n can model - degree feature interactions . it is also interesting to see that it take 5 layers for ci ##n to yield the best result on the bing news data ##set . sub ##section : performance of integrated models ( q ##2 ) x ##dee ##pf ##m integrate ##s ci ##n and d ##nn into an end - to - end model . while ci ##n and d ##nn covers two distinct properties in learning feature interactions , we are interested to know whether it is indeed necessary and effective to combine them together for jointly explicit and implicit learning . here we compare several strong baseline ##s which are not limited to individual models , and the results are shown in table [ reference ] . we observe that l ##r is far worse than all the rest models , which demonstrates that factor ##ization - based models are essential for measuring sparse features . wide & deep , dc ##n , deep ##fm and x ##dee ##pf ##m are significantly better than d ##nn , which directly reflects that , despite their simplicity , incorporating hybrid components are important for boost ##ing the accuracy of predict ##ive systems . our proposed x ##dee ##pf ##m achieve ##s the best performance on all data ##set ##s , which demonstrates that combining explicit and implicit high - order feature interaction is necessary , and x ##dee ##pf ##m is effective in learning this class of combination . another interesting observation is that , all the neural - based models do not require a very deep network structure for the best performance . typical settings for the depth hyper - parameter are 2 and 3 , and the best depth setting for x ##dee ##pf ##m is 3 , which indicates that the interactions we learned are at most 4 - order . sub ##section : hyper - parameter study ( q ##3 ) we study the impact of hyper - parameters on x ##dee ##pf ##m in this section , including ( 1 ) the number of hidden layers ; ( 2 ) the number of neurons per layer ; and ( 3 ) activation functions . we conduct experiments via holding the best settings for the d ##nn part while varying the settings for the ci ##n part . . 32 . 32 . 32 . 32 . 32 . 32 depth of network . figure [ reference ] and [ reference ] demonstrate the impact of number of hidden layers . we can observe that the performance of x ##dee ##pf ##m increases with the depth of network at the beginning . however , model performance de ##grade ##s when the depth of network is set greater than 3 . it is caused by over ##fi ##tting evidenced by that we notice that the loss of training data still keeps decreasing when we add more hidden layers . number of neurons per layer . adding the number of neurons per layer indicates increasing the number of feature maps in ci ##n . as shown in figure [ reference ] and [ reference ] , model performance on bing news data ##set increases steadily when we increase the number of neurons from to , while on dia ##np ##ing data ##set , is a more suitable setting for the number of neurons per layer . in this experiment we fix the depth of network at 3 . activation function . note that we exploit the identity as activation function on neurons of ci ##n , as shown in e ##q . [ reference ] . a common practice in deep learning literature is to employ non - linear activation functions on hidden neurons . we thus compare the results of different activation functions on ci ##n ( for neurons in d ##nn , we keep the activation function with re ##lu ) . as shown in figure [ reference ] and [ reference ] , identify function is indeed the most suitable one for neurons in ci ##n . section : related work sub ##section : classical recommend ##er systems sub ##su ##bs ##ection : non - factor ##ization models for web - scale recommend ##er systems ( rs ##s ) , the input features are usually sparse , cat ##egorical - continuous - mixed , and high - dimensional . linear models , such as log ##istic regression with ft ##rl , are widely adopted as they are easy to manage , maintain , and deploy . because linear models lack the ability of learning feature interactions , data scientists have to spend a lot of work on engineering cross features in order to achieve better performance . considering that some hidden features are hard to design manually , some researchers exploit boost ##ing decision trees to help build feature transformations . sub ##su ##bs ##ection : factor ##ization models a major downs ##ide of the aforementioned models is that they can not general ##ize to unseen feature interactions in the training set . factor ##ization machines overcome this problem via em ##bed ##ding each feature into a low dimension late ##nt vector . matrix factor ##ization ( m ##f ) , which only considers id ##s as features , can be regarded as a special kind of fm . recommendations are made via the product of two late ##nt vectors , thus it does not require the co - occurrence of user and item in the training set . m ##f is the most popular model - based collaborative filtering method in the rs literature . extend m ##f to lever ##aging side information , in which both a linear model and a m ##f model are included . on the other hand , for many recommend ##er systems , only implicit feedback data ##set ##s such as users ' watching history and brows ##ing activities are available . thus researchers extend the factor ##ization models to a bay ##esian personal ##ized ranking ( bp ##r ) framework for implicit feedback . sub ##section : recommend ##er systems with deep learning deep learning techniques have achieved great success in computer vision , speech recognition and natural language understanding . as a result , an increasing number of researchers are interested in employing d ##nn ##s for recommend ##er systems . sub ##su ##bs ##ection : deep learning for high - order interactions to avoid manually building up high - order cross features , researchers apply d ##nn ##s on field em ##bed ##ding , thus patterns from cat ##egorical feature interactions can be learned automatically . representative models include f ##nn , p ##nn , deep ##cross , n ##fm , dc ##n , wide & deep , and deep ##fm . these models are highly related to our proposed x ##dee ##pf ##m . since we have reviewed them in section [ reference ] and section [ reference ] , we do not further discuss them in detail in this section . we have demonstrated that our proposed x ##dee ##pf ##m has two special properties in comparison with these models : ( 1 ) x ##dee ##pf ##m learns high - order feature interactions in both explicit and implicit fashion ##s ; ( 2 ) x ##dee ##pf ##m learns feature interactions at the vector - wise level rather than at the bit - wise level . sub ##su ##bs ##ection : deep learning for elaborate representation learning we include some other deep learning - based rs ##s in this section due to that they are less focused on learning feature interactions . some early work employs deep learning mainly to model auxiliary information , such as visual data and audio data . recently , deep neural networks are used to model the collaborative filtering ( cf ) in rs ##s . proposes a neural collaborative filtering ( nc ##f ) so that the inner product in m ##f can be replaced with an arbitrary function via a neural architecture . model cf base on the auto ##en ##code ##r paradigm , and they have empirical ##ly demonstrated that auto ##en ##code ##r - based cf out ##per ##forms several classical m ##f models . auto ##en ##code ##rs can be further employed for jointly modeling cf and side information with the purpose of generating better late ##nt factors . employ neural networks to jointly train multiple domains ' late ##nt factors . proposes the at ##ten ##tive collaborative filtering ( ac ##f ) to learn more elaborate preference at both item - level and component - level . shows tha traditional rs ##s can not capture interest diversity and local activation effectively , so they introduce a deep interest network ( din ) to represent users ' diverse interests with an at ##ten ##tive activation mechanism . section : conclusions in this paper , we propose a novel network named compressed interaction network ( ci ##n ) , which aims to learn high - order feature interactions explicitly . ci ##n has two special virtues : ( 1 ) it can learn certain bounded - degree feature interactions effectively ; ( 2 ) it learns feature interactions at a vector - wise level . following the spirit of several popular models , we incorporate a ci ##n and a d ##nn in an end - to - end framework , and named the resulting model extreme deep factor ##ization machine ( x ##dee ##pf ##m ) . thus x ##dee ##pf ##m can automatically learn high - order feature interactions in both explicit and implicit fashion ##s , which is of great significance to reducing manual feature engineering work . we conduct comprehensive experiments and the results demonstrate that our x ##dee ##pf ##m out ##per ##forms state - of - the - art models consistently on three real - world data ##set ##s . there are two directions for future work . first , currently we simply employ a sum pool ##ing for em ##bed ##ding multi ##valent fields . we can explore the usage of the din mechanism to capture the related activation according to the candidate item . second , as discussed in section [ reference ] , the time complexity of the ci ##n module is high . we are interested in developing a distributed version of x ##dee ##pf ##m which can be trained efficiently on a gp ##u cluster . section : acknowledge ##ments the authors would like to thank the anonymous reviewers for their insight ##ful reviews , which are very helpful on the revision of this paper . this work is supported in part by youth innovation promotion association of cas . bibliography : references",
        "pred_seq": "[SEP] extreme machine [SEP] [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "extreme deep factorization machine"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "bing news dataset",
                        "bing news",
                        "bing news datasets"
                    ]
                ],
                "Method": [
                    [
                        "deep factorization machine",
                        "xdeepfm",
                        "xdeepfm model"
                    ]
                ],
                "Metric": [
                    [
                        "auc",
                        "area under roc curve",
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "predicting ad clickthrough rate",
                        "ctr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "bing news dataset",
                        "bing news",
                        "bing news datasets"
                    ]
                ],
                "Method": [
                    [
                        "deep factorization machine",
                        "xdeepfm",
                        "xdeepfm model"
                    ]
                ],
                "Metric": [
                    [
                        "loss function",
                        "log loss",
                        "logloss"
                    ]
                ],
                "Task": [
                    [
                        "predicting ad clickthrough rate",
                        "ctr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "criteo dataset"
                    ]
                ],
                "Method": [
                    [
                        "deep factorization machine",
                        "xdeepfm",
                        "xdeepfm model"
                    ]
                ],
                "Metric": [
                    [
                        "auc",
                        "area under roc curve",
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "predicting ad clickthrough rate",
                        "ctr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "criteo dataset"
                    ]
                ],
                "Method": [
                    [
                        "deep factorization machine",
                        "xdeepfm",
                        "xdeepfm model"
                    ]
                ],
                "Metric": [
                    [
                        "loss function",
                        "log loss",
                        "logloss"
                    ]
                ],
                "Task": [
                    [
                        "predicting ad clickthrough rate",
                        "ctr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "dianping dataset",
                        "dianping"
                    ]
                ],
                "Method": [
                    [
                        "deep factorization machine",
                        "xdeepfm",
                        "xdeepfm model"
                    ]
                ],
                "Metric": [
                    [
                        "auc",
                        "area under roc curve",
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "predicting ad clickthrough rate",
                        "ctr"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "dianping dataset",
                        "dianping"
                    ]
                ],
                "Method": [
                    [
                        "deep factorization machine",
                        "xdeepfm",
                        "xdeepfm model"
                    ]
                ],
                "Metric": [
                    [
                        "loss function",
                        "log loss",
                        "logloss"
                    ]
                ],
                "Task": [
                    [
                        "predicting ad clickthrough rate",
                        "ctr"
                    ]
                ]
            }
        ]
    },
    "50": {
        "doctext": "document : explaining and harness ##ing ad ##vers ##aria ##l examples several machine learning models , including neural networks , consistently mis ##class ##ify ad ##vers ##aria ##l examples \u2014 inputs formed by applying small but intentionally worst - case per ##tur ##bation ##s to examples from the data ##set , such that the per ##tur ##bed input results in the model output ##ting an incorrect answer with high confidence . early attempts at explaining this phenomenon focused on nonlinear ##ity and over ##fi ##tting . we argue instead that the primary cause of neural networks ' vulnerability to ad ##vers ##aria ##l per ##tur ##bation is their linear nature . this explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them : their general ##ization across architecture ##s and training sets . moreover , this view yields a simple and fast method of generating ad ##vers ##aria ##l examples . using this approach to provide examples for ad ##vers ##aria ##l training , we reduce the test set error of a max ##out network on the mn ##ist data ##set . * section : 0 ##pt ##1 ##pt ##1 ##pt section : introduction s ##ze ##ged ##y - ic ##lr ##20 ##14 made an intriguing discovery : several machine learning models , including state - of - the - art neural networks , are vulnerable to ad ##vers ##aria ##l examples . that is , these machine learning models mis ##class ##ify examples that are only slightly different from correctly classified examples drawn from the data distribution . in many cases , a wide variety of models with different architecture ##s trained on different subset ##s of the training data mis ##class ##ify the same ad ##vers ##aria ##l example . this suggests that ad ##vers ##aria ##l examples expose fundamental blind spots in our training algorithms . the cause of these ad ##vers ##aria ##l examples was a mystery , and speculative explanations have suggested it is due to extreme nonlinear ##ity of deep neural networks , perhaps combined with insufficient model averaging and insufficient regular ##ization of the purely supervised learning problem . we show that these speculative h ##yp ##oth ##eses are unnecessary . linear behavior in high - dimensional spaces is sufficient to cause ad ##vers ##aria ##l examples . this view enables us to design a fast method of generating ad ##vers ##aria ##l examples that makes ad ##vers ##aria ##l training practical . we show that ad ##vers ##aria ##l training can provide an additional regular ##ization benefit beyond that provided by using drop ##out drop ##out alone . generic regular ##ization strategies such as drop ##out , pre ##train ##ing , and model averaging do not con ##fer a significant reduction in a model ' s vulnerability to ad ##vers ##aria ##l examples , but changing to nonlinear model families such as rb ##f networks can do so . our explanation suggests a fundamental tension between designing models that are easy to train due to their linear ##ity and designing models that use nonlinear effects to resist ad ##vers ##aria ##l per ##tur ##bation . in the long run , it may be possible to escape this trade ##off by designing more powerful optimization methods that can su ##cc ##es ##fully train more nonlinear models . section : related work s ##ze ##ged ##y - ic ##lr ##20 ##14 demonstrated a variety of intriguing properties of neural networks and related models . those most relevant to this paper include : box - constrained l - bf ##gs can re ##lia ##bly find ad ##vers ##aria ##l examples . on some data ##set ##s , such as image ##net image ##net , the ad ##vers ##aria ##l examples were so close to the original examples that the differences were ind ##ist ##ing ##uis ##hab ##le to the human eye . the same ad ##vers ##aria ##l example is often mis ##class ##ified by a variety of class ##ifiers with different architecture ##s or trained on different subset ##s of the training data . shallow soft ##max regression models are also vulnerable to ad ##vers ##aria ##l examples . training on ad ##vers ##aria ##l examples can regular ##ize the model \u2014 however , this was not practical at the time due to the need for expensive constrained optimization in the inner loop . these results suggest that class ##ifiers based on modern machine learning techniques , even those that obtain excellent performance on the test set , are not learning the true underlying concepts that determine the correct output label . instead , these algorithms have built a pot ##em ##kin village that works well on naturally occur ##ing data , but is exposed as a fake when one visits points in space that do not have high probability in the data distribution . this is particularly disappointing because a popular approach in computer vision is to use con ##vo ##lu ##tion ##al network features as a space where euclidean distance approximate ##s per ##ce ##pt ##ual distance . this resemblance is clearly flawed if images that have an im ##me ##as ##ura ##bly small per ##ce ##pt ##ual distance correspond to completely different classes in the network ' s representation . these results have often been interpreted as being a flaw in deep networks in particular , even though linear class ##ifiers have the same problem . we regard the knowledge of this flaw as an opportunity to fix it . indeed , luca and causal have already begun the first steps toward designing models that resist ad ##vers ##aria ##l per ##tur ##bation , though no model has yet su ##cc ##es ##fully done so while maintaining state of the art accuracy on clean inputs . section : the linear explanation of ad ##vers ##aria ##l examples we start with explaining the existence of ad ##vers ##aria ##l examples for linear models . in many problems , the precision of an individual input feature is limited . for example , digital images often use only 8 bits per pixel so they disc ##ard all information below of the dynamic range . because the precision of the features is limited , it is not rational for the class ##ifier to respond differently to an input than to an ad ##vers ##aria ##l input if every element of the per ##tur ##bation is smaller than the precision of the features . formally , for problems with well - separated classes , we expect the class ##ifier to assign the same class to and so long as , where is small enough to be discarded by the sensor or data storage apparatus associated with our problem . consider the dot product between a weight vector and an ad ##vers ##aria ##l example : the ad ##vers ##aria ##l per ##tur ##bation causes the activation to grow by . we can maximize this increase subject to the max norm constraint on by assign ##ing . if has dimensions and the average magnitude of an element of the weight vector is , then the activation will grow by . since does not grow with the dimensional ##ity of the problem but the change in activation caused by per ##tur ##bation by can grow linear ##ly with , then for high dimensional problems , we can make many infinite ##si ##mal changes to the input that add up to one large change to the output . we can think of this as a sort of \" accidental ste ##gano ##graphy , \" where a linear model is forced to attend exclusively to the signal that align ##s most closely with its weights , even if multiple signals are present and other signals have much greater amplitude . this explanation shows that a simple linear model can have ad ##vers ##aria ##l examples if its input has sufficient dimensional ##ity . previous explanations for ad ##vers ##aria ##l examples invoked h ##yp ##oth ##es ##ized properties of neural networks , such as their supposed highly non - linear nature . our hypothesis based on linear ##ity is simpler , and can also explain why soft ##max regression is vulnerable to ad ##vers ##aria ##l examples . section : linear per ##tur ##bation of non - linear models the linear view of ad ##vers ##aria ##l examples suggests a fast way of generating them . we h ##yp ##oth ##es ##ize that neural networks are too linear to resist linear ad ##vers ##aria ##l per ##tur ##bation . l ##st ##ms l ##st ##m , re ##lus jarrett - icc ##v ##200 ##9 , g ##lor ##ot + al - ai - 2011 , and max ##out networks good ##fell ##ow - et - al - ic ##ml ##20 ##13 are all intentionally designed to behave in very linear ways , so that they are easier to opt ##imi ##ze . more nonlinear models such as si ##gm ##oid networks are carefully tuned to spend most of their time in the non - sat ##ura ##ting , more linear regime for the same reason . this linear behavior suggests that cheap , analytical per ##tur ##bation ##s of a linear model should also damage neural networks . let be the parameters of a model , the input to the model , the targets associated with ( for machine learning tasks that have targets ) and be the cost used to train the neural network . we can linear ##ize the cost function around the current value of , obtaining an optimal max - norm constrained per ##tub ##ation of we refer to this as the \" fast gradient sign method \" of generating ad ##vers ##aria ##l examples . note that the required gradient can be computed efficiently using back ##pro ##pa ##gation . we find that this method re ##lia ##bly causes a wide variety of models to mis ##class ##ify their input . see fig . [ reference ] for a demonstration on image ##net . we find that using , we cause a shallow soft ##max class ##ifier to have an error rate of 99 . 9 % with an average confidence of 79 . 3 % on the mn ##ist le ##cu ##n + 98 test set . in the same setting , a max ##out network mis ##class ##ifies 89 . 4 % of our ad ##vers ##aria ##l examples with an average confidence of 97 . 6 % . similarly , using , we obtain an error rate of 87 . 15 % and an average probability of 96 . 6 % assigned to the incorrect labels when using a con ##vo ##lu ##tion ##al max ##out network on a prep ##ro ##ces ##sed version of the ci ##far - 10 k ##riz ##he ##vsky ##hin ##ton ##200 ##9 test set . other simple methods of generating ad ##vers ##aria ##l examples are possible . for example , we also found that rotating by a small angle in the direction of the gradient re ##lia ##bly produces ad ##vers ##aria ##l examples . the fact that these simple , cheap algorithms are able to generate mis ##class ##ified examples serves as evidence in favor of our interpretation of ad ##vers ##aria ##l examples as a result of linear ##ity . the algorithms are also useful as a way of speeding up ad ##vers ##aria ##l training or even just analysis of trained networks . section : ad ##vers ##aria ##l training of linear models versus weight decay perhaps the simplest possible model we can consider is log ##istic regression . in this case , the fast gradient sign method is exact . we can use this case to gain some intuition for how ad ##vers ##aria ##l examples are generated in a simple setting . see fig . [ reference ] for ins ##truct ##ive images . if we train a single model to recognize labels with where is the log ##istic si ##gm ##oid function , then training consists of gradient descent on where is the soft ##pl ##us function . we can derive a simple analytical form for training on the worst - case ad ##vers ##aria ##l per ##tur ##bation of rather than itself , based on gradient sign per ##tur ##bation . note that the sign of the gradient is just , and that . the ad ##vers ##aria ##l version of log ##istic regression is therefore to minimize this is somewhat similar to regular ##ization . however , there are some important differences . most significantly , the penalty is sub ##tracted off the model ' s activation during training , rather than added to the training cost . this means that the penalty can eventually start to disappear if the model learns to make confident enough predictions that sat ##ura ##tes . this is not guaranteed to happen \u2014 in the under ##fi ##tting regime , ad ##vers ##aria ##l training will simply worse ##n under ##fi ##tting . we can thus view weight decay as being more \" worst case \" than ad ##vers ##aria ##l training , because it fails to dea ##ct ##ivate in the case of good margin . if we move beyond log ##istic regression to multi ##class soft ##max regression , weight decay becomes even more pe ##ssi ##mist ##ic , because it treats each of the soft ##max ' s outputs as independently per ##tur ##ba ##ble , when in fact it is usually not possible to find a single that align ##s with all of the class ' s weight vectors . weight decay over ##est ##imate ##s the damage ac ##hi ##eva ##ble with per ##tur ##bation even more in the case of a deep network with multiple hidden units . because weight decay over ##est ##imate ##s the amount of damage an adversary can do , it is necessary to use a smaller weight decay coefficient than the associated with the precision of our features . when training max ##out networks on mn ##ist , we obtained good results using ad ##vers ##aria ##l training with . when applying weight decay to the first layer , we found that even a coefficient of . 00 ##25 was too large , and caused the model to get stuck with over 5 % error on the training set . smaller weight decay coefficients permitted su ##cc ##es ##ful training but conferred no regular ##ization benefit . section : ad ##vers ##aria ##l training of deep networks the criticism of deep networks as vulnerable to ad ##vers ##aria ##l examples is somewhat mis ##guide ##d , because unlike shallow linear models , deep networks are at least able to represent functions that resist ad ##vers ##aria ##l per ##tur ##bation . the universal approx ##ima ##tor theorem horn ##ik ##8 ##9 guarantees that a neural network with at least one hidden layer can represent any function to an ar ##bit ##ary degree of accuracy so long as its hidden layer is permitted to have enough units . shallow linear models are not able to become constant near training points while also assign ##ing different outputs to different training points . of course , the universal approx ##ima ##tor theorem does not say anything about whether a training algorithm will be able to discover a function with all of the desired properties . obviously , standard supervised training does not specify that the chosen function be resistant to ad ##vers ##aria ##l examples . this must be encoded in the training procedure somehow . s ##ze ##ged ##y - ic ##lr ##20 ##14 showed that by training on a mixture of ad ##vers ##aria ##l and clean examples , a neural network could be regular ##ized somewhat . training on ad ##vers ##aria ##l examples is somewhat different from other data aug ##ment ##ation schemes ; usually , one aug ##ments the data with transformations such as translations that are expected to actually occur in the test set . this form of data aug ##ment ##ation instead uses inputs that are unlikely to occur naturally but that expose flaws in the ways that the model conceptual ##izes its decision function . at the time , this procedure was never demonstrated to improve beyond drop ##out on a state of the art bench ##mark . however , this was partially because it was difficult to experiment extensively with expensive ad ##vers ##aria ##l examples based on l - bf ##gs . we found that training with an ad ##vers ##aria ##l objective function based on the fast gradient sign method was an effective regular ##izer : in all of our experiments , we used . other values may work better ; our initial guess of this hyper ##para ##meter worked well enough that we did not feel the need to explore more . this approach means that we continually update our supply of ad ##vers ##aria ##l examples , to make them resist the current version of the model . using this approach to train a max ##out network that was also regular ##ized with drop ##out , we were able to reduce the error rate from 0 . 94 % without ad ##vers ##aria ##l training to 0 . 84 % with ad ##vers ##aria ##l training . we observed that we were not reaching zero error rate on ad ##vers ##aria ##l examples on the training set . we fixed this problem by making two changes . first , we made the model larger , using 1600 units per layer rather than the 240 used by the original max ##out network for this problem . without ad ##vers ##aria ##l training , this causes the model to over ##fi ##t slightly , and get an error rate of 1 . 14 % on the test set . with ad ##vers ##aria ##l training , we found that the validation set error leveled off over time , and made very slow progress . the original max ##out result uses early stopping , and terminates learning after the validation set error rate has not decreased for 100 epoch ##s . we found that while the validation set error was very flat , the ad ##vers ##aria ##l validation set error was not . we therefore used early stopping on the ad ##vers ##aria ##l validation set error . using this criterion to choose the number of epoch ##s to train for , we then re ##train ##ed on all 60 , 000 examples . five different training runs using different seeds for the random number generators used to select mini ##bat ##ches of training examples , initial ##ize model weights , and generate drop ##out masks result in four trials that each had an error rate of 0 . 77 % on the test set and one trial that had an error rate of 0 . 83 % . the average of 0 . 78 ##2 % is the best result reported on the per ##mut ##ation invariant version of mn ##ist , though statistical ##ly ind ##ist ##ing ##uis ##hab ##le from the result obtained by fine - tuning db ##ms with drop ##out drop ##out at 0 . 79 % . the model also became somewhat resistant to ad ##vers ##aria ##l examples . recall that without ad ##vers ##aria ##l training , this same kind of model had an error rate of 89 . 4 % on ad ##vers ##aria ##l examples based on the fast gradient sign method . with ad ##vers ##aria ##l training , the error rate fell to 17 . 9 % . ad ##vers ##aria ##l examples are transfer ##able between the two models but with the ad ##vers ##aria ##lly trained model showing greater robust ##ness . ad ##vers ##aria ##l examples generated via the original model yield an error rate of 19 . 6 % on the ad ##vers ##aria ##lly trained model , while ad ##vers ##aria ##l examples generated via the new model yield an error rate of 40 . 9 % on the original model . when the ad ##vers ##aria ##lly trained model does mis ##class ##ify an ad ##vers ##aria ##l example , its predictions are unfortunately still highly confident . the average confidence on a mis ##class ##ified example was 81 . 4 % . we also found that the weights of the learned model changed significantly , with the weights of the ad ##vers ##aria ##lly trained model being significantly more localized and interpret ##able ( see fig . [ reference ] ) . the ad ##vers ##aria ##l training procedure can be seen as mini ##mi ##zing the worst case error when the data is per ##tur ##bed by an adversary . that can be interpreted as learning to play an ad ##vers ##aria ##l game , or as mini ##mi ##zing an upper bound on the expected cost over noisy samples with noise from added to the inputs . ad ##vers ##aria ##l training can also be seen as a form of active learning , where the model is able to request labels on new points . in this case the human label ##er is replaced with a he ##uri ##stic label ##er that copies labels from nearby points . we could also regular ##ize the model to be ins ##ens ##itive to changes in its features that are smaller than the precision simply by training on all points within the max norm box , or sampling many points within this box . this corresponds to adding noise with max norm during training . however , noise with zero mean and zero co ##var ##iance is very in ##ef ##fi ##cie ##nt at preventing ad ##vers ##aria ##l examples . the expected dot product between any reference vector and such a noise vector is zero . this means that in many cases the noise will have essentially no effect rather than yielding a more difficult input . in fact , in many cases the noise will actual ##y result in a lower objective function value . we can think of ad ##vers ##aria ##l training as doing hard example mining among the set of noisy inputs , in order to train more efficiently by considering only those noisy points that strongly resist classification . as control experiments , we trained training a max ##out network with noise based on randomly adding to each pixel , or adding noise in to each pixel . these obtained an error rate of 86 . 2 % with confidence 97 . 3 % and an error rate of 90 . 4 % with a confidence of 97 . 8 % respectively on fast gradient sign ad ##vers ##aria ##l examples . because the derivative of the sign function is zero or und ##efined everywhere , gradient descent on the ad ##vers ##aria ##l objective function based on the fast gradient sign method does not allow the model to anti ##ci ##pate how the adversary will react to changes in the parameters . if we instead ad ##vers ##aria ##l examples based on small rotation ##s or addition of the scaled gradient , then the per ##tur ##bation process is itself different ##iable and the learning can take the reaction of the adversary into account . however , we did not find nearly as powerful of a regular ##izing result from this process , perhaps because these kinds of ad ##vers ##aria ##l examples are not as difficult to solve . one natural question is whether it is better to per ##tur ##b the input or the hidden layers or both . here the results are inconsistent . s ##ze ##ged ##y - ic ##lr ##20 ##14 reported that ad ##vers ##aria ##l per ##tur ##bation ##s yield the best regular ##ization when applied to the hidden layers . that result was obtained on a si ##gm ##oid ##al network . in our experiments with the fast gradient sign method , we find that networks with hidden units whose activation ##s are un ##bound ##ed simply respond by making their hidden unit activation ##s very large , so it is usually better to just per ##tur ##b the original input . on sat ##ura ##ting models such as the rust model we found that per ##tur ##bation of the input performed com ##para ##bly to per ##tur ##bation of the hidden layers . per ##tur ##bation ##s based on rotating the hidden layers solve the problem of un ##bound ##ed activation ##s growing to make additive per ##tur ##bation ##s smaller by comparison . we were able to su ##cc ##es ##fully train max ##out networks with rotational per ##tur ##bation ##s of the hidden layers . however , this did not yield nearly as strong of a regular ##izing effect as additive per ##tur ##bation of the input layer . our view of ad ##vers ##aria ##l training is that it is only clearly useful when the model has the capacity to learn to resist ad ##vers ##aria ##l examples . this is only clearly the case when a universal approx ##ima ##tor theorem applies . because the last layer of a neural network , the linear - si ##gm ##oid or linear - soft ##max layer , is not a universal approx ##ima ##tor of functions of the final hidden layer , this suggests that one is likely to encounter problems with under ##fi ##tting when applying ad ##vers ##aria ##l per ##tur ##bation ##s to the final hidden layer . we indeed found this effect . our best results with training using per ##tur ##bation ##s of hidden layers never involved per ##tur ##bation ##s of the final hidden layer . section : different kinds of model capacity one reason that the existence of ad ##vers ##aria ##l examples can seem counter - intuitive is that most of us have poor intuition ##s for high dimensional spaces . we live in three dimensions , so we are not used to small effects in hundreds of dimensions adding up to create a large effect . there is another way that our intuition ##s serve us poorly . many people think of models with low capacity as being unable to make many different confident predictions . this is not correct . some models with low capacity do exhibit this behavior . for example shallow rb ##f networks with are only able to confidently predict that the positive class is present in the vicinity of . elsewhere , they default to predicting the class is absent , or have low - confidence predictions . rb ##f networks are naturally immune to ad ##vers ##aria ##l examples , in the sense that they have low confidence when they are fooled . a shallow rb ##f network with no hidden layers gets an error rate of 55 . 4 % on mn ##ist using ad ##vers ##aria ##l examples generated with the fast gradient sign method and . however , its confidence on mistaken examples is only . its average confidence on clean test examples is % . we ca n ' t expect a model with such low capacity to get the right answer at all points of space , but it does correctly respond by reducing its confidence considerably on points it does not \" understand . \" rb ##f units are unfortunately not invariant to any significant transformations so they can not general ##ize very well . we can view linear units and rb ##f units as different points on a precision - recall trade ##off curve . linear units achieve high recall by responding to every input in a certain direction , but may have low precision due to responding too strongly in unfamiliar situations . rb ##f units achieve high precision by responding only to a specific point in space , but in doing so sacrifice recall . motivated by this idea , we decided to explore a variety of models involving quad ##ratic units , including deep rb ##f networks . we found this to be a difficult task \u2014 very model with sufficient quad ##ratic inhibition to resist ad ##vers ##aria ##l per ##tur ##bation obtained high training set error when trained with sg ##d . section : why do ad ##vers ##aria ##l examples general ##ize ? an intriguing aspect of ad ##vers ##aria ##l examples is that an example generated for one model is often mis ##class ##ified by other models , even when they have different arch ##ite ##cure ##s or were trained on di ##s ##jo ##int training sets . moreover , when these different models mis ##class ##ify an ad ##vers ##aria ##l example , they often agree with each other on its class . explanations based on extreme non - linear ##ity and over ##fi ##tting can not readily account for this behavior \u2014 why should multiple extremely non - linear model with excess capacity consistently label out - of - distribution points in the same way ? this behavior is especially surprising from the view of the hypothesis that ad ##vers ##aria ##l examples finely tile space like the rational numbers among the real ##s , because in this view ad ##vers ##aria ##l examples are common but occur only at very precise locations . under the linear view , ad ##vers ##aria ##l examples occur in broad subsp ##ace ##s . the direction need only have positive dot product with the gradient of the cost function , and need only be large enough . fig . [ reference ] demonstrates this phenomenon . by tracing out different values of we see that ad ##vers ##aria ##l examples occur in contiguous regions of the 1 - d subsp ##ace defined by the fast gradient sign method , not in fine pockets . this explains why ad ##vers ##aria ##l examples are abundant and why an example mis ##class ##ified by one class ##ifier has a fairly high prior probability of being mis ##class ##ified by another class ##ifier . to explain why mu ##tip ##le class ##ifiers assign the same class to ad ##vers ##aria ##l examples , we h ##yp ##oth ##es ##ize that neural networks trained with current method ##ologies all resemble the linear class ##ifier learned on the same training set . this reference class ##ifier is able to learn approximately the same classification weights when trained on different subset ##s of the training set , simply because machine learning algorithms are able to general ##ize . the stability of the underlying classification weights in turn results in the stability of ad ##vers ##aria ##l examples . to test this hypothesis , we generated ad ##vers ##aria ##l examples on a deep max ##out network and classified these examples using a shallow soft ##max network and a shallow rb ##f network . on examples that were mis ##class ##ified by the max ##out network , the rb ##f network predicted the max ##out network ' s class assignment only 16 . 0 % of the time , while the soft ##max class ##ifier predict the max ##out network ' s class correctly 54 . 6 % of the time . these numbers are largely driven by the differing error rate of the different models though . if we exclude our attention to cases where both models being compared make a mistake , then soft ##max regression predict ' s max ##out ' s class 84 . 6 % of the time , while the rb ##f network is able to predict max ##out ' s class only 54 . 3 % of the time . for comparison , the rb ##f network can predict soft ##max regression ' s class 53 . 6 % of the time , so it does have a strong linear component to its own behavior . our hypothesis does not explain all of the max ##out network ' s mistakes or all of the mistakes that general ##ize across models , but clearly a significant proportion of them are consistent with linear behavior being a major cause of cross - model general ##ization . section : alternative h ##yp ##oth ##eses we now consider and ref ##ute some alternative h ##yp ##oth ##eses for the existence of ad ##vers ##aria ##l examples . first , one hypothesis is that genera ##tive training could provide more constraint on the training process , or cause the model to learn what to distinguish \" real \" from \" fake \" data and be confident only on \" real \" data . the mp - db ##m mp ##db ##m provides a good model to test this hypothesis . its inference procedure gets good classification accuracy ( an 0 . 88 % error rate ) on mn ##ist . this inference procedure is different ##iable . other genera ##tive models either have non - different ##iable inference procedures , making it harder to compute ad ##vers ##aria ##l examples , or require an additional non - genera ##tive disc ##rim ##inator model to get good classification accuracy on mn ##ist . in the case of the mp - db ##m , we can be sure that the genera ##tive model itself is responding to ad ##vers ##aria ##l examples , rather than the non - genera ##tive class ##ifier model on top . we find that the model is vulnerable to ad ##vers ##aria ##l examples . with an of 0 . 25 , we find an error rate of 97 . 5 % on ad ##vers ##aria ##l examples generated from the mn ##ist test set . it remains possible that some other form of genera ##tive training could con ##fer resistance , but clearly the mere fact of being genera ##tive is not alone sufficient . another hypothesis about why ad ##vers ##aria ##l examples exist is that individual models have strange qui ##rks but averaging over many models can cause ad ##vers ##aria ##l examples to wash out . to test this hypothesis , we trained an ensemble of twelve max ##out networks on mn ##ist . each network was trained using a different seed for the random number generator used to initial ##ize the weights , generate drop ##out masks , and select mini ##bat ##ches of data for st ##och ##astic gradient descent . the ensemble gets an error rate of 91 . 1 % on ad ##vers ##aria ##l examples designed to per ##tur ##b the entire ensemble with . if we instead use ad ##vers ##aria ##l examples designed to per ##tur ##b only one member of the ensemble , the error rate falls to 87 . 9 % . en ##se ##mbling provides only limited resistance to ad ##vers ##aria ##l per ##tur ##bation . section : summary and discussion as a summary , this paper has made the following observations : ad ##vers ##aria ##l examples can be explained as a property of high - dimensional dot products . they are a result of models being too linear , rather than too nonlinear . the general ##ization of ad ##vers ##aria ##l examples across different models can be explained as a result of ad ##vers ##aria ##l per ##tur ##bation ##s being highly aligned with the weight vectors of a model , and different models learning similar functions when trained to perform the same task . the direction of per ##tur ##bation , rather than the specific point in space , matters most . space is not full of pockets of ad ##vers ##aria ##l examples that finely tile the real ##s like the rational numbers . because it is the direction that matters most , ad ##vers ##aria ##l per ##tur ##bation ##s general ##ize across different clean examples . we have introduced a family of fast methods for generating ad ##vers ##aria ##l examples . we have demonstrated that ad ##vers ##aria ##l training can result in regular ##ization ; even further regular ##ization than drop ##out . we have run control experiments that failed to reproduce this effect with simpler but less efficient regular ##izer ##s including weight decay and adding noise . models that are easy to opt ##imi ##ze are easy to per ##tur ##b . linear models lack the capacity to resist ad ##vers ##aria ##l per ##tur ##bation ; only structures with a hidden layer ( where the universal approx ##ima ##tor theorem applies ) should be trained to resist ad ##vers ##aria ##l per ##tur ##bation . rb ##f networks are resistant to ad ##vers ##aria ##l examples . models trained to model the input distribution are not resistant to ad ##vers ##aria ##l examples . ensembles are not resistant to ad ##vers ##aria ##l examples . some further observations concerning rubbish class examples are presented in the appendix : rubbish class examples are ubiquitous and easily generated . shallow linear models are not resistant to rubbish class examples . rb ##f networks are resistant to rubbish class examples . gradient - based optimization is the work ##horse of modern ai . using a network that has been designed to be sufficiently linear - whether it is a re ##lu or max ##out network , an l ##st ##m , or a si ##gm ##oid network that has been carefully configured not to sat ##ura ##te too much - we are able to fit most problems we care about , at least on the training set . the existence of ad ##vers ##aria ##l examples suggests that being able to explain the training data or even being able to correctly label the test data does not imply that our models truly understand the tasks we have asked them to perform . instead , their linear responses are overly confident at points that do not occur in the data distribution , and these confident predictions are often highly incorrect . this work has shown we can partially correct for this problem by explicitly identifying problematic points and correct ##ing the model at each of these points . however , one may also conclude that the model families we use are intrinsic ##ally flawed . ease of optimization has come at the cost of models that are easily mis ##led . this mo ##tiv ##ates the development of optimization procedures that are able to train models whose behavior is more locally stable . sub ##su ##bs ##ection : ac ##k ##now ##led ##gm ##ents we would like to thank geoffrey hint ##on and il ##ya su ##tsk ##ever for helpful discussions . we would also like to thank jeff dean , greg co ##rrado , and or ##iol vin ##yal ##s for their feedback on drafts of this article . we would like to thank the developers of the ##ano berg ##stra + al : 2010 - sci ##py , bas ##tie ##n - the ##ano - 2012 , p ##yle ##ar ##n ##2 p ##yle ##ar ##n ##2 _ ar ##xi ##v _ 2013 , and di ##st ##bel ##ie ##f di ##st ##bel ##ie ##f . plus 0 . 3 ##ex bibliography : references appendix : rubbish class examples a concept related to ad ##vers ##aria ##l examples is the concept of examples drawn from a \" rubbish class . \" these examples are de ##gen ##erate inputs that a human would classify as not belonging to any of the categories in the training set . if we call these classes in the training set \" the positive classes , \" then we want to be careful to avoid false positive ##s on rubbish inputs - i . e . , we do not want to classify a de ##gen ##erate input as being something real . in the case of separate binary class ##ifiers for each class , we want all classes output near zero probability of the class being present , and in the case of a multi ##no ##ull ##i distribution over only the positive classes , we would prefer that the class ##ifier output a high - entropy ( nearly uniform ) distribution over the classes . the traditional approach to reducing vulnerability to rubbish inputs is to introduce an extra , constant output to the model representing the rubbish class le ##cu ##n + 98 . fool recently re - popularized the concept of the rubbish class in the context of computer vision under the name fool ##ing images . as with ad ##vers ##aria ##l examples , there has been a mis ##con ##ception that rubbish class false positive ##s are hard to find , and that they are primarily a problem faced by deep networks . our explanation of ad ##vers ##aria ##l examples as the result of linear ##ity and high dimensional spaces also applies to analyzing the behavior of the model on rubbish class examples . linear models produce more extreme predictions at points that are far from the training data than at points that are near the training data . in order to find high confidence rubbish false positive ##s for such a model , we need only generate a point that is far from the data , with larger norms yielding more confidence . rb ##f networks , which are not able to confidently predict the presence of any class far from the training data , are not fooled by this phenomenon . we generated 10 , 000 samples from and fed them into various class ##ifiers on the mn ##ist data ##set . in this context , we consider assign ##ing a probability greater than 0 . 5 to any class to be an error . a naive ##ly trained max ##out network with a soft ##max layer on top had an error rate of 98 . 35 % on ga ##uss ##ian rubbish examples with an average confidence of 92 . 8 % on mistakes . changing the top layer to independent si ##gm ##oids dropped the error rate to 68 % with an average confidence on mistakes of 87 . 9 % . on ci ##far - 10 , using 1 , 000 samples from , a con ##vo ##lu ##tion ##al max ##out net obtain ##s an error rate of 93 . 4 % , with an average confidence of 84 . 4 % . these experiments suggest that the optimization algorithms employed by fool are over ##kill ( or perhaps only needed on image ##net ) , and that the rich geometric structure in their fool ##ing images are due to the prior ##s encoded in their search procedures , rather than those structures being uniquely able to cause false positive ##s . though fool focused their attention on deep networks , shallow linear models have the same problem . a soft ##max regression model has an error rate of 59 . 8 % on the rubbish examples , with an average confidence on mistakes of 70 . 8 % . if we use instead an rb ##f network , which does not behave like a linear function , we find an error rate of 0 % . note that when the error rate is zero the average confidence on a mistake is und ##efined . fool focused on the problem of generating fool ##ing images for a specific class , which is a harder problem than simply finding points that the network confidently class ##ifies as belonging to any one class despite being defective . the above methods on mn ##ist and ci ##far - 10 tend to have a very sk ##ew ##ed distribution over classes . on mn ##ist , 45 . 3 % of a naive ##ly trained max ##out network ' s false positive ##s were classified as 5 ##s , and none were classified as 8 ##s . likewise , on ci ##far - 10 , 49 . 7 % of the con ##vo ##lu ##tion ##al network ' s false positive ##s were classified as frogs , and none were classified as airplanes , automobiles , horses , ships , or trucks . to solve the problem introduced by fool of generating a fool ##ing image for a particular class , we propose adding to a ga ##uss ##ian sample as a fast method of generating a fool ##ing image classified as class . if we repeat this sampling process until it succeeds , we a random ##ized algorithm with variable run ##time . on ci ##far - 10 , we found that one sampling step had a 100 % success rate for frogs and trucks , and the hardest class was airplanes , with a success rate of 24 . 7 % per sampling step . averaged over all ten classes , the method has an average per - step success rate of 75 . 3 % . we can thus generate any desired class with a handful of samples and no special prior ##s , rather than tens of thousands of generations of evolution . to confirm that the resulting examples are indeed fool ##ing images , and not images of real classes rendered by the gradient sign method , see fig . [ reference ] . the success rate of this method in terms of generating members of class may de ##grade for data ##set ##s with more classes , since the risk of inadvertently increasing the activation of a different class increases in that case . we found that we were able to train a max ##out network to have a zero percent error rate on ga ##uss ##ian rubbish examples ( it was still vulnerable to rubbish examples generated by applying a fast gradient sign step to a ga ##uss ##ian sample ) with no negative impact on its ability to classify clean examples . unfortunately , unlike training on ad ##vers ##aria ##l examples , this did not result in any significant reduction of the model ' s test set error rate . in conclusion , it appears that a randomly selected input to deep or shallow models built from linear parts is overwhelmingly likely to be processed incorrectly , and that these models only behave reasonably on a very thin manifold encompassing the training data .",
        "pred_seq": "mn ##set [SEP] [SEP] [SEP] [SEP] [unused0] mn ##set [SEP] [SEP] [SEP] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "mnist dataset"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": []
            },
            {
                "Material": [
                    [
                        "mnist dataset"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "mnist dataset",
                        "mnist"
                    ]
                ],
                "Method": [
                    [
                        "adversarial training",
                        "linear explanation of adversarial examples",
                        "linear view of adversarial examples",
                        "sgd"
                    ]
                ],
                "Metric": [
                    [
                        "test set error",
                        "error",
                        "adversarial validation set error"
                    ]
                ],
                "Task": [
                    [
                        "classification"
                    ]
                ]
            }
        ]
    },
    "51": {
        "doctext": "document : an ##mm : ranking short answer texts with attention - based neural matching model as an alternative to question answering methods based on feature engineering , deep learning approaches such as con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) and long short - term memory models ( l ##st ##ms ) have recently been proposed for semantic matching of questions and answers . to achieve good results , however , these models have been combined with additional features such as word overlap or b ##m ##25 scores . without this combination , these models perform significantly worse than methods based on linguistic feature engineering . in this paper , we propose an attention based neural matching model for ranking short answer text . we adopt value - shared weight ##ing scheme instead of position - shared weight ##ing scheme for combining different matching signals and incorporate question term importance learning using question attention network . using the popular bench ##mark tre ##c q ##a data , we show that the relatively simple an ##mm model can significantly out ##per ##form other neural network models that have been used for the question answering task , and is competitive with models that are combined with additional features . when an ##mm is combined with additional features , it out ##per ##forms all baseline ##s . 2016 , october 24 - 28 , 2016 , indianapolis , in , usa \u00a1 cc ##s ##20 ##12 \u00bf \u00a1 concept \u00bf \u00a1 / concept _ id \u00bf \u00a1 concept _ des ##c \u00bf information systems retrieval models and ranking \u00a1 / concept _ des ##c \u00bf \u00a1 concept _ significance \u00bf 500 \u00a1 / concept _ significance \u00bf \u00a1 / concept \u00bf \u00a1 concept \u00bf \u00a1 / concept _ id \u00bf \u00a1 concept _ des ##c \u00bf information systems question answering \u00a1 / concept _ des ##c \u00bf \u00a1 concept _ significance \u00bf 500 \u00a1 / concept _ significance \u00bf \u00a1 / concept \u00bf \u00a1 / cc ##s ##20 ##12 \u00bf [ 500 ] information systems retrieval models and ranking systems question answering section : introduction question answering ( q ##a ) , which returns exact answers as either short facts or long passages to natural language questions issued by users , is a challenging task and plays a central role in the next generation of advanced web search . many of current q ##a systems use a learning to rank approach that en ##codes question / answer pairs with complex linguistic features including lexi ##cal , syn ##ta ##ctic and semantic features . for instance , sur ##de ##anu et al . investigated a wide range of feature types including similarity features , translation features , density / frequency features and web correlation features for learning to rank answers and show improvements in accuracy . however , such methods rely on manual feature engineering , which is often time - consuming and requires domain dependent expertise and experience . moreover , they may need additional nl ##p par ##ser ##s or external knowledge sources that may not be available for some languages . recently , researchers have been studying deep learning approaches to automatically learn semantic match between questions and answers . such methods are built on the top of neural network models such as con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) and long short - term memory models ( l ##st ##ms ) . the proposed models have the benefit of not requiring hand - crafted linguistic features and external resources . some of them achieve state - of - the art performance for the answer sentence selection task bench ##mark ##ed by the tre ##c q ##a track . however , the weakness of the existing studies is that the proposed deep models , either based on cnn ##s or l ##st ##ms , need to be combined with additional features such as word overlap features and b ##m ##25 to perform well . without combining these additional features , their performance is significantly worse than the results obtained by the state - of - the - art methods based on linguistic feature engineering . this led us to propose the following research questions : rq ##1 without combining additional features , could we build deep learning models that can achieve comparable or even better performance than methods using feature engineering ? rq ##2 by combining additional features , could our model out ##per ##form state - of - the - art models for question answering ? to address these research questions , we analyze the existing current deep learning architecture ##s for answer ranking and make the following two key observations : architecture ##s not specifically designed for question / answer matching : some methods employ cnn ##s for question / answer matching . however , cnn ##s are originally designed for computer vision ( cv ) , which uses position - shared weights with local per ##ceptive filters , to learn spatial regular ##ities in many cv tasks . however , such spatial regular ##ities may not exist in semantic matching between questions and answers , since important similarity signals between question and answer terms could appear in any position due to the complex linguistic property of natural languages . meanwhile , models based on l ##st ##ms view the question / answer matching problem in a sequential way . without direct interactions between question and answer terms , the model may not be able to capture sufficiently detailed matching signals between them . lack of modeling question focus : understanding the focus of questions , e . g . , important terms in a question , is helpful for ranking the answers correctly . for example , given a question like \" where was the first burger king restaurant opened ? \" , it is critical for the answer to talk about \" burger \" , \" king \" , \" open \" , etc . most existing text matching models do not explicitly model question focus . for example , models based on cnn ##s treat all the question terms as equally important when matching to answer terms . models based on l ##st ##ms usually model question terms closer to the end to be more important . to handle these issues in the existing deep learning architecture ##s for ranking answers , we propose an attention based neural matching model ( an ##mm ) . the novel properties of the proposed model and our contributions can be summarized as follows : deep neural network with value - shared weights : we introduce a novel value - shared weight ##ing scheme in deep neural networks as a counterpart of the position - shared weight ##ing scheme in cnn ##s , based on the idea that semantic matching between a question and answer is mainly about the ( semantic similarity ) value regular ##ities rather than spatial regular ##ities . incorporate attention scheme over question terms : we incorporate the attention scheme over the question terms using a ga ##ting function , so that we can explicitly disc ##rim ##inate the question term importance . extensive experimental evaluation and promising results . we perform a thorough experimental study based on the tre ##c q ##a data ##set from tre ##c q ##a tracks 8 - 13 , which appears to be one of the most widely used bench ##marks for answer re - ranking . unlike previous methods using cnn ##s and l ##st ##ms , which showed inferior results without combining additional features , our model can achieve better performance than a state - of - art method using linguistic feature engineering and comparable performance with previous deep learning models with combined additional features . if we combine our model with a simple additional feature like q ##l , our method can achieve the state - of - the - art performance among current existing methods for ranking answers under multiple metric ##s . road ##ma ##p . the rest of our paper is organized as follows . we will review related work in section [ reference ] . in section [ reference ] , we will present the proposed an ##mm model with two components : value - shared weights and question attention network with ga ##ting functions . two different architecture ##s will be presented and analyzed . section [ reference ] is a systematic experimental analysis using the tre ##c q ##a bench ##mark data ##set . finally , we conclude our paper and discuss future work in section [ reference ] . section : related work our work is related to several research areas , including deep learning models for text matching , facto ##id question answering , answer ranking in c ##qa and answer passage / sentence retrieval . deep learning models for text matching . recently there have been many deep learning models proposed for text matching and ranking . such deep learning models include ds ##sm , cds ##sm , arc - i / arc - ii , dc ##nn , deep ##mat ##ch , multi ##gra ##nc ##nn and match ##py ##ram ##id . ds ##sm performs a non - linear projection to map the query and the documents to a common semantic space . the neural network models are trained using click ##th ##rou ##gh data such that the conditional likelihood of the clicked document given the query is maximize ##d . deep ##mat ##ch uses a topic model to construct the interactions between two texts and then makes different levels of abstraction ##s with a deep architecture to model the relationships between topics . arc - i and arc - ii are two different architecture ##s proposed by hu et . al . for matching natural language sentences . arc - i firstly finds the representation of each sentence and then compares the representations of the two sentences with a multi - layer per ##ce ##pt ##ron ( ml ##p ) . the draw ##back of arc - i is that it def ##ers the interaction between two sentences until their individual representation mature ##s in the con ##vo ##lu ##tion model , and therefore has the risk of losing details , which could be important for the matching task . on the other hand , arc - ii is built directly on the interaction space between two sentences . thus arc - ii makes two sentences meet before their own high - level representations mature , while still retaining the space for individual development of abstraction of each sentence . our an ##mm architecture adopt ##s a similar design with arc - ii in the q ##a matching matrix where we build neural networks directly on the interaction of sentence term pairs . however , we adopt value - shared weights instead of position - shared weights as in the cnn used by arc - ii . we also add attention scheme to learn question term importance . facto ##id question answering . there have been many previous studies on facto ##id question answering , most of which use the bench ##mark data from tre ##c q ##a track . yi ##h et . al . formulated answer sentence selection as a semantic matching problem with a late ##nt word - alignment structure and conducted a series of experimental studies on lever ##aging proposed lexi ##cal semantic models . i ##y ##yer et . al . introduced a rec ##urs ##ive neural network ( rn ##n ) model that can reason over text that contains very few individual words by modeling textual composition ##ality . yu et al . proposed an approach for answer sentence selection via distributed representations , and learned to match questions with answers by considering their semantic encoding . they combined the learning results of their model with word overlap features by training a log ##istic regression class ##ifier . wang and ny ##berg proposed a method which uses a stacked bid ##ire ##ction ##al long - short term memory ( b ##ls ##tm ) network to sequential ##ly read words from question and answer sentences , and then output their relevance scores . their system needs to combine the stacked b ##ls ##tm relevance model with a b ##m ##25 score to achieve good performance . se ##very ##n and mo ##sch ##itt ##i presented a con ##vo ##lu ##tion ##al neural network architecture for re - ranking pairs of short texts , where they learned the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data . they also need to combine additional features into their model to out ##per ##form previous methods . unlike the previous research , our method can out ##per ##form previous methods using feature engineering without combining any additional features . with an additional simple feature like q ##l , our model is significantly better than the previous state - of - the - art methods including deep learning methods . answer ranking in c ##qa . there is also previous research on ranking answers from community question answering ( c ##qa ) sites . sur ##de ##anu et al . investigated a wide range of feature types such as similarity features , translation features , density / frequency features for ranking answers to non - facto ##id questions in yahoo ! answers . jan ##sen et al . presented an answer re - ranking model for non - facto ##id questions that integrate lexi ##cal semantics with discourse information driven by two representations of discourse . xu ##e et al . proposed a retrieval model that combines a translation - based language model for the question part with a query likelihood approach for the answer part . questions from c ##qa sites are mostly non - facto ##id questions . our research is closer to facto ##id questions such as questions in tre ##c q ##a data . answer passage / sentence retrieval . our work is also related to previous research on answer passage / sentence retrieval . ty ##mos ##henko and mo ##sch ##itt ##i studied the use of syn ##ta ##ctic and semantic structures obtained with shallow and deeper syn ##ta ##ctic par ##ser ##s in the answer passage re - ranking task . kei ##kha et al . developed an ann ##ota ##ted data set for non - facto ##id answer finding using tre ##c gov ##2 collections and topics . they ann ##ota ##ted passage - level answers , revisited several passage retrieval models with this data , and came to the conclusion that the current methods are not effective for this task . yang et al . developed effective methods for answer sentence retrieval using this ann ##ota ##ted data by combining semantic features , context features and basic text matching features with a learning to rank approach . our model is built on attention - based neural matching model with value - shared weight ##ing sc ##hema . unlike learning to rank approaches with feature engineering , our model can achieve good performance for ranking answers without any additional manual feature engineering , prep ##ro ##ces ##sing of nl ##p par ##ser ##s and external resources like knowledge bases . section : attention - based neural matching model in this section we present the proposed model referred as an ##mm ( attention - based neural matching model ) , which is shown in figure [ reference ] . before we introduce our model , we firstly define some term ##ino ##logies . sub ##section : terminology short answer text : we use short answer text to refer to a short fact , answer sentences or answer passages that can address users ' information needs in the issued questions . this is the ranking object in this paper and includes answers in various lengths . in the experiments of this paper , we mainly focus on ranking answer sentences that contain correct answer facts as in tre ##c q ##a data . q ##a matching matrix : we use q ##a matching matrix to refer to a matrix which represents the semantic matching information of term pairs from a question and answer pair . given a question with length and an answer with length , a q ##a matching matrix is an by matrix , where denote the semantic similarity between term and term measured by the co ##sin ##e similarity of the corresponding word em ##bed ##ding ##s of terms . if and are the same term , we assign as . q ##a matching vector : we use q ##a matching vector to refer to a row in the q ##a matching matrix . as presented before , the - th row of the q ##a matching matrix contains the semantic similarity between and all terms in answer . sub ##section : model overview our method contains three steps as follows : we construct q ##a matching matrix for each question and answer pair with pre - trained word em ##bed ##ding ##s . we then employ a deep neural network with value - shared weight ##ing scheme in the first layer , and fully connected layers in the rest to learn hierarchical abstraction of the semantic matching between question and answer terms . finally , we employ a question attention network to learn question term importance and produce the final ranking score . we propose two neural matching model architecture ##s and compare the effectiveness ##es of them . we firstly describe a basic version of the architecture , which is referred to as an ##mm - 1 . in the following sections , we will explain in detail the two major designs of an ##mm - 1 , i . e . , value - shared weights and question attention network . sub ##section : value - shared weight ##ing we first train word em ##bed ##ding ##s with the word ##2 ##ve ##c tool by mi ##ko ##lov et al . with the english wikipedia dump to construct q ##a matching matrices . given a question sentence and an answer sentence , we compute the dot product of the normal ##ized word em ##bed ##ding ##s of all term pairs to construct the q ##a matching matrix as defined in section [ reference ] . a major problem with the q ##a matching matrix is the variable size due to the different lengths of answers for a given question . to solve this problem , one can use cnn with pool ##ing strategy to handle the variable size . however , as we have mentioned before , cnn ##s basically use position - shared weight ##ing scheme which may not fit semantic matching between questions and answers . important question terms and semantic ##ally similar answer words could appear anywhere in questions / answers due to the complex linguistic property of natural languages . thus we adopt the following method to handle the various length problem : value - shared weights : for this method , the assumption is that matching signals in different ranges play different roles in deciding the final ranking score . thus we introduce the value - shared weight ##ing scheme to learn the importance of different levels of matching signals . the comparison between the position - shared weight and value - shared weight is shown in figure [ reference ] . we can see that for position - shared weights , the weight associated with a node only depends on its position or relative location as specified by the filters in cnn . however in our model , the weight associated with a node depends on its value . the value of a node denotes the strength of the matching signal between term pairs of questions and answers from the q ##a matching matrix , as explained in section [ reference ] . such a setting enables us to use the learned weights to en ##code how to combine different levels of matching signals . after this step , the size of the hidden representation becomes fixed and we can use normal fully connected layers to learn higher level representations . we use the term bin to denote a specific range of matching signals . since , if we set the size of bin ##s as , then we have bin ##s where there is a separate bin for to denote exact match of terms . specifically , value - shared weights are adopted in the forward propagation prediction process from the input layer to the hidden layer over each question term in an ##mm - 1 as follows : input layer to hidden layer . let denote a dimensional model parameter from input layer to hidden layer . denotes the sum of all matching signals within the - th value range or bin . for each q ##a matching vector of a given query , the combined score after the activation function of the - th node in hidden layer is defined as where is the index of question words in . we use the si ##gm ##oid function as the activation function , which is commonly adopted in many neural network architecture ##s . sub ##section : question attention network in addition to value - shared weight ##ing , another model component of an ##mm - 1 is the question attention network . in a committee of neural networks which consists of multiple networks , we need to combine the output of these networks to output a final decision vector . the question attention network uses the ga ##ting function to control the output of each network in this process . specifically , in an ##mm - 1 we use the soft ##max gate function to combine the output of multiple networks where each network corresponds to a question term as shown in figure [ reference ] . we feed the dot product of query word em ##bed ##ding and model parameter to the soft ##max function to represent the query term importance . in this setting , we can directly compare the relative term importance of query words within the same query with soft ##max function . we also tried si ##gm ##oid gate function , but this did not perform as well as soft ##max gate function . soft ##max gate function is used in the forward propagation process from the hidden layer to the output layer as follows : hidden layer to output layer . from the hidden layer to the output layer , we add a soft ##max gate function to learn question attention . let denote a dimensional vector which is a model parameter . we feed the dot product of query word em ##bed ##ding and to the soft ##max function to represent the query term importance as shown in equation [ reference ] . note that we normal ##ize the query word em ##bed ##ding before computing the dot product . unlike previous models like cnn ##s and b ##ls ##tm , which learn the semantic match score between questions and answers through representation learning from matching matrix or question / answer pair sequences , an ##mm achieve ##s this by combining semantic matching signals of term pairs in questions and answers weighted by the output of question attention network , where soft ##max gate functions help disc ##rim ##inate the term importance or attention on different question terms . sub ##section : model training for an ##mm - 1 , the model parameters contain two sets : 1 ) the value - shared weights for combining matching signals from the input layer to the hidden layer . 2 ) the parameters in the ga ##ting function from the hidden layer to the output layer . to learn the model parameters from the training data , we adopt a pair - wise learning strategy with a large margin objective . firstly we construct triple ##s from the training data , with matched with better than with . we have the ranking - based loss as the objective function as following : where denote the predicted matching score for q ##a pair . during training stage , we will scan all the triple ##s in training data . given a triple , we will compute . if , we will skip this triple . otherwise , we need to update model parameters with back propagation algorithm to minimize the objective function . under soft ##max gate function setting , the gradient ##s of w . r . t . from hidden layer to the output layer is shown in equation [ reference ] where can be derived as the gradient of w . r . t . from input layer to hidden layer is shown in equation [ reference ] . with the formulas of gradient ##s , we can perform st ##och ##astic gradient descent to learn model parameters . we use mini - batch gradient descent to achieve more robust performance on the ranking task . for the learning rate , we adopt adaptive learning rate : , where will approach with more iteration ##s . such a setting has better guarantee for convergence . sub ##section : extension to deep neural networks with multiple sets of value - shared weights in an ##mm - 1 , we can only use one set of value - shared weights for each q ##a matching vector . we further propose a more flexible neural network architecture which could enable us to use multiple sets of value - shared weights for each q ##a matching vector , leading to multiple intermediate nodes in the first hidden layer , as shown in figure [ reference ] by the yellow color . we refer to this extended model as an ##mm - 2 . the model architecture shown in figure [ reference ] is corresponding to an ##mm - 2 . sub ##su ##bs ##ection : forward propagation prediction for an ##mm - 2 , we add a hidden layer in the neural network where we learn multiple combined scores from the input layer . with this hidden layer , we define multiple weight vectors as . thus becomes a two dimensional matrix . the formula for the forward propagation prediction is as follows : where and denote the soft ##max gate function . is the number of nodes in hidden layer 1 . is the model parameter from hidden layer 1 to hidden layer 2 , where we feed the linear combination of outputs of nodes in hidden layer 1 to an extra activation function comparing with equation [ reference ] . then from hidden layer 2 to output layer , we sum over all outputs of nodes in hidden layer 2 weighted by the outputs of soft ##max gate functions , which also form the question attention network . sub ##su ##bs ##ection : back propagation for model training for an ##mm - 2 , we have three sets of model parameters : 1 ) from input layer to hidden layer 1 ; 2 ) from hidden layer 1 to hidden layer 2 ; 3 ) from hidden layer 2 to output layer . all three sets of parameters are updated through back propagation . the definition of the objective function is the same as equation [ reference ] . the back propagation process for model parameter learning is described as follows : from hidden layer 2 to output layer . the gradient ##s of the objective function w . r . t . is as following : where from hidden layer 1 to hidden layer 2 . the gradient ##s of the objective function w . r . t . is as following : where . from input layer to hidden layer 1 . the gradient ##s of the objective function w . r . t . is as following : where initially we will randomly give the values of model parameters . then we will use back propagation to update the model parameters . when the learning process converge , we use the learned model parameters for prediction to rank short answer texts . section : experiments sub ##section : data set and experiment settings we use the tre ##c q ##a data set created by wang et . al . from tre ##c q ##a track 8 - 13 data , with candidate answers automatically selected from each question ' s document pool using a combination of overlapping non - stop word counts and pattern matching . this data set is one of the most widely used bench ##marks for answer re - ranking . table [ reference ] shows the statistics of this data set . the data ##set contains a set of facto ##id questions with candidate answers which are limited to a single sentence . there are two training data sets : train and train - all . answers in train have manual judgments for the answer correct ##ness . the manual judgment of candidate answer sentences is provided for the entire tre ##c 13 set and for a part of questions from tre ##c 8 - 12 . train - all is another training set with much larger number of questions . the correct ##ness of candidate answer sentences in train - all is identified by matching answer sentences with answer pattern regular expressions provided by tre ##c . this data set is more noisy , however it provides many more q ##a pairs for model training . there is a dev set for hyper - parameter optimization and test set for model testing . we use the same train / dev / test partition in our experiments to directly compare our results with previous works . for data prep ##ro ##ces ##s , we perform token ##ization without stemming . we maintain stop words during the model training stage . word em ##bed ##ding ##s . we obtain pre - trained word em ##bed ##ding ##s with the word ##2 ##ve ##c tool by mi ##ko ##lov et al . with the english wikipedia dump . we use the skip - gram model with window size and filter words with frequency less than following the common practice in many neural em ##bed ##ding models . for the word vector dimension , we tune it as a hyper - parameter on the validation data starting from to . em ##bed ##ding ##s for words not present are randomly initial ##ized with sampled numbers from uniform distribution u [ - 0 . 25 , 0 . 25 ] , which follows the same setting as . model hyper - parameters . for the setting of hyper - parameters , we set the number of bin ##s as , word em ##bed ##ding dimension as for ann ##m - 1 , the number of bin ##s as , word em ##bed ##ding dimension as for ann ##m - 2 after we tune hyper - parameters on the provided dev set of tre ##c q ##a data . sub ##section : evaluation and metric ##s for evaluation , we rank answer sentences with the predicted score of each method and compare the rank list with the ground truth to compute metric ##s . we choose mean average precision ( map ) and mean reciprocal rank ( mr ##r ) , which are commonly used in information retrieval and question answering , as the metric to evaluate our model . the definition of mr ##r is as follows : where is the position of the first correct answer in the rank list . thus mr ##r is only based on the rank of the first correct answer . it is more suitable for the cases where the rank of the first correct answer is emphasized or each question only have one correct answer . on the other hand , map looks at the ranks of all correct answers . it is computed as following : where is the average precision for each query . thus map is the average performance on all correct answers . we use the official scripts for computing these metric ##s . sub ##section : model learning results in this section , we give some qu ##ali ##tative analysis and visual ##ization of our model learning results . specifically , we analyze the learned value - sha ##rd weights and question term importance by an ##mm . sub ##su ##bs ##ection : value - shared weight we take the learned value - shared weights of an ##mm - 1 as the example . figure [ reference ] shows the learned value - shared weights by an ##mm - 1 . in an ##mm - 1 , for each q ##a matching vector , there is only one bin node . thus the learned value - shared weights for an ##mm - 1 is a one dimension vector . for an ##mm - 1 , we set the number of bin ##s as as presented in section [ reference ] . note that the x - axis is the index of bin range and the y - axis is the value - shared weights corresponding to each bin range . the range of match signals is [ - 1 , 1 ] from the left to the right . we make the following observations : ( 1 ) the exact match signal which is corresponding to in the last bin is tied with a very large weight , which shows that exact match information is very important . ( 2 ) for positive matching score from , which is corresponding to bin index , the learned value - shared weights are different for matching score range ( bin index ) and matching score range ( bin index ) . we can observe many positive value - shared weights for matching score range and negative value - shared weights for matching score range . this makes sense since high semantic matching scores are positive indicators on answer correct ##ness , whereas low semantic matching scores indicate that the candidate answer sentences contain irrelevant terms . ( 3 ) for negative matching scores from , we can see there is not a lot of differences between value - shared weights for different ranges . a major reason is that most similarity scores based on word em ##bed ##ding ##s are positive . therefore , we can remove bin ##s corresponding to negative matching scores to reduce the dimension of value - shared weight vectors , which can help improve the efficiency of the model training process . we will show more quantitative results on the comparison between value - shared weights and position - shared weights in cnn in section [ reference ] . sub ##su ##bs ##ection : question term importance next we analyze the learned question term importance of our model . due to the space limit , we also use the learned question term importance of an ##mm - 1 as an example . table [ reference ] shows the examples of learned question term importance by an ##mm - 1 . we also visual ##ize the question term importance in figure [ reference ] . based on the results in the table and the figure , we can clearly see that an ##mm - 1 learns reasonable term importance . for instance , with the question attention network , an ##mm - 1 discovers important terms like \" khmer \" , \" rouge \" , \" power \" as for the question \" when did the khmer rouge come into power ? \" . terms like \" age \" , \" rossi ##nin \" , \" stop \" , \" writing \" , \" opera \" are highlighted for the question \" at what age did rossi ##ni stop writing opera ? \" . for the question \" where was the first burger king restaurant opened ? \" mentioned in section [ reference ] , \" burger \" , \" king \" , \" opened \" are treated as important question terms . an interesting question is how the learned term importance compare with traditional ir term weight ##ing methods such as idf . we design an experiment to compare an ##mm - 1 / an ##mm - 2 with an ##mm - idf , which is a de ##gen ##erate version of our model where we use idf to directly replace the output of question attention network . in this case , in equation [ reference ] is replaced by the idf of the j - th question term . table [ reference ] shows the results . we find that if we replace the output of question attention network of an ##mm with idf , it will decrease the answer ranking performance , especially on train data . thus , we can see that with the optimization process in the back propagation training process , an ##mm can learn better question term weight ##ing score than he ##uri ##stic term weight ##ing functions like idf . sub ##section : experimental results for ranking answers sub ##su ##bs ##ection : learning without combining additional features our first experimental setting is ranking answer sentences directly by the predicted score from an ##mm without combining any additional features . this will enable us to answer rq ##1 proposed in section [ reference ] . table [ reference ] shows the results of tre ##c q ##a on train and train - all without combining additional features . in this table , we compare the results of an ##mm with other previous deep learning methods including cnn and l ##st ##m . we sum ##mar ##ize our observations as follows : ( 1 ) both an ##mm - 1 and an ##mm - 2 show significant improvements for map and mr ##r on train and train - all data sets comparing with previous deep learning methods . specifically , if we compare the results of an ##mm - 1 with the strongest deep learning baseline method by se ##very ##n et al . based on cnn , we can see an ##mm - 1 out ##per ##form cnn for % in map on train , % in map on train - all . for mr ##r , we can also observe similar significant improvements of an ##mm - 1 . these results show that with the value - shared weight scheme instead of the position - shared weight scheme in cnn and term importance learning with question attention network , an ##mm can predict ranking scores with much higher accuracy comparing with previous deep learning models for ranking answers . ( 2 ) if we compare the results of an ##mm - 1 and an ##mm - 2 , we can see their results are very close . an ##mm - 1 has slightly better performance than an ##mm - 2 . this result indicates that adding one more hidden layer to incorporate multiple bin nodes does not necessarily increase the performance for answer ranking in tre ##c q ##a data . from the perspective of model efficiency , an ##mm - 1 could be a better choice since it can be trained much faster with good prediction accuracy . however , for larger training data sets than tre ##c q ##a data , an ##mm - 2 could have better performance since it has more model parameters and is suitable for fitting larger training data set . we leave the study of impact of the number of hidden layers in an ##mm to future work . table [ reference ] shows the comparison between an ##mm with previous methods using feature engineering on train - all without combining additional features . we find that both an ##mm - 1 and an ##mm - 2 achieve better performance comparing with other methods using feature engineering . specifically , comparing the results of an ##mm - 1 with the strongest baseline by yi ##h et al . based on enhanced lexi ##cal semantic models , an ##mm - 1 achieve ##s % gain for map and % gain for mr ##r . these results show that it is possible to build a uniform deep learning model such that it can achieve better performance than methods using feature engineering . to the best of our knowledge , an ##mm is the first deep learning model that can achieve good performance comparing with previous methods either based on deep learning models or feature engineering for ranking answers without any additional features , syn ##ta ##ctic par ##ser ##s and external resources except for pre - trained word em ##bed ##ding ##s . sub ##su ##bs ##ection : learning with combining additional features our second experimental setting is to address rq ##2 proposed in section [ reference ] , where we ask whether our model can out ##per ##form the state - of - the - art performance achieved by cnn and l ##st ##m for answer ranking when combining additional features . we combine the predicted score from an ##mm - 1 and an ##mm - 2 with the query likelihood ( q ##l ) score using lambda ##mart following a similar approach to . we use the implementation of lambda ##mart in j ##for ##est ##s we compare the results with previous deep learning models with additional features . table [ reference ] shows the results on train and train - all when combining additional features . we can see that with combined features , both an ##mm - 1 and an ##mm - 2 have better performance . an ##mm - 1 also out ##per ##forms cnn by se ##very ##n et al . which is the current state - of - the - art method for ranking answers in terms of both map and mr ##r on train and train - all . we also tried to combine an ##mm score with other additional features such as word overlap features , idf weighted word overlap features and b ##m ##25 as in previous research . the results were either similar or worse than combining an ##mm score with q ##l . for an ##mm , the gains after combining additional features are not as large as neural network models like cnn in and l ##st ##m in . we think the reasons for this are two - fold : ( 1 ) the q ##a matching matrix in an ##mm model can capture exact match information by assign ##ing to matrix elements if the corresponding answer term and question term are the same . this exact match information include match between numbers and proper nouns , which are highlighted in previous research work as especially important for facto ##id questions answering , where most of the questions are of type what , when , who that are looking for answers containing numbers or proper nouns . within an ##mm architecture , this problem has already been handled with q ##a matching matrix . thus incorporating word overlap features will not help much for improving the performance of an ##mm . ( 2 ) in addition to exact match information , an ##mm could also learn question term importance like idf information through question attention network . instead of empirical ##ly designing he ##uri ##stic functions like idf , an ##mm can get learning based question term importance score . as analyzed in section [ reference ] , with the optimization process in the back propagation training process , an ##mm can learn similar or even better term weight ##ing score than idf . thus combining an ##mm score with features like idf weighted word overlap features and b ##m ##25 may not increase the performance of an ##mm by a large margin as the case in related research works . sub ##su ##bs ##ection : results summary [ b ] 0 . 48 [ b ] 0 . 48 finally we sum ##mar ##ize the results of previously published systems on the q ##a answer ranking task in table [ reference ] . we can see an ##mm trained with train - all set beats all the previous state - of - the art systems including both methods using feature engineering and deep learning models . these results are very promising since an ##mm requires no manual feature engineering , no expensive processing by various nl ##p par ##ser ##s and no external results like large scale knowledge base except for pre - trained word em ##bed ##ding ##s . furthermore , even without combining additional features , an ##mm still performs well for answer ranking , showing significant improvements over previous deep learning model with no additional features and linguistic feature engineering methods . sub ##section : parameter sensitivity analysis we perform parameter sensitivity analysis of our proposed model an ##mm . we focus on an ##mm - 1 as the example due to the space limitation . for an ##mm - 1 , we fix the number of bin ##s as and change the dimension of word vectors . similarly , we fix the dimension of word vectors as and vary the number of bin ##s . figure [ reference ] shows the change of map and mr ##r on the validation data as we vary the hyper - parameters . we sum ##mar ##ize our observations as follows : ( 1 ) for word vector dimension , the range is a good choice as much lower or higher word vector dimensions will hurt the performance . the choice of word vector dimension also depends on the size of training corpus . larger corpus requires higher dimension of word vectors to em ##bed terms in vocabulary . ( 2 ) for the number of bin ##s , we can see that map and mr ##r will decrease as the bin number increase . too many bin ##s will increase the model complexity , which leads an ##mm to be more likely to over ##fi ##t the training data . thus choosing suitable number of bin ##s by opt ##imi ##zing hyper - parameter on validation data can help improve the performance of an ##mm . section : conclusions and future work in this paper , we propose an attention based neural matching model for ranking short answer text . we adopt value - shared weight ##ing scheme instead of position - shared weight ##ing scheme for comb ##ing different matching signals and incorporate question term importance learning using a question attention network . we perform a thorough experimental study with the tre ##c q ##a data ##set from tre ##c q ##a tracks 8 - 13 and show promising results . unlike previous methods including cnn as in and l ##st ##m as in , which only show inferior results without combining additional features , our model can achieve better performance than the state - of - art method using linguistic feature engineering without additional features . with a simple additional feature , our method can achieve the new state - of - the - art performance among current existing methods . for further work , we will study other deep learning architecture ##s for answer ranking and extend our work to include non - facto ##id question answering data sets . section : ac ##k ##now ##led ##gm ##ents this work was supported in part by the center for intelligent information retrieval , in part by ns ##f ii ##s - 116 ##0 ##8 ##9 ##4 , and in part by ns ##f grant # ii ##s - 141 ##9 ##6 ##9 ##3 . any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor . bibliography : references",
        "pred_seq": "[SEP] cnn ##s [SEP] b scores [SEP] question task [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "cnns"
                    ]
                ],
                "Metric": [
                    [
                        "bm25 scores"
                    ]
                ],
                "Task": [
                    [
                        "question answering task"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "trec qa dataset",
                        "trec qa"
                    ]
                ],
                "Method": [
                    [
                        "attentionbased neural matching model",
                        "attention based neural matching model",
                        "anmm"
                    ]
                ],
                "Metric": [
                    [
                        "map"
                    ]
                ],
                "Task": [
                    [
                        "question answering\u00a1",
                        "ranking systems question answering",
                        "question answering",
                        "qa",
                        "question answer matching",
                        "question answer matching problem",
                        "factoid question answering",
                        "factoid questions answering",
                        "nonfactoid question answering"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "trec qa dataset",
                        "trec qa"
                    ]
                ],
                "Method": [
                    [
                        "attentionbased neural matching model",
                        "attention based neural matching model",
                        "anmm"
                    ]
                ],
                "Metric": [
                    [
                        "mean average precision",
                        "mean reciprocal rank",
                        "mrr"
                    ]
                ],
                "Task": [
                    [
                        "question answering\u00a1",
                        "ranking systems question answering",
                        "question answering",
                        "qa",
                        "question answer matching",
                        "question answer matching problem",
                        "factoid question answering",
                        "factoid questions answering",
                        "nonfactoid question answering"
                    ]
                ]
            }
        ]
    },
    "52": {
        "doctext": "document : ad ##vers ##aria ##l disc ##rim ##ina ##tive domain adaptation ad ##vers ##aria ##l learning methods are a promising approach to training robust deep networks , and can generate complex samples across diverse domains . they also can improve recognition despite the presence of domain shift or data ##set bias : several ad ##vers ##aria ##l approaches to un ##su ##per ##vis ##ed domain adaptation have recently been introduced , which reduce the difference between the training and test domain distributions and thus improve general ##ization performance . prior genera ##tive approaches show compelling visual ##izations , but are not optimal on disc ##rim ##ina ##tive tasks and can be limited to smaller shifts . prior disc ##rim ##ina ##tive approaches could handle larger domain shifts , but imposed tied weights on the model and did not exploit a gan - based loss . we first outline a novel generalized framework for ad ##vers ##aria ##l adaptation , which sub ##sume ##s recent state - of - the - art approaches as special cases , and we use this generalized view to better relate the prior approaches . we propose a previously une ##x ##pl ##ored instance of our general framework which combines disc ##rim ##ina ##tive modeling , un ##tie ##d weight sharing , and a gan loss , which we call ad ##vers ##aria ##l disc ##rim ##ina ##tive domain adaptation ( add ##a ) . we show that add ##a is more effective yet considerably simpler than competing domain - ad ##vers ##aria ##l methods , and demonstrate the promise of our approach by exceeding state - of - the - art un ##su ##per ##vis ##ed adaptation results on standard cross - domain digit classification tasks and a new more difficult cross - mod ##ality object classification task . section : introduction deep con ##vo ##lu ##tion ##al networks , when trained on large - scale data ##set ##s , can learn representations which are generic ##ally useful ##l across a variety of tasks and visual domains . however , due to a phenomenon known as data ##set bias or domain shift , recognition models trained along with these representations on one large data ##set do not general ##ize well to novel data ##set ##s and tasks . the typical solution is to further fine - tune these networks on task - specific data ##set ##s \u2014 however , it is often prohibit ##ively difficult and expensive to obtain enough labeled data to properly fine - tune the large number of parameters employed by deep multi ##layer networks . domain adaptation methods attempt to mit ##igate the harmful effects of domain shift . recent domain adaptation methods learn deep neural transformations that map both domains into a common feature space . this is generally achieved by opt ##imi ##zing the representation to minimize some measure of domain shift such as maximum mean disc ##re ##pan ##cy or correlation distances . an alternative is to rec ##ons ##truct the target domain from the source representation . ad ##vers ##aria ##l adaptation methods have become an increasingly popular incarnation of this type of approach which seeks to minimize an approximate domain disc ##re ##pan ##cy distance through an ad ##vers ##aria ##l objective with respect to a domain disc ##rim ##inator . these methods are closely related to genera ##tive ad ##vers ##aria ##l learning , which pits two networks against each other \u2014 a generator and a disc ##rim ##inator . the generator is trained to produce images in a way that confuse ##s the disc ##rim ##inator , which in turn tries to distinguish them from real image examples . in domain adaptation , this principle has been employed to ensure that the network can not distinguish between the distributions of its training and test domain examples . however , each algorithm makes different design choices such as whether to use a generator , which loss function to employ , or whether to share weights across domains . for example , share weights and learn a symmetric mapping of both source and target images to the shared feature space , while deco ##up ##le some layers thus learning a partially as ##ym ##metric mapping . in this work , we propose a novel unified framework for ad ##vers ##aria ##l domain adaptation , allowing us to effectively examine the different factors of variation between the existing approaches and clearly view the similarities they each share . our framework un ##ifies design choices such as weight - sharing , base models , and ad ##vers ##aria ##l losses and sub ##sume ##s previous work , while also facilitating the design of novel instant ##iation ##s that improve upon existing ones . in particular , we observe that genera ##tive modeling of input image distributions is not necessary , as the ultimate task is to learn a disc ##rim ##ina ##tive representation . on the other hand , as ##ym ##metric mapping ##s can better model the difference in low level features than symmetric ones . we therefore propose a previously une ##x ##pl ##ored un ##su ##per ##vis ##ed ad ##vers ##aria ##l adaptation method , ad ##vers ##aria ##l disc ##rim ##ina ##tive domain adaptation ( add ##a ) , illustrated in figure [ reference ] . add ##a first learns a disc ##rim ##ina ##tive representation using the labels in the source domain and then a separate encoding that maps the target data to the same space using an as ##ym ##metric mapping learned through a domain - ad ##vers ##aria ##l loss . our approach is simple yet surprisingly powerful and achieve ##s state - of - the - art visual adaptation results on the mn ##ist , us ##ps , and sv ##hn digits data ##set ##s . we also test its potential to bridge the gap between even more difficult cross - mod ##ality shifts , without requiring instance constraints , by transferring object class ##ifiers from r ##gb color images to depth observations . section : related work there has been extensive prior work on domain transfer learning , see e . g . , . recent work has focused on transferring deep neural network representations from a labeled source data ##set ##s to a target domain where labeled data is sparse or non - existent . in the case of un ##lab ##ele ##d target domains ( the focus of this paper ) the main strategy has been to guide feature learning by mini ##mi ##zing the difference between the source and target feature distributions . several methods have used the maximum mean disc ##re ##pan ##cy ( mm ##d ) loss for this purpose . mm ##d compute ##s the norm of the difference between two domain means . the dd ##c method used mm ##d in addition to the regular classification loss on the source to learn a representation that is both disc ##rim ##ina ##tive and domain invariant . the deep adaptation network ( dan ) applied mm ##d to layers embedded in a rep ##rod ##uc ##ing kernel hilbert space , effectively matching higher order statistics of the two distributions . in contrast , the deep correlation alignment ( coral ) method proposed to match the mean and co ##var ##iance of the two distributions . other methods have chosen an ad ##vers ##aria ##l loss to minimize domain shift , learning a representation that is simultaneously disc ##rim ##ina ##tive of source labels while not being able to distinguish between domains . proposed adding a domain class ##ifier ( a single fully connected layer ) that predict ##s the binary domain label of the inputs and designed a domain confusion loss to encourage its prediction to be as close as possible to a uniform distribution over binary labels . the gradient reversal algorithm ( reverse ##grad ) proposed in also treats domain in ##var ##iance as a binary classification problem , but directly maximize ##s the loss of the domain class ##ifier by rev ##ers ##ing its gradient ##s . dr ##c ##n takes a similar approach but also learns to rec ##ons ##truct target domain images . in related work , ad ##vers ##aria ##l learning has been explored for genera ##tive tasks . the genera ##tive ad ##vers ##aria ##l network ( gan ) method is a genera ##tive deep model that pits two networks against one another : a genera ##tive model g that captures the data distribution and a disc ##rim ##ina ##tive model d that distinguishes between samples drawn from g and images drawn from the training data by predicting a binary label . the networks are trained jointly using back ##pro ##p on the label prediction loss in a mini - max fashion : simultaneously update g to minimize the loss while also up ##dating d to maximize the loss ( fool ##ing the disc ##rim ##inator ) . the advantage of gan over other genera ##tive methods is that there is no need for complex sampling or inference during training ; the downs ##ide is that it may be difficult to train . gan ##s have been applied to generate natural images of objects , such as digits and faces , and have been extended in several ways . the big ##an approach extends gan ##s to also learn the inverse mapping from the image data back into the late ##nt space , and shows that this can learn features useful for image classification tasks . the conditional genera ##tive ad ##vers ##aria ##l net ( c ##gan ) is an extension of the gan where both networks g and d receive an additional vector of information as input . this might contain , say , information about the class of the training example . the authors apply c ##gan to generate a ( possibly multi - mod ##al ) distribution of tag - vectors conditional on image features . recently the co ##gan approach applied gan ##s to the domain transfer problem by training two gan ##s to generate the source and target images respectively . the approach achieve ##s a domain invariant feature space by tying the high - level layer parameters of the two gan ##s , and shows that the same noise input can generate a corresponding pair of images from the two distributions . domain adaptation was performed by training a class ##ifier on the disc ##rim ##inator output and applied to shifts between the mn ##ist and us ##ps digit data ##set ##s . however , this approach relies on the generators finding a mapping from the shared high - level layer feature space to full images in both domains . this can work well for say digits which can be difficult in the case of more distinct domains . in this paper , we observe that modeling the image distributions is not strictly necessary to achieve domain adaptation , as long as the late ##nt feature space is domain invariant , and propose a disc ##rim ##ina ##tive approach . section : generalized ad ##vers ##aria ##l adaptation we present a general framework for ad ##vers ##aria ##l un ##su ##per ##vis ##ed adaptation methods . in un ##su ##per ##vis ##ed adaptation , we assume access to source images and labels drawn from a source domain distribution , as well as target images drawn from a target distribution , where there are no label observations . our goal is to learn a target representation , and class ##ifier that can correctly classify target images into one of categories at test time , despite the lack of in domain ann ##ota ##tions . since direct supervised learning on the target is not possible , domain adaptation instead learns a source representation mapping , , along with a source class ##ifier , , and then learns to adapt that model for use in the target domain . in ad ##vers ##aria ##l adaptive methods , the main goal is to regular ##ize the learning of the source and target mapping ##s , and , so as to minimize the distance between the empirical source and target mapping distributions : and . if this is the case then the source classification model , , can be directly applied to the target representations , eli ##mat ##ing the need to learn a separate target class ##ifier and instead setting , . the source classification model is then trained using the standard supervised loss below : we are now able to describe our full general framework view of ad ##vers ##aria ##l adaptation approaches . we note that all approaches minimize source and target representation distances through alternating mini ##mi ##zation between two functions . first a domain disc ##rim ##inator , , which class ##ifies whether a data point is drawn from the source or the target domain . thus , is opt ##imi ##zed according to a standard supervised loss , where the labels indicate the origin domain , defined below : second , the source and target mapping ##s are opt ##imi ##zed according to a constrained ad ##vers ##aria ##l objective , whose particular instant ##iation may vary across methods . thus , we can derive a generic formulation for domain ad ##vers ##aria ##l techniques below : in the next sections , we demonstrate the value of our framework by positioning recent domain ad ##vers ##aria ##l approaches within our framework . we describe the potential mapping structure , mapping optimization constraints ( ) choices and finally choices of ad ##vers ##aria ##l mapping loss , . sub ##section : source and target mapping ##s in the case of learning a source mapping alone it is clear that supervised training through a late ##nt space disc ##rim ##ina ##tive loss using the known labels results in the best representation for final source recognition . however , given that our target domain is un ##lab ##ele ##d , it remains an open question how best to minimize the distance between the source and target mapping ##s . thus the first choice to be made is in the particular parameter ##ization of these mapping ##s . because un ##su ##per ##vis ##ed domain adaptation generally considers target disc ##rim ##ina ##tive tasks such as classification , previous adaptation methods have generally relied on adapting disc ##rim ##ina ##tive models between domains . with a disc ##rim ##ina ##tive base model , input images are mapped into a feature space that is useful for a disc ##rim ##ina ##tive task such as image classification . for example , in the case of digit classification this may be the standard len ##et model . however , liu and tu ##zel achieve state of the art results on un ##su ##per ##vis ##ed mn ##ist - us ##ps using two genera ##tive ad ##vers ##aria ##l networks . these genera ##tive models use random noise as input to generate samples in image space \u2014 generally , an intermediate feature of an ad ##vers ##aria ##l disc ##rim ##inator is then used as a feature for training a task - specific class ##ifier . once the mapping parameter ##ization is determined for the source , we must decide how to para ##met ##rize the target mapping . in general , the target mapping almost always matches the source in terms of the specific functional layer ( architecture ) , but different methods have proposed various regular ##ization techniques . all methods initial ##ize the target mapping parameters with the source , but different methods choose different constraints between the source and target mapping ##s , . the goal is to make sure that the target mapping is set so as to minimize the distance between the source and target domains under their respective mapping ##s , while crucial ##ly also maintaining a target mapping that is category disc ##rim ##ina ##tive . consider a layered representations where each layer parameters are denoted as , or , for a given set of equivalent layers , . then the space of constraints explored in the literature can be described through layer ##wise equality constraints as follows : where each individual layer can be constrained independently . a very common form of constraint is source and target layer ##wise equality : it is also common to leave layers un ##con ##stra ##ined . these equality constraints can easily be imposed within a con ##vo ##lu ##tion ##al network framework through weight sharing . for many prior ad ##vers ##aria ##l adaptation methods , all layers are constrained , thus enforcing exact source and target mapping consistency . learning a symmetric transformation reduces the number of parameters in the model and ensures that the mapping used for the target is disc ##rim ##ina ##tive at least when applied to the source domain . however , this may make the optimization poorly conditioned , since the same network must handle images from two separate domains . an alternative approach is instead to learn an as ##ym ##metric transformation with only a subset of the layers constrained , thus enforcing partial alignment . ro ##zan ##ts ##ev et al . showed that partially shared weights can lead to effective adaptation in both supervised and un ##su ##per ##vis ##ed settings . as a result , some recent methods have favored un ##ty ##ing weights ( fully or partially ) between the two domains , allowing models to learn parameters for each domain individually . sub ##section : ad ##vers ##aria ##l losses once we have decided on a para ##met ##rization of , we employ an ad ##vers ##aria ##l loss to learn the actual mapping . there are various different possible choices of ad ##vers ##aria ##l loss functions , each of which have their own unique use cases . all ad ##vers ##aria ##l losses train the ad ##vers ##aria ##l disc ##rim ##inator using a standard classification loss , , previously stated in equation [ reference ] . however , they differ in the loss used to train the mapping , . the gradient reversal layer of opt ##imi ##zes the mapping to maximize the disc ##rim ##inator loss directly : this optimization corresponds to the true mini ##max objective for genera ##tive ad ##vers ##aria ##l networks . however , this objective can be problematic , since early on during training the disc ##rim ##inator converge ##s quickly , causing the gradient to vanish . when training gan ##s , rather than directly using the mini ##max loss , it is typical to train the generator with the standard loss function with inverted labels . this splits the optimization into two independent objectives , one for the generator and one for the disc ##rim ##inator , where remains unchanged , but becomes : this objective has the same fixed - point properties as the mini ##max loss but provides stronger gradient ##s to the target mapping . we refer to this modified loss function as the \" gan loss function \" for the remainder of this paper . note that , in this setting , we use independent mapping ##s for source and target and learn only ad ##vers ##aria ##lly . this mimic ##s the gan setting , where the real image distribution remains fixed , and the generating distribution is learned to match it . the gan loss function is the standard choice in the setting where the generator is attempting to mimic another un ##chang ##ing distribution . however , in the setting where both distributions are changing , this objective will lead to os ##ci ##llation \u2014 when the mapping converge ##s to its opt ##imum , the disc ##rim ##inator can simply flip the sign of its prediction in response . t ##zen ##g et al . instead proposed the domain confusion objective , under which the mapping is trained using a cross - entropy loss function against a uniform distribution : this loss ensures that the ad ##vers ##aria ##l disc ##rim ##inator views the two domains identical ##ly . section : ad ##vers ##aria ##l disc ##rim ##ina ##tive domain adaptation the benefit of our generalized framework for domain ad ##vers ##aria ##l methods is that it directly enables the development of novel adaptive methods . in fact , designing a new method has now been simplified to the space of making three design choices : whether to use a genera ##tive or disc ##rim ##ina ##tive base model , whether to tie or un ##tie the weights , and which ad ##vers ##aria ##l learning objective to use . in light of this view we can sum ##mar ##ize our method , ad ##vers ##aria ##l disc ##rim ##ina ##tive domain adaptation ( add ##a ) , as well as its connection to prior work , according to our choices ( see table [ reference ] \" add ##a \" ) . specifically , we use a disc ##rim ##ina ##tive base model , un ##sha ##red weights , and the standard gan loss . we illustrate our overall sequential training procedure in figure [ reference ] . first , we choose a disc ##rim ##ina ##tive base model , as we h ##yp ##oth ##es ##ize that much of the parameters required to generate convincing in - domain samples are irrelevant for disc ##rim ##ina ##tive adaptation tasks . most prior ad ##vers ##aria ##l adaptive methods opt ##imi ##ze directly in a disc ##rim ##ina ##tive space for this reason . one counter - example is co ##gan ##s . however , this method has only shown dominance in settings where the source and target domain are very similar such as mn ##ist and us ##ps , and in our experiments we have had difficulty getting it to converge for larger distribution shifts . next , we choose to allow independent source and target mapping ##s by un ##ty ##ing the weights . this is a more flexible lear ##ing paradigm as it allows more domain specific feature extraction to be learned . however , note that the target domain has no label access , and thus without weight sharing a target model may quickly learn a de ##gen ##erate solution if we do not take care with proper initial ##ization and training procedures . therefore , we use the pre - trained source model as an int ##itia ##lization for the target representation space and fix the source model during ad ##vers ##aria ##l training . in doing so , we are effectively learning an as ##ym ##metric mapping , in which we modify the target model so as to match the source distribution . this is most similar to the original genera ##tive ad ##vers ##aria ##l learning setting , where a generated space is updated until it is ind ##ist ##ing ##uis ##hab ##le with a fixed real space . therefore , we choose the inverted label gan loss described in the previous section . our proposed method , add ##a , thus corresponds to the following un ##con ##stra ##ined optimization : we choose to opt ##imi ##ze this objective in stages . we begin by opt ##imi ##zing over and by training using the labeled source data , and . because we have opted to leave fixed while learning , we can thus opt ##imi ##ze and without rev ##isi ##ting the first objective term . a summary of this entire training process is provided in figure [ reference ] . we note that the unified framework presented in the previous section has enabled us to compare prior domain ad ##vers ##aria ##l methods and make informed decisions about the different factors of variation . through this framework we are able to mo ##tiv ##ate a novel domain adaptation method , add ##a , and offer insight into our design decisions . in the next section we demonstrate promising results on un ##su ##per ##vis ##ed adaptation bench ##mark tasks , studying adaptation across digits and across mod ##ali ##ties . section : experiments we now evaluate add ##a for un ##su ##per ##vis ##ed classification adaptation across four different domain shifts . we explore three digits data ##set ##s of varying difficulty : mn ##ist , us ##ps , and sv ##hn . we additionally evaluate on the nyu ##d data ##set to study adaptation across mod ##ali ##ties . example images from all experimental data ##set ##s are provided in figure [ reference ] . for the case of digit adaptation , we compare against multiple state - of - the - art un ##su ##per ##vis ##ed adaptation methods , all based upon domain ad ##vers ##aria ##l learning objectives . in 3 of 4 of our experimental setup ##s , our method out ##per ##forms all competing approaches , and in the last domain shift studied , our approach out ##per ##forms all but one competing approach . we also valid ##ate our model on a real - world mod ##ality adaptation task using the nyu depth data ##set . despite a large domain shift between the r ##gb and depth mod ##ali ##ties , add ##a learns a useful depth representation without any labeled depth data and improves over the non ##ada ##ptive baseline by over 50 % ( relative ) . sub ##section : mn ##ist , us ##ps , and sv ##hn digits data ##set ##s we experimental ##ly valid ##ate our proposed method in an un ##su ##per ##vis ##ed adaptation task between the mn ##ist , us ##ps , and sv ##hn digits data ##set ##s , which consist 10 classes of digits . example images from each data ##set are visual ##ized in figure [ reference ] and table [ reference ] . for adaptation between mn ##ist and us ##ps , we follow the training protocol established in , sampling 2000 images from mn ##ist and 1800 from us ##ps . for adaptation between sv ##hn and mn ##ist , we use the full training sets for comparison against . all experiments are performed in the un ##su ##per ##vis ##ed settings , where labels in the target domain are with ##held , and we consider adaptation in three directions : mn ##ist us ##ps , us ##ps mn ##ist , and sv ##hn mn ##ist . for these experiments , we use the simple modified len ##et architecture provided in the caf ##fe source code . when training with add ##a , our ad ##vers ##aria ##l disc ##rim ##inator consists of 3 fully connected layers : two layers with 500 hidden units followed by the final disc ##rim ##inator output . each of the 500 - unit layers uses a re ##lu activation function . results of our experiment are provided in table [ reference ] . on the easier mn ##ist and us ##ps shifts add ##a achieve ##s comparable performance to the current state - of - the - art , co ##gan ##s , despite being a considerably simpler model . this provides compelling evidence that the machinery required to generate images is largely irrelevant to enabling effective adaptation . additionally , we show convincing results on the challenging sv ##hn and mn ##ist task in comparison to other methods , indicating that our method has the potential to general ##ize to a variety of settings . in contrast , we were unable to get co ##gan ##s to converge on sv ##hn and mn ##ist \u2014 because the domains are so di ##spar ##ate , we were unable to train coupled generators for them . sub ##section : mod ##ality adaptation bath ##tub bed books ##hel ##f box chair counter desk door dresser garbage bin lamp monitor night stand pillow sink sofa table television toilet overall we use the nyu depth data ##set , which contains bound ##ing box ann ##ota ##tions for 19 object classes in 144 ##9 images from indoor scenes . the data ##set is split into a train ( 381 images ) , val ( 41 ##4 images ) and test ( 65 ##4 ) sets . to perform our cross - mod ##ality adaptation , we first crop out tight bound ##ing boxes around instances of these 19 classes present in the data ##set and evaluate on a 19 - way classification task over object crops . in order to ensure that the same instance is not seen in both domains , we use the r ##gb images from the train split as the source domain and the depth images from the val split as the target domain . this corresponds to 2 , 186 labeled source images and 2 , 401 un ##lab ##ele ##d target images . figure [ reference ] visual ##izes samples from each of the two domains . we consider the task of adaptation between these r ##gb and h ##ha encoded depth images , using them as source and target domains respectively . because the bound ##ing boxes are tight and relatively low resolution , accurate classification is quite difficult , even when evaluating in - domain . in addition , the data ##set has very few examples for certain classes , such as toilet and bath ##tub , which directly translates to reduced classification performance . for this experiment , our base architecture is the v ##gg - 16 architecture , initial ##izing from weights pre ##train ##ed on image ##net . this network is then fully fine - tuned on the source domain for 20 , 000 iteration ##s using a batch size of 128 . when training with add ##a , the ad ##vers ##aria ##l disc ##rim ##inator consists of three additional fully connected layers : 102 ##4 hidden units , 204 ##8 hidden units , then the ad ##vers ##aria ##l disc ##rim ##inator output . with the exception of the output , these additionally fully connected layers use a re ##lu activation function . add ##a training then proceeds for another 20 , 000 iteration ##s , again with a batch size of 128 . we find that our method , add ##a , greatly improves classification accuracy for this task . for certain categories , like counter , classification accuracy goes from 2 . 9 % under the source only baseline up to 44 . 7 % after adaptation . in general , average accuracy across all classes improves significantly from 13 . 9 % to 21 . 1 % . however , not all classes improve . three classes have no correctly labeled target images before adaptation , and adaptation is unable to recover performance on these classes . additionally , the classes of pillow and nightstand suffer performance loss after adaptation . for additional insight on what effect add ##a has on classification , figure [ reference ] plots confusion matrices before adaptation , after adaptation , and in the hypothetical best - case scenario where the target labels are present . examining the confusion matrix for the source only baseline reveals that the domain shift is quite large \u2014 as a result , the network is poorly conditioned and incorrectly predict ##s pillow for the majority of the data ##set . this tendency to output pillow also explains why the source only model achieve ##s such abnormal ##ly high accuracy on the pillow class , despite poor performance on the rest of the classes . in contrast , the class ##ifier trained using add ##a predict ##s a much wider variety of classes . this leads to decreased accuracy for the pillow class , but significantly higher acc ##ura ##cies for many of the other classes . additionally , comparison with the \" train on target \" model reveals that many of the mistakes the add ##a model makes are reasonable , such as confusion between the chair and table classes , indicating that the add ##a model is learning a useful representation on depth images . section : conclusion we have proposed a unified framework for un ##su ##per ##vis ##ed domain adaptation techniques based on ad ##vers ##aria ##l learning objectives . our framework provides a simplified and co ##hesive view by which we may understand and connect the similarities and differences between recently proposed adaptation methods . through this comparison , we are able to understand the benefits and key ideas from each approach and to combine these strategies into a new adaptation method , add ##a . we present evaluation across four domain shifts for our un ##su ##per ##vis ##ed adaptation approach . our method general ##izes well across a variety of tasks , achieving strong results on bench ##mark adaptation data ##set ##s as well as a challenging cross - mod ##ality adaptation task . additional analysis indicates that the representations learned via add ##a resemble features learned with supervisory data in the target domain much more closely than una ##da ##pt ##ed features , providing further evidence that add ##a is effective at partially undo ##ing the effects of domain shift . bibliography : references",
        "pred_seq": "[SEP] ad adaptation [SEP] [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "adversarial discriminative domain adaptation"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "svhn and mnist",
                        "svhn mnist"
                    ]
                ],
                "Method": [
                    [
                        "adversarial discriminative domain adaptation",
                        "adda",
                        "adversarial adaptation methods",
                        "adversarial unsupervised adaptation methods"
                    ]
                ],
                "Metric": [
                    [
                        "classification performance",
                        "classification accuracy",
                        "accuracy",
                        "accuracies"
                    ]
                ],
                "Task": [
                    [
                        "unsupervised adaptation"
                    ]
                ]
            }
        ]
    },
    "53": {
        "doctext": "document : exploit ##ing temporal information for 3d human pose estimation in this work , we address the problem of 3d human pose estimation from a sequence of 2d human poses . although the recent success of deep networks has led many state - of - the - art methods for 3d pose estimation to train deep networks end - to - end to predict from images directly , the top - performing approaches have shown the effectiveness of dividing the task of 3d pose estimation into two steps : using a state - of - the - art 2d pose est ##ima ##tor to estimate the 2d pose from images and then mapping them into 3d space . they also showed that a low - dimensional representation like 2d locations of a set of joints can be disc ##rim ##ina ##tive enough to estimate 3d pose with high accuracy . however , estimation of 3d pose for individual frames leads to temporal ##ly inc ##oh ##ere ##nt estimates due to independent error in each frame causing ji ##tter . therefore , in this work we utilize the temporal information across a sequence of 2d joint locations to estimate a sequence of 3d poses . we designed a sequence - to - sequence network composed of layer - normal ##ized l ##st ##m units with short ##cut connections connecting the input to the output on the deco ##der side and imposed temporal smooth ##ness constraint during training . we found that the knowledge of temporal consistency improves the best reported result on human ##3 . 6 m data ##set by approximately and helps our network to recover temporal ##ly consistent 3d poses over a sequence of images even when the 2d pose detector fails . section : introduction the task of est ##imating 3d human pose from 2d representations like mono ##cular images or videos is an open research problem among the computer vision and graphics community for a long time . an understanding of human posture and limb art ##ic ##ulation is important for high level computer vision tasks such as human action or activity recognition , sports analysis , augmented and virtual reality . a 2d representation of human pose , which is considered to be much easier to estimate , can be used for these tasks . however , 2d poses can be ambiguous because of o ##cc ##lusion and fore ##sho ##rte ##ning . additionally poses that are totally different can appear to be similar in 2d because of the way they are projected as shown in figure [ reference ] . the depth information in 3d representation of human pose makes it free from such am ##bi ##gui ##ties and hence can improve performance for higher level tasks . moreover , 3d pose can be very useful in computer animation , where the articulated pose of a person in 3d can be used to accurately model human posture and movement . however , 3d pose estimation is an ill - posed problem because of the inherent ambiguity in back - projecting a 2d view of an object to the 3d space maintaining its structure . since the 3d pose of a person can be projected in an infinite number of ways on a 2d plane , the mapping from a 2d pose to 3d is not unique . moreover , obtaining a data ##set for 3d pose is difficult and expensive . unlike the 2d pose data ##set ##s where the users can manually label the key ##points by mouse clicks , 3d pose data ##set ##s require a complicated laboratory setup with motion capture sensors and cameras . hence , there is a lack of motion capture data ##set ##s for images in - the - wild . over the years , different techniques have been used to address the problem of 3d pose estimation . earlier methods used to focus on extract ##ing features , invariant to factors such as background scenes , lighting , and skin color from images and mapping them into 3d human pose . with the success of deep networks , recent methods tend to focus on training a deep con ##vo ##lu ##tion ##al neural network ( cnn ) end - to - end to estimate 3d poses from images directly . some approaches divided the 3d pose estimation task into first predicting the joint locations in 2d using 2d pose est ##ima ##tors and then back - projecting them to estimate the 3d joint locations . these results suggest the effectiveness of deco ##up ##ling the task of 3d pose estimation where 2d pose est ##ima ##tor abstracts the complex ##ities in the image . in this paper , we also adopt the deco ##up ##led approach to 3d pose estimation . however , predicting 3d pose for each frame individually can lead to ji ##tter in videos because the errors in each frame are independent of each other . therefore , we designed a sequence - to - sequence network with short ##cut connections on the deco ##der side that predict ##s a sequence of temporal ##ly consistent 3d poses given a sequence of 2d poses . each unit of our network is a long short - term memory ( l ##st ##m ) unit with layer normal ##ization and rec ##urrent drop ##out . we also imposed a temporal smooth ##ness constraint on the predicted 3d poses during training to ensure that our predictions are smooth over a sequence . our network achieve ##s the state - of - the - art result on the human ##3 . 6 m data ##set improving the previous best result by approximately . we also obtained the lowest error for every action class in human ##3 . 6 m data ##set . moreover , we observed that our network predicted meaningful 3d poses on youtube videos , even when the detection ##s from the 2d pose detector were extremely noisy or meaningless . this shows the effectiveness of using temporal information . in short our contributions in this work are : designing an efficient sequence - to - sequence network that achieve ##s the state - of - the - art results for every action class of human ##3 . 6 m data ##set and can be trained very fast . exploit ##ing the ability of sequence - to - sequence networks to take into account the events in the past , to predict temporal ##ly consistent 3d poses . effectively imposing temporal consistency constraint on the predicted 3d poses during training so that the errors in the predictions are distributed smoothly over the sequence . using only the previous frames to understand temporal context so that it can be deployed online and real - time . section : related work paragraph : representation of 3d pose both model - based and model - free representations of 3d human pose have been used in the past . the most common model - based representation is a skeleton defined by a kin ##ema ##tic tree of a set of joints , parameter ##ized by the offset and rotational parameters of each joint relative to its parent . several 3d pose methods have used this representation . others model 3d pose as a sparse linear combination of an over - complete dictionary of basis poses . however , we have chosen a model - free representation of 3d pose , where a 3d pose is simply a set of 3d joint locations relative to the root node like several recent approaches . this representation is much simpler and low - dimensional . paragraph : est ##imating 3d pose from 2d joints lee and chen were the first to in ##fer 3d joint locations from their 2d projections given the bone lengths using a binary decision tree where each branch corresponds to two possible states of a joint relative to its parent . jiang used the 2d joint locations to estimate a set of hypothesis 3d poses using taylor ' s algorithm and used them to query a large database of motion capture data to find the nearest neighbor . gupta et al . and chen and rama ##nan also used this idea of using the detected 2d pose to query a large database of ex ##em ##pl ##ar poses to find the nearest nearest neighbor 3d pose . another common approach to est ##imating 3d joint locations given the 2d pose is to separate the camera pose variability from the intrinsic deformation of the human body , the latter of which is modeled by learning an over - complete dictionary of basis 3d poses from a large database of motion capture data . a valid 3d pose is defined by a sparse linear combination of the bases and by transforming the points using transformation matrix representing camera ex ##tri ##ns ##ic parameters . moreno - no ##ug ##uer used the pair - wise distance matrix of 2d joints to learn a distance matrix for 3d joints , which they found invariant up to a rigid similarity transform with the ground truth 3d and used multi - dimensional scaling ( md ##s ) with pose - prior ##s to rule out the am ##bi ##gui ##ties . martinez et al . designed a fully connected network with short ##cut connections every two linear layers to estimate 3d joint locations relative to the root node in the camera coordinate space . paragraph : deep network based methods with the success of deep networks , many have designed networks that can be trained end - to - end to predict 3d poses from images directly . li et al . and park et al . designed cnn ##s to jointly predict 2d and 3d poses . me ##hta et al . and sun et al . used transfer learning to transfer the knowledge learned for 2d human pose estimation to the task of 3d pose estimation . pa ##v ##lak ##os et al . extended the stacked - hour ##glass network originally designed to predict 2d heat ##ma ##ps of each joint to make it predict 3d volume ##tric heat ##ma ##ps . tome et al . also extended a 2d pose est ##ima ##tor called con ##vo ##lu ##tion ##al pose machine ( cp ##m ) to make it predict 3d pose . ro ##ges ##z and sc ##hmi ##d and var ##ol et al . augmented the training data with synthetic images and trained cnn ##s to predict 3d poses from real images . sun et al . designed a unified network that can reg ##ress both 2d and 3d poses at the same time given an image . hence during training time , in - the - wild images which do not have any ground truth 3d poses can be combined with the data with ground truth 3d poses . a similar idea of exploit ##ing in - the - wild images to learn pose structure was used by fang et al . . they learned a pose grammar that en ##codes the possible human pose configurations . paragraph : using temporal information since est ##imating poses for each frame individually leads to inc ##oh ##ere ##nt and ji ##tter ##y predictions over a sequence , many approaches tried to exploit temporal information . and ##ril ##uka et al . used tracking - by - detection to associate 2d poses detected in each frame individually and used them to retrieve 3d pose . te ##kin et al . used a cnn to first align bound ##ing boxes of successive frames so that the person in the image is always at the center of the box and then extracted 3d hog features densely over the spat ##io - temporal volume from which they reg ##ress the 3d pose of the central frame . me ##hta et al . implemented a real - time system for 3d pose estimation that applies temporal filtering across 2d and 3d poses from previous frames to predict a temporal ##ly consistent 3d pose . lin et al . performed a multi - stage sequential ref ##ine ##ment using l ##st ##ms to predict 3d pose sequences using previously predicted 2d pose representations and 3d pose . we focus on predicting temporal ##ly consistent 3d poses by learning the temporal context of a sequence using a form of sequence - to - sequence network . unlike lin et al . our method does not need multiple stages of ref ##ine ##ment . it is simpler and requires fewer parameters to train , leading to much improved performance . section : our approach paragraph : network design we designed a sequence - to - sequence network with l ##st ##m units and residual connections on the deco ##der side to predict a temporal ##ly coherent sequence of 3d poses given a sequence of 2d joint locations . figure [ reference ] shows the architecture of our network . the motivation behind using a sequence - to - sequence network comes from its application on the task of neural machine translation ( nm ##t ) by su ##tsk ##ever et al . , where their model translates a sentence in one language to a sentence in another language e . g . english to french . in a language translation model , the input and output sentences can have different lengths . although our case is analogous to the nm ##t , the input and output sequences always have the same length while the input vectors to the en ##code ##r and deco ##der have different dimensions . the en ##code ##r side of our network takes a sequence of 2d poses and en ##codes them in a fixed size high dimensional vector in the hidden state of its final l ##st ##m unit . since the l ##st ##ms are excellent in memo ##riz ##ing events and information from the past , the encoded vector stores the 2d pose information of all the frames . the initial state of the deco ##der is initial ##ized by the final state of the en ##code ##r . a token is passed as initial input to the deco ##der , which in our case is a vector of ones , telling it to start deco ##ding . given a 3d pose estimate at a time step each deco ##der unit predict ##s the 3d pose for next time step . note that the order of the input sequence is reversed as recommended by su ##tsk ##ever et al . . the short ##cut connections on the deco ##der side cause each deco ##der unit to estimate the amount of per ##tur ##bation in the 3d pose from the previous frame instead of having to estimate the actual 3d pose for each frame . as suggested by he et al . , such a mapping is easier to learn for the network . we use layer normal ##ization and rec ##urrent drop ##out to regular ##ize our network . ba et al . came up with the idea of layer normal ##ization which estimates the normal ##ization statistics ( mean and standard deviation ) from the sum ##med inputs to the rec ##urrent neurons of hidden layer on a single training example to regular ##ize the rn ##n units . similarly , za ##rem ##ba et al . proposed the idea of applying drop ##out only on the non - rec ##urrent connections of the network with a certain probability while always keeping the rec ##urrent connections intact because they are necessary for the rec ##urrent units to remember the information from the past . paragraph : loss function given a sequence of 2d joint locations as input , our network predict ##s a sequence of 3d joint locations relative to the root node ( central hip ) . we predict each 3d pose in the camera coordinate space instead of predicting them in an arbitrary global frame as suggested by martinez et al . . we impose a temporal smooth ##ness constraint on the predicted 3d joint locations to ensure that the prediction of each joint in one frame does not differ too much from its previous frame . because the 2d pose detectors work on individual frames , even with the minimal movement of the subject in the image , the detection ##s from successive frames may vary , particularly for the joints which move fast or are prone to o ##cc ##lusion . hence , we made an assumption that the subject does not move too much in successive frames given the frame rate is high enough . therefore , we added the l ##2 norm of the first order derivative on the 3d joint locations with respect to time to our loss function during training . this constraint helps us to estimate 3d poses re ##lia ##bly even when the 2d pose detector fails for a few frames within the temporal window without any post - processing . empirical ##ly we found that certain joints are more difficult to estimate accurately e . g . wrist , ankle , elbow compared to others . to address this issue , we partition ##ed the joints into three di ##s ##jo ##int sets , and based on their contribution to overall error . we observed that the joints connected to the torso and the head e . g . hips , shoulders , neck are always predicted with high accuracy compared to those joints belonging to the limbs and therefore put them in the set . the joints of the limbs , especially the joints on the arms , are always more difficult to predict due to their high range of motion and o ##cc ##lusion . we put the knees and the ankles in the set and the elbow and wrist in . we multi ##ply the derivatives of each set of joints with different scala ##r values based on their contribution to the overall error . therefore our loss function consists of the sum of two separate terms : mean squared error ( ms ##e ) of different sequences of 3d joint locations ; and the mean of the l ##2 norm of the first order derivative of sequences of 3d joint locations with respect to time , where the joints are divided into three di ##s ##jo ##int sets . the ms ##e over sequences , each of time - steps , of 3d joint locations is given by here , denotes the estimated 3d joint locations while denotes 3d ground truth . the mean of l ##2 norm of the first order derivative of sequences of 3d joint locations , each of length , with respect to time is given by in the above equation , , and denotes the predicted 3d locations of joints belonging to the sets , and respectively . the and are scala ##r hyper - parameters to control the significance of the derivatives of 3d locations of each of the three set of joints . a higher weight is assigned to the set of joints which are generally predicted with higher error . the overall loss function for our network is given as here and are scala ##r hyper - parameters regulating the importance of each of the two terms in the loss function . section : experimental evaluation paragraph : data ##set ##s and protocols we perform quantitative evaluation on the human 3 . 6 m data ##set and on the humane ##va data ##set . human 3 . 6 m , to the best of our knowledge , is the largest publicly available data ##set for human 3d pose estimation . the data ##set contains 3 . 6 million images of 7 different professional actors performing 15 everyday activities like walking , eating , sitting , making a phone call . the data ##set consists of 2d and 3d joint locations for each corresponding image . each video is captured using 4 different cal ##ib ##rated high resolution cameras . in addition to 2d and 3d pose ground truth , the data ##set also provides ground truth for bound ##ing boxes , the camera parameters , the body proportion of all the actors and high resolution body scans or mesh ##es of each actor . humane ##va , on the other hand , is a much smaller data ##set . it has been largely used to bench ##mark previous work over the last decade . most of the methods report results on two different actions and on three actors . for qu ##ali ##tative evaluation , we used the some videos from youtube and the human ##3 . 6 m data ##set . we follow the standard protocols of the human ##3 . 6 m data ##set used in the literature . we used subjects 1 , 5 , 6 , 7 , and 8 for training , and subjects 9 and 11 for testing and the error is evaluated on the predicted 3d pose without any transformation . we refer this as protocol # 1 . another common approach used by many to evaluate their methods is to align the predicted 3d pose with the ground truth using a similarity transformation ( pro ##cr ##ust ##es analysis ) . we refer this as protocol # 2 . we use the average error per joint in mill ##imeters between the estimated and the ground truth 3d pose relative to the root node as the error metric . for the humane ##va data ##set , we report results on each subject and action separately after performing rigid alignment with the ground truth data , following the protocol used by the previous methods . paragraph : 2d detection ##s we fine - tuned a model of stacked - hour ##glass network , initially trained on the mp ##ii data ##set ( a bench ##mark data ##set for 2d pose estimation ) , on the images of the human ##3 . 6 m data ##set to obtain 2d pose estimation ##s for each image . we used the bound ##ing box information provided with the data ##set to first compute the center of the person in the image and then crop ##ped a region across the person and res ##ized it to . we fine - tuned the network for 250 iteration ##s and used a batch size of 3 and a learning rate of . paragraph : baseline ##s since many of the previous methods are based on single frame predictions , we used two baseline ##s for comparison . to show that our method is much better than naive post processing , we applied a mean filter and a median filter on the 3d pose predictions of martinez et al . . we used a window size of 5 frames and a stride length of 1 to apply the filters . although non - rigid structure from motion ( nr ##sf ##m ) is one of the most general approaches for any 3d reconstruction problem from a sequence of 2d correspondence ##s , we did not use it as a baseline because zhou et al . did not find nr ##sf ##m techniques to be effective for 3d human pose estimation . they found that the nr ##sf ##m techniques do not work well with slow camera motion . since the videos in the human ##3 . 6 m data ##set are captured by stationary cameras , the subjects in the data ##set do not rotate that much to provide alternative views for nr ##sf ##m algorithm to perform well . another reason is that human pose reconstruction is a specialized problem in which constraints from human body structure apply . paragraph : data pre - processing we normal ##ized the 3d ground truth poses , the noisy 2d pose estimates from stacked - hour ##glass network and the 2d ground truth by sub ##tra ##cting the mean and dividing by standard deviation . we do not predict the 3d location of the root joint i . e . central hip joint and hence zero center the 3d joint locations relative to the global position of the root node . to obtain the ground truth 3d poses in camera coordinate space , an inverse rigid body transformation is applied on the the ground truth 3d poses in global coordinate space using the given camera parameters . to generate both training and test sequences , we translated a sliding window of length by one frame . hence there is an overlap between the sequences . this gives us more data to train on , which is always an advantage for deep learning systems . during test time , we initially predict the first frames of the sequence and slide the window by a stride length of 1 to predict the next frame using the previous frames . paragraph : training details we trained our network for 100 epoch ##s , where each epoch makes a complete pass over the entire human 3 . 6 m data ##set . we used the adam opt ##imi ##zer for training the network with a learning rate of which is decay ##ed exponential ##ly per iteration . the weights of the l ##st ##m units are initial ##ized by xavier uniform initial ##izer . we used a mini - batch batch size of 32 i . e . 32 sequences . for most of our experiments we used a sequence length of 5 , because it allows faster training with high accuracy . we experimented with different sequence lengths and found sequence length 4 , 5 and 6 to generally give better results , which we will discuss in detail in the results section . we trained a single model for all the action classes . our code is implemented in tensor ##flow . we perform cross - validation on the training set to select the hyper - parameter values and of our loss function to and respectively . similarly , using cross - validation , the three hyper - parameters of the temporal consistency constraint and , are set to and respectively . a single training step for sequences of length 5 takes only 34 ms approximately , while a forward pass takes only about 16 ##ms on n ##vid ##ia titan x gp ##u . therefore given the 2d joint locations from a pose detector , our network takes about 3 . 2 ##ms to predict 3d pose per frame . sub ##section : quantitative results paragraph : evaluation on estimated 2d pose as mentioned before , we used a sequence length of 5 to perform both qu ##ali ##tative and quantitative evaluation of our network . the results on human ##3 . 6 m data ##set under protocol # 1 are shown in table [ reference ] . from the table we observe that our model achieve ##s the lowest error for every action class under protocol # 1 , unlike many of the previous state - of - the - art methods . note that we train a single model for all the action classes unlike many other methods which trained a model for each action class . our network significantly improves the state - of - the - art result of sun et al . by approximately ( by mm ) . the results under protocol # 2 , which align ##s the predictions to the ground truth using a rigid body similarity transform before computing the error , is reported in table [ reference ] . our network improves the reported state - of - the - art results by ( by mm ) and achieve ##s the lowest error for each action in protocol # 2 as well . from the results , we observe the effectiveness of exploit ##ing temporal information across multiple sequences . by using the information of temporal context , our network reduced the overall error in est ##imating 3d joint locations , especially on actions like phone , photo , sit and sitting down on which most previous methods did not perform well due to heavy o ##cc ##lusion . we also observe that our method out ##per ##forms both the baseline ##s by a large margin on both the protocols . this shows that our method learned the temporal context of the sequences and predicted temporal ##ly consistent 3d poses , which naive post - processing techniques like temporal mean and median filters over frame - wise prediction failed to do . like most previous methods , we report the results on action classes walking and jogging of the humane ##va data ##set in table [ reference ] . we obtained the lowest error in four of the six cases and the lowest average error for the two actions . we also obtained the second best result on subject 2 of action walking . however , humane ##va is a smaller data ##set than human ##3 . 6 m and the same subjects appear in both training and testing . paragraph : evaluation on 2d ground truth as suggested by martinez et al . , we also found that the more accurate the 2d joint locations are , the better are the estimates for 3d pose . we trained our model on ground truth 2d poses for a sequence length of 5 . the results under protocol # 1 are reported in table [ reference ] . as seen from the table , our model improves the lower bound error of martinez et al . by almost . the results on ground truth 2d joint input for protocol # 2 are reported in table [ reference ] . when there is no noise in 2d joint locations , our network performs better than the models by martinez et al . and moreno - no ##ug ##uer . these results suggest that the information of temporal consistency from previous frames is a valuable cue for the task of est ##imating 3d pose even when the detection ##s are noise free . paragraph : robust ##ness to noise we carried out some experiments to test the tolerance of our model to different levels of noise in the input data by training our network on 2d ground truth poses and testing on inputs corrupted by different levels of ga ##uss ##ian noise . table [ reference ] shows how our final model compares against the models by moreno - no ##ug ##uer and martinez et al . . our network is significantly more robust than moreno - no ##ug ##uer ' s model . when compared against martinez et al . our network performs better when the level of input noise is low i . e . standard deviation less than or equal to 10 . however , for higher levels of noise our network performs slightly worse than martinez et al . . we would like to attribute the cause of this to the temporal smooth ##ness constraint imposed during training which distribute ##s the error of individual frames over the entire sequence . however , its useful ##ness can be observed in the qu ##ali ##tative results ( see figure [ reference ] and figure [ reference ] ) . paragraph : ab ##lative analysis to show the useful ##ness of each component and design decision of our network , we perform an ab ##lative analysis . we follow protocol # 1 for performing ab ##lative analysis and trained a single model for all the actions . the results are reported in table [ reference ] . we observe that the biggest improvement in result is due the the residual connections on the deco ##der side , which agrees with the hypothesis of he et al . . removing the residual connections massive ##ly increases the error by mm . when we do not apply layer normal ##ization on l ##st ##m units , the error increases by mm . on the other hand when drop ##out is not performed , the error raises by mm . when both layer normal ##ization and rec ##urrent drop ##out are not used the results get worse by mm . although the temporal consistency constraint may seem to have less impact ( only mm ) quantitative ##ly on the performance of our network , it ensures that the predictions over a sequence are smooth and temporal ##ly consistent which is apparent from our qu ##ali ##tative results as seen in figure [ reference ] and figure [ reference ] . to show the effectiveness of our model on detection ##s from different 2d pose detectors , we also experimented with the detection ##s from cp ##m and from stacked - hour ##glass ( sh ) module which is not fine - tuned on human ##3 . 6 m data ##set . we observe that even for the non - fine tuned stacked hour ##glass detection ##s , our model achieve ##s the state - of - the - art results . for detection ##s from cp ##m , our model achieve ##s competitive accuracy for the predictions . paragraph : performance on different sequence lengths the results reported so far have been for input and output sequences of length 5 . we carried out experiments to see how our network performs for different sequence lengths ranging from 2 to 10 . the results are shown in figure [ reference ] . as can be seen , the performance of our network remains stable for sequences of varying lengths . even for a sequence length of 2 , which only considers the previous and the current frame , our model generates very good results . particularly the best results were obtained for length 4 , 5 and 6 . however , we chose sequence length 5 for carrying out our experiments as a compromise between training time and accuracy . sub ##section : qu ##ali ##tative analysis we provide qu ##ali ##tative results on some videos of human ##3 . 6 m and youtube . we apply the model trained on the human ##3 . 6 m data ##set on some videos gathered from youtube , the bound ##ing box for each person in the youtube video is labeled manually and for human ##3 . 6 m the ground truth bound ##ing box is used . the 2d poses are detected using the stacked - hour ##glass model fine - tuned on human ##3 . 6 m data . the qu ##ali ##tative result for youtube videos is shown in figure [ reference ] and for human ##3 . 6 m in figure [ reference ] . the real advantage of using the temporal smooth ##ness constraint during training is apparent in these figures . for figure [ reference ] , we can see that even when the 2d pose est ##ima ##tor breaks or generates extremely noisy detection ##s , our system can recover temporal ##ly coherent 3d poses by exploit ##ing the temporal consistency information . a similar trend can also be found for human ##3 . 6 m videos in figure [ reference ] , particularly for the action sitting down of subject 11 . we have provided more qu ##ali ##tative results in the supplementary material . section : conclusion both the quantitative and qu ##ali ##tative results for our network show the effectiveness of exploit ##ing temporal information over multiple sequences to estimate 3d poses which are temporal ##ly smooth . our network achieved the best accuracy till date on all of the 15 action classes in the human ##3 . 6 m data ##set . particularly , most of the previous methods struggled with actions which have a high degree of o ##cc ##lusion like taking photo , talking on the phone , sitting and sitting down . our network has significantly better results on these actions . additionally we found that our network is reasonably robust to noisy 2d poses . although the contribution of temporal smooth ##ness constraint is not apparent in the ab ##lative analysis in table [ reference ] , its effectiveness is clearly visible in the qu ##ali ##tative results , particularly on challenging youtube videos ( see figure [ reference ] ) . our network effectively demonstrates the power of using temporal context information which we achieved using a sequence - to - sequence network that can be trained efficiently in a reasonably quick time . also our network makes predictions from 2d poses at 3 ##ms per frame on average which suggests that , given the 2d pose detector is real time , our network can be applied in real - time scenarios . bibliography : references",
        "pred_seq": "human ##set [SEP] [SEP] [SEP] 3d estimation [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "human3 6 m dataset"
                    ]
                ],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "3d human pose estimation"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "human36 m dataset",
                        "human 36 m dataset",
                        "human 36 m",
                        "human36 m"
                    ]
                ],
                "Method": [
                    [
                        "sequencetosequence network",
                        "sequencetosequence networks"
                    ]
                ],
                "Metric": [
                    [
                        "average error"
                    ]
                ],
                "Task": [
                    [
                        "3d human pose estimation",
                        "estimating 3d human pose",
                        "3d poses",
                        "2d human pose estimation",
                        "2d pose estimation",
                        "2d pose estimations"
                    ]
                ]
            }
        ]
    },
    "54": {
        "doctext": "pro ##ba ##bilis ##tic model - ag ##nostic meta - learning section : abstract meta - learning for few - shot learning en ##tails acquiring a prior over previous tasks and experiences , such that new tasks be learned from small amounts of data . however , a critical challenge in few - shot learning is task ambiguity : even when a powerful prior can be meta - learned from a large number of prior tasks , a small data ##set for a new task can simply be too ambiguous to acquire a single model ( e . g . , a class ##ifier ) for that task that is accurate . in this paper , we propose a pro ##ba ##bilis ##tic meta - learning algorithm that can sample models for a new task from a model distribution . our approach extends model - ag ##nostic meta - learning , which adapt ##s to new tasks via gradient descent , to incorporate a parameter distribution that is trained via a variation ##al lower bound . at meta - test time , our algorithm adapt ##s via a simple procedure that in ##ject ##s noise into gradient descent , and at meta - training time , the model is trained such that this st ##och ##astic adaptation procedure produces samples from the approximate model posterior . our experimental results show that our method can sample plausible class ##ifiers and reg ##ress ##ors in ambiguous few - shot learning problems . section : introduction learning from a few examples is a key aspect of human intelligence . one way to make it possible to acquire solutions to complex tasks from only a few examples is to leverage past experience to learn a prior over tasks . the process of learning this prior en ##tails discovering the shared structure across different tasks from the same family , such as commonly occurring visual features or semantic cues . structure is useful ins ##of ##ar as it yields efficient learning of new tasks - a mechanism known as learning - to - learn , or meta - learning [ reference ] . however , when the end goal of few - shot meta - learning is to learn solutions to new tasks from small amounts of data , a critical issue that must be dealt with is task ambiguity : even with the best possible prior , there might simply not be enough information in the examples for a new task to resolve that task with high certainty . it is therefore quite desire ##able to develop few - shot meta - learning methods that can propose multiple potential solutions to an ambiguous few - shot learning problem . such a method could be used to evaluate uncertainty ( by measuring agreement between the samples ) , perform active learning , or eli ##cit direct human supervision about which sample is prefer ##able . for example , in safety - critical applications , such as few - shot medical image classification , uncertainty is crucial for determining if the learned class ##ifier should be trusted . when learning from such small amounts of data , uncertainty estimation can also help predict if additional data would be beneficial for learning and improving the estimate of the rewards . finally , while we do not experiment with this in this paper , we expect that modeling this ambiguity will be helpful for reinforcement learning problems , where it can be used to aid in exploration . while recognizing and accounting for ambiguity is an important aspect of the few - shot learning problem , it is particularly challenging to model when scaling to high - dimensional data , large function approx ##ima ##tors , and multi ##mo ##dal task structure . representing distributions over functions is relatively straightforward when using simple function approx ##ima ##tors , such as linear functions , and has been done extensively in early few - shot learning approaches using bay ##esian models [ reference ] [ reference ] . but this problem becomes substantially more challenging when reasoning over high - dimensional function approx ##ima ##tors such as deep neural networks , since explicitly representing expressive distributions over thousands or millions of parameters if often intra ##ctable . as a result , recent more scala ##ble approaches to few - shot learning have focused on acquiring deter ##mini ##stic learning algorithms that disregard ambiguity over the underlying function . can we develop an approach that has the benefits of both classes of few - shot learning methods - scala ##bility and uncertainty awareness ? to do so , we build upon tools in amor ##tized variation ##al inference for developing a pro ##ba ##bilis ##tic meta - learning approach . in particular , our method builds on model - ag ##nostic meta - learning ( ma ##ml ) [ reference ] , a few shot metal ##ear ##ning algorithm that uses standard gradient descent to adapt the model at meta - test time to a new few - shot task , and trains the model parameters at meta - training time to enable rapid adaptation , essentially opt ##imi ##zing for a neural network initial ##ization that is well - suited for few shot learning . ma ##ml can be shown to retain the general ##ity of black - box meta - learners such as rn ##ns [ reference ] , while being applicable to standard neural network architecture ##s . our approach extends ma ##ml to model a distribution over prior model parameters , which leads to an appealing simple st ##och ##astic adaptation procedure that simply in ##ject ##s noise into gradient descent at meta - test time . the meta - training procedure then opt ##imi ##zes for this simple inference process to produce samples from an approximate model posterior . the primary contribution of this paper is a ref ##ram ##ing of ma ##ml as a graphical model inference problem , where variation ##al inference can provide us with a principle ##d and natural mechanism for modeling uncertainty and ambiguity . our approach enables sampling multiple potential solutions to a few - shot learning problem at meta - test time , and our experiments show that this ability can be utilized to sample multiple possible reg ##ress ##ors for an ambiguous regression problem , as well as multiple possible class ##ifiers for ambiguous few - shot attribute classification tasks . section : related work hierarchical bay ##esian models are a long - standing approach for few - shot learning that naturally allow for the ability to reason about uncertainty over functions [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . while these approaches have been demonstrated on simple few - shot image classification data ##set ##s [ reference ] , they have yet to scale to the more complex problems , such as the experiments in this paper . a number of works have approached the problem of few - shot learning from a meta - learning perspective [ reference ] [ reference ] , including black - box [ reference ] [ reference ] [ reference ] and optimization - based approaches [ reference ] [ reference ] . while these approaches scale to large - scale image data ##set ##s [ reference ] and visual reinforcement learning problems [ reference ] , they typically lack the ability to reason about uncertainty . our work is most related to methods that combine deep networks and pro ##ba ##bilis ##tic methods for few - shot learning [ reference ] [ reference ] [ reference ] . one approach that considers hierarchical bay ##esian models for few - shot learning is the neural stat ##istic ##ian [ reference ] , which uses an explicit task variable to model task distributions . our method is fully model ag ##nostic , and directly samples model weights for each task for any network architecture . our experiments show that our approach improves on ma ##ml [ reference ] , which out ##per ##forms the model by edwards and st ##or ##key [ reference ] . other work that considers model uncertainty in the few - shot learning setting is the ll ##ama method [ reference ] , which also builds on the ma ##ml algorithm . ll ##ama makes use of a local lap ##lace approximation for modeling the task parameters ( post - update parameters ) , which introduces the need to approximate a high dimensional co ##var ##iance matrix . we instead propose a method that approximately in ##fers the pre - update parameters , which we make tract ##able through a choice of approximate posterior parameter ##ized by gradient operations . bay ##esian neural networks [ reference ] [ reference ] [ reference ] [ reference ] have been studied extensively as a way to incorporate uncertainty into deep networks . although exact inference in bay ##esian neural networks is imp ##rac ##tical , approximation ##s based on back ##pro ##pa ##gation and sampling [ reference ] [ reference ] [ reference ] [ reference ] have been effective in incorporating uncertainty into the weights of generic networks . our approach differs from these methods in that we explicitly train a hierarchical bay ##esian model over weights , where a posterior task - specific parameter distribution is in ##fer ##red at meta - test time conditioned on a learned weight prior and a ( few - shot ) training set , while conventional bay ##esian neural networks directly learn only the posterior weight distribution for a single task . our method draws on amor ##tized variation ##al inference methods [ reference ] [ reference ] [ reference ] to make this possible , but the key modification is that the model and inference networks share the same parameters . the resulting method corresponds structurally to a bay ##esian version of model - ag ##nostic meta - learning [ reference ] . into the center model after performing inference over ##\u03c6 ##i . we find it beneficial to introduce additional depend ##encies of the prior on the training data to compensate for using the map estimate to approximate p ( \u03c6 ##i ) , as shown on the right . section : pre ##lim ##ina ##ries in the meta - learning problem setting that we consider , the goal is to learn models that can learn new tasks from small amounts of data . to do so , meta - learning algorithms require a set of meta - training and meta - testing tasks drawn from some distribution p ( t ) . the key assumption of learning - to - learn is that the tasks in this distribution share common structure that can be exploited for faster learning of new tasks . thus , the goal of the meta - learning process is to discover that structure . in this section , we will introduce notation and overview the model - ag ##nostic meta - learning ( ma ##ml ) algorithm [ reference ] . meta - learning algorithms proceed by sampling data from a given task , and splitting the sampled data into a set of a few data ##points , d where ##\u03c6 i is used to denote the parameters updated by gradient descent and where the loss corresponds to negative log likelihood of the data . in particular , in the case of supervised classification with inputs { x j } , their corresponding labels { y j } , and a class ##ifier f ##\u03b8 , we will denote the negative log likelihood of the data under the class ##ifier as l ( \u03b8 , d ) = \u2212 ( x ##j , y ##j ) \u2208 ##d log p ( y j | x j , \u03b8 ) . this corresponds to the cross entropy loss function . section : method our goal is to build a meta - learning method that can handle the uncertainty and ambiguity that occurs when learning from small amounts of data , while scaling to highly - expressive function approx ##ima ##tors such as neural networks . to do so , we set up a graphical model for the few - shot learning problem . in particular , we want a hierarchical bay ##esian model that includes random variables for the prior distribution over function parameters , \u03b8 , the distribution over parameters for a particular task , \u03c6 i , and the task training and test data ##points . this graphical model is illustrated in figure 1 ( left ) , where tasks are indexed over i and data ##points are indexed over j . we will use the short ##hand x . therefore , posterior inference over ##\u03c6 i must take into account both the evidence ( training set ) and the prior imposed by p ( \u03b8 ) and p ( \u03c6 i | \u03b8 ) . conventional ma ##ml can be interpreted as approx ##imating maximum a posterior ##i inference under a simplified model where p ( \u03b8 ) is a delta function , and inference is performed by running gradient descent on log p ( y tr | x tr , \u03c6 i ) for a fixed number of iteration ##s starting from ##\u03c6 [ reference ] . the corresponding distribution p ( \u03c6 i | \u03b8 ) is approximately ga ##uss ##ian , with a mean that depends on the step size and number of gradient steps . when p ( \u03b8 ) is not deter ##mini ##stic , we must make a further approximation to account for the random variable ##\u03b8 . one way we can do this is by using structured variation ##al inference . in structured variation ##al inference , we approximate the distribution over the hidden variables ##\u03b8 and ##\u03c6 i for each task with some approximate distribution q i ( \u03b8 , \u03c6 i ) . there are two reasonable choices we can make for q i ( \u03b8 , \u03c6 i ) . first , we can approximate it as a product of independent marginal ##s , according to however , this approximation does not permit uncertainty to prop ##aga ##te effectively from ##\u03b8 to ##\u03c6 i . a more expressive approximation is the structured variation ##al approximation we can further avoid storing a separate variation ##al distribution q i ( \u03c6 i | \u03b8 ) and q i ( \u03b8 ) for each task t i by employing an amor ##tized variation ##al inference technique [ reference ] [ reference ] [ reference ] , where we instead set , where q ##\u03c8 is defined by some function approx ##ima ##tor with parameters ##\u03c8 that takes x tr i , y tr i as input , and the same q ##\u03c8 is used for all tasks . similarly , we can define we can now write down the variation ##al lower bound on the log - likelihood as the likelihood terms on the first line can be evaluated efficiently : given a sample ##\u03b8 , \u03c6 [UNK] q ( \u03b8 , \u03c6 i | x , the training and test likelihood ##s simply correspond to the loss of the network with parameters ##\u03c6 i . the prior p ( \u03b8 ) can be chosen to be ga ##uss ##ian , with a learned mean and ( diagonal ) co ##var ##iance to provide for flexibility to choose the prior parameters . this corresponds to a bay ##esian version of the ma ##ml algorithm . we will define these parameters as ##\u00b5 ##\u03b8 and ##\u03c3 2 ##\u03b8 . lastly , p ( \u03c6 i | \u03b8 ) must be chosen . this choice is more delicate . one way to ensure a tract ##able likelihood is to use a ga ##uss ##ian with mean ##\u03b8 . this choice is reasonable , because it encourages ##\u03c6 i to stay close to the prior parameters ##\u03c6 i , but we will see in the next section how a more expressive implicit conditional can be obtained using gradient descent , resulting in a procedure that more closely resembles the original ma ##ml algorithm while still modeling the uncertainty . lastly , we must choose a form for the inference networks q ##\u03c8 ( \u03c6 i | \u03b8 , they must be chosen so that their en ##tro ##pies on the second line of the above equation are tract ##able . furthermore , note that both of these distributions model very high - dimensional random variables : a deep neural network can have hundreds of thousands or millions of parameters . so while we can use an arbitrary function approx ##ima ##tor , we would like to find a scala ##ble solution . one convenient solution is to allow q ##\u03c8 to re ##use the learned mean of the prior ##\u00b5 ##\u03b8 . we observe that adapting the parameters with gradient descent is a good way to update them to a given training set x where v q is a learned ( diagonal ) co ##var ##iance , and the mean has an additional parameter beyond ##\u00b5 ##\u03b8 , which is a \" learning rate \" vector ##\u03b3 q that is point ##wise multiplied with the gradient . while this choice may at first seem arbitrary , there is a simple intuition : the inference network should produce a sample of ##\u03b8 that is close to the posterior p ( \u03b8 | x a reasonable way to arrive at a value of ##\u03b8 close to this posterior is to adapt it to both the training set and test set . [ reference ] note that this is only done during meta - training . it remains to choose q ##\u03c8 ( \u03c6 i | \u03b8 , , which can also be formulated as a conditional ga ##uss ##ian with mean given by applying gradient descent . although this variation ##al distribution is substantially more compact in terms of parameters than a separate neural network , it only provides estimates of the posterior during meta - training . at meta - test time , we must obtain the posterior p ( \u03c6 i | x . we can train a separate set of inference networks to perform this operation , potentially also using gradient descent within the inference network . however , these networks do not receive any gradient information during meta - training , and may not work well in practice . in the next section we propose an even simpler and more practical approach that uses only a single inference network during meta - training , and none during meta - testing . section : algorithm 1 meta - training , differences from ma ##ml in red require : p ( t ) : distribution over tasks 1 : initial ##ize ##\u03b8 : = { \u00b5 ##\u03b8 , \u03c3 2 ##\u03b8 , v ##q , \u03b3 ##p , \u03b3 ##q } 2 : while not done do 3 : sample batch of tasks [UNK] p ( t ) 4 : for all ti do 5 : evaluate compute adapted parameters with gradient descent : compute ##\u2207 ##\u03b8 update ##\u03b8 using adam section : algorithm 2 meta - testing section : pro ##ba ##bilis ##tic model - ag ##nostic meta - learning approach with hybrid inference to formula ##te a simpler variation ##al meta - learning procedure , we recall the pro ##ba ##bilis ##tic interpretation of ma ##ml : as discussed by grant et al . [ reference ] , ma ##ml can be interpreted as approximate inference for the posterior p ( y where we use the maximum a posterior ##i ( map ) value ##\u03c6 i . it can be shown that , for likelihood ##s that are ga ##uss ##ian in ##\u03c6 i , gradient descent for a fixed number of iteration ##s using x tr i , y tr i corresponds exactly to maximum a posterior ##i inference under a ga ##uss ##ian prior p ( \u03c6 i | \u03b8 ) [ reference ] . in the case of non - ga ##uss ##ian likelihood ##s , the equivalence is only locally approximate , and the exact form of the prior p ( \u03c6 i | \u03b8 ) is intra ##ctable . however , in practice this implicit prior can actually be prefer ##able to an explicit ( and simple ) ga ##uss ##ian prior , since it incorporates the rich nonlinear structure of the neural network parameter manifold , and produces good performance in practice [ reference ] [ reference ] . we can interpret this map approximation as in ##fer ##ring an approximate posterior on ##\u03c6 i of the form p ( \u03c6 i | x where ##\u03c6 i is obtained via gradient descent on the training set x tr i , y tr i starting from ##\u03b8 . incorporating this approximate inference procedure transforms the graphical model in figure 1 ( a ) into the one in figure 1 ( b ) , where there is now a factor over p ( \u03c6 i | x tr i , y tr i , \u03b8 ) . while this is a crude approximation to the likelihood , it provides us with an empirical ##ly effective and simple tool that greatly sim ##pl ##ifies the variation ##al inference procedure described in the previous section , in the case where we aim to model a distribution over the global parameters p ( \u03b8 ) . after using gradient descent to estimate p ( \u03c6 i | x is not observed . thus , we can now write down a variation ##al lower bound for the log ##ari ##th ##m of the approximate likelihood on the second line , which is given by in this bound , we essentially perform approximate inference via map on ##\u03c6 i to obtain p ( \u03c6 i | x to evaluate the variation ##al lower bound during training , we can use the following procedure : first , we evaluate the mean by starting from ##\u00b5 ##\u03b8 and taking one ( or more ) gradient steps on log p ( y test i | x test i , \u03b8 current ) , where ##\u03b8 current starts at ##\u00b5 ##\u03b8 . we then add noise with variance v q , which is made different ##iable via the rep ##ara ##meter ##ization trick [ reference ] . we then take additional gradient steps on the training likelihood log p ( y tr i | x tr i , \u03b8 current ) . this accounts for the map inference procedure on ##\u03c6 i . training of ##\u00b5 ##\u03b8 , \u03c3 2 ##\u03b8 , and v q is performed by back ##pro ##pa ##gating gradient ##s through this entire procedure with respect to the variation ##al lower bound , which includes a term for the likelihood tr , y tr , \u03c6 i ) and the k ##l - diver ##gence between the [UNK] q ##\u03c8 and the prior p ( \u03b8 ) . this meta - training procedure is detailed in algorithm 1 . at meta - test time , the inference procedure is much simpler . the test labels are not available , so we simply [UNK] p ( \u03b8 ) and perform map inference on ##\u03c6 i using the training set , which corresponds to gradient steps on log p ( y tr i | x tr i , \u03b8 current ) , where ##\u03b8 current starts at the sampled ##\u03b8 . this meta - testing procedure is detailed in algorithm 2 . section : adding additional depend ##encies in the transformed graphical model , the training data x tr i , y tr i and the prior ##\u03b8 are conditional ##ly independent . however , since we have only a crude approximation to p ( \u03c6 i | x tr i , y tr i , \u03b8 ) , this independence often does n ' t actually hold . we can allow the model to compensate for this approximation by additionally conditioning the learned prior p ( \u03b8 ) on the training data . in this case , the learned \" prior \" has the form p ( \u03b8 i | x tr i , y tr i ) , where ##\u03b8 i is now task - specific , but with global parameters ##\u00b5 ##\u03b8 and ##\u03c3 2 ##\u03b8 . we thus obtain the modified graphical model in figure 1 ( c ) . similarly to the inference network q ##\u03c8 , we parameter ##ize the learned prior as follows : with this new form for distribution over ##\u03b8 , the variation ##al training objective uses the likelihood term log p ( \u03b8 i | x in our experiments , we find that this more expressive distribution often leads to better performance . section : experiments the goal of our experimental evaluation is to answer the following questions : ( 1 ) can our approach enable sampling from the distribution over potential functions underlying the training data ? , ( 2 ) does our approach improve upon the ma ##ml algorithm when there is ambiguity over the class of functions ? , and ( 3 ) can our approach scale to deep con ##vo ##lu ##tion ##al networks ? we study two ill ##ust ##rative toy examples and a realistic ambiguous few - shot image classification problem . for the both experimental domains , we compare ma ##ml to our pro ##ba ##bilis ##tic approach . we will refer to our version of ma ##ml as a pl ##ati ##pus ( pro ##ba ##bilis ##tic late ##nt model for incorporating prior ##s and uncertainty in few - shot learning ) , due to its unusual combination of two approximate inference methods : amor ##tized inference and map . both pl ##ati ##pus and ma ##ml use the same neural network architecture and the same number of inner gradient steps . we additionally provide a comparison on the mini ##ima ##gen ##et bench ##mark and specify the hyper ##para ##meter ##s in the supplementary appendix . , and ga ##uss ##ian noise with a standard deviation of 0 . 3 is added to the labels . we trained both ma ##ml and pl ##ati ##pus for 5 - shot regression . in figure 2 , we show the qu ##ali ##tative performance of both methods , where the ground truth underlying function is shown in gray and the data ##points in d tr are shown as purple triangles . we show the function f ##\u03c6 ##i learned by ma ##ml in black . for pl ##ati ##pus , we sample 10 sets of parameters from p ( \u03c6 i | \u03b8 ) and plot the resulting functions in different colors . in the top row , we can see that pl ##ati ##pus allows the model to effectively reason over the set of functions underlying the provided data ##points , with increased variance in parts of the function where there is more uncertainty . further , we see that pl ##ati ##pus is able to capture the multi ##mo ##dal structure , as the curves are all linear or sin ##uso ##idal . a particularly useful application of uncertainty estimates in few - shot learning is est ##imating when more data would be helpful . in particular , seeing a large variance in a particular part of the input space suggests that more data would be helpful for learning the function in that part of the input space . on the bottom of figure 2 , we show the results for a single task at meta - test time with increasing numbers of training data ##points . even though the model was only trained on training set sizes of 5 data ##points , we observe that pl ##ati ##pus is able to effectively reduce its uncertainty as more and more data ##points are available . this suggests that the uncertainty provided by pl ##ati ##pus can be used for approximately ga ##ug ##ing when more data would be helpful for learning a new task . consisting of both positive and negative examples . we plot the results using the same scheme as before , except that we plot the decision boundary ( rather than the regression function ) and visual ##ize the single positive data ##point with a green plus . as seen in figure 3 , we see that pl ##ati ##pus captures a broad distribution over possible decision boundaries , all of which are roughly circular . ma ##ml provides a single decision boundary of average size . ambiguous image classification . the ambiguity illustrated in the previous settings is common in real world tasks where images can share multiple attributes . we study an ambiguous extension to the ce ##le ##ba attribute classification task . our meta - training data ##set is formed by sampling two attributes at random to form a positive class and taking the same number of random examples without either attribute to from the negative classes . to evaluate the ability to capture multiple decision boundaries while simultaneously obtaining good performance , we evaluate our method as follows : we sample from a test set of three attributes and a corresponding set of images with those attributes . since the tasks involve classify ##ing images that have two attributes , this task is ambiguous , and there are three possible combinations of two attributes that explain the training set . we sample models from our prior as described in section 4 and assign each of the sampled models to one of the three possible tasks based on its log - likelihood . if each of the three possible tasks is assigned a non ##zer ##o number of samples , this means that the model effectively covers all three possible modes that explain the ambiguous training set . we can measure coverage and accuracy from this protocol . the coverage score indicates the average number of tasks ( between 1 and 3 ) that receive at least one sample for each ambiguous training set , and the accuracy score is the average number of correct classifications on these tasks ( according to the sampled models assigned to them ) . a highly random method will achieve good coverage but poor accuracy , while a deter ##mini ##stic method will have a coverage of 1 . our results are summarized in table 5 and fig . 4 . the accuracy of our method is comparable to standard , deter ##mini ##stic ma ##ml . however , the deter ##mini ##stic algorithm only ever captures one mode observes five positive ##s that share three attributes , and five negative ##s . a class ##ifier that uses any two attributes can correctly classify the training set . on the right , we show each of the possible two - attribute tasks that this training set can correspond to , and illustrate the labels ( positive indicated by red border ) assigned by the best sample for that task . we see that the different samples are able to make reasonable predictions with no hats ( 2nd column ) or pay attention to them ( 1st and 3rd column ) , and can effectively capture the three possible explanations . for each ambiguous task , where the maximum is three . our method on average captures between two and three modes . the qu ##ali ##tative analysis in figure 4 illustrates 3 an example ambiguous training set , example images for the three possible two - attribute pairs that can correspond to this training set , and the classifications made by different sampled class ##ifiers trained on the ambiguous training set . note that the different samples each pay attention to different attributes , indicating that pl ##ati ##pus is effective at capturing the different modes of the task . section : discussion and future work we introduced an algorithm for few - shot meta - learning that enables simple and effective sampling of models for new tasks at meta - test time . our algorithm , pl ##ati ##pus , adapt ##s to new tasks by running gradient descent with injected noise . during meta - training , the model parameters are opt ##imi ##zed with respect to a variation ##al lower bound on the likelihood for the meta - training tasks , so as to enable this simple adaptation procedure to produce approximate samples from the model posterior when conditioned on a few - shot training set . this approach has a number of benefits . the adaptation procedure is exceeding ##ly simple , and the method can be applied to any standard model architecture . the algorithm introduces a modest number of additional parameters : besides the initial model weights , we must learn a variance on each parameter for the inference network and prior , and the number of parameters scales only linear ##ly with the number of model weights . our experimental results show that our method can be used to effectively sample diverse solutions to both regression and classification tasks at meta - test time , including for task families that have multi - mod ##al task distributions . although our approach is simple and broadly applicable , it has a number of potential limitations that could be addressed in future work . first , the current form of the method provides a relatively impoverished est ##ima ##tor of posterior variance , which might be less effective at ga ##ug ##ing uncertainty in settings where different tasks have very different degrees of ambiguity . in these cases , finding a way to make the variance dependent on the few - shot training set might produce better results , and investigating how to do this without adding a large number of additional parameters would be an interesting direction for future work . another exciting direction for future research would be to study how our approach could be applied in settings where ambiguity and uncertainty can directly guide data acquisition , so as to devi ##se better few - shot active learning and reinforcement learning algorithms . section : appendix a ambiguous ce ##le ##ba details to construct our ambiguous few - shot variant of ce ##le ##ba , we take the entire base set of attributes holding out 10 attributes for testing . we consider every combination of 2 attributes , disc ##arding those with insufficient numbers of examples . this leave us with a total of 38 ##7 training tasks and 43 testing attributes . we partition our meta - training set and meta - validation set to 337 / 50 respectively . during meta - training , we sample 2 random attributes to construct a positive class and randomly sample examples with neither attribute as negative examples . during testing of our approach , we sample 3 attributes from the test set , and sample the 3 corresponding 2 - up ##les to form the test task . the training attributes are : section : b experimental details in the ill ##ust ##rative experiments , we use a fully connected network with 3 re ##lu layers of size 100 . for ce ##le ##ba , we adapt the base con ##vo ##lu ##tion ##al architecture described in finn et al . [ reference ] which we refer the readers to for more detail . our approximate posterior and prior have dimensional ##ity matching the underlying model . we tune our approach over the inner learning rate ##\u03b1 , a weight on the d k ##l , the scale of the initial ##ization of ##\u00b5 ##\u03b8 , \u03c3 2 ##\u03b8 , v q , \u03b3 p , \u03b3 q , with early stopping on the validation set . at meta - test time , we evaluate our approach by taking 10 samples from the prior before determining the assignments . the assignments are made based on the complete likelihood of the testing examples ( including the negative ##s ) . section : c mini ##ima ##gen ##et comparison we provide an additional comparison on the mini ##ima ##gen ##et data ##set . since this bench ##mark does not contain a large amount of ambiguity , we do not aim to show state - of - the - art performance . instead , our goal with this experiment is to compare our approach on to ma ##ml and prior methods that build upon ma ##ml on this standard bench ##mark . since our goal is to compare algorithms , rather than achieving maximal performance , we deco ##up ##le the effect of the meta - learning algorithm and the architecture used by using the standard 4 - block con ##vo ##lu ##tion ##al architecture used by vin ##yal ##s et al . [ reference ] , ravi and la ##ro ##chel ##le [ reference ] , finn et al . [ reference ] and others . we note that better performance can likely be achieved by tuning the architecture . the results , in table 2 indicate that our method slightly out ##per ##forms ma ##ml and achieve ##s comparable performance to a number of other prior methods . mini ##ima ##gen ##et 5 - way , 1 - shot accuracy ma ##ml [ reference ] 48 . 70 ##\u00b1 1 . 84 % ll ##ama [ reference ] 49 . 40 ##\u00b1 1 . 83 % rep ##tile [ reference ] 49 . 97 ##\u00b1 0 . 32 % pl ##ati ##pus ( ours ) 50 . 13 ##\u00b1 1 . 86 % meta - sg ##d [ reference ] 50 . 71 ##\u00b1 1 . 87 % matching nets [ reference ] 43 . 56 ##\u00b1 0 . 84 % meta - learn ##er l ##st ##m [ reference ] 43 . 44 ##\u00b1 0 . 77 % snail [ reference ] * 45 . 10 ##\u00b1 0 . 00 % proto ##typical networks [ reference ] 46 . 61 ##\u00b1 0 . 78 % map - dl ##m [ reference ] 49 . 82 ##\u00b1 0 . 78 % g ##nn [ reference ] 50 . 33 ##\u00b1 0 . 36 % relation net [ reference ] 50 . 44 ##\u00b1 0 . 82 % table 2 : comparison between our approach and prior ma ##ml - based methods ( top ) , and other prior few - shot learning techniques on the 5 - way , 1 - shot mini ##ima ##gen ##et bench ##mark . our approach gives a small boost over ma ##ml , and is comparable to other approaches . we bold the approaches that are above the highest confidence interval lower - bound . * accuracy using comparable network architecture . section : section : ac ##k ##now ##led ##gm ##ents we thank marvin zhang and di ##by ##a g ##hosh for comments on an earlier draft of this paper . this research was supported by an ns ##f graduate research fellowship , ns ##f ii ##s - 1651 ##8 ##43 , the office of naval research , and n ##vid ##ia . section :",
        "pred_seq": "[SEP] pro learning [SEP] [SEP] few learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning learning [CLS]",
        "pred_templates": [],
        "gold_templates": [
            {
                "Material": [
                    [
                        "miniimagenet benchmark",
                        "miniimagenet dataset",
                        "miniimagenet 5way",
                        "5way 1shot miniimagenet benchmark"
                    ]
                ],
                "Method": [
                    [
                        "platipus",
                        "probabilistic latent model for incorporating priors and uncertainty in fewshot learning"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy"
                    ]
                ],
                "Task": [
                    [
                        "fewshot medical image classification",
                        "ambiguous fewshot attribute classification tasks",
                        "fewshot image classification datasets",
                        "ambiguous fewshot image classification problem"
                    ]
                ]
            }
        ]
    },
    "55": {
        "doctext": "document : pixel ##2 ##mes ##h : generating 3d mesh models from single r ##gb images we propose an end - to - end deep learning architecture that produces a 3d shape in triangular mesh from a single color image . limited by the nature of deep neural network , previous methods usually represent a 3d shape in volume or point cloud , and it is non - trivial to convert them to the more ready - to - use mesh model . unlike the existing methods , our network represents 3d mesh in a graph - based con ##vo ##lu ##tion ##al neural network and produces correct geometry by progressively def ##or ##ming an el ##lip ##so ##id , lever ##aging per ##ce ##pt ##ual features extracted from the input image . we adopt a coarse - to - fine strategy to make the whole deformation procedure stable , and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3d geometry . extensive experiments show that our method not only qu ##ali ##tative ##ly produces mesh model with better details , but also achieve ##s higher 3d shape estimation accuracy compared to the state - of - the - art . indicates equal contributions . indicates corresponding author . section : introduction in ##fer ##ring 3d shape from a single perspective is a fundamental human vision functionality but is extremely challenging for computer vision . recently , great success has been achieved for 3d shape generation from a single color image using deep learning techniques . taking advantage of con ##vo ##lu ##tion ##al layers on regular grid ##s or multi - layer perception , the estimated 3d shape , as the output of the neural network , is represented as either a volume or point cloud . however , both representations lose important surface details , and is non - trivial to rec ##ons ##truct a surface model ( fig . [ reference ] ) , i . e . a mesh , which is more desirable for many real applications since it is lightweight , capable of modelling shape details , easy to def ##or ##m for animation , to name a few . in this paper , we push along the direction of single image reconstruction , and propose an algorithm to extract a 3d triangular mesh from a single color image . rather than directly synth ##es ##izing , our model learns to def ##or ##m a mesh from a mean shape to the target geometry . this benefits us from several aspects . first , deep network is better at predicting residual , e . g . a spatial deformation , rather than structured output , e . g . a graph . second , a series of deformation ##s can be added up together , which allows shape to be gradually refined in detail . it also enables the control of the trade - off between the complexity of the deep learning model and the quality of the result . lastly , it provides the chance to en ##code any prior knowledge to the initial mesh , e . g . topology . as a pioneer study , in this work , we specifically work on objects that can be approximate ##d using 3d mesh with genus 0 by def ##or ##ming an el ##lip ##so ##id with a fixed size . in practice , we found most of the commonly seen categories can be handled well under this setting , e . g . car , plane , table , etc . to achieve this goal , there are several inherent challenges . the first challenge is how to represent a mesh model , which is essentially an irregular graph , in a neural network and still be capable of extract ##ing shape details effectively from a given color image represented in a 2d regular grid . it requires the integration of the knowledge learned from two data mod ##ali ##ties . on the 3d geometry side , we directly build a graph based fully con ##vo ##lu ##tion ##al network ( g ##c ##n ) on the mesh model , where the vertices and edges in the mesh are directly represented as nodes and connections in a graph . network feature encoding information for 3d shape is saved on each vertex . through forward propagation , the con ##vo ##lu ##tion ##al layers enable feature exchanging across neighboring nodes , and eventually reg ##ress the 3d location for each vertex . on the 2d image side , we use a v ##gg - 16 like architecture to extract features as it has been demonstrated to be successful for many tasks . to bridge these two , we design a per ##ce ##pt ##ual feature pool ##ing layer which allows each node in the g ##c ##n to pool image features from its 2d projection on the image , which can be readily obtained by assuming known camera intrinsic matrix . the per ##ce ##pt ##ual feature pool ##ing is enabled once after several con ##vo ##lu ##tions ( i . e . a deformation block described in sec . [ reference ] ) using updated 3d locations , and hence the image features from correct locations can be effectively integrated with 3d shapes . given the graph representation , the next challenge is how to update the vertex location effectively towards ground truth . in practice , we observe that network trained to directly predict mesh with a large number of vertices is likely to make mistake in the beginning and hard to fix later . one reason is that a vertex can not effectively retrieve features from other vertices with a number of edges away , i . e . the limited rec ##eptive field . to solve this problem , we design a graph un ##pool ##ing layer , which allows the network to initiate with a smaller number of vertices and increase during the forward propagation . with fewer vertices at the beginning stages , the network learns to distribute the vertices around to the most representative location , and then add local details as the number of vertices increases later . besides the graph un ##pool ##ing layer , we use a deep g ##c ##n enhanced by short ##cut connections as the backbone of our architecture , which enables large rec ##eptive fields for global context and more steps of movements . representing the shape in graph also benefits the learning procedure . the known connectivity allows us to define higher order loss functions across neighboring nodes , which are important to regular ##ize 3d shapes . specifically , we define a surface normal loss to favor smooth surface ; an edge loss to encourage uniform distribution of mesh vertices for high recall ; and a lap ##la ##cian loss to prevent mesh faces from intersecting each other . all of these losses are essential to generate quality appealing mesh model , and none of them can be trivial ##ly defined without the graph representation . the contributions of this paper are mainly in three aspects . first , we propose a novel end - to - end neural network architecture that generates a 3d mesh model from a single r ##gb image . second , we design a projection layer which incorporates per ##ce ##pt ##ual image features into the 3d geometry represented by g ##c ##n . third , our network predict 3d geometry in a coarse to fine fashion , which is more reliable and easy to learn . section : related work 3d reconstruction has been well studied based on the multi - view geometry ( mv ##g ) in the literature . the major research directions include structure from motion ( sf ##m ) for large - scale high - quality reconstruction and simultaneous local ##ization and mapping ( slam ) for navigation . though they are very successful in these scenarios , they are restricted by 1 ) the coverage that the multiple views can give and 2 ) the appearance of the object that wants to rec ##ons ##truct . the former restriction means mv ##g can not rec ##ons ##truct unseen parts of the object , and thus it usually takes a long time to get enough views for a good reconstruction ; the latter restriction means mv ##g can not rec ##ons ##truct non - lambert ##ian ( e . g . reflective or transparent ) or texture ##less objects . these restrictions lead to the trend of resort ##ing to learning based approaches . learning based approaches usually consider single or few images , as it largely relies on the shape prior ##s that it can learn from data . early works can be traced back to ho ##ie ##m et al . and saxe ##na et al . . most recently , with the success of deep learning architecture ##s and the release of large - scale 3d shape data ##set ##s such as shape ##net , learning based approaches have achieved great progress . huang et al . and su et al . retrieve shape components from a large data ##set , assemble them and def ##or ##m the assembled shape to fit the observed image . however , shape retrieval from images itself is an ill - posed problem . to avoid this problem , ka ##r et al . learns a 3d def ##or ##mable model for each object category and capture the shape variations in different images . however , the reconstruction is limited to the popular categories and its reconstruction result is usually lack of details . another line of research is to directly learn 3d shapes from single images . restricted by the prevalent grid - based deep learning architecture ##s , most works outputs 3d vox ##els , which are usually with low resolutions due to the memory constraint on a modern gp ##u . most recently , tatar ##chenko et al . have proposed an oct ##ree representation , which allows to rec ##ons ##tructing higher resolution outputs with a limited memory budget . however , a 3d vox ##el is still not a popular shape representation in game and movie industries . to avoid draw ##backs of the vox ##el representation , fan et al . propose to generate point clouds from single images . the point cloud representation has no local connections between points , and thus the point positions have a very large degree of freedom . consequently , the generated point cloud is usually not close to a surface and can not be used to recover a 3d mesh directly . besides these typical 3d representations , there is an interesting work which uses a so - called \" geometry image \" to represent a 3d shape . thus , their network is a 2d con ##vo ##lu ##tion ##al neural network which conducts an image to image mapping . our works are mostly related to the two recent works and . however , the former adopt ##s simple silhouette supervision , and hence does not perform well for complicated objects such as car , lamp , etc ; the latter needs a large model repository to generate a combined model . our base network is a graph neural network ; this architecture has been adopted for shape analysis . in the meanwhile , there are charting - based methods which directly apply con ##vo ##lu ##tions on surface manifold ##s for shape analysis . as far as we know , these architecture ##s have never been adopted for 3d reconstruction from single images , though graph and surface manifold are natural representations for mesh ##ed objects . for a comprehensive understanding of the graph neural network , the charting - based methods and their applications , please refer to this survey . section : method sub ##section : preliminary : graph - based con ##vo ##lu ##tion we first provide some background about graph based con ##vo ##lu ##tion ; more detailed introduction can be found in . a 3d mesh is a collection of vertices , edges and faces that defines the shape of a 3d object ; it can be represented by a graph , where is the set of vertices in the mesh , is the set of edges with each connecting two vertices , and are the feature vectors attached on vertices . a graph based con ##vo ##lu ##tion ##al layer is defined on irregular graph as : where are the feature vectors on vertex before and after the con ##vo ##lu ##tion , and is the neighboring vertices of ; and are the learn ##able parameter matrices of that are applied to all vertices . note that is shared for all edges , and thus ( [ reference ] ) works on nodes with different vertex degrees . in our case , the attached feature vector is the con ##cate ##nation of the 3d vertex coordinate , feature encoding 3d shape , and feature learned from the input color image ( if they exist ) . running con ##vo ##lu ##tions updates the features , which is equivalent as applying a deformation . sub ##section : system overview our model is an end - to - end deep learning framework that takes a single color image as input and produces a 3d mesh model in camera coordinate . the overview of our framework is illustrated in fig . [ reference ] . the whole network consists an image feature network and a cascade ##d mesh deformation network . the image feature network is a 2d cnn that extract per ##ce ##pt ##ual feature from the input image , which is leverage ##d by the mesh deformation network to progressively def ##or ##m an el ##lip ##so ##id mesh into the desired 3d model . the cascade ##d mesh deformation network is a graph - based con ##vo ##lu ##tion network ( g ##c ##n ) , which contains three deformation blocks intersect ##ed by two graph un ##pool ##ing layers . each deformation block takes an input graph representing the current mesh model with the 3d shape feature attached on vertices , and produces new vertices locations and features . whereas the graph un ##pool ##ing layers increase the number of vertices to increase the capacity of handling details , while still maintain the triangular mesh topology . starting from a smaller number of vertices , our model learns to gradually def ##or ##m and add details to the mesh model in a coarse - to - fine fashion . in order to train the network to produce stable deformation and generate an accurate mesh , we extend the cha ##m ##fer distance loss used by fan et al . with three other mesh specific loss - surface normal loss , lap ##la ##cian regular ##ization loss , and edge length loss . the remaining part of this section describes details of these components . sub ##section : initial el ##lip ##so ##id our model does not require any prior knowledge of the 3d shape , and always def ##or ##m from an initial el ##lip ##so ##id with average size placed at the common location in the camera coordinate . the el ##lip ##so ##id is centered at 0 . 8 m in front of the camera with 0 . 2 m , 0 . 2 m , 0 . 4 m as the radius of three axis . the mesh model is generated by implicit surface algorithm in mesh ##lab and contains 156 vertices . we use this el ##lip ##so ##id to initial ##ize our input graph , where the initial feature contains only the 3d coordinate of each vertex . sub ##section : mesh deformation block the architecture of mesh deformation block is shown in fig . [ reference ] ( a ) . in order to generate 3d mesh model that is consistent with the object shown in the input image , the deformation block need to pool feature ( ) from the input image . this is done in conjunction with the image feature network and a per ##ce ##pt ##ual feature pool ##ing layer given the location of vertex ( ) in the current mesh model . the poole ##d per ##ce ##pt ##ual feature is then con ##cate ##nated with the 3d shape feature attached on the vertex from the input graph ( ) and fed into a series of graph based res ##net ( g - res ##net ) . the g - res ##net produces , also as the output of the mesh deformation block , the new coordinates ( ) and 3d shape feature ( ) for each vertex . sub ##su ##bs ##ection : per ##ce ##pt ##ual feature pool ##ing layer we use a v ##gg - 16 architecture up to layer con ##v ##5 _ 3 as the image feature network as it has been widely used . given the 3d coordinate of a vertex , we calculate its 2d projection on input image plane using camera intrinsic ##s , and then pool the feature from four nearby pixels using bi ##line ##ar inter ##pol ##ation . in particular , we con ##cate ##nate feature extracted from layer ' con ##v ##3 _ 3 ' , ' con ##v ##4 _ 3 ' , and ' con ##v ##5 _ 3 ' , which results in a total dimension of 128 ##0 . this per ##ce ##pt ##ual feature is then con ##cate ##nated with the 128 - dim 3d feature from the input mesh , which results in a total dimension of 140 ##8 . this is illustrated in fig . [ reference ] ( b ) . note that in the first block , the per ##ce ##pt ##ual feature is con ##cate ##nated with the 3 - dim feature ( coordinate ) since there is no learnt shape feature at the beginning . sub ##su ##bs ##ection : g - res ##net after obtaining 140 ##8 - dim feature for each vertex representing both 3d shape and 2d image information , we design a graph based con ##vo ##lu ##tion ##al neural network to predict new location and 3d shape feature for each vertex . this requires efficient exchange of the information between vertices . however , as defined in ( [ reference ] ) , each con ##vo ##lu ##tion only enables the feature exchanging between neighboring pixels , which severely imp ##air ##s the efficiency of information exchanging . this is equivalent as the small rec ##eptive field issue on 2d cnn . to solve this issue , we make a very deep network with short ##cut connections and denote it as g - res ##net ( fig . [ reference ] ( a ) ) . in this work , the g - res ##net in all blocks has the same structure , which consists of 14 graph residual con ##vo ##lu ##tion ##al layers with 128 channels . the serial of g - res ##net block produces a new 128 - dim 3d feature . in addition to the feature output , there is a branch which applies an extra graph con ##vo ##lu ##tion ##al layer to the last layer features and outputs the 3d coordinates of the vertex . sub ##section : graph un ##pool ##ing layer the goal of un ##pool ##ing layer is to increase the number of vertex in the g ##c ##nn . it allows us to start from a mesh with fewer vertices and add more only when necessary , which reduces memory costs and produces better results . a straightforward approach is to add one vertex in the center of each triangle and connect it with the three vertices of the triangle ( fig . [ reference ] ( b ) face - based ) . however , this causes im ##balance ##d vertex degrees , i . e . number of edges on vertex . inspired by the vertex adding strategy of the mesh subdivision algorithm prevalent in computer graphics , we add a vertex at the center of each edge and connect it with the two end - point of this edge ( fig . [ reference ] ( a ) ) . the 3d feature for newly added vertex is set as the average of its two neighbors . we also connect three vertices if they are added on the same triangle ( dashed line . ) consequently , we create 4 new triangles for each triangle in the original mesh , and the number of vertex is increased by the number of edges in the original mesh . this edge - based un ##pool ##ing uniformly ups ##amp ##les the vertices as shown in fig . [ reference ] ( b ) edge - based . sub ##section : losses we define four kinds of losses to con ##stra ##in the property of the output shape and the deformation procedure to guarantee appealing results . we adopt the cha ##m ##fer loss to con ##stra ##in the location of mesh vertices , a normal loss to enforce the consistency of surface normal , a lap ##la ##cian regular ##ization to maintain relative location between neighboring vertices during deformation , and an edge length regular ##ization to prevent out ##lier ##s . these losses are applied with equal weight on both the intermediate and final mesh . unless otherwise stated , we use for a vertex in the predicted mesh , for a vertex in the ground truth mesh , for the neighboring pixel of , till the end of this section . sub ##su ##bs ##ection : cha ##m ##fer loss the cha ##m ##fer distance measures the distance of each point to the other set : it is reasonably good to reg ##ress the vertices close to its correct position , however is not sufficient to produce nice 3d mesh ( see the result of fan et al . in fig . [ reference ] ) . sub ##su ##bs ##ection : normal loss we further define loss on surface normal to character ##ize high order properties : where is the closest vertex for that is found when calculating the cha ##m ##fer loss , is the neighboring pixel of , is the inner product of two vectors , and is the observed surface normal from ground truth . essentially , this loss requires the edge between a vertex with its neighbors to perpendicular to the observation from the ground truth . one may find that this loss does not equal to zero unless on a plan ##ar surface . however , opt ##imi ##zing this loss is equivalent as forcing the normal of a locally fitted tangent plane to be consistent with the observation , which works practically well in our experiment . moreover , this normal loss is fully different ##iable and easy to opt ##imi ##ze . sub ##su ##bs ##ection : regular ##ization even with the cha ##m ##fer loss and normal loss , the optimization is easily stuck ##ed in some local minimum . more specifically , the network may generate some super large deformation to favor some local consistency , which is especially harmful at the beginning when the estimation is far from ground truth , and causes flying vertices ( fig . [ reference ] ) . paragraph : lap ##la ##cian regular ##ization to handle these problem , we first propose a lap ##la ##cian term to prevent the vertices from moving too freely , which potentially avoids mesh self - intersection . the lap ##lai ##cian term serves as a local detail preserving operator , that encourages neighboring vertices to have the same movement . in the first deformation block , it acts like a surface smooth ##ness term since the input to this block is a smooth - everywhere el ##lip ##so ##id ; starting from the second block , it prevents the 3d mesh model from def ##or ##ming too much , so that only fine - grain ##ed details are added to the mesh model . to calculate this loss , we first define a lap ##la ##cian coordinate for each vertex as and the lap ##la ##cian regular ##ization is defined as : where and are the lap ##la ##cian coordinate of a vertex after and before a deformation block . paragraph : edge length regular ##ization . to penal ##ize flying vertices , which us ##usa ##lly cause long edge , we add an edge length regular ##ization loss : the overall loss is a weighted sum of all four losses , , where , and are the hyper ##para ##meter ##s which balance the losses and fixed for all the experiments . section : experiment in this section , we perform an extensive evaluation on our model . in addition to comparing with previous 3d shape generation works for evaluating the reconstruction accuracy , we also anal ##yse the importance of each component in our model . qu ##ali ##tative results on both synthetic and real - world images further show that our model produces triangular mesh ##es with smooth surfaces and still maintains details depicted in the input images . sub ##section : experimental setup sub ##su ##bs ##ection : data . we use the data ##set provided by cho ##y et al . . the data ##set contains rendering images of 50 ##k models belonging to 13 object categories from shape ##net , which is a collection of 3d cad models that are organized according to the word ##net hierarchy . a model is rendered from various camera viewpoint ##s , and camera intrinsic and ex ##tri ##ns ##ic matrices are recorded . for fair comparison , we use the same training / testing split as in cho ##y et . al . . sub ##su ##bs ##ection : evaluation metric . we adopt the standard 3d reconstruction metric . we first uniformly sample points from our result and ground truth . we calculate precision and recall by checking the percentage of points in prediction or ground truth that can find a nearest neighbor from the other within certain threshold . a f - score as the harmonic mean of precision and recall is then calculated . following fan et . al . , we also report the cha ##m ##fer distance ( cd ) and earth move ##r ' s distance ( em ##d ) . for f - score , larger is better . for cd and em ##d , smaller is better . on the other hand , we realize that the commonly used evaluation metric ##s for shape generation may not thoroughly reflect the shape quality . they often capture o ##cc ##up ##ancy or point - wise distance rather than surface properties , such as continuity , smooth ##ness , high - order details , for which a standard evaluation metric is barely missing in literature . thus , we recommend to pay attention on qu ##ali ##tative results for better understanding of these aspects . sub ##su ##bs ##ection : baseline ##s . we compare the presented approach to the most recent single image reconstruction approaches . specifically , we compare with two state - of - the - art methods - cho ##y et . al . ( 3d - r ##2 ##n ##2 ) producing 3d volume , and fan et . al . ( ps ##g ) producing point cloud . since the metric ##s are defined on point cloud , we can evaluate ps ##g directly on its output , our method by uniformly sampling point on surface , and 3d - r ##2 ##n ##2 by uniformly sampling point from mesh created using the marching cube method . we also compare to neural 3d mesh render ##er ( n ##3 ##m ##r ) which is so far the only deep learning based mesh generation model with code public available . for fair comparison , the models are trained with the same data using the same amount of time . sub ##su ##bs ##ection : training and run ##time . our network receives input images of size , and initial el ##lip ##so ##id with 156 vertices and 46 ##2 edges . the network is implemented in tensor ##flow and opt ##imi ##zed using adam with weight decay 1 ##e - 5 . the batch size is 1 ; the total number of training epoch is 50 ; the learning rate is initial ##ized as 3 ##e - 5 and drops to 1 ##e - 5 after 40 epoch ##s . the total training time is 72 hours on a n ##vid ##ia titan x . during testing , our model takes 15 . 58 ##ms to generate a mesh with 246 ##6 vertices . sub ##section : comparison to state of the art tab . [ reference ] shows the f - score with different threshold ##s of different methods . our approach out ##per ##forms the other methods in all categories except water ##craft . notably , our results are significantly better than the others in all categories under a smaller threshold , showing at least 10 % f - score improvement . n ##3 ##m ##r does not perform well , and its result is about 50 % worse than ours , probably because their model only learns from limited silhouette signal in images and lacks of explicit handling of the 3d mesh . we also show the cd and em ##d for all categories in tab . [ reference ] . our approach out ##per ##forms the other methods in most categories and achieve ##s the best mean score . the major competitor is ps ##g , which produces a point cloud and has the most freedom ; this freedom leads to smaller cd and em ##d , however does not necessarily leads to a better mesh model without proper regular ##ization . to demonstrate this , we show the qu ##ali ##tative results to analyze why our approach out ##per ##forms the others . fig . [ reference ] shows the visual results . to compare the quality of mesh model , we convert volume ##tric and point cloud to mesh using standard approaches . as we can see , the 3d volume results produced by 3d - r ##2 ##n ##2 lack of details due to the low resolution , e . g . , the legs are missing in the chair example as shown in the 4 - th row of fig . [ reference ] . we tried oct ##ree based solution to increase the volume resolution , but found it still hard to recover surface level details as much as our model . ps ##g produces sparse 3d point clouds , and it is non - trivial to recover mesh ##es from them . this is due to the applied cha ##m ##fer loss acting like a regression loss which gives too much degree of freedom to the point cloud . n ##3 ##m ##r produces very rough shape , which might be sufficient for some rendering tasks , however can not recover complicated objects such as chairs and tables . in contrast , our model does not suffer from these issues by lever ##aging a mesh representation , integration of per ##ce ##pt ##ual feature , and carefully defined losses during the training . our result is not restricted by the resolution due to the limited memory budget and contains both smooth continuous surface and local details . sub ##section : ab ##lation study now we conduct controlled experiments to anal ##yse the importance of each component in our model . tab . [ reference ] reports the performance of each model by removing one component from the full model . again , we argue that these commonly used evaluation metric ##s does not necessarily reflect the quality of the recovered 3d geometry . for example , the model with no edge length regular ##ization achieve ##s the best performance across all , however , in fact produces the worst mesh ( fig . [ reference ] , the last 2nd column ) . as such , we use qu ##ali ##tative result fig . [ reference ] to show the contribution of each component in our system . sub ##su ##bs ##ection : graph un ##pool ##ing we first remove the graph un ##pool ##ing layers , and thus each block has the same number of vertices as in the last block of our full model . it is observed that the deformation makes mistake easier at beginning , which can not be fixed later on . consequently , there are some obvious artifacts in some parts of the objects . sub ##su ##bs ##ection : g - res ##net we then remove the short ##cut connections in g - res ##net , and make it regular g ##c ##n . as can be seen from tab . [ reference ] , there is a huge performance gap in all four measurement metric ##s , which means the failure of opt ##imi ##zing cha ##m ##fer distance . the main reason is the degradation problem observed in the very deep 2d con ##vo ##lu ##tion ##al neural network . such problem leads to a higher training error ( and thus higher testing error ) when adding more layers to a suit ##ably deep model . es ##set ##ial ##ly , our network has 42 graph con ##vo ##lu ##tion ##al layers . thus , this phenomenon has also been observed in our very deep graph neural network experiment . sub ##su ##bs ##ection : loss terms we evaluate the function of each additional terms besides the cha ##m ##fer loss . as can be seen in fig . [ reference ] , removing normal loss severely imp ##air ##s the surface smooth ##ness and local details , e . g . seat back ; removing lap ##la ##cian term causes intersecting geometry because the local topology changes , e . g . the hand held of the chair ; removing edge length term causes flying vertices and surfaces , which completely ruins the surface characteristics . these results demonstrate that all the components presented in this work contribute to the final performance . sub ##su ##bs ##ection : number of deformation blocks we now analyze the effects of the number of blocks . figure fig . [ reference ] ( left ) shows the mean f - score ( ) and cd with regard to the number of blocks . the results indicate that increasing the number of blocks helps , but the benefit is getting saturated with more blocks , e . g . from 3 to 4 . in our experiment , we found that 4 blocks results in too many vertices and edges , which slow down our approach dramatically even though it provides better accuracy on evaluation metric ##s . therefore , we use 3 blocks in all our experiment for the best balance of performance and efficiency . fig . [ reference ] ( right ) shows the output of our model after each deformation block . notice how mesh is den ##si ##fied with more vertices and new details are added . sub ##section : rec ##ons ##tructing real - world images following cho ##y et . al . , we test our network on the online products data ##set and internet images for qu ##ali ##tative evaluation on real images . we use the model trained from shape ##net data ##set and directly run on real images without fine ##tu ##ning , and show results in fig . [ reference ] . as can be seen , our model trained on synthetic data general ##izes well to the real - world images across various categories . section : conclusion we have presented an approach to extract 3d triangular mesh ##es from sing ##e images . we exploit the key advantages the mesh presentation can bring to us , and the key issues required to solve for success . the former includes surface normal constraints and information propagation along edges ; the latter includes per ##ce ##pt ##ual features extracted from images as a guidance . we carefully design our network structure and propose a very deep cascade ##d graph con ##vo ##lu ##tion ##al neural network with \" short ##cut \" connections . mesh ##es are progressively refined by our network trained end - to - end with the cha ##m ##fer loss and normal loss . our results are significantly better than the previous state - of - the - art using other shape representations such as 3d volume or 3d point cloud . thus , we believe mesh representation is the next big thing in this direction , and we hope that the key components discovered in our work can support follow - up works that will further advance direct 3d mesh reconstruction from single images . sub ##su ##bs ##ection : future work our method only produces mesh ##es with the same topology as the initial mesh . in the future , we will extend our approach to more general cases , such as scene level reconstruction , and learn from multiple images for multi - view reconstruction . sub ##su ##bs ##ection : acknowledge ##ments this work was supported by two projects from ns ##fc ( # 61 ##6 ##22 ##20 ##4 and # 61 ##57 ##21 ##34 ) , two projects from st ##cs ##m ( # 16 ##j ##c ##14 ##20 ##40 ##1 and # 16 ##qa ##14 ##00 ##500 ) , eastern scholar ( t ##p ##20 ##17 ##00 ##6 ) , and the thousand talents plan of china ( for young professionals , d ##14 ##100 ##0 ##9 ) . bibliography : references",
        "pred_seq": "[SEP] pixel ##h [SEP] 3d accuracy [SEP] 3d accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy 3d 3d 3d 3d 3d 3d 3d 3d 3d 3d 3d 3d shape [CLS]",
        "pred_templates": [],
        "gold_templates": [
            {
                "Material": [
                    [
                        "shapenet dataset"
                    ]
                ],
                "Method": [
                    [
                        "pixel2mesh"
                    ]
                ],
                "Metric": [
                    [
                        "fscore",
                        "fscore improvement",
                        "mean fscore"
                    ]
                ],
                "Task": [
                    [
                        "3d reconstruction"
                    ]
                ]
            }
        ]
    },
    "56": {
        "doctext": "document : curriculum domain adaptation for semantic segment ##ation of urban scenes during the last half decade , con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) have triumph ##ed over semantic segment ##ation , which is a core task of various emerging industrial applications such as autonomous driving and medical imaging . however , to train cnn ##s requires a huge amount of data , which is difficult to collect and labor ##ious to ann ##ota ##te . recent advances in computer graphics make it possible to train cnn models on photo - realistic synthetic data with computer - generated ann ##ota ##tions . despite this , the domain mis ##mat ##ch between the real images and the synthetic data significantly decreases the models ' performance . hence we propose a curriculum - style learning approach to minimize the domain gap in semantic segment ##ation . the curriculum domain adaptation solve ##s easy tasks first in order to in ##fer some necessary properties about the target domain ; in particular , the first task is to learn global label distributions over images and local distributions over landmark super ##pi ##x ##els . these are easy to estimate because images of urban traffic scenes have strong id ##ios ##yn ##cr ##asi ##es ( e . g . , the size and spatial relations of buildings , streets , cars , etc . ) . we then train the segment ##ation network in such a way that the network predictions in the target domain follow those in ##fer ##red properties . in experiments , our method significantly out ##per ##forms the baseline ##s as well as the only known existing approach to the same problem . 1 ] yang zhang 2 ] philip david 1 ] bo ##qing gong [ 1 ] center for research in computer vision , university of central florida [ 2 ] computational and information sciences directorate , u . s . army research laboratory yang ##zh ##ang @ knights . uc ##f . ed ##u , , b ##gong @ cr ##c ##v . uc ##f . ed ##u section : introduction this paper is concerned with domain adaptation for semantic image segment ##ation of urban scenes , i . e . , assign ##ing a category label to every pixel of an image or video frame . our interest in this problem is partially due to the exciting vision of autonomous driving , where understanding complex inner - city traffic scenes is an essential module and semantic segment ##ation is one of its key constituents . machine learning methods for automatic semantic segment ##ation require massive amounts of high - quality ann ##ota ##ted imagery in order to produce effective class ##ifiers that general ##ize well to novel scenes . however , ann ##ota ##ting training imagery for semantic segment ##ation is a very cum ##bers ##ome task for humans . cord ##ts et al . report that the ann ##ota ##tion and quality control take more than 1 . 5 hours on a single image of the city ##sca ##pes data ##set . besides , it is very difficult and time - consuming to collect imagery that depicts the large number of var ##ia ##bilities possible of urban scenes in different countries , seasons , and lighting conditions , etc . to overcome both short ##coming ##s , simulated urban environments may be used to automatically generate large amounts of ann ##ota ##ted training imagery . this , however , introduces a new problem , that of domain mis ##mat ##ch between the source ( simulated ) domain and the target ( real ) domain . figure [ reference ] illustrates some examples drawn from the synthetic synth ##ia data ##set and the real city ##sca ##pes data ##set . it is readily apparent that there are significant visual differences between the two data ##set ##s . domain adaptation techniques may be used by machine learning methods to bridge this gap between the two domains . in computer vision , learning domain - invariant features has been a prevalent and successful strategy to tackle the disc ##re ##pan ##cy between two domains , mainly for classification and regression problems . the core idea is to in ##fer a new feature space such that the marginal distributions of the source domain ( s ) and the target domain ( t ) are about the same , i . e . , . furthermore , the prediction function from that space is assumed to be the same across the domains so that one can leverage the rich labeled data in the source domain to train class ##ifiers that general ##ize well to the target . it is hard to verify the assumption , but the work along this line is rich and has led to impressive practical results regardless , such as the algorithms using linear transformation , kernel methods , and the recent deep learning methods that directly extract domain - invariant features from raw input images . in contrast to prior arts , the semantic segment ##ation we study in this paper is a highly structured prediction problem , for which domain adaptation is only sparsely explored in the literature . under structured prediction , can we still achieve good domain adaptation results by following the above principles ? our intuition and experimental studies ( cf . section [ reference ] ) tell us no . learning a decision function for structured prediction is more involved than classification because it has to resolve the predictions in an exponential ##ly large label space . as a result , the assumption that the source and target domains share the same prediction function becomes less likely to hold . besides , some disc ##rim ##ina ##tive cues in the data would be suppressed if one matches the feature representations of the two domains without taking careful account of the structured labels . finally , data instances are the proxy to measure the domain difference . however , it is not immediately clear what comprises the instances in semantic segment ##ation , especially given that the top - performing segment ##ation methods are built upon deep neural networks . hoffman et al . take each spatial unit in the fully con ##vo ##lu ##tion ##al network ( fc ##n ) as an instance . we contend that such instances are actually non - i . i . d . in either individual domain , as their rec ##eptive fields overlap with each other . how can we avoid the assumption that the source and target domains share the same prediction function in a transformed domain - invariant feature space ? our proposed solution draws on two key observations . one is that the urban traffic scene images have strong id ##ios ##yn ##cr ##asi ##es ( e . g . , the size and spatial relations of buildings , streets , cars , etc . ) . therefore , some tasks are \" easy \" and , more importantly , suffer less because of the domain disc ##re ##pan ##cy . second , the structured output in semantic segment ##ation enables convenient posterior regular ##ization , as opposed to the popular ( e . g . , ) regular ##ization over model parameters . accordingly , we propose a curriculum - style domain adaptation approach . recall that , in domain adaptation , only the source domain supplies many labeled data while there are no or only scarce labels from the target . the curriculum domain adaptation begins with the easy tasks , in order to gain some high - level properties about the unknown pixel - level labels for each target image . it then learns a semantic segment ##ation network \u2014 the hard task , whose predictions over the target images are forced to follow those necessary properties as much as possible . to develop the easy tasks in the curriculum , we consider label distributions over both ho ##listic images and some landmark super ##pi ##x ##els of the target domain . take the former for instance . the label distribution of an image indicates the percentage of pixels that belong to each category , respectively . we argue that such tasks are easier , despite the domain mis ##mat ##ch , than assign ##ing pixel - wise labels . indeed , we may directly estimate the label distributions without in ##fer ##ring the pixel - wise labels . moreover , the relative sizes of road , vehicle , pedestrian , etc . con ##stra ##in the shape of the distributions , effectively reducing the search space . finally , models to estimate the label distributions over super ##pi ##x ##els may benefit from the urban scenes ' canonical layout that trans ##cend ##s domains , e . g . , buildings stand beside streets . why and when are the seemingly simple label distributions useful for the domain adaptation of semantic segment ##ation ? in our experiments , we find that the segment ##ation networks trained on the source domain perform poorly on many target images , giving rise to di ##sp ##rop ##ort ##ion ##ate label assignments ( e . g . , many more pixels are classified to sidewalks than to streets ) . to rec ##tify this , the image - level label distribution informs the segment ##ation network how to update the predictions while the label distributions of the landmark super ##pi ##x ##els tell the network where to update . jointly , they guide the adaptation of the networks to the target domain to , at least , generate proportional label predictions . note that additional \" easy tasks \" can be convenient ##ly incorporated into our framework in the future . our main contribution is on the proposed curriculum - style domain adaptation for the semantic segment ##ation of urban scenes . we select into the curriculum the easy and useful tasks of in ##fer ##ring label distributions for the target images and landmark super ##pi ##x ##els , in order to gain some necessary properties about the target domain . built upon these , we learn a pixel - wise disc ##rim ##ina ##tive segment ##ation network from the labeled source data and , meanwhile , conduct a \" sanity check \" to ensure the network behavior is consistent with the previously learned knowledge about the target domain . our approach effectively el ##udes the assumption about the existence of a common prediction function for both domains in a transformed feature space . it readily applies to different segment ##ation networks , as it does not change the network architecture or tax any intermediate layers . section : related work we discuss some related work on domain adaptation and semantic segment ##ation , with special focus on that transferring knowledge from virtual images to real photos . paragraph : domain adaptation . conventional machine learning algorithms rely on the assumption that the training and test data are drawn i . i . d . from the same underlying distribution . however , it is often the case that there exists some disc ##re ##pan ##cy from the training to the test stage . domain adaptation aims to rec ##tify this mis ##mat ##ch and tune the models toward better general ##ization at testing . the existing work on domain adaptation mostly focuses on classification and regression problems , e . g . , learning from online commercial images to classify real - world objects , and , more recently , aims to improve the adapt ##ability of deep neural networks . among them , the most relevant work to ours is that exploring simulated data . sun and sa ##enko train generic object detectors from the synthetic images , while va ##zquez et al . use the virtual images to improve pedestrian detection ##s in real environment . the other way around , i . e . , how to improve the quality of the simulated images using the real ones , is studied in . paragraph : semantic segment ##ation . semantic segment ##ation is the task of assign ##ing an object label to each pixel of an image . traditional methods rely on local image features manually designed by domain experts . after the pioneering work that introduced the con ##vo ##lu ##tion ##al neural network ( cnn ) to semantic segment ##ation , most recent top - performing methods are built on cnn ##s . an enormous amount of labor - intensive work is required to ann ##ota ##te the many images that are needed to obtain accurate segment ##ation models . the pascal vo ##c ##20 ##12 challenge contains nearly 10 , 000 ann ##ota ##ted images for the segment ##ation competition , and the ms coco challenge includes over 200 , 000 ann ##ota ##ted images . according to , it took about 60 minutes to manually segment each image in and about 90 minutes for each in . a plausible approach to reducing the human work ##load is to utilize weakly supervised information such as image labels and bound ##ing boxes . we instead explore the use of almost effortlessly labeled virtual images for training high - quality segment ##ation networks . in , ann ##ota ##ting a synthetic image took only 7 seconds on average through a computer game . for the urban scenes , we use the synth ##ia data ##set which contains images of a virtual city . paragraph : domain adaptation for semantic segment ##ation . upon observing the obvious mis ##mat ##ch between virtual and real data , we expect domain adaptation to enhance the segment ##ation performance on real images by networks trained on virtual ones . to the best of our knowledge , the only attempt to algorithm ##ically address this problem is . while it regular ##izes the intermediate layers and con ##stra ##ins the output of the network , we propose a different curriculum domain adaptation strategy . we solve the easy task first and then use the learned knowledge about the target domain to regular ##ize the network predictions . section : approach in this section , we present the details of the proposed curriculum domain adaptation for semantic segment ##ation of urban scene images . unlike previous work that align ##s the domains via an intermediate feature space and thereby implicit ##ly assumes the existence of the same decision function for the two domains , it is our intuition that , for structured prediction ( i . e . , semantic segment ##ation here ) , the cross - domain general ##ization of machine learning models can be more efficiently improved if we avoid this assumption and instead train them subject to necessary properties they should retain in the target domain . paragraph : pre ##lim ##ina ##ries . in particular , the properties are about the pixel - wise category labels of an arbitrary image from the target domain , where and are the width and height of the image , respectively , and is the number of categories . we use one - hot vector encoding for the ground ##tr ##uth labels , i . e . , takes the value of 0 or 1 and the latter means that the - th label is assigned by a human ann ##ota ##tor to the pixel at . corresponding ##ly , the prediction by a segment ##ation network is realized by a soft ##max function per pixel . we express each target property in the form of a distribution over the categories , where represents the o ##cc ##up ##ancy proportion of the category over the - th target image or a super ##pi ##x ##el of the image . therefore , one can immediately calculate the distribution given the human ann ##ota ##tions to the image . for instance , the image level label distribution is expressed by similarly , we can compute the target property / distribution from the network predictions and denote it by . sub ##section : domain adaptation using the target properties ideally , we would like to have a segment ##ation network to im ##itate human ann ##ota ##tors on the target domain . therefore , necessarily , the properties of their ann ##ota ##tion results should be the same too . we capture this notion by mini ##mi ##zing the cross entropy at training , where the first term of the right - hand side is the entropy and the second is the k ##l - diver ##gence . given a mini - batch consisting of both source images ( ) and target images ( ) , the overall objective function for training the cross - domain general ##izing segment ##ation network is , where is the pixel - wise cross - entropy loss defined over the sufficiently labeled source domain images , enforcing the network to have the pixel level disc ##rim ##ina ##tive capabilities , and the second term is over the un ##lab ##ele ##d target domain images , hint ##ing the network what necessary properties its predictions should have in the target domain . we use to balance the two strengths in training and super ##script to index different types of label distributions . note that in the domain adaptation context , we actually can not directly compute the label distribution from the ground ##tr ##uth ann ##ota ##tions of the target domain . nonetheless , est ##imating them using the labeled source data is easier than assign ##ing labels to every single pixel of the target images . we present the details in the next section . paragraph : remarks . mathematical ##ly , the objective function has a similar form as in model compression . we thus borrow some concepts to gain more intuitive understanding about our domain adaptation procedure . the \" student \" network follows a curriculum to learn simple knowledge about the target domain before it addresses the hard one of semantic ##ally segment ##ing images . the models in ##fer ##ring the target properties act like \" teachers \" , as they hint what label distributions the final solution ( image ann ##ota ##tion ) may have in the target domain at the image and super ##pi ##x ##el levels . another perspective is to understand the target properties as a posterior regular ##ization for the network . the posterior regular ##ization can convenient ##ly en ##code a prior ##i knowledge into the objective function . some applications include weakly supervised segment ##ation and detection , and rule - regular ##ized training of neural networks . in addition to the domain adaptation setting and novel target properties , another key distinction of our work is that we deco ##up ##le the label distributions from the network predictions and thus avoid the em type of optimization ##s . our approach learns the segment ##ation network with almost effort ##less changes to the popular deep learning tools . sub ##section : in ##fer ##ring the target properties thus far we have presented the \" hard \" task in the curriculum domain adaptation . in this section , we describe the \" easy \" ones , i . e . , how to in ##fer the target properties without access ##ing the image ann ##ota ##tions of the target domain . our contributions also include selecting the particular property of label distributions to constitute the simple tasks . sub ##su ##bs ##ection : global label distributions of images due to the domain di ##spar ##ity , a baseline segment ##ation network trained on the source domain ( i . e . , using the first term of e ##q . ( [ reference ] ) ) could be easily crippled given the target images . in our experiments , we find that our baseline network constantly mistakes streets for sidewalks and / or cars ( cf . figure [ reference ] ) . consequently , the predicted labels for the pixels are highly di ##sp ##rop ##ort ##ion ##ate . to rec ##tify this , we employ the label distribution over the global image as our first property ( cf . e ##q . ( [ reference ] ) ) . without access to the target labels , we have to train machine learning models from the labeled source images to estimate the label distribution for the target image . nonetheless , we argue that this is less challenging than generating the per - pixel predictions despite that both tasks are influenced by the domain mis ##mat ##ch . in our experiments , we examine different approaches to this task . we extract image features using the inception - res ##net - v ##2 as the input to the following models . although multi ##no ##mia ##l log ##istic regression ( l ##r ) is mainly used for classification , its output is actually a valid distribution over the categories . for our purpose , we thus train it by replacing the one - hot vectors in the cross - entropy loss with the ground ##tr ##uth label distribution , which is calculated using e ##q . ( [ reference ] ) and the available human labels of the source domain . given a target image , we directly take the l ##r ' s output as the predicted label distribution . we also test a non ##para ##metric method by simply re ##tri ##eving the nearest neighbors ( n ##ns ) for a target image and then transferring the mean of the n ##ns ' label distributions to the target image . we use the distance for the n ##n retrieval . finally , we include two dumb predictions as the control experiment . one is , for any target image , to output the mean of all the label distributions in the source domain ( source mean ) , and the other is to output a uniform distribution . sub ##su ##bs ##ection : local label distributions of landmark super ##pi ##x ##els the image level label distribution globally penal ##izes potentially di ##sp ##rop ##ort ##ional segment ##ation output on the target domain , and yet is inadequate in providing spatial constraints . in this section , we consider the use of label distributions over some super ##pi ##x ##els as the anchors to drive the network towards spatial ##ly desired target properties . note that it is not necessary , and is even harmful , to use all of the super ##pi ##x ##els in a target image to regular ##ize the segment ##ation network , because that would be too strong a force and may over ##ru ##le the pixel - wise disc ##rim ##ina ##tive ##ness revealed by the labeled source images , especially when the label distributions are not in ##fer ##red accurately enough . in order to have the dual effect of both est ##imating the label distributions of super ##pi ##x ##els and filtering the super ##pi ##x ##els , we sim ##plify the problem and employ a linear sv ##m in this work . in particular , we segment each image into 100 super ##pi ##x ##els using linear spectral cluster ##ing . for the super ##pi ##x ##els of the source domain , we are able to assign a single dominant label to each of them , and then use the labels and the corresponding features extracted from the super ##pi ##x ##els to train a multi - class sv ##m . given a test super ##pi ##x ##el of a target image , the multi - class sv ##m returns a class label as well as a decision value , which is interpreted as the confidence score about classify ##ing this super ##pi ##x ##el . we keep the top 60 % super ##pi ##x ##els , called landmark super ##pi ##x ##els , in the target domain and calculate their label distributions as the second type of \" easy \" tasks . in particular , the class label of a landmark super ##pi ##x ##el is encoded into a one - hot vector , which serves as a valid distribution about the categories in the landmark super ##pi ##x ##el . albeit simple , we find this method works very well in our experiments . we en ##code both visual and context ##ual information to represent a super ##pi ##x ##el . first , we use the fc ##n - 8 ##s pre - trained on the pascal context data ##set , which has 59 distinct classes , to obtain 59 detection scores for each pixel . we then average them within each super ##pi ##x ##el . finally , we represent a super ##pi ##x ##el by the con ##cate ##nation of the 59 ##d vectors of itself , its left and right super ##pi ##x ##els , as well as the two respectively above and below it . sub ##section : curriculum domain adaptation : rec ##ap ##it ##ulation we rec ##ap the proposed curriculum domain adaptation using figure [ reference ] before presenting the experiments in the next section . our main idea is to execute the domain adaptation step by step , starting from the easy tasks that are less sensitive to the domain disc ##re ##pan ##cy than the semantic segment ##ation . we choose the labels distributions over global images and local landmark super ##pi ##x ##els in this work ; more tasks will be explored in the future . the solutions to them provide useful gradient ##s originating from the target domain ( cf . the arrows with brown color in figure [ reference ] ) , while the source domain feeds the network with well - labeled images and segment ##ation masks ( cf . the dark blue arrows in figure [ reference ] ) . section : experiments in this section , we describe the experimental setup and compare the results of our approach , its variations , and some existing baseline methods . sub ##section : segment ##ation network and optimization in our experiments , we use fc ##n - 8 ##s as our semantic segment ##ation network . we initial ##ize its con ##vo ##lu ##tion ##al layers with v ##gg - 19 , and then train it using the ada ##del ##ta opt ##imi ##zer with default parameters . each mini - batch is comprised of five source images and five randomly chosen target images . when we train the baseline network with no adaptation , however , we try to use the largest possible mini - batch that includes 15 source images . the network is implemented in ke ##ras and the ##ano . we train different versions of the network on a single tesla k ##40 gp ##u . unlike the existing deep domain adaptation methods which introduce regular ##ization to the intermediate layers , we only rev ##ise the loss function over the output . hence , our curriculum domain adaptation can be readily applied to other segment ##ation networks ( e . g . , ) . sub ##section : data ##set ##s and evaluation we use the publicly available city ##sc ##pa ##es and synth ##ia data ##set ##s in our experiments . city ##sca ##pes is a real - world , vehicle - ego ##centric image data ##set collected in 50 cities in germany and nearby countries . it provides four di ##s ##jo ##int subset ##s : 2 , 99 ##3 training images , 50 ##3 validation image , 1 , 53 ##1 test images , and 20 , 02 ##1 auxiliary images . all the training , validation , and test images are accurately ann ##ota ##ted with per pixel category labels , while the auxiliary set is coarse ##ly labeled . there are 34 distinct categories in the data ##set . synth ##ia is a large data ##set of synthetic images and provides a particular subset , called synth ##ia - rand - city ##sca ##pes , to pair with city ##sca ##pes . this subset contains 9 , 400 images that are automatically ann ##ota ##ted with 12 object categories , one void class , and some unnamed classes . note that the virtual city used to generate the synthetic images does not correspond to any of the real cities covered by city ##sca ##pes . we ab ##bre ##via ##te synth ##ia - rand - city ##sca ##pes to synth ##ia here ##on . paragraph : domain id ##ios ##yn ##cr ##asi ##es . although both data ##set ##s depict urban scenes , and synth ##ia is created to be as photo - realistic as possible , they are mis ##mat ##ched domains in several ways . the most noticeable difference is probably the coarse - grain ##ed textures in synth ##ia ; very similar texture patterns repeat in a regular manner across different images . in contrast , the city ##sca ##pes images are captured by high - quality dash - cameras . another major distinction is the variability in view angles . since city ##sca ##pes images are recorded by the dash cameras mounted on a moving car , they are viewed from almost a constant angle that is about parallel to the ground . more diverse view angles are employed by synth ##ia \u2014 it seems like some cameras are placed on the buildings that are significantly higher than a bus . finally , some of the synth ##ia images are severely shadowed by extreme lighting conditions , while we find no such conditions in the city ##sca ##pes images . these combined factors , among others , make domain adaptation from synth ##ia to city ##sca ##pes a very challenging problem . figure [ reference ] shows some example images from both data ##set ##s . we pair each city ##sc ##pa ##es image with its nearest neighbor in synth ##ia , retrieved by the inception - res ##net - v ##2 features . however , the cross - data ##set nearest neighbors are visually very different from the query images , verify ##ing the dramatic di ##spar ##ity between the two domains . paragraph : experiment setup . since our ultimate goal is to solve the semantic segment ##ation problem for real images of urban scenes , we take city ##sca ##pes as the target domain and synth ##ia as the source domain . the city ##sca ##pes validation set is used as our test set . we split 500 images out of the city ##sc ##pa ##es training set for the validation purpose ( e . g . , to monitor the convergence of the networks ) . in training , we randomly sample mini - batch ##es from both the images ( and their labels ) of synth ##ia and the remaining images of city ##sca ##pes yet with no labels . as in , we manually find 16 common classes between the two data ##set ##s : sky , building , road , sidewalk , fence , vegetation , pole , car , traffic sign , person , bicycle , motorcycle , traffic light , bus , wall , and rider . the last four are unnamed and yet labeled in synth ##ia . paragraph : evaluation . we use the evaluation code released along with the city ##sca ##pes data ##set to evaluate our results . it calculate ##s the pascal vo ##c intersection - over - union , i . e . , , where t ##p , f ##p , and f ##n are the numbers of true positive , false positive , and false negative pixels , respectively , determined over the whole test set . since we have to res ##ize the images before feeding them to the segment ##ation network , we res ##ize the output segment ##ation mask back to the original image size before running the evaluation against the ground ##tr ##uth ann ##ota ##tions . sub ##section : results of in ##fer ##ring global label distributions before presenting the final semantic segment ##ation results , we first compare the different approaches to in ##fer ##ring the global label distributions of the target images ( cf . section [ reference ] ) . we report the results on the held - out validation images of city ##sca ##pes in this experiment , and then select the best method for the remaining experiments . in table [ reference ] , we compare the estimated label distributions with the ground ##tr ##uth ones using the distance , the smaller the better . we see that the baseline network ( no ##ada ##pt ) , which is directly learned from the source domain without any adaptation methods , out ##per ##forms the dumb uniform distribution ( uniform ) and yet no other methods . this confirms that the baseline network gives rise to severely di ##sp ##rop ##ort ##ion ##ate predictions over the target domain . another dumb prediction ( sr ##c mean ) , i . e . , using the mean of all label distributions over the source domain as the prediction for the target images , however , performs reasonably well . to some extent , this indicates the value of the simulated source domain for the semantic segment ##ation task of urban scenes . finally , the nearest neighbors ( n ##n ) based method and the multi ##no ##mia ##l log ##istic regression ( l ##r ) ( cf . section [ reference ] ) perform the best . we use the output of l ##r on the target domain in our remaining experiments . sub ##section : comparison results bike fence wall t - sign pole mb ##ike t - light sky bus rider ve ##g b ##ld ##g car person sidewalk road we report the final semantic segment ##ation results on the test data of the target domain in this section . we compare our approach to the following competing methods . we directly train the fc ##n - 8 ##s model on synth ##ia without applying any domain adaptation methods . this is the most basic baseline for our experiments . recall that we have trained a multi - class sv ##m using the dominant labels of the super ##pi ##x ##els in the source domain . we then use them to classify the target super ##pi ##x ##els . since we keep the top 60 % most confidently classified super ##pi ##x ##els as the landmarks to regular ##ize our segment ##ation network during training ( cf . section [ reference ] ) , it is also interesting to examine the classification results of these super ##pi ##x ##els . we run the evaluation after assign ##ing the void class label to the other pixels of the images . in addition to the io ##u , we have also evaluated the classification results of the super ##pi ##x ##els by accuracy . we find that the classification accuracy is 71 % for all the super ##pi ##x ##els of the target domain , while for the selected 60 % landmark super ##pi ##x ##els , the classification accuracy is more than 88 % . hoffman et al . ' s work is the only existing one addressing the same problem as ours , to the best of our knowledge . they introduce a pixel - level ad ##vers ##aria ##l loss to the intermediate layers of the network and impose constraints to the network output . their experimental setup is about identical to ours except that they do not specify which part of city ##sca ##pes is considered as the test set . nonetheless , we include their results for comparison to put our work in a better perspective . the comparison results are shown in table [ reference ] . immediately , we note that all our domain adaptation results are significantly better than those without adaptation ( no ##ada ##pt ) . we denote by ( ours ( i ) ) the network trained using the global label distributions over the target images ( and the labeled source images ) . although one may wonder that the image - wise label distributions are too high - level to supervise the pixel - wise disc ##rim ##ina ##tive network , the gain is actually significant . they are able to correct some obvious errors of the baseline network , such as the di ##sp ##rop ##ort ##ional predictions about road and sidewalk ( cf . the results of ours ( i ) vs . no ##ada ##pt in the last two columns ) . it is interesting to see that both super ##pi ##x ##el classification - based segment ##ation results ( sp and sp l ##nd ##m ##k ) are also better than the baseline network ( no ##ada ##pt ) . the label distributions obtained over the landmark super ##pi ##x ##els boost the segment ##ation network ( ours ( sp ) ) to the mean io ##u of 28 . 1 % , which is better than those by either super ##pi ##x ##el classification or the baseline network individually . we have also tried to use the label distributions over all the super ##pi ##x ##els to train the network , and observe little improvement over no ##ada ##pt . this is probably because it is too forceful to regular ##ize the network output at every single super ##pi ##x ##el especially when the estimated label distributions are not accurate enough . the super ##pi ##x ##el - based methods , including ours ( sp ) , miss small objects such as fences , traffic lights ( t - light ) , and traffic signs ( t - sign ) , and instead are very accurate for categories like the sky , road , and building , that typically occupy larger image regions . on the contrary , the label distributions on the images give rise to a network ( ours ( i ) ) that performs better on the small objects than ours ( sp ) . in other words , they mutually complement to some extent . re - training the network by using the label distributions over both global images and local landmark super ##pi ##x ##els ( ours ( i + sp ) ) , we achieve the best semantic segment ##ation results on the target domain . in the future work , it is worth exploring other target properties , perhaps still in the form of label distributions , that handle the small objects well , in order to further complement the super ##pi ##x ##el - level label distributions . paragraph : comparison with fc ##ns in the wild . although we use the same segment ##ation network ( fc ##n - 8 ##s ) as , our baseline results ( no ##ada ##pt ) are better than those reported in . this may be due to subtle differences in terms of implementation or experimental setup . although our own baseline results are superior , we gain larger improvements ( 7 % ) over them than the performance gain of ( 3 % ) over the seemingly under ##per ##form ##ing baseline network there . paragraph : comparison with learning domain - invariant features . at our first attempt to solve the domain adaptation problem for the semantic segment ##ation of urban scenes , we tried to learn domain invariant features following the deep domain adaptation methods for classification . in particular , we impose the maximum mean disc ##re ##pan ##cy over the layer before the output . we name such network layer the feature layer . since there are virtually three output layers in fc ##n - 8 ##s , we experiment with all the three feature layers corresponding ##ly . we have also tested the domain adaptation by rev ##ers ##ing the gradient ##s of a domain class ##ifier . however , none of these efforts lead to any noticeable gain over the baseline network so the results are omitted . section : conclusion in this paper , we address domain adaptation for the semantic segment ##ation of urban scenes . we propose a curriculum style approach to this problem . we learn to estimate the global label distributions of the images and local label distributions of the landmark super ##pi ##x ##els of the target domain . such tasks are easier to solve than the pixel - wise label assignment . therefore , we use their results to effectively regular ##ize our training of the semantic segment ##ation network such that its predictions meet the in ##fer ##red label distributions over the target domain . our method out ##per ##forms several competing methods that do domain adaptation from simulated images to real photos of urban traffic scenes . in future work , we will explore more target properties that can be convenient ##ly in ##fer ##red to en ##rich our curriculum domain adaptation framework . paragraph : acknowledge ##ments . this work is supported by the ns ##f award ii ##s # 156 ##65 ##11 , a gift from adobe systems inc . , and a gp ##u from n ##vid ##ia . we thank the anonymous reviewers and area chairs for their insight ##ful comments . bibliography : references bike fence wall t - sign pole mb ##ike t - light sky bus rider ve ##g terrain train b ##ld ##g car person truck sidewalk road appendix : gt ##a city ##sca ##pes the main text above has been accepted to ieee international conference on computer vision ( icc ##v ) 2017 . after the paper submission , we have been continuously working on the project and have got more results . we include them below to complement the experiments in the main text . the new experiment is basically the same as the one in the main text except that we replace synth ##ia with the gt ##a data ##set . gt ##a is a synthetic , vehicle - ego ##centric image data ##set collected from the open world in the realistic ##ally rendered computer game grand theft auto v ( gt ##a , or gt ##a ##5 ) . it contains 24 , 99 ##6 images , whose semantic segment ##ation ann ##ota ##tions are fully compatible with the classes used in city ##sca ##pes . hence we use all the 19 official training classes in our experiment . the results are shown in table [ reference ] . as in the main text , the same observations about our approach apply here . additionally , we note that the results are overall better than those adapting from synth ##ia to city ##sca ##pes . this is not surprising , because the gt ##a images are more photo - realistic than synth ##ia ' s .",
        "pred_seq": "[SEP] [SEP] [SEP] semantic ##ation [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [],
                "Task": [
                    [
                        "semantic segmentation"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "gta cityscapes"
                    ]
                ],
                "Method": [
                    [
                        "curriculum domain adaptation",
                        "curriculumstyle domain adaptation approach",
                        "curriculumstyle domain adaptation",
                        "curriculum domain adaptation strategy",
                        "crossdomain generalization of machine learning models",
                        "crossdomain generalizing segmentation network",
                        "curriculum domain adaptation framework"
                    ]
                ],
                "Metric": [
                    [
                        "mean iou"
                    ]
                ],
                "Task": []
            }
        ]
    },
    "57": {
        "doctext": "bb ##8 : a scala ##ble , accurate , robust to partial o ##cc ##lusion method for predicting the 3d poses of challenging objects without using depth section : abstract we introduce a novel method for 3d object detection and pose estimation from color images only . we first use segment ##ation to detect the objects of interest in 2d even in presence of partial o ##cc ##lusion ##s and cl ##uttered background . by contrast with recent patch - based methods , we rely on a \" ho ##listic \" approach : we apply to the detected objects a con ##vo ##lu ##tion ##al neural network ( cnn ) trained to predict their 3d poses in the form of 2d projections of the corners of their 3d bound ##ing boxes . this , however , is not sufficient for handling objects from the recent t - less data ##set : these objects exhibit an axis of rotational symmetry , and the similarity of two images of such an object under two different poses makes training the cnn challenging . we solve this problem by restricting the range of poses used for training , and by introducing a class ##ifier to identify the range of a pose at run - time before est ##imating it . we also use an optional additional step that ref ##ines the predicted poses . we improve the state - of - the - art on the line ##mo ##d data ##set from 73 . 7 % [ 2 ] to 89 . 3 % of correctly registered r ##gb frames . we are also the first to report results on the o ##cc ##lusion data ##set [ 1 ] using color images only . we obtain 54 % of frames passing the pose 6 ##d criterion on average on several sequences of the t - less data ##set , compared to the 67 % of the state - of - the - art [ 10 ] on the same sequences which uses both color and depth . the full approach is also scala ##ble , as a single network can be trained for multiple objects simultaneously . section : introduction 3d pose estimation of object instances has recently become a popular problem again , because of its application in robotics , virtual and augmented reality . many recent approaches rely on depth maps , sometimes in conjunction with color images [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . however , it is not always possible to use depth cameras , as they fail ( c ) ( d ) figure 1 . zoom ##s on estimated poses for ( a ) the ape of the line ##mo ##d data ##set [ reference ] , ( b ) the drill ##er of the o ##cc ##lusion data ##set [ reference ] , ( c ) and ( d ) three objects of the t - less [ reference ] data ##set . the green bound ##ing boxes correspond to the ground truth poses , and the blue bound ##ing boxes to the poses estimated with our method . the two boxes often overlap almost perfectly , showing the accuracy of our estimated poses . the parts of the bound ##ing boxes o ##cc ##lu ##ded by the object were removed using the object mask rendered from our estimated pose . in ( b ) , we can still obtain a good pose despite the large o ##cc ##lusion by the bench vis ##e . in ( c ) and ( d ) , we also obtain very good estimates despite large o ##cc ##lusion ##s , the similarities between the objects , and the fact that the sy ##mme ##tries challenge the learning algorithms . outdoor or on spec ##ular objects . in addition , they drain the batteries of mobile devices , being an active sensor . it is therefore desirable to rely only on color images for 3d pose estimation , even if it is more challenging . recent methods [ reference ] [ reference ] [ reference ] work by identifying the ' object coordinates ' of the pixels , which are the pixels ' 3d coordinates in a coordinate system related to the object [ reference ] . the object 3d pose can then be estimated using a p ##np algorithm from these 2d - 3d correspondence ##s . [ reference ] obtain similar correspondence ##s by ass ##oc ##iating some pixels in selected parts of the object with virtual 3d points . however , obtaining these 2d - 3d correspondence ##s from local patches is difficult and the output is typically very noisy for these methods . a robust optimization is then needed to estimate the pose . in this paper , we argue for a \" ho ##listic \" approach , in the sense that we predict the pose of an object directly from its appearance , instead of identifying its individual surface points . as we will show , this approach provides significantly better results . we first detect the target objects in 2d . we show that using object segment ##ation performs better for this task compared to a standard sliding window detector , in particular in presence of partial o ##cc ##lusion . we then apply a cnn to predict the 3d pose of the detected objects . while the predicted 3d pose can be represented directly by a translation and a rotation , we achieve better accuracy by using a representation similar to the one used in [ reference ] for object parts : we predict the 2d projections of the corners of the object ' s bound ##ing box , and compute the 3d pose from these 2d - 3d correspondence ##s with a p ##np algorithm . compared to the object coordinate approaches the predictions are typically out ##lier - free , and no robust estimation is thus needed . compared to the direct prediction of the pose , this also avoids the need for a meta - parameter to balance the translation and rotation terms . unfortunately , this simple approach performs badly on the recent and challenging t - less data ##set . this data ##set is made of manufactured objects that are not only similar to each other , but also have one axis of rotational symmetry . for example , the squared box of fig . 1 ( c ) has an angle of symmetry of 90 \u2022 and the other object has an angle of symmetry of 0 \u2022 since it is an object of revolution ; object # 5 in fig . 1 ( d ) is not perfectly symmetrical but only because of the small screw on the top face . the approach described above fails on these objects because it tries to learn a mapping from the image space to the pose space . since two images of a symmetrical object under two different poses look identical , the image - pose correspondence is in fact a one - to - many relationship . this issue is actually not restricted to our approach . for example , [ reference ] , which relies on object coordinates , does not provide results on the bowl object of the line ##mo ##d data ##set , an object with an axis of symmetry : it is not clear which coordinates should be assigned to the 3d points of this object , as all the points on a circle orthogonal to the axis of symmetry have the same appearance . to solve this problem , we train the method described above using images of the object under rotation in a restricted range , such that the training set does not contain ambiguous images . in order to recover the object pose under a larger range of rotation , we train a class ##ifer to tell under which range the object rotation is . again , this is easy to do with a \" ho ##listic \" approach , and this class ##ifier takes an image of the entire object as input . as we will explain in more details , we can then always use the cnn trained on the restricted range to estimate any pose . in addition , we will show how to adapt this idea to handle \" approx ##ima ##tively symmetrical \" objects like object # 5 . this approach allows us to obtain good performance on the t - less data ##set . finally , we show that we can add an optional last step to ref ##ine the pose estimates by using the \" feedback loop \" proposed in [ reference ] for hand detection in depth images : we train a network to improve the prediction of the 2d projections by comparing the input image and a rendering of the object for the initial pose estimate . this allows us to improve even more our results on the line ##mo ##d and o ##cc ##lusion data ##set ##s . our full approach , which we call bb ##8 , for the 8 corners of the bound ##ing box , is also very fast , as it only requires to apply deep networks to the input image a few times . in the remainder of the paper , we first discuss related work , describe our approach , and compare it against the state - of ##the - art on the three available data ##set ##s . section : related work the literature on 3d object detection is very large , thus we will focus only on recent works . key ##point - based methods [ reference ] [ reference ] were popular for a long time and perform well but only on very texture ##d objects . the app ##ari ##tion of inexpensive 3d cameras favored the development of methods suitable for un ##text ##ured objects : [ reference ] [ reference ] rely on depth data only and use votes from pairs of 3d points and their normal ##s to detect 3d objects . [ reference ] uses a decision tree applied to r ##gb - d images to simultaneously recognize the objects and predict their poses . [ reference ] [ reference ] consider a template - based representation computed from r ##gb - d or r ##gb data , which allows for large scale detection [ reference ] . however , this template approach is sensitive to partial o ##cc ##lusion ##s . to tackle cl ##utter and partial o ##cc ##lusion ##s , [ reference ] and [ reference ] rely on local patches recognition performed with random forests . in particular , [ reference ] considers ' 3d object coordinates ' : a random forest is trained to predict the 3d location in the object coordinate system of each image location . the prediction of this forest is integrated in an energy function together with a term that compares the depth map with a rendering of the object and a term that penal ##izes pixels that lie on the object rendering but predicted by the forest to not be an object point . this energy function is opt ##imi ##zed by a ran ##sa ##c procedure . [ reference ] replaces this energy function by an energy computed from the output of a cnn trained to compare observed image features and features computed from a 3d rendering of the potentially detected object . this makes the approach very robust to partial o ##cc ##lusion ##s . these works , however , are designed for r ##gb - d data . [ reference ] extends this work and relies on r ##gb data only , as we do . they use auto - context [ reference ] to obtain better predictions from the random forests , estimate a distribute over the object coordinates to handle the prediction uncertain ##ties better , and propose a more sophisticated ran ##sa ##c - like method that scales with the number of objects . this results in an efficient and accurate method , however , robust ##ness to partial o ##cc ##lusion ##s are not demonstrated . [ 3 ] is related to [ reference ] [ reference ] [ reference ] but focuses on providing sparse 2d - 3d correspondence ##s from reliable object parts . unfortunately , it provides results on its own data ##set only , not on more broadly available data ##set ##s . like us , [ reference ] relies on a cnn to directly predict a 3d pose , but in the form of a translation and a rotation . it considers camera re ##lo ##cal ##isation in urban environment rather than 3d object detection , and uses the full image as input to the cnn . by predicting the 2d projections of the corners of the bound ##ing box , we avoid the need for a meta - parameter to balance the position and orientation errors . as shown in our experiments , the pose appears to be more accurate when predicted in this form . intuitive ##ly , this should not be surprising , as predicting 2d locations from a color images seems easier than predicting a 3d translation and a qu ##ater ##nio ##n , for example . [ 6 ] also uses a cnn to predict the 3d pose of generic objects but from r ##gb - d data . it first segments the objects of interest to avoid the influence of cl ##utter . we tried segment ##ing the objects before predicting the pose as well , however , this performed poorly on the line ##mo ##d data ##set , because the segment ##ed si ##lho ##ut ##tes were not very accurate , even with state - of - the - art segment ##ation methods . in summary , our method appears to be one of the first to deal with r ##gb data only to detect 3d objects and estimate their poses on recent data ##set ##s . as we will show in the experiments , it out ##per ##forms the accuracy of the state - of - the ##art [ reference ] by a large margin . section : proposed approach in our approach , we first find the objects in 2d , we obtain a first estimate of the 3d poses , including objects with a rotational symmetry , and we finally ref ##ine the initial pose estimates . we describe each step in this section . section : local ##izing the objects in 2d we first identify the 2d centers of the objects of interest in the input images . we could use a standard 2d object detector , but we developed an approach based on segment ##ation that resulted in better performance as it can provide accurate locations even under partial o ##cc ##lusion ##s . compared to our initial tests using a sliding window , this ap - pro ##ach improved our 2d detection results from about 75 % to 98 . 8 % correct detection rate based on a io ##u of 0 . 5 . we only need a low resolution segment ##ation and thus do not need a hour ##glass - shaped architecture [ reference ] , which makes our segment ##ation more efficient . as shown in fig . 2 , our approach performs a two - level coarse - to - fine object segment ##ation . for each level , we train a single network for all the objects . the first network is obtained by replacing the last layer of v ##gg [ reference ] by a fully connected layer with the required number of output required by each step , and fine - tune it . the second network has a simple , ad hoc architecture . more exactly , the first network is trained to provide a very low resolution binary segment ##ation of the objects given an image region j of size 128 ##\u00d7 128 by mini ##mi ##zing the following objective function : where t s is a training set made of image regions j , and the corresponding segment ##ations s for object o , ( f is the output of network f 1 ##\u03c6 for region j and object o . \u03c6 denotes the network ' s parameters , opt ##imi ##zed during training . for the line ##mo ##d and o ##cc ##lusion data ##set ##s , there is at most one object for a given region j , but more objects can be present for the t - less data ##set . at run - time , to get the segment ##ations , we compute : where s 1 , o is a 8 ##\u00d7 8 binary segment ##ation of j for object o , and ##\u03c4 1 is a threshold used to bin ##ari ##ze the network ' s output . to obtain a binary segment ##ation for the full input image , we split this image into regions and compute the s 1 , o for each region . this gives us one binary segment ##ation s 1 , o for the full input image , and each possible object . this usually results in a single connected component per visible object ; if several components are present , we keep only the largest one for each object . if the largest component in a segment ##ation s 1 , o is small , object o is likely not visible . for the remaining object ( s ) , we ref ##ine the shape of the largest component by applying a second network to each 16 ##\u00d7 16 image patch p that corresponds to an active location in s 1 : using notation ##s similar to the ones in e ##q . [ reference ] . since the input to f 2 ##\u03c8 ( p ) has a low resolution , we do not need a complex network such as v ##gg [ reference ] , and we use a much simpler architecture with 2 con ##vo ##lu ##tion ##al layers and 2 pool ##ing layers . we finally obtain a segment ##ation s 2 , o with resolution 64 ##\u00d7 48 for the full input image and each visible object o . we therefore get the identities o of the visible object ( s ) , and for these objects , we use the segment ##ation centro ##ids as their 2d centers , to compute the 3d poses of the objects as described below . section : predicting the 3d pose we predict the 3d pose of an object by applying a deep network to an image window w centered on the 2d object center estimated as described in the previous section . as for the segment ##ation , we use v ##gg [ reference ] as a basis for this network . this allows us to handle all the objects of the target data ##set with a single network . it is possible to directly predict the pose in the form of a 3 - vector and an exponential map for example , as in [ reference ] . however , a more accurate approach was proposed in [ reference ] for predicting the poses of object parts . to apply it here , we minimize the following cost function over the parameters ##\u03b8 of network g ##\u03b8 : where t is a training set made of image windows w containing object o under a pose defined by an exponential map e and a 3 - vector t . the m o i are the 3d coordinates of the corners of the bound ##ing box of object o in the object coordinate system . pro ##j e , t ( m ) projects the 3d point m on the image from the pose defined by e and t . returns the two components of the output of g ##\u03b8 corresponding to the predicted 2d coordinates of the i - th corner for object o . \u2022 mod ##ulo 180 \u2022 ( c ) . our solution is to restrict the range during training to be between 0 \u2022 and 90 \u2022 . we use a class ##ifier to detect if the pose in an input image is between 90 \u2022 and 180 \u2022 . if this is the case ( d ) , we mirror the input image ( e ) , and mirror back the predicted projections for the corners ( f ) . at run - time , the segment ##ation gives the identity and the 2d locations of the visible object ( s ) o . the 3d pose can then be estimated for the correspondence ##s between the 3d points m o i and the predicted m i ( ( g ##\u03b8 ( w ) ) [ o ] ) using a p ##np algorithm . other 3d points could be used here , however , the corners of the bound ##ing box are a natural choice as they frame the object and are well spread in space 1 . section : handling objects with an axis of symmetry if we apply the method described so far to the t - less data ##set , the performances are significantly lower than the performances on the line ##mo ##d data ##set . as mentioned in the introduction , this is because training images w in e ##q . ( 4 ) for the objects of this data ##set can be identical while having very different expected predictions pro ##j e , t ( m o i ) , because of the rotational symmetry of the objects . we first remark that for an object with an angle of symmetry ##\u03b1 , its 3d rotation around its axis of symmetry can be defined only mod ##ulo ##\u03b1 , not 2 ##\u03c0 . for an object with an angle of symmetry ##\u03b1 , we can therefore restrict the poses used for training to the poses where the angle of rotation around the symmetry axis is within the range [ 0 ; \u03b1 [ , to avoid the ambiguity between images . however , this solve ##s our problem only partially : images at one ex ##tre ##mity of this range of poses and the images at the other ex ##tre ##mity , while not identical , still look very similar . as a result , for input images with an angle of rotation close to 0 mod ##ulo ##\u03b1 , the pose prediction can still be very bad , as illustrated in fig . 3 . to explain our solution , let us first denote by ##\u03b2 the rotation angle , and introduce the intervals r 1 = [ 0 ; \u03b1 / 2 [ and r 2 = [ \u03b1 / 2 ; \u03b1 [ . to avoid ambiguity , we restrict ##\u03b2 to be in r 1 for the training images used in the optimization problem of e ##q . ( 4 ) . the draw ##back is of course that , without doing anything else , we would not be able to estimate the poses when ##\u03b2 is in r 2 . we therefore introduce a cnn class ##ifier k ( \u00b7 ) to predict at run - time if ##\u03b2 is in r 1 or r 2 : if ##\u03b2 is in r 1 , we can estimate the pose as before ; if ##\u03b2 is in r 2 , one option would be to apply another g ##\u03b8 ( \u00b7 ) network trained for this range . however , it is actually possible to use the same network g ##\u03b8 ( \u00b7 ) for both r 1 and r 2 , as follows . if the class ##ifier predict ##s that ##\u03b2 in in r 2 , we mirror the input image w : as illustrated in fig . 3 ( e ) , the object appears in the mirror image with a rotation angle equal to ##\u03b1 ##\u2212 ##\u03b2 , which is in r 1 . therefore we can apply g ##\u03b8 ( \u00b7 ) to the mirrored w . to obtain the correct pose , we finally mirror back the projections of the corners predicted by g ##\u03b8 ( \u00b7 ) . we currently consider the case where the axis of symmetry is more or less vertical in the image , and mirror the image from left to right . when the axis is closer to be horizontal , we should mirror the image from top to bottom . objects of revolution are a special and simpler case : since their angle of symmetry is 0 \u2022 , we predict their poses under the same angle of rotation . for training the pose predict ##or g ##\u03b8 ( \u00b7 ) , we use the original training images with angles of rotation in r 1 , and mirror the training images with angles of rotation in r 2 . handling objects that are ' not exactly symmetrical ' as mentioned in the introduction , some objects of the t - less data ##set are only approximately symmetrical , such as object # 5 in fig . 1 ( d ) . the small details that make the object not perfectly symmetrical , however , do not help the optimization problem of e ##q . ( 4 ) , but we would still like to predict the pose of this object . in the case of object # 5 , we consider 4 regions instead of 2 : r 1 = [ 0 ; \u03c0 / 2 [ , r 1 = [ \u03c0 / 2 ; \u03c0 [ , r 3 = [ \u03c0 ; 3 ##\u03c0 / 2 [ , and r 4 = [ 3 ##\u03c0 / 2 ; 2 ##\u03c0 [ , and we train the class ##ifier k ( \u00b7 ) to predict in which of these four regions the angle of rotation ##\u03b2 is . if ##\u03b2 ##\u2208 r 2 or ##\u03b2 ##\u2208 r 4 , we mirror the image before computing the pose as before . then , if ##\u03b2 ##\u2208 r 3 or ##\u03b2 ##\u2208 r 4 , we still have to add ##\u03c0 to the angle of rotation of the recovered pose to get an angle between 0 and 2 ##\u03c0 . section : refining the pose we also introduce an optional additional stage to improve the accuracy of the pose estimates inspired by [ reference ] . as illustrated in fig . 4 , we train another cnn that predict ##s an update to improve the pose . because this cnn takes 4 or 6 channels as input , it is not clear how we can use v ##gg , as we did for the previously introduced networks , and we use here one cnn per object . however , this stage is optional , and without it , we already out ##per ##form the - state - of - the - art . the first image is the image window w as for g ##\u03b8 ( \u00b7 ) . the second image depends on the current estimate of the pose : while [ reference ] generates a depth map with a deep network , we render ( using open ##gl ) either a binary mask or a color rendering of the target object as seen from this current estimate . more formally we train this cnn by mini ##mi ##zing : where h ##\u00b5 denotes the cnn , \u00b5 its parameters ; n ( e , t ) is a set of poses sampled around pose ( e , t ) , and render ( e , t ) a function that returns a binary mask , or a color rendering , of the target object seen from pose ( e , t ) . at run - time , given a current estimate of the object pose represented by the projections of the corners ##v = [ . . . m i . . . ] , and the corresponding parameter ##isation ( [UNK] , t ) , we can update this estimate by in ##voking h ##\u00b5 ( \u00b7 ) : section : generating training images in section 4 , we will compare our method to the state ##of - the art for 3d object detection in color images [ reference ] , and like them , for each of 15 objects of the line ##mo ##d data ##set , we use 15 % of the images for training and use the rest for testing . the training images are selected as in [ reference ] , such that relative orientation between them should be larger than a threshold . we also tried a random selection , and there was only a slight drop in performance , for some objects only . the selection method thus does not seem critical . the t - less data ##set provides regularly sampled training images . as shown in fig . 5 , we also use a similar method as [ reference ] to aug ##ment the training set : we extract the objects ' silhouette ##s from these images , which can be done as the ground figure 5 . two generated training images for different objects from the line ##mo ##d data ##set [ reference ] . the object is shifted from the center to handle the ina ##cc ##ura ##cy of the detection method , and the background is random to make sure that the network g ##\u03b8 can not exploit the context specific to the data ##set . truth poses and the objects ' 3d models are available . note that this means the results are not influenced by the scene context , which makes the pose estimation more difficult . to be robust to cl ##utter and scale changes , we scale the segment ##ed objects by a factor of s ##\u2208 [ 0 . 8 , 1 . 2 ] , and change the background by a patch extracted from a randomly picked image from the image ##net data ##set [ reference ] . moreover , the object is shifted by some pixels from the center of the image window in both x and y directions . this helps us to handle small object local ##ization errors made during detection . section : experiments in this section , we present and discuss the results of our evaluation . we first describe the three evaluation metric ##s used in the literature and in this paper . we evaluate our method on all the possible data ##set ##s with color images for instance 3d detection and pose estimation we are aware of : the line ##mo ##d [ reference ] , o ##cc ##lusion [ reference ] , and t - less [ reference ] data ##set ##s . section : evaluation metric ##s as in [ reference ] , we use the percentage of correctly predicted poses for each sequence and each object , where a pose is considered correct if it passes the tests presented below . section : 2d projections [ 2 ] this is a metric suited for applications such as augmented reality . a pose is considered correct if the average of the 2d distances between the projections of the object ' s vertices from the estimated pose and the ground truth pose is less than 5 pixels . 6 ##d pose [ reference ] with this metric , a pose is considered correct if the average of the 3d distances between the transformed of the object ' s vertices table 1 . evaluation using the 2d projections metric of using the 2d projections of the bound ##ing box ( ' bb ' ) , compared to the direct prediction of the pose ( ' direct ' ) , and of the ref ##ine ##ment methods . for this evaluation , we used the ground truth 2d object center to avoid the influence of the detection . for the objects marked with a ( * ) , we opt ##imi ##ze the value of the weight balancing the rotation and translation terms on the test set , giving an advantage to the ' direct ' pose method . for the other objects , we used the value that is optimal for both the ape and the drill ##er . is less than 10 % of the object ' s diameter . v is the set of the object ' s vertices , ( [UNK] , t ) the estimated pose and ( [UNK] , t ) the ground truth pose , and tr e , t ( \u00b7 ) a rigid transformation by rotation e , translation t . for the objects with am ##bi ##gio ##us poses due to sy ##mme ##tries , [ reference ] replaces this measure by : 5 cm 5 \u2022 metric [ reference ] with this metric , a pose is considered correct if the translation and rotation errors are below 5 cm and 5 \u2022 respectively . section : contributions of the different steps the columns ' bb ' , ' mask ref . [ reference ] , and ' r ##gb ref . ' of table 1 compare the results of our method before and after two iteration ##s of ref ##ine ##ment , using either a binary mask or a color rendering . for this evaluation , we used the ground truth 2d object center to avoid the influence of the detection . using ref ##ine ##ment improves the results on average by 4 . 5 % and 6 . 3 % for the mask and color rendering respectively . using a color rendering systematically yields the best results , but using the binary mask yields already a significant improvement , showing that an un ##text ##ured model can be used . [ reference ] and our method without and with r ##gb ref ##ine ##ment using our segment ##ation - based method to obtain the 2d object centers on the line ##mo ##d data ##set . [ reference ] does not provide results for the bowl and the cup , hence for the sake of comparison the average is taken over the first 13 objects . section : the line ##mo ##d data ##set : comparison with [ 2 ] table 2 compares our bb ##8 method with and without r ##gb ref ##ine ##ment against the one presented in [ reference ] on the line ##mo ##d data ##set . because of lack of space , we provide the results without ref ##ine ##ment only for the 2d projection metric , however , the results for the other metric ##s are comparable . for this evaluation , we used the results of our detection method presented in section 3 . 1 , not the ground truth 2d object center . our method out ##per ##forms [ reference ] by a large margin : 15 . 6 % for 2d projection , 12 . 6 % for 6 ##d pose and 28 . 4 % for the 5 cm 5 \u2022 metric . fig . 7 shows qu ##ali ##tative results for our method on this data ##set . for most of the images , the two bound ##ing boxes , for the ground truth pose and for the pose we estimate , overlap almost perfectly . section : the o ##cc ##lusion data ##set : robust ##ness to partial o ##cc ##lusion ##s the o ##cc ##lusion data ##set was created by [ reference ] from the line ##mo ##d data ##set . the partial o ##cc ##lusion ##s make it significantly more difficult , and to the best of our knowledge , the only published results use both color and depth data . [ reference ] provide results using only color images , but limited to 2d detection , not 3d pose estimation . we only use images from the line ##mo ##d data ##set to generate our training images by using the approach explained in section 3 . 5 , except that we also randomly super ##im ##pose objects extracted from the other sequences to the target ob - je ##ct to be robust to o ##cc ##lusion ##s . we do not use any image of the test sequence to avoid having o ##cc ##lusion ##s similar to the ones presented in the test sequence . although all the poses in the test sets are not visible in the training sequences , we can estimate accurate poses with a 2d projection error lower than 15 ##p ##x for about 80 % of the frames for these seven objects . we do not report the performance of our method for the egg ##box , as more than 70 % of close poses are not seen in the training sequence . some qu ##ali ##tative results are shown in the second row of fig . 7 . to the best of our knowledge , we are the first to present results on this data ##set using color images only . section : the t - less data ##set : handling objects with an axis of symmetry the test sequences of the t - less data ##set are very challenging , with sometimes multiple instances of the same objects and a high amount of cl ##utter and o ##cc ##lusion . we considered only scenes # 1 , # 2 , # 4 , # 5 , and # 7 in our experiments . it is also difficult to compare against the only published work on t - less [ reference ] , as it provides the 6 ##d pose metric averaged per object or per scene , computed using r ##gb - d data , while , to the best of our knowledge , we are the first to report results on the t - less data ##set using r ##gb images figure 7 . some qu ##ali ##tative results . first row : line ##mo ##d data ##set ; second row : o ##cc ##lusion data ##set ; third row : t - less data ##set ( for objects of revolution , we represent the pose with a cylinder rather than a box ) ; last row : some failure cases . from left to right : an example of a pose rejected by the 2d projections metric , a failure due to the lack of corresponding poses in the training set , two examples from t - less rejected by the 6 ##d pose metric , and one failure due to the fact that some objects are made of several instances of another object . table 3 . our quantitative results on t - less [ reference ] . most of the errors are along the z axis of the camera , as we rely on color images . only . similarly to [ reference ] , we evaluate the poses with more than 10 % of the object surface visible in the ground truth poses . as shown in table 3 , the 6 ##d pose average per scene with our method is 54 % . the object 3d orientation and translation along the x and y axes of the camera are typically very well estimated , and most of the error is along the z axis , which should not be surprising for a method using color images only . section : computation times our implementation takes 140 ms for the segment ##ation , 130 ms for the pose prediction , and 21 ms for each ref ##ine ##ment iteration , on an intel core i ##7 - 58 ##20 k 3 . 30 ghz desktop with a ge ##force titan x . if there is only one object of interest , we can replace v ##gg by a specific network with a simpler architecture , the computation times then become 20 ms for the segment ##ation and 12 ms for the pose prediction , with similar accuracy . section : conclusion our \" ho ##listic \" approach , made possible by the remarkable abilities of deep networks for regression , allowed us to significantly advance the state - of - the - art on 3d pose estimation from color images , even on challenging objects from the t - less data ##set . section : section : ac ##k ##now ##led ##gm ##ent : this work was funded by the christian do ##pp ##ler laboratory for semantic 3d computer vision . section :",
        "pred_seq": "t ##set [SEP] bb ##8 [SEP] [SEP] 3d detection [SEP] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] [unused0] bb bb bb bb bb bb bb ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 ##8 [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "tless dataset"
                    ]
                ],
                "Method": [
                    [
                        "bb8"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "3d object detection"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "linemod dataset",
                        "linemod"
                    ]
                ],
                "Method": [
                    [
                        "bb8"
                    ]
                ],
                "Metric": [
                    [
                        "accuracy"
                    ]
                ],
                "Task": []
            }
        ]
    },
    "58": {
        "doctext": "document : simple and accurate dependency par ##sing using bid ##ire ##ction ##al l ##st ##m feature representations we present a simple and effective scheme for dependency par ##sing which is based on bid ##ire ##ction ##al - l ##st ##ms ( bi ##ls ##tm ##s ) . each sentence token is associated with a bi ##ls ##tm vector representing the token in its sent ##ential context , and feature vectors are constructed by con ##cate ##nat ##ing a few bi ##ls ##tm vectors . the bi ##ls ##tm is trained jointly with the par ##ser objective , resulting in very effective feature extract ##ors for par ##sing . we demonstrate the effectiveness of the approach by applying it to a greedy transition - based par ##ser as well as to a globally opt ##imi ##zed graph - based par ##ser . the resulting par ##ser ##s have very simple architecture ##s , and match or sur ##pass the state - of - the - art acc ##ura ##cies on english and chinese . no ##ite ##ms ##ep , tops ##ep = 0 ##pt , par ##se ##p = 0 ##pt , part ##ops ##ep = 0 ##pt 1 . 0 ##em section : introduction the focus of this paper is on feature representation for dependency par ##sing , using recent techniques from the neural - networks ( ' ' deep learning ' ' ) literature . modern approaches to dependency par ##sing can be broadly categorized into graph - based and transition - based par ##ser ##s . graph - based par ##ser ##s treat par ##sing as a search - based structured prediction problem in which the goal is learning a scoring function over dependency trees such that the correct tree is scored above all other trees . transition - based par ##ser ##s treat par ##sing as a sequence of actions that produce a par ##se tree , and a class ##ifier is trained to score the possible actions at each stage of the process and guide the par ##sing process . perhaps the simplest graph - based par ##ser ##s are arc - factor ##ed ( first order ) models , in which the scoring function for a tree deco ##mp ##oses over the individual arcs of the tree . more elaborate models look at larger ( overlapping ) parts , requiring more sophisticated inference and training algorithms . the basic transition - based par ##ser ##s work in a greedy manner , performing a series of locally - optimal decisions , and bo ##ast very fast par ##sing speeds . more advanced transition - based par ##ser ##s introduce some search into the process using a beam or dynamic programming . regardless of the details of the par ##sing framework being used , a crucial step in par ##ser design is choosing the right feature function for the underlying statistical model . recent work ( see section [ reference ] for an overview ) attempt to alleviate parts of the feature function design problem by moving from linear to non - linear models , enabling the model ##er to focus on a small set of ' ' core ' ' features and leaving it up to the machine - learning machinery to come up with good feature combinations . however , the need to carefully define a set of core features remains . for example , the work of chen ##20 ##14 ##fast uses 18 different elements in its feature function , while the work of pei ##20 ##15 ##ef ##fect ##ive uses 21 different elements . other works , notably dyer ##20 ##15 ##tra ##ns ##ition ##base ##d and le ##20 ##14 ##ins ##ide ##outs ##ide , propose more sophisticated feature representations , in which the feature engineering is replaced with architecture engineering . in this work , we suggest an approach which is much simpler in terms of both feature engineering and architecture engineering . our proposal ( section [ reference ] ) is centered around bi ##rn ##ns , and more specifically bi ##ls ##tm ##s , which are strong and train ##able sequence models ( see section [ reference ] ) . the bi ##ls ##tm excel ##s at representing elements in a sequence ( i . e . , words ) together with their contexts , capturing the element and an ' ' infinite ' ' window around it . we represent each word by its bi ##ls ##tm encoding , and use a con ##cate ##nation of a minimal set of such bi ##ls ##tm encoding ##s as our feature function , which is then passed to a non - linear scoring function ( multi - layer per ##ce ##pt ##ron ) . crucial ##ly , the bi ##ls ##tm is trained with the rest of the par ##ser in order to learn a good feature representation for the par ##sing problem . if we set aside the inherent complexity of the bi ##ls ##tm itself and treat it as a black box , our proposal results in a pleasing ##ly simple feature extract ##or . we demonstrate the effectiveness of the approach by using the bi ##ls ##tm feature extract ##or in two par ##sing architecture ##s , transition - based ( section [ reference ] ) as well as a graph - based ( section [ reference ] ) . in the graph - based par ##ser , we jointly train a structured - prediction model on top of a bi ##ls ##tm , prop ##aga ##ting errors from the structured objective all the way back to the bi ##ls ##tm feature - en ##code ##r . to the best of our knowledge , we are the first to perform such end - to - end training of a structured prediction model and a rec ##urrent feature extract ##or for non - sequential outputs . aside from the novelty of the bi ##ls ##tm feature extract ##or and the end - to - end structured training , we rely on existing models and techniques from the par ##sing and structured prediction literature . we stick to the simplest par ##ser ##s in each category - greedy inference for the transition - based architecture , and a first - order , arc - factor ##ed model for the graph - based architecture . despite the simplicity of the par ##sing architecture ##s and the feature functions , we achieve near state - of - the - art par ##sing acc ##ura ##cies in both english ( 93 . 1 ua ##s ) and chinese ( 86 . 6 ua ##s ) , using a first - order par ##ser with two features and while training solely on tree ##bank data , without relying on semi - supervised signals such as pre - trained word em ##bed ##ding ##s , word - clusters , or techniques such as tri - training . when also including pre - trained word em ##bed ##ding ##s , we obtain further improvements , with acc ##ura ##cies of 93 . 9 ua ##s ( english ) and 87 . 6 ua ##s ( chinese ) for a greedy transition - based par ##ser with 11 features , and 93 . 6 ua ##s ( en ) / 87 . 4 ( ch ) for a greedy transition - based par ##ser with 4 features . section : background and notation paragraph : notation we use to denote a sequence of vectors . is a function parameter ##ized with parameters . we write as short ##hand for - an instant ##iation of with a specific set of parameters . we use to denote a vector con ##cate ##nation operation , and to denote an index ##ing operation taking the th element of a vector . sub ##section : feature functions in dependency par ##sing traditionally , state - of - the - art par ##ser ##s rely on linear models over hand - crafted feature functions . the feature functions look at core components ( e . g . ' ' word on top of stack ' ' , ' ' left ##most child of the second - to - top word on the stack ' ' , ' ' distance between the head and the mod ##ifier words ' ' ) , and are comprised of several template ##s , where each template instant ##iate ##s a binary indicator function over a conjunction of core elements ( resulting in features of the form ' ' word on top of stack is x and left ##most child is y and \u2026 ' ' ) . the design of the feature function - which components to consider and which combinations of components to include - is a major challenge in par ##ser design . once a good feature function is proposed in a paper it is usually adopted in later works , and sometimes t ##we ##ake ##d to improve performance . examples of good feature functions are the feature - set proposed by zhang ##11 ##ac ##l for transition - based par ##sing ( including roughly 20 core components and 72 feature template ##s ) , and the feature - set proposed by ms ##t for graph - based par ##sing , with the paper listing 18 template ##s for a first - order par ##ser , while the first order feature - extract ##or in the actual implementation ' s code ( ms ##tp ##ars ##er ) includes roughly a hundred feature template ##s . the core features in a transition - based par ##ser usually look at information such as the word - identity and part - of - speech ( po ##s ) tags of a fixed number of words on top of the stack , a fixed number of words on the top of the buffer , the mod ##ifiers ( usually left - most and right - most ) of items on the stack and on the buffer , the number of mod ##ifiers of these elements , parents of words on the stack , and the length of the spans spanned by the words on the stack . the core features of a first - order graph - based par ##ser usually take into account the word and po ##s of the head and mod ##ifier items , as well as po ##s - tags of the items around the head and mod ##ifier , po ##s tags of items between the head and mod ##ifier , and the distance and direction between the head and mod ##ifier . sub ##section : related research efforts coming up with a good feature - set for a par ##ser is a hard and time consuming task , and many researchers attempt to reduce the required manual effort . the work of lei - eta ##l : 2014 : p ##14 - 1 suggests a low - rank tensor representation to automatically find good feature combinations . tau ##bt ##abi ##b ##20 ##15 ##tem ##plate suggest a kernel - based approach to implicit ##ly consider all possible feature combinations over sets of core - features . the recent popularity of neural networks prompted a move from template ##s of sparse , binary indicator features to dense core feature encoding ##s fed into non - linear class ##ifiers . chen ##20 ##14 ##fast en ##code each core feature of a greedy transition - based par ##ser as a dense low - dimensional vector , and the vectors are then con ##cate ##nated and fed into a non - linear class ##ifier ( multi - layer per ##ce ##pt ##ron ) which can potentially capture arbitrary feature combinations . weiss ##20 ##15 ##st ##ructured showed further gains using the same approach coupled with a somewhat improved set of core features , a more involved network architecture with skip - layers , beam search - deco ##ding , and careful hyper - parameter tuning . pei ##20 ##15 ##ef ##fect ##ive apply a similar methodology to graph - based par ##sing . while the move to neural - network class ##ifiers alleviate ##s the need for hand - craft ##ing feature - combinations , the need to carefully define a set of core features remain . for example , the feature representation in chen ##20 ##14 ##fast is a con ##cate ##nation of 18 word vectors , 18 po ##s vectors and 12 dependency - label vectors . the above works tackle the effort in hand - craft ##ing effective feature combinations . a different line of work attacks the feature - engineering problem by suggesting novel neural - network architecture ##s for encoding the par ##ser state , including intermediate ##ly - built sub ##tree ##s , as vectors which are then fed to non - linear class ##ifiers . tito ##v and henderson en ##code the par ##ser state using inc ##rem ##ental si ##gm ##oid - belief networks tito ##v - henderson : 2007 : i ##w ##pt ##200 ##7 . in the work of dyer ##20 ##15 ##tra ##ns ##ition ##base ##d , the entire stack and buffer of a transition - based par ##ser are encoded as a stack - l ##st ##ms , where each stack element is itself based on a composition ##al representation of par ##se trees . le ##20 ##14 ##ins ##ide ##outs ##ide en ##code each tree node as two composition ##al representations capturing the inside and outside structures around the node , and feed the representations into a re ##rank ##er . a similar re ##rank ##ing approach , this time based on con ##vo ##lu ##tion ##al neural networks , is taken by zhu ##20 ##15 ##rera ##nk ##ing . finally , in ki ##per ##was ##ser ##20 ##16 ##ef we present an easy - first par ##ser based on a novel hierarchical - l ##st ##m tree encoding . in contrast to these , the approach we present in this work results in much simpler feature functions , without resort ##ing to elaborate network architecture ##s or composition ##al tree representations . work by vin ##lay ##s ##20 ##14 ##gram ##mar employs a sequence - to - sequence with attention architecture for constituency par ##sing . each token in the input sentence is encoded in a deep - bi ##ls ##tm representation , and then the token ##s are fed as input to a deep - l ##st ##m that predict ##s a sequence of bracket ##ing actions based on the already predicted bracket ##ing as well as the encoded bi ##ls ##tm vectors . a train ##able attention mechanism is used to guide the par ##ser to relevant bi ##ls ##tm vectors at each stage . this architecture shares with ours the use of bi ##ls ##tm encoding and end - to - end training . the sequence of bracket ##ing actions can be interpreted as a sequence of shift and reduce operations of a transition - based par ##ser . however , while the par ##ser of vin ##yal ##s et al . relies on a train ##able attention mechanism for focusing on specific bi ##ls ##tm vectors , par ##ser ##s in the transition - based family we use in section [ reference ] use a human designed stack and buffer mechanism to manually direct the par ##ser ' s attention . while the effectiveness of the train ##able attention approach is impressive , the stack - and - buffer guidance of transition - based par ##ser ##s results in more robust learning . indeed , work by cross ##20 ##16 ##in ##cre ##mental , published while working on the camera - ready version of this paper , show that the same methodology as ours is highly effective also for greedy , transition - based constituency par ##sing , surpassing the beam - based architecture of vin ##yal ##s et al . ( 88 . 3 ##f vs . 89 . 8 ##f points ) when trained on the penn tree ##bank data ##set and without using orthogonal methods such as en ##se ##mbling and up - training . sub ##section : bid ##ire ##ction ##al rec ##urrent neural networks rec ##urrent neural networks ( rn ##ns ) are statistical learners for modeling sequential data . an rn ##n allows one to model the th element in the sequence based on the past - the elements up to and including it . the rn ##n model provides a framework for conditioning on the entire history without resort ##ing to the marko ##v assumption which is traditionally used for modeling sequences . rn ##ns were shown to be capable of learning to count , as well as to model line lengths and complex phenomena such as bracket ##ing and code ind ##entation . our proposed feature extract ##ors are based on a bid ##ire ##ction ##al rec ##urrent neural network ( bi ##rn ##n ) , an extension of rn ##ns that take into account both the past and the future . we use a specific flavor of rn ##n called a long short - term memory network ( l ##st ##m ) . for br ##ev ##ity , we treat rn ##n as an abstraction , without getting into the mathematical details of the implementation of the rn ##ns and l ##st ##ms . for further details on rn ##ns and l ##st ##ms , the reader is referred to goldberg - prime ##r and cho - prime ##r . the rec ##urrent neural network ( rn ##n ) abstraction is a parameter ##ized function mapping a sequence of input vectors , to a sequence of output vectors . each output vector is conditioned on all the input vectors , and can be thought of as a summary of the prefix of . in our notation , we ignore the intermediate vectors and take the output of to be the vector . a bid ##ire ##ction ##al rn ##n is composed of two rn ##ns , and , one reading the sequence in its regular order , and the other reading it in reverse . concrete ##ly , given a sequence of vectors and a desired index , the function is defined as : the vector is then a representation of the th item in , taking into account both the entire history and the entire future by con ##cate ##nat ##ing the matching rn ##n s . we can view the bi ##rn ##n encoding of an item as representing the item together with a context of an infinite window around it . paragraph : computational complexity computing the bi ##rn ##n vectors encoding of the th element of a sequence requires time for computing the two rn ##ns and con ##cate ##nat ##ing their outputs . a naive approach of computing the bid ##ire ##ction ##al representation of all elements result in computation . however , it is trivial to compute the bi ##rn ##n encoding of all sequence items in linear time by pre - computing and , keeping the intermediate representations , and con ##cate ##nat ##ing the required elements as needed . paragraph : bi ##rn ##n training initially , the bi ##rn ##n encoding ##s do not capture any particular information . during training , the encoded vectors are fed into further network layers , until at some point a prediction is made , and a loss is incurred . the back - propagation algorithm is used to compute the gradient ##s of all the parameters in the network ( including the bi ##rn ##n parameters ) with respect to the loss , and an opt ##imi ##zer is used to update the parameters according to the gradient ##s . the training procedure causes the bi ##rn ##n function to extract from the input sequence the relevant information for the task task at hand . paragraph : going deeper we use a variant of deep bid ##ire ##ction ##al rn ##n ( or - layer bi ##rn ##n ) which is composed of bi ##rn ##n functions that feed into each other : the output of becomes the input of . stack ##ing bi ##rn ##ns in this way has been empirical ##ly shown to be effective . in this work , we use bi ##rn ##ns and deep - bi ##rn ##ns interchange ##ably , specify ##ing the number of layers when needed . paragraph : historical notes rn ##ns were introduced by elm ##an ##19 ##90 ##fin ##ding , and extended to bi ##rn ##ns by schuster ##19 ##9 ##7 ##bid ##ire ##ction ##al . the l ##st ##m variant of rn ##ns is due to hoc ##hre ##iter ##19 ##9 ##7 ##long . bi ##ls ##tm ##s were recently popularized by graves ##200 ##8 ##su ##per ##vis ##ed , and deep bi ##rn ##ns were introduced to nl ##p by irs ##oy ##20 ##14 ##op ##ini ##on , who used them for sequence tag ##ging . in the context of par ##sing , lewis ##20 ##16 ##ls ##tm and va ##sw ##ani : 2016 : na ##ac ##l use a bi ##ls ##tm sequence tag ##ging model to assign a cc ##g super ##tag for each token in the sentence . lewis ##20 ##16 ##ls ##tm feeds the resulting super ##tag ##s sequence into an a * cc ##g par ##ser . va ##sw ##ani : 2016 : na ##ac ##l adds an additional layer of l ##st ##m which receives the bi ##ls ##tm representation together with the k - best super ##tag ##s for each word and outputs the most likely super ##tag given previous tags , and then feeds the predicted super ##tag ##s to a disc ##rim ##ini ##tively trained par ##ser . in both works , the bi ##ls ##tm is trained to produce accurate cc ##g super ##tag ##s , and is not aware of the global par ##sing objective . section : our approach we propose to replace the hand - crafted feature functions in favor of minimal ##ly - defined feature functions which make use of automatically learned bid ##ire ##ction ##al l ##st ##m representations . given - words input sentence with words together with the corresponding po ##s tags , we associate each word and po ##s with em ##bed ##ding vectors and , and create a sequence of input vectors in which each is a con ##cate ##nation of the corresponding word and po ##s vectors : the em ##bed ##ding ##s are trained together with the model . this en ##codes each word in isolation , disregard ##ing its context . we introduce context by representing each input element as its ( deep ) bi ##ls ##tm vector , : our feature function is then a con ##cate ##nation of a small number of bi ##ls ##tm vectors . the exact feature function is par ##ser dependent and will be discussed when discussing the corresponding par ##ser ##s . the resulting feature vectors are then scored using a non - linear function , namely a multi - layer per ##ce ##pt ##ron with one hidden layer ( ml ##p ) : where are the model parameters . beside using the bi ##ls ##tm - based feature functions , we make use of standard par ##sing techniques . crucial ##ly , the bi ##ls ##tm is trained jointly with the rest of the par ##sing objective . this allows it to learn representations which are suitable for the par ##sing task . consider a con ##cate ##nation of two bi ##ls ##tm vectors ( ) scored using an ml ##p . the scoring function has access to the words and po ##s - tags of and , as well as the words and po ##s - tags of the words in an infinite window surrounding them . as l ##st ##ms are known to capture length and sequence position information , it is very plausible that the scoring function can be sensitive also to the distance between and , their ordering , and the sequential material between them . paragraph : par ##sing - time complexity once the bi ##ls ##tm is trained , par ##sing is performed by first computing the bi ##ls ##tm encoding for each word in the sentence ( a linear time operation ) . then , par ##sing proceeds as usual , where the feature extraction involves a con ##cate ##nation of a small number of the pre - computed vectors . section : transition - based par ##ser we begin by integrating the feature extract ##or in a transition - based par ##ser . we follow the notation in ta ##cl ##20 ##13 ##dy ##nami ##c . the transition - based par ##sing framework assumes a transition system , an abstract machine that processes sentences and produces par ##se trees . the transition system has a set of configurations and a set of transitions which are applied to configurations . when par ##sing a sentence , the system is initial ##ized to an initial configuration based on the input sentence , and transitions are repeatedly applied to this configuration . after a finite number of transitions , the system arrives at a terminal configuration , and a par ##se tree is read off the terminal configuration . in a greedy par ##ser , a class ##ifier is used to choose the transition to take in each configuration , based on features extracted from the configuration itself . the par ##sing algorithm is presented in algorithm [ reference ] below . [ h ] greedy transition - based par ##sing [ 1 ] input : sentence , parameter ##ized function with parameters . not given a sentence , the par ##ser is initial ##ized with the configuration ( line [ reference ] ) . then , a feature function represents the configuration as a vector , which is fed to a scoring function score assign ##ing scores to ( configuration , transition ) pairs . score scores the possible transitions , and the highest scoring transition is chosen ( line [ reference ] ) . the transition is applied to the configuration , resulting in a new par ##ser configuration . the process ends when reaching a final configuration , from which the resulting par ##se tree is read and returned ( line [ reference ] ) . transition systems differ by the way they define configurations , and by the particular set of transitions available to them . a par ##ser is determined by the choice of a transition system , a feature function and a scoring function score . our choices are detailed below . paragraph : the arc - hybrid system many transition systems exist in the literature . in this work , we use the arc - hybrid transition system , which is similar to the more popular arc - standard system , but for which an efficient dynamic oracle is available . in the arc - hybrid system , a configuration consists of a stack , a buffer , and a set of dependency arcs . both the stack and the buffer hold integer indices pointing to sentence elements . given a sentence , the system is initial ##ized with an empty stack , an empty arc set , and , where is the special root index . any configuration with an empty stack and a buffer containing only is terminal , and the par ##se tree is given by the arc set of . the arc - hybrid system allows 3 possible transitions , shift , and , defined as : the shift transition moves the first item of the buffer ( ) to the stack . the left ##\u2113 transition removes the first item on top of the stack ( ) and attache ##s it as a mod ##ifier to with label , adding the arc . the right ##\u2113 transition removes from the stack and attache ##s it as a mod ##ifier to the next item on the stack ( ) , adding the arc . paragraph : scoring function traditionally , the scoring function is a disc ##rim ##ina ##tive linear model of the form . the linear ##ity of score required the feature function to en ##code non - linear ##ities in the form of combination features . we follow chen and manning chen ##20 ##14 ##fast and replace the linear scoring model with an ml ##p . paragraph : simple feature function the feature function is typically complex ( see section [ reference ] ) . our feature function is the con ##cate ##nated bi ##ls ##tm vectors of the top 3 items on the stack and the first item on the buffer . i . e . , for a configuration the feature extract ##or is defined as : this feature function is rather minimal : it takes into account the bi ##ls ##tm representations of and , which are the items affected by the possible transitions being scored , as well as one extra stack context . figure 1 depicts transition scoring with our architecture and this feature function . note that , unlike previous work , this feature function does not take into account , the already built structure . the high par ##sing acc ##ura ##cies in the experimental sections suggest that the bi ##ls ##tm encoding is capable of est ##imating a lot of the missing information based on the provided stack and buffer elements and the sequential content between them . while not explored in this work , relying on only four word indices for scoring an action results in very compact state signatures , making our proposed feature representation very appealing for use in transition - based par ##ser ##s that employ dynamic - programming search . paragraph : extended feature function one of the benefits of the greedy transition - based par ##sing framework is precisely its ability to look at arbitrary features from the already built tree . if we allow somewhat less minimal feature function , we could add the bi ##ls ##tm vectors corresponding to the right - most and left - most mod ##ifiers of , and , as well as the left - most mod ##ifier of , reaching a total of 11 bi ##ls ##tm vectors . we refer to this as the extended feature set . as we ' ll see in section [ reference ] , using the extended set does indeed improve par ##sing acc ##ura ##cies when using pre - trained word em ##bed ##ding ##s , but has a minimal effect in the fully - supervised case . sub ##section : details of the training algorithm the training objective is to set the score of correct transitions above the scores of incorrect transitions . we use a margin - based objective , aiming to maximize the margin between the highest scoring correct action and the highest scoring incorrect action . the hi ##nge loss at each par ##sing configuration is defined as : where is the set of possible transitions and is the set of correct ( gold ) transitions at the current stage . at each stage of the training process the par ##ser scores the possible transitions , inc ##urs a loss , selects a transition to follow , and moves to the next configuration based on it . the local losses are sum ##med throughout the par ##sing process of a sentence , and the parameters are updated with respect to the sum of the losses at sentence boundaries . the gradient ##s of the entire network ( including the ml ##p and the bi ##ls ##tm ) with respect to the sum of the losses are calculated using the back ##pro ##pa ##gation algorithm . as usual , we perform several training iteration ##s over the training corpus , shuffling the order of sentences in each iteration . paragraph : error - exploration and dynamic oracle training we follow ta ##cl ##20 ##13 ##dy ##nami ##c ; colin ##g ##20 ##12 ##dy ##nami ##c in using error exploration training with a dynamic - oracle , which we briefly describe below . at each stage in the training process , the par ##ser assigns scores to all the possible transitions . it then selects a transition , applies it , and moves to the next step . which transition should be followed ? a common approach follows the highest scoring transition that can lead to the gold tree . however , when training in this way the par ##ser sees only configurations that result from following correct actions , and as a result tends to suffer from error propagation at test time . instead , in error - exploration training the par ##ser follows the highest scoring action in during training even if this action is incorrect , exposing it to configurations that result from er ##rone ##ous decisions . this strategy requires defining the set such that the correct actions to take are well - defined also for states that can not lead to the gold tree . such a set is called a dynamic oracle . we perform error - exploration training using the dynamic - oracle defined by ta ##cl ##20 ##13 ##dy ##nami ##c . paragraph : aggressive exploration we found that even when using error - exploration , after one iteration the model remembers the training set quite well , and does not make enough errors to make error - exploration effective . in order to expose the par ##ser to more errors , we follow an aggressive - exploration scheme : we sometimes follow incorrect transitions also if they score below correct transitions . specifically , when the score of the correct transition is greater than that of the wrong transition but the difference is smaller than a margin constant , we chose to follow the incorrect action with probability ( we use in our experiments ) . paragraph : summary the greedy transition - based par ##ser follows standard techniques from the literature ( margin - based objective , dynamic oracle training , error exploration , ml ##p - based non - linear scoring function ) . we depart from the literature by replacing the hand - crafted feature function over carefully selected components of the configuration with a con ##cate ##nation of bi ##ls ##tm representations of a few prominent items on the stack and the buffer , and training the bi ##ls ##tm en ##code ##r jointly with the rest of the network . section : graph - based par ##ser graph - based par ##sing follows the common structured prediction paradigm : given an input sentence ( and the corresponding sequence of vectors ) we look for the highest - scoring par ##se tree in the space of valid dependency trees over . in order to make the search tract ##able , the scoring function is deco ##mp ##osed to the sum of local scores for each part independently . in this work , we focus on arc - factor ##ed graph based approach presented in ms ##t . arc - factor ##ed par ##sing deco ##mp ##oses the score of a tree to the sum of the score of its head - mod ##ifier arcs : given the scores of the arcs the highest scoring projective tree can be efficiently found using e ##is ##ner ' s deco ##ding algorithm e ##is ##ner ##19 ##9 ##6 ##de ##p . mcdonald et al . and most subsequent work estimate the local score of an arc by a linear model parameter ##ized by a weight vector , and a feature function assign ##ing a sparse feature vector for an arc linking mod ##ifier to head . we follow pei ##20 ##15 ##ef ##fect ##ive and replace the linear scoring function with an ml ##p . the feature extract ##or is usually complex , involving many elements ( see section [ reference ] ) . in contrast , our feature extract ##or uses merely the bi ##ls ##tm encoding of the head word and the mod ##ifier word : the final model is : the architecture is illustrated in figure [ reference ] . paragraph : training the training objective is to set the score function such that correct tree is scored above incorrect ones . we use a margin - based objective , aiming to maximize the margin between the score of the gold tree and the highest scoring incorrect tree . we define a hi ##nge loss with respect to a gold tree as : each of the tree scores is then calculated by act ##ivating the ml ##p on the arc representations . the entire loss can viewed as the sum of multiple neural networks , which is sub - different ##iable . we calculate the gradient ##s of the entire network ( including to the bi ##ls ##tm en ##code ##r and word em ##bed ##ding ##s ) . paragraph : labeled par ##sing up to now , we described un ##lab ##ele ##d par ##sing . a possible approach for adding labels is to score the combination of an un ##lab ##ele ##d arc and its label by considering the label as part of the arc . this results in parts that need to be scored , leading to slow par ##sing speeds and arguably a harder learning problem . instead , we chose to first predict the un ##lab ##ele ##d structure using the model given above , and then predict the label of each resulting arc . using this approach , the number of parts stays small , enabling fast par ##sing . the labeling of an arc is performed using the same feature representation fed into a different ml ##p predict ##or : as before we use a margin based hi ##nge loss . the label ##er is trained on the gold trees . the bi ##ls ##tm en ##code ##r responsible for producing and is shared with the arc - factor ##ed par ##ser : the same bi ##ls ##tm en ##code ##r is used in the par ##er and the label ##er . this sharing of parameters can be seen as an instance of multi - task learning . as we show in section [ reference ] , the sharing is effective : training the bi ##ls ##tm feature en ##code ##r to be good at predicting arc - labels significantly improves the par ##ser ' s un ##lab ##ele ##d accuracy . paragraph : loss augmented inference in initial experiments , the network learned quickly and over ##fi ##t the data . in order to remedy this , we found it useful to use loss augmented inference . the intuition behind loss augmented inference is to update against trees which have high model scores and are also very wrong . this is done by aug ##ment ##ing the score of each part not belonging to the gold tree by adding a constant to its score . formally , the loss transforms as follows : paragraph : speed improvements the arc - factor ##ed model requires the scoring of arcs . scoring is performed using an ml ##p with one hidden layer , resulting in matrix - vector multiplication ##s from the input to the hidden layer , and multiplication ##s from the hidden to the output layer . the first multiplication ##s involve larger dimensional input and output vectors , and are the most time consuming . fortunately , these can be reduced to multiplication ##s and vector additions , by observing that the multiplication can be written as where and are are the first and second half of the matrix and re ##using the products across different pairs . summary the graph - based par ##ser is straight - forward first - order par ##ser , trained with a margin - based hi ##nge - loss and loss - augmented inference . we depart from the literature by replacing the hand - crafted feature function with a con ##cate ##nation of bi ##ls ##tm representations of the head and mod ##ifier words , and training the bi ##ls ##tm en ##code ##r jointly with the structured objective . we also introduce a novel multi - task learning approach for labeled par ##sing by training a second - stage arc - label ##er sharing the same bi ##ls ##tm en ##code ##r with the un ##lab ##ele ##d par ##ser . section : experiments and results we evaluated our par ##sing model on english and chinese data . for comparison purposes we follow the setup of dyer ##20 ##15 ##tra ##ns ##ition ##base ##d . paragraph : data for english , we used the stanford dependency ( sd ) conversion of the penn tree ##bank , using the standard train / dev / test splits with the same predicted po ##s - tags as used in dyer ##20 ##15 ##tra ##ns ##ition ##base ##d ; chen ##20 ##14 ##fast . this data ##set contains a few non - projective trees . pun ##ct ##uation symbols are excluded from the evaluation . for chinese , we use the penn chinese tree ##bank 5 . 1 ( ct ##b ##5 ) , using the train / test / dev splits of with gold part - of - speech tags , also following . when using external word em ##bed ##ding ##s , we also use the same data as dyer ##20 ##15 ##tra ##ns ##ition ##base ##d . paragraph : implementation details the par ##ser ##s are implemented in python , using the p ##y ##c ##nn tool ##kit for neural network training . the code is available at the gi ##th ##ub repository . we use the l ##st ##m variant implemented in p ##y ##c ##nn , and opt ##imi ##ze using the adam opt ##imi ##zer . unless otherwise noted , we use the default values provided by p ##y ##c ##nn ( e . g . for random initial ##ization , learning rates etc ) . the word and po ##s em ##bed ##ding ##s and are initial ##ized to random values and trained together with the rest of the par ##ser ##s ' networks . in some experiments , we introduce also pre - trained word em ##bed ##ding ##s . in those cases , the vector representation of a word is a con ##cate ##nation of its randomly - initial ##ized vector em ##bed ##ding with its pre - trained word vector . both are tuned during training . we use the same word vectors as in dyer ##20 ##15 ##tra ##ns ##ition ##base ##d during training , we employ a variant of word drop ##out , and replace a word with the unknown - word symbol with probability that is inverse ##ly proportional to the frequency of the word . a word appearing times in the training corpus is replaced with the unknown symbol with probability . if a word was dropped the external em ##bed ##ding of the word is also dropped with probability . we train the par ##ser ##s for up to 30 iteration ##s , and choose the best model according to the ua ##s accuracy on the development set . paragraph : hyper ##para ##meter tuning we performed a very minimal hyper - parameter search with the graph - based par ##ser , and use the same hyper - parameters for both par ##ser ##s . the hyper - parameters of the final networks used for all the reported experiments are detailed in table [ reference ] . main results table [ reference ] lists the test - set acc ##ura ##cies of our best par ##sing models , compared to other state - of - the - art par ##ser ##s from the literature . it is clear that our par ##ser ##s are very competitive , despite using very simple par ##sing architecture ##s and minimal feature extract ##ors . when not using external em ##bed ##ding ##s , the first - order graph - based par ##ser with 2 features out ##per ##forms all other systems that are not using external resources , including the third - order turbo ##par ##ser . the greedy transition based par ##ser with 4 features also matches or out ##per ##forms most other par ##ser ##s , including the beam - based transition par ##ser with heavily engineered features of zhang and ni ##vre ( 2011 ) and the stack - l ##st ##m par ##ser of dyer ##20 ##15 ##tra ##ns ##ition ##base ##d , as well as the same par ##ser when trained using a dynamic oracle . moving from the simple ( 4 features ) to the extended ( 11 features ) feature set leads to some gains in accuracy for both english and chinese . interesting ##ly , when adding external word em ##bed ##ding ##s the accuracy of the graph - based par ##ser de ##grade ##s . we are not sure why this happens , and leave the exploration of effective semi - supervised par ##sing with the graph - based model for future work . the greedy par ##ser does manage to benefit from the external em ##bed ##ding ##s , and using them we also see gains from moving from the simple to the extended feature set . both feature sets result in very competitive results , with the extended feature set yielding the best reported results for chinese , and ranked second for english , after the heavily - tuned beam - based par ##ser of weiss ##20 ##15 ##st ##ructured . paragraph : additional results we perform some ab ##lation experiments in order to quan ##tify the effect of the different components on our best models ( table [ reference ] ) . loss augmented inference is crucial for the success of the graph - based par ##ser , and the multi - task learning scheme for the arc - label ##er contributes nicely to the un ##lab ##ele ##d scores . dynamic oracle training yields nice gains for both english and chinese . section : conclusion we presented a pleasing ##ly effective approach for feature extraction for dependency par ##sing based on a bi ##ls ##tm en ##code ##r that is trained jointly with the par ##ser , and demonstrated its effectiveness by integrating it into two simple par ##sing models : a greedy transition - based par ##ser and a globally opt ##imi ##zed first - order graph - based par ##ser , yielding very competitive par ##sing acc ##ura ##cies in both cases . paragraph : acknowledge ##ments this research is supported by the intel collaborative research institute for computational intelligence ( ic ##ri - ci ) and the israeli science foundation ( grant number 155 ##5 / 15 ) . we thank lillian lee for her important feedback and efforts invested in editing this paper . we also thank the reviewers for their valuable comments . bibliography : references",
        "pred_seq": "[SEP] bid ##m [SEP] [SEP] dependency ##sing [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "bidirectional lstm"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "dependency parsing"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "treebank data",
                        "penn treebank dataset",
                        "penn treebank",
                        "ctb5"
                    ]
                ],
                "Method": [
                    [
                        "globally optimized graphbased parser",
                        "graphbased",
                        "graphbased parsers",
                        "graphbased parser",
                        "graphbased parsing",
                        "beambased parser"
                    ]
                ],
                "Metric": [
                    [
                        "local score",
                        "loss"
                    ]
                ],
                "Task": [
                    [
                        "dependency parsing",
                        "parsing"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "treebank data",
                        "penn treebank dataset",
                        "penn treebank",
                        "ctb5"
                    ]
                ],
                "Method": [
                    [
                        "greedy transitionbased parser",
                        "transitionbased parsers",
                        "parser",
                        "transitionbased",
                        "transitionbased architecture",
                        "transitionbased parser",
                        "transitionbased family",
                        "transitionbased parsing framework",
                        "transition system",
                        "greedy transitionbased parsing",
                        "transition systems",
                        "shift transition",
                        "greedy transitionbased parsing framework",
                        "parer",
                        "greedy transition based parser",
                        "beambased transition parser"
                    ]
                ],
                "Metric": [
                    [
                        "local score",
                        "loss"
                    ]
                ],
                "Task": [
                    [
                        "dependency parsing",
                        "parsing"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "treebank data",
                        "penn treebank dataset",
                        "penn treebank",
                        "ctb5"
                    ]
                ],
                "Method": [
                    [
                        "globally optimized graphbased parser",
                        "graphbased",
                        "graphbased parsers",
                        "graphbased parser",
                        "graphbased parsing",
                        "beambased parser"
                    ]
                ],
                "Metric": [
                    [
                        "pos",
                        "postags"
                    ]
                ],
                "Task": [
                    [
                        "dependency parsing",
                        "parsing"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "treebank data",
                        "penn treebank dataset",
                        "penn treebank",
                        "ctb5"
                    ]
                ],
                "Method": [
                    [
                        "greedy transitionbased parser",
                        "transitionbased parsers",
                        "parser",
                        "transitionbased",
                        "transitionbased architecture",
                        "transitionbased parser",
                        "transitionbased family",
                        "transitionbased parsing framework",
                        "transition system",
                        "greedy transitionbased parsing",
                        "transition systems",
                        "shift transition",
                        "greedy transitionbased parsing framework",
                        "parer",
                        "greedy transition based parser",
                        "beambased transition parser"
                    ]
                ],
                "Metric": [
                    [
                        "pos",
                        "postags"
                    ]
                ],
                "Task": [
                    [
                        "dependency parsing",
                        "parsing"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "treebank data",
                        "penn treebank dataset",
                        "penn treebank",
                        "ctb5"
                    ]
                ],
                "Method": [
                    [
                        "globally optimized graphbased parser",
                        "graphbased",
                        "graphbased parsers",
                        "graphbased parser",
                        "graphbased parsing",
                        "beambased parser"
                    ]
                ],
                "Metric": [
                    [
                        "parsing accuracies",
                        "uas"
                    ]
                ],
                "Task": [
                    [
                        "dependency parsing",
                        "parsing"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "treebank data",
                        "penn treebank dataset",
                        "penn treebank",
                        "ctb5"
                    ]
                ],
                "Method": [
                    [
                        "greedy transitionbased parser",
                        "transitionbased parsers",
                        "parser",
                        "transitionbased",
                        "transitionbased architecture",
                        "transitionbased parser",
                        "transitionbased family",
                        "transitionbased parsing framework",
                        "transition system",
                        "greedy transitionbased parsing",
                        "transition systems",
                        "shift transition",
                        "greedy transitionbased parsing framework",
                        "parer",
                        "greedy transition based parser",
                        "beambased transition parser"
                    ]
                ],
                "Metric": [
                    [
                        "parsing accuracies",
                        "uas"
                    ]
                ],
                "Task": [
                    [
                        "dependency parsing",
                        "parsing"
                    ]
                ]
            }
        ]
    },
    "59": {
        "doctext": "ho ##listic , instance - level human par ##sing section : abstract object par ##sing - the task of deco ##mp ##osing an object into its semantic parts - has traditionally been formulated as a category - level segment ##ation problem . consequently , when there are multiple objects in an image , current methods can not count the number of objects in the scene , nor can they determine which part belongs to which object . we address this problem by segment ##ing the parts of objects at an instance - level , such that each pixel in the image is assigned a part label , as well as the identity of the object it belongs to . moreover , we show how this approach benefits us in obtaining segment ##ations at coarse ##r gran ##ular ##ities as well . our proposed network is trained end - to - end given detection ##s , and begins with a category - level segment ##ation module . thereafter , a different ##iable conditional random field , defined over a variable number of instances for every input image , reasons about the identity of each part by ass ##oc ##iating it with a human detection . in contrast to other approaches , our method can handle the varying number of people in each image and our ho ##listic network produces state - of - the - art results in instance - level part and human segment ##ation , together with competitive results in category - level part segment ##ation , all achieved by a single forward - pass through our neural network . section : introduction object par ##sing , the segment ##ation of an object into semantic parts , is naturally performed by humans to obtain a more detailed understanding of the scene . when performed automatically by computers , it has many practical applications , such as in human - robot interaction , human behaviour analysis and image descriptions for the visually impaired . furthermore , detailed part information has been shown to be beneficial in other visual recognition tasks such as fine - grain ##ed recognition [ reference ] , human pose estimation [ reference ] and object detection [ reference ] . in this paper , we focus on the application of par ##sing humans as it is more commonly studied , although our method makes no assumptions on the type of object it is segment ##ing . in contrast to existing human par ##sing approaches [ reference ] [ reference ] [ reference ] , we operate at an instance level ( to our knowledge , we are the first work to do so ) . as shown in fig . 1 , not only do we segment the various body parts of humans ( fig . 1b ) , but we associate each of these parts to one of the humans in the scene ( fig . 1 ##c ) , which is particularly important for understanding scenes with multiple people . in contrast to existing instance segment ##ation work [ reference ] part segment ##ation human segment ##ation figure 1 : our proposed approach segments human parts at an instance level ( c ) ( which to our knowledge is the first work to do so ) from category - level part segment ##ations produced earlier in the network ( b ) . moreover , we can easily obtain human instance segment ##ations ( d ) by taking the union of all pixels associated to a particular person . therefore , our proposed end - to - end trained neural network par ##ses humans into semantic parts at both category and instance level in a single forward - pass . best viewed in colour . [ reference ] , we operate at a more detailed part level , enabling us to extract more comprehensive information of the scene . furthermore , with our part - level instance segment ##ation of humans , we can easily recover human - level instance segment ##ation ( by taking the union of all parts assigned to a particular instance as shown in fig . 1 ##d ) , and we show significant improvement over previous state - of - the - art in human instance - segment ##ation when doing so . our approach is based on a deep con ##vo ##lu ##tion ##al neural network ( cnn ) , which consists of an initial category - level part segment ##ation module . using the output of a human detector , we are then able to associate segment ##ed parts with detected humans in the image using a different ##iable conditional random field ( cr ##f ) , producing a part - level instance segment ##ation of the image . our formulation is robust to false - positive detection ##s as well as imperfect bound ##ing boxes which do not cover the entire human , in contrast to other instance segment ##ation methods based on object detectors [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . given object detection ##s , our network is trained end - to - end , given detection ##s , with a novel loss function which allows us to handle a variable number of human instances on every image . we evaluate our approach on the pascal person - parts [ reference ] data ##set , which contains humans in a diverse set of poses and o ##cc ##lusion ##s . we achieve state - of - the - art results on instance ##lev ##el segment ##ation of both body parts and humans . moreover , our results on semantic part segment ##ation ( which is not - instance aware ) is also competitive with current state - of - the ##art . all of these results are achieved with a ho ##listic , end - to - end trained model which par ##ses humans at both an instance and category level , and outputs a dynamic number of instances per image , all in a single forward - pass through the network . section : related work the problem of object par ##sing , which aims to deco ##mp ##ose objects into their semantic parts , has been addressed by numerous works [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] , most of which have concentrated on par ##sing humans . however , none of the aforementioned works have par ##sed objects at an instance level as shown in fig . 1 , but rather category level . in fact , a lot of work on human par ##sing has focus ##sed on data ##set ##s such as fashion ##ista [ reference ] , at ##r [ reference ] and deep fashion [ reference ] where images typically contain only one , centred person . the notion of instance - level segment ##ation only matters when more than one person is present in an image , mo ##tiv ##ating us to evaluate our method on the pascal person - parts data ##set [ reference ] where multiple people can appear in un ##con ##stra ##ined environments . recent human par ##sing approaches have typically been similar to semantic segment ##ation works using fully con ##vo ##lu ##tion ##al networks ( fc ##ns ) [ reference ] , but trained to label parts [ reference ] [ reference ] [ reference ] instead of object classes . however , methods using only fc ##ns do not explicitly model the structure of a human body , and typically do not perform as well as methods which do [ reference ] . structural prior ##s of the human body have been encoded using pictorial structures [ reference ] [ reference ] , conditional random fields ( cr ##fs ) [ reference ] [ reference ] [ reference ] [ reference ] and more recently , with l ##st ##ms [ reference ] [ reference ] . the ha ##z ##n approach of [ reference ] addressed the problem that some parts are often very small compared to other parts and difficult to segment with scale - variant cnn ##s . this scale variation was handled by a cascade of three separately ##train ##ed fc ##ns , each par ##sing different regions of the image at different scales . an early instance segment ##ation work by win ##n et al . [ reference ] predicted the parts of an object , and then encouraged these parts to maintain a spatial ordering , characteristic of an instance , using as ##ym ##metric pair ##wise potential ##s in a cr ##f . however , subsequent work has not operated at a part level . zhang et al . [ reference ] [ reference ] performed instance segment ##ation of vehicles using an mr ##f . however , this graphical model was not trained end - to - end as done by [ reference ] [ reference ] [ reference ] and our approach . furthermore , they assumed a maximum of 9 cars per image . approaches using rec ##urrent neural networks [ reference ] [ reference ] can handle a variable number of instances per image by segment ##ing an instance per time - step , but are currently restricted to only one object category . our method , on the other hand , is able to handle both an arbitrary number of objects , and multiple object categories in the image with a single forward - pass through the network . various methods of instance segment ##ation have also involved modifying object detection systems to output segments instead of bound ##ing boxes [ reference ] [ reference ] [ reference ] [ reference ] . however , these methods can not produce a segment ##ation map of the image , as shown in fig . 1 , without post ##pro ##ces ##sing as they consider each detection independently . although our method also uses an object detector , it considers all detection ##s in the image jointly with an initial category - level segment ##ation , and produces segment ##ation maps naturally where one pixel can not belong to multiple instances in contrast to the aforementioned approaches . the idea of combining the outputs of a category - level segment ##ation network and an object detector to reason about different instances was also presented by [ reference ] . however , that system was not trained end - toe ##nd , could not segment instances outside the detector ' s bound ##ing box , and did not operate at a part level . section : proposed approach our network ( fig . 2 ) consists of two components : a category - level part segment ##ation module , and an instance segment ##ation module . as both of these modules are different ##iable , they can be integrated into a single network and trained jointly . the instance segment ##ation module ( sec . 3 . 2 ) uses the output of the first category - level segment ##ation module ( sec . 3 . 1 ) as well as the outputs of an object detector as its input . it associates each pixel in the category ##lev ##el segment ##ation with an object detection , resulting in an instance - level segment ##ation of the image . given a h ##\u00d7 ##w ##\u00d7 3 input image , i , the category - level part segment ##ation module produces a h ##\u00d7 ##w ##\u00d7 ( p + 1 ) dimensional output q where p is the number of part classes in the data ##set and one background class . there can be a variable number , d , of human detection ##s per image , and the output of the instance segment ##ation module is an h ##\u00d7 ##w ##\u00d7 ( pd + 1 ) tensor den ##oting the pro ##ba ##bilities , at each pixel in the image , of each of the p part classes belonging to one of the d detection ##s . two challenges of instance segment ##ation are the variable number of instances in every image , and the fact that per ##mut ##ations of instance labels lead to identical results ( in fig . 1 , how we order the different people does not matter ) . zhang et al . [ reference ] [ reference ] resolve these issues by assuming a maximum number of instances and using the ground - truth depth ordering of instances respectively . others have bypassed both of these issues by predicting each instance independently [ reference ] [ reference ] [ reference ] [ reference ] , but this also allows a pixel to belong to multiple instances . instead , we use a loss function ( sec 3 . 3 ) that is based on \" matching \" the prediction to the ground - truth , allowing us to handle per ##mut ##ations of the ground truth . furthermore , weight - sharing in our instance segment ##ation module allows us to segment a variable number of instances per image . as a result , we do not assume a maximum number of instances , consider all instances jointly , and train our network end - to - end , given object detection ##s . section : category - level part segment ##ation module the part segment ##ation module is a fully con ##vo ##lu ##tion ##al network [ reference ] based on res ##net - 101 [ reference ] . a common technique , presented in [ reference ] [ reference ] , is to predict the image at three different scales ( with network weights shared among all the scales ) , and combine predictions together with learned , image - dependent weights . we take a different approach of fu ##sing information at multiple scales - we pool the features after res ##5 ##c [ reference ] at five different resolutions ( by varying the pool ##ing stride ) , ups ##amp ##le the features to the resolution before pool ##ing , and then con ##cate ##nate these features before passing them to the final con ##vo ##lu ##tion ##al class ##ifier , as proposed in [ reference ] . as we show in sec 4 . 4 , this approach achieve ##s better semantic segment ##ation results than [ reference ] [ reference ] . we denote the output of this module by the tensor , q , where q i ( l ) is the probability of pixel i being assigned label l ##\u2208 { 0 , 1 , 2 , . . . , p } . further details of this module are included in the appendix . section : instance - level segment ##ation module this module creates an instance - level segment ##ation of the image by ass ##oc ##iating each pixel in the input category - level segment ##ation , q , with one of the d input human - detection ##s or the background label . let there be d input human - detection ##s for the image , where the i - th detection is represented by b i , the set of pixels lying within the four corners of its bound ##ing box , and s i ##\u2208 [ 0 , 1 ] , the detection score . we assume that the 0 - th detection refers to the background label . furthermore , we define a multi ##no ##mia ##l random variable , v i , at each of the n pixels in the image , and let . this variable can take on a label from the set { 1 , 2 , . . . , d } \u00d7 { 1 , 2 , . . . , p } \u222a { ( 0 , 0 ) } since each of the p part labels can be associated with one of the d human detection ##s , or that pixel could belong to the background label , ( 0 , 0 ) . we formula ##te a conditional random field over these v variables , where the energy of the assignment v to all of the instance variables v consists of two una ##ry terms , and one pair ##wise term ( whose weight ##ing co - efficient ##s are all learned via back ##pro ##pa ##gation ) : ( the una ##ry and pair ##wise potential ##s are computed within our neural network , different ##iable with respect to their input and parameters , and described in sec . 3 . 2 . 1 through 3 . 2 . 3 . the maximum - a - posterior ##i ( map ) estimate of our cr ##f ( since the energy in e ##q . 1 character ##ises a gibbs distribution ) is computed as the final label ##ling produced by our network . we perform the it ##erative mean - field inference algorithm to approximately compute the map solution by mini ##mis ##ing e ##q . 1 . as shown by zheng et al . [ reference ] , this can be formulated as a rec ##urrent neural network ( rn ##n ) , allowing it to be trained end - to - end as part of a larger network . however , as our network is input a variable number of detection ##s per image , d , the label space of the cr ##f is dynamic . therefore , unlike [ reference ] , the parameters of our cr ##f are not class - specific to allow for this variable number of \" channels \" . section : box consistency term we observe that in most cases , a body part belonging to a person is located inside the bound ##ing box of the person . based on this observation , the box consistency term is employed to encourage pixel locations inside a human bound ##ing box b i to be associated with the i - th human detection . the box term potential at spatial location k for body part j of a human i is assigned either 0 for k / \u2208 b i , or the product of the detection score , s i , and the category - level part segment ##ation confidence , note that this potential may be robust to false - positive detection ##s when the category - level segment ##ation and human detection do not agree with each other , since q k ( l ) , the probability of a pixel k taking on body - part label l , is low . furthermore , note that we use one human ##de ##tec ##tion to reason about the identity of all parts which constitute that human . section : global term a possible short ##coming for the box consistency potential is that if some pixels belonging to a human instance fall outside the bound ##ing box and are consequently assigned 0 for the box consistency term potential , they would be lost in the final instance segment ##ation prediction . visually , the generated instance masks would appear truncated along the bound ##ing box boundaries - a problem suffered by [ reference ] [ reference ] [ reference ] [ reference ] . to overcome this und ##es ##ira ##ble effect , we introduce the global potential : it complement ##s the box consistency term by assuming that a pixel is equally likely to belong to any one of the detected humans . it is expressed as prediction , p original ground - truth , y \" matched \" ground - truth , y * figure 3 : as different per ##mut ##ations of the ground - truth are equivalent in the case of instance segment ##ation , we \" match \" the original ground - truth , y , to our network ' s prediction , p , to obtain the \" matched \" ground - truth which we use to compute our loss during training . section : pair ##wise term our pair ##wise term is composed of densely - connected ga ##uss ##ian kernel ##s [ reference ] which are commonly used in segment ##ation literature [ reference ] [ reference ] . this pair ##wise potential encourages both spatial and appearance consistency , and we find these prior ##s to be suitable in the case of instance ##lev ##el segment ##ation as well . as in [ reference ] , the weight ##ing parameters of these potential ##s are learned via back ##pro ##pa ##gation , though in our case , the weights are shared among all classes . section : loss function and network training we first pre - train the category - level segment ##ation part of our network , as described in the appendix . thereafter , we add the instance segment ##ation module , and train with a per ##mut ##ation ##in ##var ##ian ##t loss function which is back ##pro ##pa ##gated through both our instance - and category ##lev ##el segment ##ation networks . since all per ##mut ##ations of an instance segment ##ation have the same qu ##ali ##tative result , we \" match \" the original ground - truth to our prediction before computing the loss , as shown in fig . 3 . this matching is based on the intersection over union ( io ##u ) [ reference ] of a predicted and ground - truth instance , similar to [ reference ] . let y = { y 1 , y 2 , . . . , y m } , a set of m segments , denote the ground - truth label ##ling of an image , where each segment is an instance and has a part label assigned to it . similarly , let p = { p 1 , p 2 , . . . , p n } denote our n predicted instances , each with an associated part label . note that m and n need not be the same as we may predict greater or fewer instances than there actually are in the image . the \" matched \" ground truth , y * is the per ##mut ##ation of the original ground - truth label ##ling which maxim ##ises the io ##u between our prediction , p and ground - truth where ##\u03c0 ( y ) denotes the set of all per ##mut ##ations of y . note that we define the io ##u between all segments of different labels to be 0 . e ##q . 4 can be solved efficiently using the hungarian algorithm as it can be formulated as a bi ##par ##tite graph matching problem , and once we have the \" matched \" ground - truth , y * , we can apply any loss function to it and train our network for segment ##ation . in our case , we use the standard cross - entropy loss function on the \" matched \" ground truth . in addition , we employ online hard example mining ( oh ##em ) , and only compute our loss over the top k pixels with the highest loss in the training mini - batch . we found that during training , many pixels already had a high probability of being assigned to the correct class . by only selecting the top k pixels with the highest loss , we are able to encourage our network to improve on the pixels it is currently mis ##class ##ifying , as opposed to increasing the probability of a pixel it is already classify ##ing correctly . this approach was inspired by \" boots ##tra ##pping \" [ reference ] [ reference ] or \" hard - negative mining \" [ reference ] commonly used in training object detectors . however , these methods mined hard examples from the entire data ##set . our approach is most similar to [ reference ] , who mined hard examples online from each mini - batch in the context of detection . similar to the aforementioned works , we found oh ##em to improve our overall results , as shown in sec . 4 . 2 . section : obtaining segment ##ations at other gran ##ular ##ities given the part instance prediction produced by our proposed network , we are able to easily obtain human instance segment ##ation and semantic part segment ##ation . in order to achieve human instance segment ##ation , we map the predicted part instance labels ( i , j ) , i . e . part j of person i , to i . whereas to obtain semantic part segment ##ation , we map predicted part instance labels ( i , j ) to j instead . section : experiments we describe our data ##set and experimental set - up in sec . 4 . 1 , before presenting results on instance - level part segment ##ation ( fig . 1 ##c ) , instance - level human segment ##ation ( fig . 1 ##d ) and semantic part segment ##ation ( fig . 1b ) . additional quantitative and qu ##ali ##tative results , failure cases and experimental details are included in the appendix . section : experimental set - up we evaluate our proposed method on the pascal person - part data ##set [ reference ] which contains 1716 training images , and 1817 test images . this data ##set contains multiple people per image in un ##con ##stra ##ined poses and environments , and contains six human body part classes ( fig . 1b ) , as well as the background label . as described in sec . 3 . 3 , we initially pre - train our category ##lev ##el segment ##ation module before training for instance - level segment ##ation . this module is first trained on the 21 classes of the pascal vo ##c data ##set [ reference ] , and then fine ##tu ##ned on the seven classes of the pascal part training set using category - level ann ##ota ##tions . finally , we train for instance segment ##ation with instance - level ground truth . full details of our training process , including all hyper ##para ##meter ##s such as learning rate , are in the appendix . to clarify these details , we will also release our code . we use the standard ap r metric [ reference ] for evaluating instance - level segment ##ation : the mean average precision of our predictions is computed where a prediction is considered correct if its io ##u with a ground - truth instance is above a certain threshold . this is similar to the ap metric used in object detection . however , in detection , the io ##u between ground ##tr ##uth and predicted bound ##ing boxes is computed , whereas here , the io ##u between regions is computed . furthermore , in detection , an overlap threshold of 0 . 5 is used , whereas we vary this threshold . finally , we define the ap r vol which is the mean of the ap r score for overlap threshold ##s varying from 0 . 1 to 0 . 9 in inc ##rem ##ents of 0 . 1 . we use the publicly available r - fc ##n detection framework [ reference ] , and train a new model with data from vo ##c 2012 [ reference ] that do not overlap with any of our test sets . we train with all object classes of vo ##c , and only use the output for the human class . non - maximal suppression is performed on all detection ##s before being fed into our network . table 1 shows our results on part - level instance segment ##ation on the pascal person - part data ##set . to our knowledge , we are the first work to do this , and hence we study the effects of various design choices on overall performance . we also use the publicly available code for mn ##c [ reference ] , which won the ms - coco 2016 instance segment ##ation challenge , and fine ##tu ##ne their public model trained on vo ##c 2011 [ reference ] on person - part instances as a baseline . section : results on instance - level part segment ##ation we first train our model in a piece ##wise manner , by first opt ##imi ##sing the parameters of the category - level segment ##ation module , and then \" freezing \" the weights of this module and only training the instance network . initially , we only use the box consistency term ( sec . 3 . 2 . 1 ) in the instance cr ##f , resulting in an ap r at 0 . 5 of 38 . 0 % . note that this model is equivalent to our rei ##mple ##ment ##ation of [ reference ] . adding in the global potential ( sec . 3 . 2 . 2 ) helps us cope with bound ##ing boxes which do not cover the whole human , and we see an improvement at all io ##u threshold ##s . training our entire network end - to - end gives further benefits . we then train all variants of our model with oh ##em , and observe consistent improvements across all io ##u threshold ##s with respect to the corresponding baseline . here , we set k = 2 [ reference ] , meaning that we computed our loss over 2 [ reference ] or approximately 12 % of the hardest pixels in each training image ( since we train at full resolution ) . we also employ oh ##em when pre - training the category - level segment ##ation module of our network , and observe minimal difference in the final result if we use oh ##em when training the category - level segment ##ation module but not the instance segment ##ation module . training end - to - end with oh ##em achieve ##s 2 . 6 % higher in ap r at 0 . 5 , and 1 . 8 % higher ap r vol over a piece ##wise - trained baseline model without oh ##em and only the box term ( second row ) , which is equivalent to the model of [ reference ] . furthermore , our ap r vol is 1 . 7 % greater than the strong mn ##c [ reference ] baseline . note that although [ reference ] also performed instance - level segment ##ation on the same data ##set , their evaluation was only done using human instance labels , which is similar to our following experiment on human instance segment ##ation . section : results on human instance segment ##ation we can trivial ##ly obtain instance - level segment ##ations of humans ( fig 1 ##d ) , as mentioned in sec . 3 . 4 . table 2 shows our state - of - the - art instance segment ##ation results for humans on the vo ##c 2012 validation set [ reference ] . we use the best model from the previous section as there is deep ##lab * [ reference ] 53 . 0 attention [ reference ] 56 . 4 ha ##z ##n [ reference ] 57 . 5 l ##g - l ##st ##m [ reference ] 58 . 0 graph l ##st ##m [ reference ] 60 . 2 deep ##lab v ##2 [ reference ] 64 . 9 ref ##inen ##et [ reference ] 68 . 6 ours , pre - trained 65 . 9 ours , final network 66 . 3 * result reported in [ reference ] no overlap between the pascal person - part training set , and the vo ##c 2012 validation set . as tab . 2 shows , our proposed approach out ##per ##forms previous state - of - the - art by a significant margin , particularly at high io ##u threshold ##s . our model receives extra supervision in its part labels , but the fact that our network can implicit ##ly in ##fer relationships between different parts whilst training may help it handle o ##cc ##lu ##ding instances better than other approaches , leading to better instance segment ##ation performance . the fact that our network is trained with part - level ann ##ota ##tions may also help it identify small features of humans better , leading to more precise segment ##ations and thus improvements at high ap r threshold ##s . our ap r at each io ##u threshold for human instance segment ##ation is higher than that for part instance segment ##ation ( tab . 1 ) . this is because parts are smaller than entire humans , and thus more difficult to local ##ise accurately . an alternate method of performing instance - level part segment ##ation may be to first obtain an instance - level human segment ##ation using another method from tab . 2 , and then partition it into the various body parts of a human . however , our approach , which groups parts into instances , is valid ##ated by the fact that it achieve ##s state - of - the - art instance - level human segment ##ation performance . section : results on category - level part segment ##ation finally , our model is also able to produce category - level segment ##ations ( as shown in fig . 1b ) . this can be obtained from the output of the category - level segment ##ation module , or from our instance module as described in sec . 3 . 4 . as shown in tab . 3 , our semantic segment ##ation results are competitive with current state - of - the - art . by training our entire network consisting of the category - level and instance - level segment ##ation modules jointly , and then obtaining the semantic segment ##ation from the final instance segment ##ation output by our network , we are able to obtain a small improvement of 0 . 4 % in mean io ##u over the output of the initial semantic segment ##ation module . section : conclusion our proposed , end - to - end trained network outputs instance - level body part and human segment ##ations , as well as category - level part segment ##ations in a single forward - pass . moreover , section : input semantic segment ##ation instance segment ##ation ground truth figure 4 : some results of our system . the first column shows the input image and the input detection ##s we obtained from training the r - fc ##n detector [ reference ] . the second and third columns show our final semantic segment ##ation ( sec . 3 . 4 ) and instance - level part segment ##ation . first row : our network can deal with poor bound ##ing box local ##isation , as it manages to segment the third person from the left although the bound ##ing box only partially covers her . second row : our method is robust against false positive detection ##s because of the box term . observe that the bowl of the right ##most person in the bottom row is falsely detected as a person , but rejected in the final prediction . following rows : we are able to handle overlapping bound ##ing boxes by reasoning globally using the instance cr ##f . we have shown how segment ##ing objects into their constituent parts helps us segment the object as a whole with our state - of - the - art results on instance - level segment ##ation of both body parts and entire humans . furthermore , our category - level segment ##ations improve after training for instance - level segment ##ation . our future work is to train the object detector end - to - end as well . moreover , the improvement that we obtained in instance segment ##ation of humans as a result of first segment ##ing parts mo ##tiv ##ates us to explore weakly - supervised methods which do not require explicit object part ann ##ota ##tions . in our main paper , we reported our ap r results averaged over all classes . fig . 5 visual ##ises the per ##class results of our best model at different io ##u threshold ##s . fig . 6 displays the success cases of our method , while fig . 7 shows examples of failure cases . furthermore , we illustrate the strengths and weaknesses of our part instance segment ##ation method in comparison to mn ##c [ reference ] in fig . 8 , and compare our instance - level human segment ##ation results , which we obtain by the simple mapping described in sec . 3 . 4 of our main paper , to mn ##c in fig . 9 . finally , we attach an additional video . we run our system off ##line , on a frame - by - frame basis on the entire music video , and show how our method is able to accurately par ##se humans at both category and instance level on internet data outside the pascal data ##set . instance - level segment ##ation of videos requires data association . we use a simple , greedy method which operates on a frame - by - frame basis . segments from one frame are associated to segments in the next frame based on the io ##u , using the same method we use for our loss function as described in sec . 3 . 3 of the main paper . it shows that our method achieve ##s best instance accuracy for the head category , and finds lower arms and lower legs most challenging to segment correctly . this is likely because of the thin shape of the lower limbs which is known to pose difficulty for semantic segment ##ation . section : input semantic segment ##ation instance segment ##ation ground truth figure 6 : success cases of our method . the first column shows the input image and the input detection ##s we obtained from training the r - fc ##n detector [ reference ] . the second column shows our final semantic segment ##ation ( as described in sec . 3 . 4 of the main paper ) . our proposed method is able to leverage an initial category - level segment ##ation network and human detection ##s to produce accurate instance - level part segment ##ation as shown in the third column . first row : unlike mn ##c which predict ##s for each part instance independently , our method reasons globally and jointly . as a result , mn ##c predict ##s two instances of lower legs for the same lower leg of the second and third person from the left . furthermore , with a dedicated category - level segment ##ation module , we are less prone to false negative ##s , whereas mn ##c misses the legs of the right ##most person , and the lower arm of the second person from the right . second row : while we can handle poor bound ##ing box local ##isation because of our global potential term , mn ##c is unable to segment regions outside the bound ##ing boxes it generates . consequently , only one lower arm of the person on the left is segment ##ed as the other one is outside the bound ##ing box . the square corners of the segment ##ed lower arm correspond to the limits imposed by the bound ##ing box which mn ##c internally uses ( box generation is the first stage of the cascade [ reference ] ) . third row : by anal ##ys ##ing an image globally and employing a different ##iable cr ##f , our method can produce more precise boundaries . as mn ##c does not perform category - level segment ##ation over the entire image , it has no incentive to produce a coherent and continuous prediction . visually , this is reflected in the gaps of \" background \" between body parts of the same person . fourth row : mn ##c predict ##s two instances of lower leg for the second person from the right , and fails to segment any lower arms for all four people due to the aforementioned problems . [ reference ] using the default parameters and extract only its human instance predictions . in contrast with proposal - driven methods such as mn ##c , our approach assigns each pixel to only one instance , is robust against non - ideal bound ##ing boxes , and often produces better boundaries due to the instance cr ##f which is trained end ##to - end . first and second row : since mn ##c predict ##s instances independently , it is prone to predicting multiple instances for a single person . third row : due to the global potential term , we can segment regions outside of a detection bound ##ing box which fails to cover the entire person , whereas mn ##c is unable to recover from such imperfect bound ##ing boxes , leading to its frequent occurrences of truncated instance predictions . fourth row : a case where mn ##c and our method show different failure modes . mn ##c predict ##s three people where there are only two , and our method can only predict one instance due to a missing detection . mn ##c is unable to recover from a false positive detection and predict ##s two people . second row : while both mn ##c and our method start off with poor bound ##ing box local ##isation that does not cover the whole instance , we are able to segment the entire person , whereas mn ##c is bounded by its flawed region proposal . third row : mn ##c performs better in this case as it is able to segment the infant , whereas we miss her completely due to a false negative person detection . section : b additional information we detail our initial category - level segment ##ation module and compare it to deep ##lab - v ##2 [ reference ] in sec . b . 1 , present our network training details in sec . b . 2 , and finally describe how we train the mn ##c model which serves as our baseline in sec . b . 3 . section : b . 1 details of the category - level segment ##ation module as shown in fig 10 ##b , the structure of our category - level segment ##ation module consists of a res ##net - 101 backbone , and a class ##ifier that extracts multi - scale features from the res ##net - 101 output by using average pool ##ing with different kernel sizes . while our category - level segment ##ation module and the deep ##lab - v ##2 network ( fig . 10 ##a ) of chen et al . [ reference ] both attempt to exploit multi - scale information in the image , the approach of [ reference ] en ##tails executing three forward passes for each image , whereas we only need a single forward pass . in comparison to deep ##lab - v ##2 , our network saves both memory and time , and achieve ##s better performance . to carry out a single forward pass , our network uses 4 . 3 gb of memory while deep ##lab - v ##2 [ reference ] needs 9 . 5 gb , 120 % more than ours . speed - wise , our network runs forward passes at 0 . 255 seconds per image ( 3 . 9 f ##ps ) , whereas deep ##lab - v ##2 takes 55 % longer , at 0 . 39 ##6 seconds per image ( 2 . 5 f ##ps ) on average . when deep ##lab - v ##2 adds a cr ##f with 10 mean - field iteration ##s to post - process the network output , it gains a small improvement in mean io ##u by 0 . 54 % [ reference ] , but it requires 11 . 2 gb of memory to make a forward pass ( 140 % of the total amount used by our full network including the instance - level segment ##ation module ) , and takes 0 . 960 seconds per image ( 1 . 0 f ##ps ) , almost a qu ##ater of our frame rate . tests are done on a single ge ##force gt ##x titan x ( maxwell ) card . overall , we are able to achieve better segment ##ation accuracy ( as shown in tab . 3 of our main paper ) and is more memory - and time - efficient than deep ##lab - v ##2 . section : b . 2 training our proposed network b . 2 . 1 training the category - level segment ##ation module we initial ##ise our semantic segment ##ation network with the coco pre - trained res ##net - 101 weights provided by [ reference ] . training is first performed on the pascal vo ##c 2012 training set using the extra ann ##ota ##tions from [ reference ] , which combine to a total of 90 ##12 training images . care is taken to ensure that all images from the pascal person - parts test set is excluded from this training set . a polynomial learning rate policy is adopted such that the effective learning rate at iteration i is given by l i = l 0 ( 1 ##\u2212 i i max ) p , where the base learning rate , l 0 , is set to 6 . 25 ##\u00d7 10 ##\u2212 ##4 , the total number of iteration ##s , i max , is set to 30 ##k , and the power , p , is set to 0 . 9 . a batch size of 16 is used . however , due to memory constraints , we simulate this batch size by \" acc ##um ##ulating gradient ##s \" : we carry out 16 forward and backward passes with one image per iteration , and only perform the weight update after completing all 16 passes . we use a momentum of 0 . 9 and weight decay of 1 ##\u00d7 10 ##\u2212 ##4 for these experiments . after 30 ##k of iteration ##s are completed , we take the best performing model and fine ##tu ##ne on the pascal person - parts training set using the same training scheme as described above . note that the parameters of the batch normal ##isation modules are kept unchanged in the whole learning process . online data - aug ##ment ##ation is performed during training to regular ##ise the model . the training images are randomly mirrored , scaled by a ratio between 0 . 5 and 2 , rotated by an angle between - 10 and 10 degrees , translated by a random amount in the hs ##v colour space , and blurred with a randomly - sized ga ##uss ##ian kernel , all on - the - fly . we observe that these techniques are effective at reducing the accuracy gap between training and testing , leading to overall higher test acc ##ura ##cies . [ reference ] and our network structure . the numbers following the layer type denote the kernel size and number of filters . for pool ##ing layers , only their kernel sizes are shown as the number of filters is not applicable . the ups ##amp ##ling ratios can be in ##fer ##red from the context . fig . 10 ##a : in the deep ##lab - v ##2 architecture , a 51 ##3 ##\u00d7 ##51 ##3 ##\u00d7 ##3 input image is downs ##amp ##led by two different ratios ( 0 . 75 and 0 . 5 ) to produce multi - scale input at three different resolutions . the three resolutions are independently processed by a res ##net - 101 - based network using shared weights ( shown by the individually coloured paths ) . the output feature maps are then ups ##amp ##led where appropriate , combined by taking the element ##wise maximum , and finally ups ##amp ##led back to 51 ##3 ##\u00d7 ##51 ##3 . fig . 10 ##b : the category - level segment ##ation module proposed in this paper forwards an input image of size 521 ##\u00d7 ##52 ##1 ##\u00d7 ##3 through a res ##net - 101 - based cnn , producing a feature map of resolution 66 ##\u00d7 ##66 ##\u00d7 ##20 ##48 . this feature map is average - poole ##d with four different kernel sizes , giving us four feature maps with spatial resolutions 1 ##\u00d7 ##1 , 2 ##\u00d7 ##2 , 3 ##\u00d7 ##3 , and 6 ##\u00d7 ##6 respectively . each feature map undergoes con ##vo ##lu ##tion and ups ##amp ##ling , before being con ##cate ##nated together with each other and the 66 ##\u00d7 ##66 ##\u00d7 ##20 ##48 res ##net - 101 output . this is followed by a con ##vo ##lu ##tion layer that reduces the dimension of the con ##cate ##nated features to 512 , and a con ##vo ##lu ##tion ##al class ##ifier that maps the 512 channels to the size of label space in the data ##set . finally , the prediction is ups ##amp ##led back to 521 ##\u00d7 ##52 ##1 . in both fig . 10 ##a and 10 ##b , the res ##net - 101 backbone uses dil ##ated con ##vo ##lu ##tion such that its output at res ##5 ##c is at 1 / 8 of the input resolution , instead of 1 / 32 for the original res ##net - 101 [ reference ] . the con ##vo ##lu ##tion ##al class ##ifiers ( coloured in purple ) output c channels , corresponding to the number of classes in the data ##set including a background class . for the pascal person - parts data ##set , c is 7 . best viewed in colour . section : b . 2 . 2 training the instance - level segment ##ation module in our model , the pair ##wise term of the fully - connected cr ##f takes the following form : where ##\u00b5 ( \u00b7 , \u00b7 ) is a compatibility function , k ( \u00b7 , \u00b7 ) is a kernel function , and f i is a feature vector at spatial location i containing the 3 - dimensional colour vector i i and the 2 - dimensional position vector p i [ reference ] . we further define the kernel as follows : where w [ reference ] and w [ reference ] are the linear combination weights for the bilateral term and the ga ##uss ##ian term respectively . in order to determine the initial values for the parameters in the instance cr ##f to train from , we carry out a random search . according to the search results , the best prediction accuracy is obtained by initial ##ising w ( 1 ) = 8 , w ( 2 ) = 2 , \u03b8 ##\u03b1 = 2 , \u03b8 ##\u03b2 = 8 , \u03b8 ##\u03b3 = 2 . furthermore , we use a fixed learning rate of 1 ##\u00d7 10 ##\u2212 ##6 , momentum of 0 . 9 , and weight decay of 1 ##\u00d7 10 ##\u2212 ##4 for training both the instance - level and category - level segment ##ation modules jointly . although we previously use the polynomial learning rate policy , we find that for training the instance - level segment ##ation module , a fixed learning rate leads to better results . furthermore , our experiments show that a batch size of one works best at this training stage . using this scheme , we train for 175 ##k iteration ##s , or approximately 100 epoch ##s . section : b . 3 training multi - task network cascade ##s ( mn ##c ) we use the publicly available multi - task network cascade ##s ( mn ##c ) framework [ reference ] , and train a new model for instance - level part segment ##ation using the pascal person - parts data ##set . the weights are initial ##ised with the officially released mn ##c model 1 which has been trained on pascal vo ##c 2011 / sb ##d [ reference ] . the base learning rate is set to 1 ##\u00d7 10 ##\u2212 ##3 , which is reduced by 10 times after 20 ##k iteration ##s . a total of 25 ##k training iteration ##s are carried out . a batch size of 8 , momentum of 0 . 9 and weight decay of 5 ##\u00d7 10 ##\u2212 ##4 are used . these settings are identical to the ones used in training the original mn ##c and provided in their public source code . using these settings , we are also able to reproduce the experimental results obtained in the original mn ##c paper [ reference ] , and hence we believe that the mn ##c model we have trained acts as a strong baseline for our proposed approach . section : section : section : section : appendix in this appendix , we present additional results of our proposed approach in sec . a , and provide additional training and implementation details in sec . b ( both for our model , and the strong mn ##c baseline [ reference ] ) . section :",
        "pred_seq": "[SEP] [SEP] ho ##sing [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [],
                "Metric": [
                    [
                        "holistic instancelevel human parsing"
                    ]
                ],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "pascal personparts",
                        "pascal personpart dataset",
                        "pascal part training set",
                        "personpart instances",
                        "pascal personpart training set",
                        "pascal dataset"
                    ]
                ],
                "Method": [
                    [
                        "instancelevel part",
                        "partlevel instance segmentation",
                        "semantic part segmentation",
                        "instancelevel segmentation",
                        "instancelevel segmentation module",
                        "human instance segmentation",
                        "instancelevel part segmentation",
                        "instancelevel human segmentation",
                        "instancelevel segmentations",
                        "instance module"
                    ]
                ],
                "Metric": [
                    [
                        "ap r metric",
                        "ap metric",
                        "ap r"
                    ]
                ],
                "Task": [
                    [
                        "holistic instancelevel human parsing",
                        "parsing humans",
                        "human parsing"
                    ]
                ]
            }
        ]
    },
    "60": {
        "doctext": "disc ##rim ##ina ##tive un ##su ##per ##vis ##ed feature learning with con ##vo ##lu ##tion ##al neural networks current methods for training con ##vo ##lu ##tion ##al neural networks depend on large amounts of labeled samples for supervised training . in this paper we present an approach for training a con ##vo ##lu ##tion ##al neural network using only un ##lab ##ele ##d data . we train the network to disc ##rim ##inate between a set of sur ##rogate classes . each sur ##rogate class is formed by applying a variety of transformations to a randomly sampled ' seed ' image patch . we find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition . the feature representation learned by our algorithm achieve ##s classification results matching or out ##per ##form ##ing the current state - of - the - art for un ##su ##per ##vis ##ed learning on several popular data ##set ##s ( st ##l - 10 , ci ##far - 10 , cal ##tech - 101 ) . 1 introduction con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) trained via back ##pro ##pa ##gation were recently shown to perform well on image classification tasks with millions of training images and thousands of categories [ 1 , 2 ] . the feature representation learned by these networks achieve ##s state - of - the - art performance not only on the classification task for which the network was trained , but also on various other visual recognition tasks , for example : classification on cal ##tech - 101 [ 2 , 3 ] , cal ##tech - 256 [ 2 ] and the cal ##tech ##uc ##sd birds data ##set [ 3 ] ; scene recognition on the sun - 39 ##7 database [ 3 ] ; detection on the pascal vo ##c data ##set [ 4 ] . this capability to general ##ize to new data ##set ##s makes supervised cnn training an attractive approach for generic visual feature learning . the downs ##ide of supervised training is the need for expensive labeling , as the amount of required labeled samples grows quickly the larger the model gets . the large performance increase achieved by methods based on the work of k ##riz ##he ##vsky et al . [ 1 ] was , for example , only possible due to massive efforts on manually ann ##ota ##ting millions of images . for this reason , un ##su ##per ##vis ##ed learning - although currently under ##per ##form ##ing - remains an appealing paradigm , since it can make use of raw un ##lab ##ele ##d images and videos . furthermore , on vision tasks outside classification it is not even certain whether training based on object class labels is advantage ##ous . for example , un ##su ##per ##vis ##ed feature learning is known to be beneficial for image restoration [ 5 ] and recent results show that it out ##per ##forms supervised feature learning also on des ##cript ##or matching [ 6 ] . in this work we combine the power of a disc ##rim ##ina ##tive objective with the major advantage of un ##su ##per ##vis ##ed feature learning : cheap data acquisition . we introduce a novel training procedure for con ##vo ##lu ##tion ##al neural networks that does not require any labeled data . it rather relies on an automatically generated sur ##rogate task . the task is created by taking the idea of data aug ##ment ##ation - which is commonly used in supervised learning - to the extreme . starting with trivial sur ##rogate classes consisting of one random image patch each , we aug ##ment the data by applying a random set of transformations to each patch . then we train a cnn to classify these sur ##rogate classes . we refer to this method as ex ##em ##pl ##ar training of con ##vo ##lu ##tion ##al neural networks ( ex ##em ##pl ##ar - cnn ) . the feature representation learned by ex ##em ##pl ##ar - cnn is , by construction , disc ##rim ##ina ##tive and invariant to typical transformations . we confirm this both theoretically and empirical ##ly , showing that this approach matches or out ##per ##forms all previous un ##su ##per ##vis ##ed feature learning methods on the standard image classification bench ##marks st ##l - 10 , ci ##far - 10 , and cal ##tech - 101 . 1 . 1 related work our approach is related to a large body of work on un ##su ##per ##vis ##ed learning of invariant features and training of con ##vo ##lu ##tion ##al neural networks . con ##vo ##lu ##tion ##al training is commonly used in both supervised and un ##su ##per ##vis ##ed methods to utilize the in ##var ##iance of image statistics to translations ( e . g . le ##cu ##n et al . [ 7 ] , ka ##vu ##k ##cu ##og ##lu et al . [ 8 ] , k ##riz ##he ##vsky et al . [ 1 ] ) . similar to our approach the current surge of successful methods employing con ##vo ##lu ##tion ##al neural networks for object recognition often rely on data aug ##ment ##ation to generate additional training samples for their classification objective ( e . g . k ##riz ##he ##vsky et al . [ 1 ] , ze ##ile ##r and fergus [ 2 ] ) . while we share the architecture ( a con ##vo ##lu ##tion ##al neural network ) with these approaches , our method does not rely on any labeled training data . in un ##su ##per ##vis ##ed learning , several studies on learning invariant representations exist . den ##ois ##ing auto ##en ##code ##rs [ 9 ] , for example , learn features that are robust to noise by trying to rec ##ons ##truct data from randomly per ##tur ##bed input samples . z ##ou et al . [ 10 ] learn invariant features from video by enforcing a temporal slow ##ness constraint on the feature representation learned by a linear auto ##en ##code ##r . so ##hn and lee [ 11 ] and hui [ 12 ] learn features invariant to local image transformations . in contrast to our disc ##rim ##ina ##tive approach , all these methods rely on directly modeling the input distribution and are typically hard to use for jointly training multiple layers of a cnn . the idea of learning features that are invariant to transformations has also been explored for supervised training of neural networks . the research most similar to ours is early work on tangent propagation [ 13 ] ( and the related double back ##pro ##pa ##gation [ 14 ] ) which aims to learn in ##var ##iance to small pre ##de ##fine ##d transformations in a neural network by directly penal ##izing the derivative of the output with respect to the magnitude of the transformations . in contrast , our algorithm does not regular ##ize the derivative explicitly . thus it is less sensitive to the magnitude of the applied transformation . this work is also loosely related to the use of un ##lab ##ele ##d data for regular ##izing supervised algorithms , for example self - training [ 15 ] or entropy regular ##ization [ 16 ] . in contrast to these semi - supervised methods , ex ##em ##pl ##ar - cnn training does not require any labeled data . finally , the idea of creating an auxiliary task in order to learn a good data representation was used by ahmed et al . [ 17 ] , col ##lo ##bert et al . [ 18 ] . 2 creating sur ##rogate training data the input to the training procedure is a set of un ##lab ##ele ##d images , which come from roughly the same distribution as the images to which we later aim to apply the learned features . we randomly sample n ##\u2208 [ 50 , 320 ##00 ] patches of size 32 ##\u00d7 ##32 pixels from different images at varying positions and scales forming the initial training set x = { x ##1 , . . . x ##n } . we are interested in patches containing objects or parts of objects , hence we sample only from regions containing considerable gradient ##s . we define a family of transformations { t ##\u03b1 | \u03b1 ##\u2208 a } parameter ##ized by vectors ##\u03b1 ##\u2208 a , where a is the set of all possible parameter vectors . each transformation t ##\u03b1 is a composition of elementary transformations from the following list : \u2022 translation : vertical or horizontal translation by a distance within 0 . 2 of the patch size ; \u2022 scaling : multiplication of the patch scale by a factor between 0 . 7 and 1 . 4 ; \u2022 rotation : rotation of the image by an angle up to 20 degrees ; \u2022 contrast 1 : multi ##ply the projection of each patch pixel onto the principal components of the set of all pixels by a factor between 0 . 5 and 2 ( factors are independent for each principal component and the same for all pixels within a patch ) ; \u2022 contrast 2 : raise sat ##uration and value ( s and v components of the hs ##v color representation ) of all pixels to a power between 0 . 25 and 4 ( same for all pixels within a patch ) , multi ##ply these values by a factor between 0 . 7 and 1 . 4 , add to them a value between ##\u2212 ##0 . 1 and 0 . 1 ; \u2022 color : add a value between ##\u2212 ##0 . 1 and 0 . 1 to the hue ( h component of the hs ##v color representation ) of all pixels in the patch ( the same value is used for all pixels within a patch ) . all numerical parameters of elementary transformations , when con ##cate ##nated together , form a single parameter vector ##\u03b1 . for each initial patch xi ##\u2208 x we sample k ##\u2208 [ 1 , 300 ] random parameter vectors { \u03b1 ##1 ##i , . . . , \u03b1 ##ki } and apply the corresponding transformations ti = { t ##\u03b1 ##1 ##i , . . . , t ##\u03b1 ##ki } to the patch xi . this yields the set of its transformed versions s ##xi = ti ##xi = { tx ##i | t ##\u2208 ti } . afterwards we sub ##tra ##ct the mean of each pixel over the whole resulting data ##set . we do not apply any other prep ##ro ##ces ##sing . exemplary patches sampled from the st ##l - 10 un ##lab ##ele ##d data ##set are shown in fig . 1 . examples of transformed versions of one patch are shown in fig . 2 . 3 learning algorithm given the sets of transformed image patches , we declare each of these sets to be a class by assign ##ing label i to the class s ##xi . we next train a cnn to disc ##rim ##inate between these sur ##rogate classes . formally , we minimize the following loss function : l ( x ) = [UNK] [UNK] t ##\u2208 ##ti l ( i , tx ##i ) , ( 1 ) where l ( i , tx ##i ) is the loss on the transformed sample tx ##i with ( sur ##rogate ) true label i . we use a cnn with a soft ##max output layer and opt ##imi ##ze the multi ##no ##mia ##l negative log likelihood of the network output , hence in our case l ( i , tx ##i ) = m ( e ##i , f ( tx ##i ) ) , m ( y , f ) = \u2212 \u3008 y , log f \u3009 = [UNK] k y ##k log fk , ( 2 ) where f ( \u00b7 ) denotes the function computing the values of the output layer of the cnn given the input data , and e ##i is the it ##h standard basis vector . we note that in the limit of an infinite number of transformations per sur ##rogate class , the objective function ( 1 ) takes the form [UNK] ( x ) = [UNK] xi ##\u2208 ##x e ##\u03b1 [ l ( i , t ##\u03b1 ##xi ) ] , ( 3 ) which we shall analyze in the next section . intuitive ##ly , the classification problem described above serves to ensure that different input samples can be distinguished . at the same time , it enforce ##s in ##var ##iance to the specified transformations . in the following sections we provide a foundation for this intuition . we first present a formal analysis of the objective , separating it into a well defined classification problem and a regular ##izer that enforce ##s in ##var ##iance ( resembling the analysis in wage ##r et al . [ 19 ] ) . we then discuss the derived properties of this classification problem and compare it to common practices for un ##su ##per ##vis ##ed feature learning . 3 . 1 formal analysis we denote by ##\u03b1 ##\u2208 a the random vector of transformation parameters , by g ( x ) the vector of activation ##s of the second - to - last layer of the network when presented the input patch x , by w the matrix of the weights of the last network layer , by h ( x ) = w ##g ( x ) the last layer activation ##s before applying the soft ##max , and by f ( x ) = soft ##max ( h ( x ) ) the output of the network . by plug ##ging in the definition of the soft ##max activation function soft ##max ( z ) = ex ##p ( z ) / \u2016 ex ##p ( z ) \u2016 1 ( 4 ) the objective function ( 3 ) with loss ( 2 ) takes the [UNK] xi ##\u2208 ##x e ##\u03b1 [ \u2212 \u3008 e ##i , h ( t ##\u03b1 ##xi ) \u3009 + log \u2016 ex ##p ( h ( t ##\u03b1 ##xi ) ) \u2016 1 ] . ( 5 ) with [UNK] = e ##\u03b1 [ g ( t ##\u03b1 ##xi ) ] being the average feature representation of transformed versions of the image patch xi we can re ##write e ##q . ( 5 ) [UNK] xi ##\u2208 ##x [ \u2212 \u3008 e ##i , [UNK] \u3009 + log \u2016 ex ##p ( [UNK] ) \u2016 1 ] + [UNK] xi ##\u2208 ##x [ e ##\u03b1 [ log \u2016 ex ##p ( h ( t ##\u03b1 ##xi ) ) \u2016 1 ] \u2212 log \u2016 ex ##p ( [UNK] ) \u2016 1 ] . ( 6 ) the first sum is the objective function of a multi ##no ##mia ##l log ##istic regression problem with input - target pairs ( [UNK] , e ##i ) . this objective falls back to the transformation - free instance classification problem l ( x ) = [UNK] xi ##\u2208 ##x l ( i , xi ) if g ( xi ) = e ##\u03b1 [ g ( t ##\u03b1 ##x ) ] . in general , this equality does not hold and thus the first sum enforce ##s correct classification of the average representation e ##\u03b1 [ g ( t ##\u03b1 ##xi ) ] for a given input sample . for a truly invariant representation , however , the equality is achieved . similarly , if we suppose that t ##\u03b1 ##x = x for ##\u03b1 = 0 , that for small values of ##\u03b1 the feature representation g ( t ##\u03b1 ##xi ) is approximately linear with respect to ##\u03b1 and that the random variable ##\u03b1 is centered , i . e . e ##\u03b1 [ \u03b1 ] = 0 , then [UNK] = e ##\u03b1 [ g ( t ##\u03b1 ##xi ) ] \u2248 e ##\u03b1 [ g ( xi ) + \u2207 ##\u03b1 ( g ( t ##\u03b1 ##xi ) ) | \u03b1 = 0 ##\u03b1 ] = g ( xi ) . the second sum in e ##q . ( 6 ) can be seen as a regular ##izer enforcing all h ( t ##\u03b1 ##xi ) to be close to their average value , i . e . , the feature representation is sought to be approximately invariant to the transformations t ##\u03b1 . to show this we use the convex ##ity of the function log \u2016 ex ##p ( \u00b7 ) \u2016 1 and jensen ' s inequality , which yields ( proof in supplementary material ) e ##\u03b1 [ log \u2016 ex ##p ( h ( t ##\u03b1 ##xi ) ) \u2016 1 ] \u2212 log \u2016 ex ##p ( [UNK] ) \u2016 1 ##\u2265 0 . ( 7 ) if the feature representation is perfectly invariant , then h ( t ##\u03b1 ##xi ) = [UNK] and inequality ( 7 ) turns to equality , meaning that the regular ##izer reaches its global minimum . 3 . 2 conceptual comparison to previous un ##su ##per ##vis ##ed learning methods suppose we want to un ##su ##per ##vis ##edly learn a feature representation useful for a recognition task , for example classification . the mapping from input images x to a feature representation g ( x ) should then satisfy two requirements : ( 1 ) there must be at least one feature that is similar for images of the same category y ( in ##var ##iance ) ; ( 2 ) there must be at least one feature that is sufficiently different for images of different categories ( ability to disc ##rim ##inate ) . most un ##su ##per ##vis ##ed feature learning methods aim to learn such a representation by modeling the input distribution p ( x ) . this is based on the assumption that a good model of p ( x ) contains information about the category distribution p ( y | x ) . that is , if a representation is learned , from which a given sample can be reconstructed perfectly , then the representation is expected to also en ##code information about the category of the sample ( ability to disc ##rim ##inate ) . additionally , the learned representation should be invariant to variations in the samples that are irrelevant for the classification task , i . e . , it should adhere to the manifold hypothesis ( see e . g . ri ##fa ##i et al . [ 20 ] for a recent discussion ) . in ##var ##iance is classical ##ly achieved by regular ##ization of the late ##nt representation , e . g . , by enforcing spa ##rs ##ity [ 8 ] or robust ##ness to noise [ 9 ] . in contrast , the disc ##rim ##ina ##tive objective in e ##q . ( 1 ) does not directly model the input distribution p ( x ) but learns a representation that disc ##rim ##inates between input samples . the representation is not required to rec ##ons ##truct the input , which is unnecessary in a recognition or matching task . this leaves more degrees of freedom to model the desired variability of a sample . as shown in our analysis ( see e ##q . ( 7 ) ) , we achieve partial in ##var ##iance to transformations applied during sur ##rogate data creation by forcing the representation g ( t ##\u03b1 ##xi ) of the transformed image patch to be predict ##ive of the sur ##rogate label assigned to the original image patch xi . it should be noted that this approach assumes that the transformations t ##\u03b1 do not change the identity of the image content . if we , for example , use a color transformation we will force the network to be invariant to this change and can not expect the extracted features to perform well in a task relying on color information ( such as different ##iating black panthers from pu ##mas ) 1 . 4 experiments to compare our disc ##rim ##ina ##tive approach to previous un ##su ##per ##vis ##ed feature learning methods , we report classification results on the st ##l - 10 [ 21 ] , ci ##far - 10 [ 22 ] and cal ##tech - 101 [ 23 ] data ##set ##s . moreover , we assess the influence of the aug ##ment ##ation parameters on the classification performance and study the in ##var ##iance properties of the network . 4 . 1 experimental setup the data ##set ##s we test on differ in the number of classes ( 10 for ci ##far and st ##l , 101 for cal ##tech ) and the number of samples per class . st ##l is especially well suited for un ##su ##per ##vis ##ed learning as it contains a large set of 100 , 000 un ##lab ##ele ##d samples . in all experiments ( except for the data ##set transfer experiment in the supplementary material ) we extracted sur ##rogate training data from the un ##lab ##ele ##d subset of st ##l - 10 . when testing on ci ##far - 10 , we res ##ized the images from 32 ##\u00d7 ##32 pixels to 64 ##\u00d7 ##64 pixels so that the scale of depicted objects roughly matches the two other data ##set ##s . we worked with two network architecture ##s . a \" small \" network was used to evaluate the influence of different components of the aug ##ment ##ation procedure on classification performance . it consists of two con ##vo ##lu ##tion ##al layers with 64 filters each followed by a fully connected layer with 128 neurons . this last layer is succeeded by a soft ##max layer , which serves as the network output . a \" large \" network , consisting of three con ##vo ##lu ##tion ##al layers with 64 , 128 and 256 filters respectively followed by a fully connected layer with 512 neurons , was trained to compare our method to the state - of - the ##art . in both models all con ##vo ##lu ##tion ##al filters are connected to a 5 ##\u00d7 ##5 region of their input . 2 ##\u00d7 ##2 max ##pool ##ing was performed after the first and second con ##vo ##lu ##tion ##al layers . drop ##out [ 24 ] was applied to the fully connected layers . we trained the networks using an implementation based on caf ##fe [ 25 ] . details on the training , the hyper ##para ##meter settings , and an analysis of the performance depending on the network architecture is provided in the supplementary material . our code and training data are available at http : / / l ##mb . inform ##ati ##k . un ##i - freiburg . de / resources . we applied the feature representation to images of arbitrary size by con ##vo ##lu ##tion ##ally computing the responses of all the network layers except the top soft ##max . to each feature map , we applied the pool ##ing method that is commonly used for the respective data ##set : 1 ) 4 - quadrant max - pool ##ing , resulting in 4 values per feature map , which is the standard procedure for st ##l - 10 and ci ##far - 10 [ 26 , 10 , 27 , 12 ] ; 2 ) 3 - layer spatial pyramid , i . e . max - pool ##ing over the whole image as well as within 4 quadrant ##s and within the cells of a 4 ##\u00d7 4 grid , resulting in 1 + 4 + 16 = 21 values per feature map , which is the standard for cal ##tech - 101 [ 28 , 10 , 29 ] . finally , we trained a linear support vector machine ( sv ##m ) on the poole ##d features . on all data ##set ##s we used the standard training and test protocols . on st ##l - 10 the sv ##m was trained on 10 pre - defined folds of the training data . we report the mean and standard deviation achieved on the fixed test set . for ci ##far - 10 we report two results : ( 1 ) training the sv ##m on the whole ci ##far - 10 training set ( ' ci ##far - 10 ' ) ; ( 2 ) the average over 10 random selections of 400 training samples per class ( ' ci ##far - 10 ( 400 ) ' ) . for cal ##tech - 101 we followed the usual protocol of selecting 30 random samples per class for training and not more than 50 samples per class for testing . this was repeated 10 times . 4 . 2 classification results in table 1 we compare ex ##em ##pl ##ar - cnn to several un ##su ##per ##vis ##ed feature learning methods , including the current state - of - the - art on each data ##set . we also list the state - of - the - art for supervised learning ( which is not directly comparable ) . additionally we show the dimensional ##ity of the feature vectors 1 ##su ##ch cases could be covered either by careful selection of applied transformations or by combining features from multiple networks trained with different sets of transformations and letting the final class ##ifier choose which features to use . produced by each method before final pool ##ing . the small network was trained on 800 ##0 sur ##rogate classes containing 150 samples each and the large one on 1600 ##0 classes with 100 samples each . the features extracted from the larger network match or out ##per ##form the best prior result on all data ##set ##s . this is despite the fact that the dimensional ##ity of the feature vector is smaller than that of most other approaches and that the networks are trained on the st ##l - 10 un ##lab ##ele ##d data ##set ( i . e . they are used in a transfer learning manner when applied to ci ##far - 10 and cal ##tech 101 ) . the increase in performance is especially pronounced when only few labeled samples are available for training the sv ##m ( as is the case for all the data ##set ##s except full ci ##far - 10 ) . this is in agreement with previous evidence that with increasing feature vector dimensional ##ity and number of labeled samples , training an sv ##m becomes less dependent on the quality of the features [ 26 , 12 ] . remarkably , on st ##l - 10 we achieve an accuracy of 72 . 8 % , which is a large improvement over all previously reported results . 4 . 3 detailed analysis we performed additional experiments ( using the \" small \" network ) to study the effect of three design choices in ex ##em ##pl ##ar - cnn training and valid ##ate the in ##var ##iance properties of the learned features . experiments on sampling ' seed ' patches from different data ##set ##s can be found in the supplementary . 4 . 3 . 1 number of sur ##rogate classes we varied the number n of sur ##rogate classes between 50 and 320 ##00 . as a sanity check , we also tried classification with random filters . the results are shown in fig . 3 . clearly , the classification accuracy increases with the number of sur ##rogate classes until it reaches an opt ##imum at about 800 ##0 sur ##rogate classes after which it did not change or even decreased . this is to be expected : the larger the number of sur ##rogate classes , the more likely it is to draw very similar or even identical samples , which are hard or impossible to disc ##rim ##inate . few such cases are not detrimental to the classification performance , but as soon as such collisions dominate the set of sur ##rogate labels , the disc ##rim ##ina ##tive loss is no longer reasonable and training the network to the sur ##rogate task no longer succeeds . to check the validity of this explanation we also plot in fig . 3 the classification error on the validation set ( taken from the sur ##rogate data ) computed after training the network . it rapidly grows as the number of sur ##rogate classes increases . we also observed that the optimal number of sur ##rogate classes increases with the size of the network ( not shown in the figure ) , but eventually sat ##ura ##tes . this demonstrates the main limitation of our approach to randomly sample ' seed ' patches : it does not scale to ar ##bit ##rar ##ily large amounts of un ##lab ##ele ##d data . however , we do not see this as a fundamental restriction and discuss possible solutions in section 5 . 4 . 3 . 2 number of samples per sur ##rogate class fig . 4 shows the classification accuracy when the number k of training samples per sur ##rogate class varies between 1 and 300 . the performance improves with more samples per sur ##rogate class and 2 on cal ##tech - 101 one can either measure average accuracy over all samples ( average overall accuracy ) or calculate the accuracy for each class and then average these values ( average per - class accuracy ) . these differ , as some classes contain fewer than 50 test samples . most researchers in ml use average overall accuracy . sat ##ura ##tes at around 100 samples . this indicates that this amount is sufficient to approximate the formal objective from e ##q . ( 3 ) , hence further increasing the number of samples does not significantly change the optimization problem . on the other hand , if the number of samples is too small , there is insufficient data to learn the desired in ##var ##iance properties . 4 . 3 . 3 types of transformations we varied the transformations used for creating the sur ##rogate data to analyze their influence on the final classification performance . the set of ' seed ' patches was fixed . the result is shown in fig . 5 . the value ' 0 ' corresponds to applying random compositions of all elementary transformations : scaling , rotation , translation , color variation , and contrast variation . different columns of the plot show the difference in classification accuracy as we discarded some types of elementary transformations . several tendencies can be observed . first , rotation and scaling have only a minor impact on the performance , while translations , color variations and contrast variations are significantly more important . secondly , the results on st ##l - 10 and ci ##far - 10 consistently show that spatial in ##var ##iance and color - contrast in ##var ##iance are approximately of equal importance for the classification performance . this indicates that variations in color and contrast , though often neglected , may also improve performance in a supervised learning scenario . third ##ly , on cal ##tech - 101 color and contrast transformations are much more important compared to spatial transformations than on the two other data ##set ##s . this is not surprising , since cal ##tech - 101 images are often well aligned , and this data ##set bias makes spatial in ##var ##iance less useful . 4 . 3 . 4 in ##var ##iance properties of the learned representation in a final experiment , we analyzed to which extent the representation learned by the network is invariant to the transformations applied during training . we randomly sampled 500 images from the st ##l - 10 test set and applied a range of transformations ( translation , rotation , contrast , color ) to each image . to avoid empty regions beyond the image boundaries when applying spatial transformations , we crop ##ped the central 64 ##\u00d7 ##64 pixel sub - patch from each 96 ##\u00d7 ##9 ##6 pixel image . we then applied two measures of in ##var ##iance to these patches . first , as an explicit measure of in ##var ##iance , we calculated the normal ##ized euclidean distance between normal ##ized feature vectors of the original image patch and the transformed one [ 10 ] ( see the supplementary material for details ) . the downs ##ide of this approach is that the distance between extracted features does not take into account how inform ##ative and disc ##rim ##ina ##tive they are . we there - fore evaluated a second measure - classification performance depending on the magnitude of the transformation applied to the classified patches - which does not come with this problem . to compute the classification accuracy , we trained an sv ##m on the central 64 ##\u00d7 64 pixel patches from one fold of the st ##l - 10 training set and measured classification performance on all transformed versions of 500 samples from the test set . the results of both experiments are shown in fig . 6 . due to space restrictions we show only few representative plots . overall the experiment empirical ##ly confirms that the ex ##em ##pl ##ar - cnn objective leads to learning invariant features . features in the third layer and the final poole ##d feature representation compare favorably to a hog baseline ( fig . 6 ( a ) ) . furthermore , adding stronger transformations in the sur ##rogate training data leads to more invariant classification with respect to these transformations ( fig . 6 ( b ) - ( d ) ) . however , adding too much contrast variation may deter ##ior ##ate classification performance ( fig . 6 ( d ) ) . one possible reason is that level of contrast can be a useful feature : for example , strong edges in an image are usually more important than weak ones . 5 discussion we have proposed a disc ##rim ##ina ##tive objective for un ##su ##per ##vis ##ed feature learning by training a cnn without class labels . the core idea is to generate a set of sur ##rogate labels via data aug ##ment ##ation . the features learned by the network yield a large improvement in classification accuracy compared to features obtained with previous un ##su ##per ##vis ##ed methods . these results strongly indicate that a disc ##rim ##ina ##tive objective is superior to objectives previously used for un ##su ##per ##vis ##ed feature learning . one potential short ##coming of the proposed method is that in its current state it does not scale to ar ##bit ##rar ##ily large data ##set ##s . two probable reasons for this are that ( 1 ) as the number of sur ##rogate classes grows larger , many of them become similar , which contra ##dict ##s the disc ##rim ##ina ##tive objective , and ( 2 ) the sur ##rogate task we use is relatively simple and does not allow the network to learn in ##var ##iance to complex variations , such as 3d viewpoint changes or inter - instance variation . we h ##yp ##oth ##es ##ize that the presented approach could learn more powerful higher - level features , if the sur ##rogate data were more diverse . this could be achieved by using additional weak supervision , for example , by means of video or a small number of labeled samples . another possible way of obtaining richer sur ##rogate training data and at the same time avoiding similar sur ##rogate classes would be ( un ##su ##per ##vis ##ed ) merging of similar sur ##rogate classes . we see these as interesting directions for future work . acknowledge ##ments we acknowledge funding by the er ##c starting grant video ##lea ##rn ( 279 ##40 ##1 ) ; the work was also partly supported by the brain ##link ##s - brain ##to ##ols cluster of excellence funded by the german research foundation ( d ##f ##g , grant number ex ##c 1086 ) .",
        "pred_seq": "ci 10 [SEP] disc learning [SEP] [SEP] visual recognition [SEP] [unused0] st 10 [SEP] disc learning [SEP] [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "cifar10"
                    ]
                ],
                "Method": [
                    [
                        "discriminative unsupervised feature learning"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "visual object recognition"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "stl10"
                    ]
                ],
                "Method": [
                    [
                        "discriminative unsupervised feature learning"
                    ]
                ],
                "Metric": [],
                "Task": []
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "cifar10",
                        "cifar"
                    ]
                ],
                "Method": [
                    [
                        "discriminative unsupervised feature learning with convolutional neural networks",
                        "convolutional neural networks",
                        "cnns",
                        "cnn"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "classification",
                        "image classification tasks",
                        "classification task",
                        "classification problem",
                        "ml"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "stl10",
                        "stl"
                    ]
                ],
                "Method": [
                    [
                        "discriminative unsupervised feature learning with convolutional neural networks",
                        "convolutional neural networks",
                        "cnns",
                        "cnn"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "classification",
                        "image classification tasks",
                        "classification task",
                        "classification problem",
                        "ml"
                    ]
                ]
            }
        ]
    },
    "61": {
        "doctext": "document : couple ##net : coupling global structure with local parts for object detection the region - based con ##vo ##lu ##tion ##al neural network ( cnn ) detectors such as faster r - cnn or r - fc ##n have already shown promising results for object detection by combining the region proposal sub ##net ##work and the classification sub ##net ##work together . although r - fc ##n has achieved higher detection speed while keeping the detection performance , the global structure information is ignored by the position - sensitive score maps . to fully explore the local and global properties , in this paper , we propose a novel fully con ##vo ##lu ##tion ##al network , named as couple ##net , to couple the global structure with local parts for object detection . specifically , the object proposals obtained by the region proposal network ( r ##p ##n ) are fed into the the coupling module which consists of two branches . one branch adopt ##s the position - sensitive roi ( ps ##roi ) pool ##ing to capture the local part information of the object , while the other employs the roi pool ##ing to en ##code the global and context information . next , we design different coupling strategies and normal ##ization ways to make full use of the complementary advantages between the global and local branches . extensive experiments demonstrate the effectiveness of our approach . we achieve state - of - the - art results on all three challenging data ##set ##s , a map of on vo ##c ##0 ##7 , on vo ##c ##12 , and on coco . codes will be made publicly available . section : introduction general object detection requires to accurately locate and classify all targets in the image or video . compared to specific object detection , such as face , pedestrian and vehicle detection , general object detection often faces more challenges due to the large inter - class appearance differences . the variations arise not only from changes in a variety of non - rigid deformation ##s , but also due to the tr ##un ##cation ##s , o ##cc ##lusion ##s and inter - class interference . however , no matter how complicated the objects are , when humans identify a target , the recognition of object categories is sub ##ser ##ved by both a global process that retrieve ##s structural information and a local process that is sensitive to individual parts . this mo ##tiv ##ates us to build a detection model that fused both global and local information . with the revival of con ##vo ##lu ##tion ##al neural networks ( cnn ) , cnn - based object detection pipeline ##s have been proposed consecutive ##ly and made impressive improvements in generic bench ##marks , pascal vo ##c and ms coco . as two representative region - based cnn approaches , fast / faster r - cnn uses a certain sub ##net ##work to predict the category of each region proposal while r - fc ##n conducts the inference with the position - sensitive score maps . through removing the roi - wise sub ##net ##work , r - fc ##n has achieved higher detection speed while keeping the detection performance . however , the global structure information is ignored by the ps ##roi pool ##ing . as shown in figure [ reference ] , using ps ##roi pool ##ing to extract local part information for final object category prediction , r - fc ##n leads to a low confidence score of 0 . 08 for the sofa detection since the local responses of sofa are disturbed by a women and a dog ( they are also the categories that need to be detected ) . conversely , the global structure of sofa could be extracted by the roi pool ##ing , but the confidence score is 0 . 45 , which is also very low for the incomplete structure of sofa . by coupling the global confidence with the local part confidence together , we can obtain a more reliable prediction with the confidence score of 0 . 78 . in fact , the idea of fu ##sing global and local information together is widely used in lots of visual tasks . in finger ##print recognition , gu combined the global orientation field and local min ##uti ##ae cue to largely improve the performance . in cl ##ique - graph matching , ni ##e proposed a cl ##ique - graph matching method by preserving global cl ##ique - to - cl ##ique correspondence and local una ##ry and pair ##wise correspondence ##s . in scene par ##sing , zhao designed a pyramid pool ##ing module to effectively extract hierarchical global context ##ual prior , and then con ##cate ##nated it with the local fc ##n feature to improve the performance . in traditional object detection , fe ##lz ##ens ##z ##wal ##b incorporated a global root model and several finer local part models to represent highly variable objects . all of which show that effective combination of the global structural properties and local fine - grain ##ed details can achieve complementary advantages . therefore , to fully explore the global and local clues , in this paper , we propose a novel full con ##vo ##lu ##tion ##al network named as couple ##net , to couple the global structure and local parts to boost the detection accuracy . specifically , the object proposals obtained by the r ##p ##n are fed into the coupling module which consists of two branches . one branch adopt ##s the ps ##roi pool ##ing to capture the local part information of the object , while the other employs the roi pool ##ing to en ##code the global and context information . moreover , we design different coupling strategies and normal ##ization ways to make full use of the complementary advantages between the global and local branches . with the coupling structure , our network can jointly learn the local , global and context expression of the objects , which makes the model have a more powerful representation capacity and general ##ization ability . extensive experiments demonstrate that couple ##net can significantly improve the detection performance . our detector shows competitive results on pascal vo ##c 07 / 12 and ms coco compared to other state - of - the - art detectors , even with model ensemble approaches . in summary , our main contributions are as follows : 1 . we propose a unified fully con ##vo ##lu ##tion ##al network to jointly learn the local , global and context information for object detection . 2 . we design different normal ##ization methods and coupling strategies to mine the compatibility and complement ##ari ##ty between the global and local branches . 3 . we achieve the state - of - the - art results on all three challenging data ##set ##s , a map of on vo ##c ##0 ##7 , on vo ##c ##12 , and on ms coco . section : related work before the arrival of cnn , visual tasks have been dominated by traditional paradigm ##s . as one of an outstanding framework , d ##pm described the object system using mixture ##s of multi - scale def ##or ##mable part models , including a coarse global root model and several finer local part models . the root model extracts structural information of the objects , while the part models capture local appearance properties of an object . the sum of root response and weighted average response of each part is used as the final confidence of an object . although d ##pm provides an elegant framework for object detection , the hand - crafted features , improved hog , are not disc ##rim ##ina ##tive enough to express the diversity of object categories . this is also the main reason that cnn completely surpassed the traditional methods in a short period time . in order to leverage the great success of deep neural networks for image classification , considerable object detection methods based on deep learning have been proposed . although there are end - to - end detection framework ##s , like ss ##d , yo ##lo and dense ##box , region - based systems ( fast / faster r - cnn and r - fc ##n ) still dominate the detection accuracy on generic bench ##marks . compared to the end - to - end framework , the region - based systems have several advantages . firstly , by exploit ##ing a divide - and - conquer strategy , the two - step framework is more stable and easier to converge . secondly , without the complicated data aug ##ment ##ation and training skills , you can still easily achieve state - of - the - art performance . the main reason for these advantages is that there is a certain structure to en ##code translation variance features for each proposal , since in deep networks , higher - layers contain more semantic meaning and less location information . as a consequence , a roi - wise sub ##net ##work or a position - sensitive roi pool ##ing layer is used to achieve the translation variance in region - based systems . however , all the existing region - based systems utilize either the region - level or part - level features to learn the variations , where each one alone is not representative enough for a variety of challenging situations . therefore , this mo ##tiv ##ates us to design a certain structure to take advantages of both the global and local features . in addition , context is known to play an important role in visual recognition . considerable works have been proposed for ex ##pl ##oting context in object detection . bell explored the use of rec ##urrent neural networks to model the context ##ual information . gi ##dar ##is proposed to utilize multiple context ##ual regions around the object . cai collected the context by pad ##ding the proposals for pedestrian and car detection . similar to these works , we also absorb the context prior to enhance the global feature representation . section : couple ##net in this section , we first introduce the architecture of the proposed couple ##net for object detection . then we explain in detail how we incorporate local representations , global appearance and context ##ual information for robust object detection . sub ##section : network architecture the architecture of our proposed couple ##net is illustrated in figure [ reference ] . our couple ##net includes two different branches : a ) a local part - sensitive fully con ##vo ##lu ##tion ##al network to learn the object - specific parts , denoted as local fc ##n ; b ) a global region - sensitive fully con ##vo ##lu ##tion ##al network to en ##code the whole appearance structure and context prior of the object , denoted as global fc ##n . we first use the image ##net pre - trained res ##net - 101 released in to initial ##ize our network . for our detection task , we remove the last average pool ##ing layer and the fc layer . given an input image , we extract candidate proposals by using the region proposal network ( r ##p ##n ) , which also shares con ##vo ##lu ##tion features with couple ##net following . then each proposal flows to two different branches : the local fc ##n and the global fc ##n . finally , the output of global and local fc ##n are coupled together as the final score of the object . we also perform class - ag ##nostic bound ##ing box regression in a similar way . sub ##section : local fc ##n to effectively capture the specific fine - grain ##ed parts in local fc ##n , we construct a set of part - sensitive score maps by app ##ending a 1 ##x ##1 con ##vo ##lu ##tion ##al layer with channels , where means we divide the object into local parts ( here is set to the default value 7 ) and is the number of object categories plus background . for each category , there are totally channels and each channel is responsible for encoding a specific part of the object . the final score of a category is determined by voting the responses . here we use position - sensitive roi pool ##ing layer in to extract object - specific parts and we simply perform average pool ##ing for voting . then , we obtain a - d vector which indicates the probability that the object belongs to each class . this procedure is equivalent to dividing a strong object category decision into the sum of multiple weak class ##ifiers , which serves as the ensemble of several part models . here we refer this part ensemble as local structure representation . as shown in figure [ reference ] ( a ) , for the truncated person , one can hardly get a strong response from the global description of the person due to tr ##un ##cation , on the contrary , our local fc ##n can effectively capture several specific parts , such as human nose , mouth , , which correspond to the regions with large responses in the feature map . we argue that the local fc ##n is much concerned with the internal structure and components , which can effectively reflect the local properties of visual object , especially when the object is o ##cc ##lu ##ded or the whole boundary is incomplete . however , for those having simple spatial structure and encompassing considerable background in the bound ##ing box , dining table , the local fc ##n alone is difficult to make robust predictions . thus it is necessary to add the global structure information to enhance the discrimination . sub ##section : global fc ##n for the global fc ##n , we aim to describe the object by using the whole region - level features . firstly , we attach a 102 ##4 - d 1 ##x ##1 con ##vo ##lu ##tion ##al layer after the last con ##vo ##lu ##tion ##al block in res ##net - 101 for reducing the dimension . due to the diverse size of the object , we insert a roi pool ##ing layer in to extract a fixed - length feature vector as the global structure description of the object . secondly , we use two con ##vo ##lu ##tion ##al layers with kern ##al size and respectively ( is set to the default value 7 ) to further abstract the global representation of roi . finally , the output of 1 ##x ##1 con ##vo ##lu ##tion is fed into the class ##ifier whose output is also a - d vector . in addition , context prior is the most basic and important factor for visual recognition tasks . for example , the boat usually travels in the water while is unlikely to fly in the sky . despite the higher layers in deep neural network can involve the spatial context information around the objects due to the large rec ##eptive field , zhou have shown that the practical rec ##eptive field is actually much smaller than the theoretical one . therefore , it is necessary to explicitly collect the surrounding information to reduce the chance of mis ##class ##ification . to enhance the feature representation ability of the global fc ##n , here we introduce the context ##ual information as an effective supplement . specifically , we extend the context region by 2 times larger than the size of original proposal . then the features roi poole ##d from the original region and context region are con ##cate ##nated together and fed into the latter roi - wise sub ##net ##work . as shown in figure [ reference ] , the context region is embedded into the global branch to extract a more complete appearance structure and disc ##rim ##ina ##tive prior representation , which will help the class ##ifier to better identity the object categories . due to the roi pool ##ing operation , the global fc ##n describes the proposal as a whole with cnn features , which can be seen as a global structure description of the object . therefore , it can easily deal with the objects with intact structure and finer scale . as shown in figure [ reference ] ( b ) , our global fc ##n shows a large confidence for the dining table . however , in most cases , natural scenes consist of considerable objects with o ##cc ##lusion ##s or tr ##un ##cation ##s , making the detection more difficult . figure [ reference ] ( a ) shows that using the global structure information alone can hardly make a confident prediction for the truncated person . by adding local part structural supports , the detection performance can be significantly boosted . therefore , it is essential to combine both local and global descriptions for a robust detection . sub ##section : coupling structure to match the same order of magnitude , we apply a normal ##ization operation to the output of local and global fc ##n before they are combined together . we explored two different methods to perform normal ##ization : an l ##2 normal ##ization layer or a 1 ##x ##1 con ##vo ##lu ##tion ##al layer to model the scale . meanwhile , how to couple the local and global output is also a problem that needs to be researched . here , we investigated three different coupling methods : element - wise sum , element - wise product and element - wise maximum . our experiments show that using 1 ##x ##1 con ##vo ##lu ##tion along with element - wise sum achieve ##s the best performance and we will discuss it in section [ reference ] . with the coupling structure , couple ##net simultaneously exploits the local parts , global structure and context prior for object detection . the whole network is fully con ##vo ##lu ##tion ##al and benefits from approximate joint training and multi - task learning . we also note that the global branch can be regarded as a lightweight faster r - cnn , in which all learn ##able parameters are from con ##vo ##lu ##tion ##al layers and the depth of roi - wise sub ##net ##work is only two . therefore , the computational complexity is far less than the sub ##net ##work in res ##net - based faster r - cnn system whose depth is ten . as a consequence , our couple ##net can perform the inference efficiently , which runs slightly slower than r - fc ##n but much more faster than faster r - cnn . section : experiments we train and evaluate our method on three challenging object detection data ##set ##s : pascal vo ##c ##200 ##7 , vo ##c ##20 ##12 and ms coco . since all these three data ##set ##s contain a variety of circumstances , which can sufficiently verify the effectiveness of our method . we demonstrate state - of - the - art results on all three data ##set ##s without bells and whistle ##s . sub ##section : ab ##lation studies on vo ##c ##200 ##7 we first perform experiments on pascal vo ##c 2007 with 20 object categories for detailed analysis of our proposed couple ##net detector . we train the models on the union set of vo ##c 2007 train ##val and vo ##c 2012 train ##val ( \" 07 + 12 \" ) following , and evaluate on vo ##c 2007 test set . object detection accuracy is measured by mean average precision ( map ) , all the ab ##lation experiments use single - scale training and testing , and we did not add the context prior . normal ##ization . since features extracted form different layers of cnn show various of scales , it is essential to normal ##ize different features before coupling them together . bell proposed to use l ##2 normal ##ization to each roi - poole ##d feature and re - scale back up by a empirical scale , which shows a great gain on vo ##c data ##set . in this paper , we also explore two different normal ##ization ways to normal ##ize the output of local and global fc ##n : an l ##2 normal ##ization layer or a 1 ##x ##1 con ##vo ##lu ##tion ##al layer to learn the scale . as shown in table [ reference ] , we find that the use of l ##2 normal ##ization decreases the performance greatly , even worse than the direct addition ( without any normal ##ization ways ) . to explain such a phenomenon , we measured the outputs of two branches before and after l ##2 normal ##ization . we further found that l ##2 normal ##ization reduces the output gap between different categories , which results in a smaller score gap . as we know , a small score gap between different categories always means the class ##ifier can not make a confident prediction . therefore , we assume that this is the reason for the performance degradation . moreover , we also exploit a 1 ##x ##1 con ##vo ##lu ##tion to adaptive ##ly learn the scales between the global and local branches . table [ reference ] shows that using 1 ##x ##1 con ##vo ##lu ##tion increases by points compared to the direct addition and points over r - fc ##n . therefore , we use 1 ##x ##1 con ##vo ##lu ##tion to replace the l ##2 normal ##ization in the following experiments . coupling strategy . we explore three different response coupling strategies : element - wise sum , element - wise product and element - wise maximum . table [ reference ] shows the comparison results for the above three different implementations . we can see that the element - wise sum always achieve ##s the best performance even though in different normal ##ization methods . generally , current advanced residual networks also use element - wise sum as the effective way to integrate information from previous layers , which greatly facilitates the circulation of information and achieve ##s the complementary advantages . for element - wise product , we argue that the system is relatively unstable and is susceptible to the weak side , which results in a large gradient to update the weak branch that makes it difficult to converge . for element - wise maximum , it equals to an ensemble model within the network to some extent , which lost ##s the advantages of mutual support compared to element - wise sum when both two branches are failed to detect the object . moreover , a better coupling strategy can be taken into consideration as the future work to further improve the accuracy , such as designing a more subtle nonlinear structure to learn the coupling relationship . model ensemble . model ensemble is commonly used to improve the final detection performance , since diverse initial ##ization of parameters and the random ##ness of training samples both lead to different performance for the same model . although the differences and complement ##ari ##ties will be more pronounced for different models , the promotion is often very limited . as shown in table [ reference ] , we also compare our couple ##net with the model ensemble . for a fair comparison , we first re - implemented faster r - cnn using res ##net - 101 and online hard example mining ( oh ##em ) , which achieve ##s a map of on vo ##c ##0 ##7 ( in original paper without oh ##em ) . we also re - implemented r - fc ##n with appropriate joint training using the public available code p ##y - r - fc ##n , which achieve ##s a slightly lower result compared to ( vs . ) . we use our rei ##mple ##ment ##ation models to conduct the comparisons for consistency . we found that the promotion brought by model ensemble is less than 1 point . as shown in table [ reference ] , it is far less than our method ( ) . on the one hand , we argue that the naive model ensemble just combines the results together and does not essentially guide the learning process of the network , while our couple ##net can simultaneously utilize the global and local information to update the network and to in ##fer the final results . on the other hand , our method enjoys end - to - end training and there is no need to train multiple models , thus greatly reducing the training time . amount of parameters . since our couple ##net introduces a few more parameters compared with the single branch detectors , to further verify effectiveness of the coupling structure , here we increase the parameters of the prediction head for each single branch implementation to maintain the same amount of parameters with couple ##net for comparison . in detail , we add a new residual variant block with three con ##vo ##lu ##tion layers , where the kernel size is 1 ##x ##1 ##x ##25 ##6 , 3 ##x ##3 ##x ##25 ##6 and 1 ##x ##1 ##x ##10 ##24 respectively , to the prediction sub - network . we found that the standard r - fc ##n with one or two extra heads got a map of and respectively in vo ##c ##0 ##7 , which is slightly higher than our re - implemented version ( ) in as shown in table [ reference ] . meanwhile , our global fc ##n , which performs the roi pool ##ing on top of con ##v ##5 , got a relative higher gain ( a map of for one head , for two heads ) . the results indicate that simply adding more prediction layers obtain ##s a very limited performance gain , while our coupling structure shows more disc ##rim ##ina ##tive power with the same amount of parameters . sub ##section : results on vo ##c ##200 ##7 using the public available res ##net - 101 as the initial ##ization model , we note that our method is easy to follow and the hyper - parameters for training are the same as in . similarly , we use the dil ##ation strategy to reduce the effective stride of res ##net - 101 , just as shows , thus both the global and local branches have a stride of 16 . we also use a 1 - gp ##u implementation , and the effective mini - batch size is 2 images by setting the to 2 . the whole network is trained for 80 ##k iteration ##s with a learning rate of 0 . 001 and then for 30 ##k iteration ##s with 0 . 000 ##1 . in addition , the context prior is proposed to further boost the performance while keeping the iteration ##s unchanged . finally , we also perform multi - scale training with the shorter sides of images are randomly res ##ized from 480 to 86 ##4 . table [ reference ] shows the detailed comparisons with faster r - cnn and r - fc ##n . as we can see that our single model achieve ##s a map of , which out ##per ##forms the r - fc ##n by 2 . 2 points . however , while em ##bed ##ding the context prior to the global branch , our map rises up to , which is the current best single model detector to our knowledge . moreover , we also evaluate the inference time of our network using a n ##vid ##ia titan x gp ##u ( pascal ) along with cu ##da 8 . 0 and cu ##d ##nn - v ##5 . 1 . as shown in the last column of table [ reference ] , our method is slightly slower than r - fc ##n , which also reaches a real - time speed ( 8 . 2 f ##ps or 9 . 8 f ##ps without context ) and achieve ##s the best trade - off between accuracy and speed . we argue that the sharing process of feature extraction between two branches and the design of lightweight roi - wise sub ##net ##work after roi pool ##ing both greatly reduce the model complexity . as shown in table [ reference ] , we also compared our method with other state - of - the - art single model . we found that our method out ##per ##forms the others with a large margin , including the advanced end - to - end ss ##d method , which requires complicated data aug ##ment ##ation and careful training skills . just as discussed earlier , couple ##net shows a large gain over the classes with o ##cc ##lusion ##s , tr ##un ##cation ##s and considerable background information , like sofa , person , table and chair , which ve ##ri ##fies our analyses . we also observed a large improvement for airplane , bird , boat and pot ##ted ##pl ##ant , which usually have class - specific backgrounds , the sky for airplane and bird , water for boat and so on . therefore , the context surrounding the objects provides an extra auxiliary discrimination . sub ##section : results on vo ##c ##20 ##12 we also evaluate our method on the more challenging vo ##c ##20 ##12 data ##set by submit ##ting results to the public evaluation server . we use vo ##c ##0 ##7 train ##val , vo ##c ##0 ##7 test and vo ##c ##12 train ##val as the training set , which consists of 21 ##k images in total . we also follow the similar hyper - parameter settings in vo ##c ##0 ##7 but change the iteration ##s , since there are more training images . we train our models with 4 gp ##us , and the effective mini - batch size thus becomes 4 ( 1 per gp ##u ) . as a result , the network is trained for 60 ##k iteration ##s with a learning rate of 0 . 001 and 0 . 000 ##1 for the following 20 ##k iteration ##s . table [ reference ] shows the results on the vo ##c ##20 ##12 test set . our method obtain ##s a top map of , which is 2 . 8 points higher than r - fc ##n . we note that without using the extra tricks in the testing phase , our detector is the first one with a map higher than . similar promotions over the specific classes anal ##yse ##d in vo ##c ##0 ##7 are also observed , which once again valid ##ates the effectiveness of our method . figure [ reference ] shows some detection examples on vo ##c 2012 test set . sub ##section : results on ms coco next we present more results on the microsoft coco object detection data ##set . the data ##set consists of 80 ##k training set , 40 ##k validation set and 20 ##k test - dev set , which involves 80 object categories . all our models are trained on the union set of 80 ##k training set and 40 ##k validation set , and evaluated on 20 ##k test - dev set . the coco standard metric denotes as ap , which is evaluated at . following the vo ##c ##20 ##12 , a 4 - gp ##u implementation is used to accelerate the training process . we use an initial learning rate of 0 . 001 for the first 510 ##k iteration ##s and 0 . 000 ##1 for the next 70 ##k iteration ##s . in addition , we conduct multi - scale training with the scales are randomly sampled from while testing in a single scale . table [ reference ] shows our results . our single - scale trained detector has already achieved a result of , which out ##per ##forms the r - fc ##n by 3 . 9 points . in addition , the multi - scale training further improves the performance up to . interesting ##ly , we observed that the more challenging the data ##set , the more the promotion ( , for vo ##c ##0 ##7 , for vo ##c ##12 and for coco , all in multi - scale training ) , which directly proves that our approach can effectively cope with a variety of complex situations . section : conclusion in this paper , we present the couple ##net , a con ##cise yet effective network that simultaneously couples global , local and context cues for accurate object detection . our system naturally combines the advantages of different region - based approaches with the coupling structure . with the combination of local part representation , global structural information and the context ##ual assistance , our couple ##net achieve ##s state - of - the - art results on the challenging pascal vo ##c and coco data ##set ##s without using any extra tricks in the testing phase , which valid ##ates the effectiveness of our method . bibliography : references",
        "pred_seq": "vo ##7 [SEP] couple ##net [SEP] map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map map maps maps maps maps maps maps maps maps maps maps maps maps maps [CLS]",
        "pred_templates": [],
        "gold_templates": [
            {
                "Material": [
                    [
                        "voc07",
                        "pascal voc 07",
                        "voc2007",
                        "union set of voc 2007 trainval",
                        "voc 2007 test set"
                    ]
                ],
                "Method": [
                    [
                        "couplenet"
                    ]
                ],
                "Metric": [
                    [
                        "map",
                        "computational complexity",
                        "mean average precision"
                    ]
                ],
                "Task": [
                    [
                        "object detection",
                        "detection",
                        "sofa detection",
                        "detection task",
                        "robust detection"
                    ]
                ]
            }
        ]
    },
    "62": {
        "doctext": "outrageous ##ly large neural networks : the sparsely - gate ##d mixture - of - experts layer section : abstract the capacity of a neural network to absorb information is limited by its number of parameters . conditional computation , where parts of the network are active on a per - example basis , has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation . in practice , however , there are significant algorithm ##ic and performance challenges . in this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000 ##x improvements in model capacity with only minor losses in computational efficiency on modern gp ##u clusters . we introduce a sparsely - gate ##d mixture - of - experts layer ( moe ) , consisting of up to thousands of feed - forward sub - networks . a train ##able ga ##ting network determines a sparse combination of these experts to use for each example . we apply the moe to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corp ##ora . we present model architecture ##s in which a moe with up to 137 billion parameters is applied con ##vo ##lu ##tion ##ally between stacked l ##st ##m layers . on large language modeling and machine translation bench ##marks , these models achieve significantly better results than state - of - the - art at lower computational cost . section : introduction and related work section : conditional computation exploit ##ing scale in both training data and model size has been central to the success of deep learning . when data ##set ##s are sufficiently large , increasing the capacity ( number of parameters ) of neural networks can give much better prediction accuracy . this has been shown in domains such as text [ reference ] [ reference ] [ reference ] [ reference ] , images [ reference ] [ reference ] , and audio [ reference ] [ reference ] . for typical deep learning models , where the entire model is activated for every example , this leads to a roughly quad ##ratic blow - up in training costs , as both the model size and the number of training examples increase . unfortunately , the advances in computing power and distributed computation fall short of meeting such demand . various forms of conditional computation have been proposed as a way to increase model capacity without a proportional increase in computational costs [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] [ reference ] . in these schemes , large parts of a network are active or inactive on a per - example basis . the ga ##ting decisions may be binary or sparse and continuous , st ##och ##astic or deter ##mini ##stic . various forms of reinforcement learning and back - propagation are proposed for tr ##ari ##ning the ga ##ting decisions . \u2022 model capacity is most critical for very large data sets . the existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600 , 000 images . it is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters . in this work , we for the first time address all of the above challenges and finally realize the promise of conditional computation . we obtain greater than 1000 ##x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state - of - the - art results on public language modeling and translation data sets . section : our approach : the sparsely - gate ##d mixture - of - experts layer our approach to conditional computation is to introduce a new type of general purpose neural network component : a sparsely - gate ##d mixture - of - experts layer ( moe ) . the moe consists of a number of experts , each a simple feed - forward neural network , and a train ##able ga ##ting network which selects a sparse combination of the experts to process each input ( see figure 1 ) . all parts of the network are trained jointly by back - propagation . while the introduced technique is generic , in this paper we focus on language modeling and machine translation tasks , which are known to benefit from very large models . in particular , we apply a moe con ##vo ##lu ##tion ##ally between stacked l ##st ##m layers [ reference ] , as in figure 1 . the moe is called once for each position in the text , selecting a potentially different combination of experts at each position . the different experts tend to become highly specialized based on syntax and semantics ( see appendix e table 9 ) . on both language modeling and machine translation bench ##marks , we improve on best published results at a fraction of the computational cost . section : related work on mixture ##s of experts since its introduction more than two decades ago [ reference ] [ reference ] , the mixture - of - experts approach has been the subject of much research . different types of expert architecture ##s ha ##e been proposed such as sv ##ms [ reference ] , ga ##uss ##ian processes [ reference ] [ reference ] [ reference ] , dir ##ich ##let processes [ reference ] , and deep networks . other work has focused on different expert configurations such as a hierarchical structure [ reference ] , infinite numbers of experts [ reference ] , and adding experts sequential ##ly [ reference ] . [ reference ] suggest an ensemble model in the format of mixture of experts for machine translation . the ga ##ting network is trained on a pre - trained ensemble nm ##t model . the works above concern top - level mixture ##s of experts . the mixture of experts is the whole model . [ reference ] introduce the idea of using multiple moe ##s with their own ga ##ting networks as parts of a deep model . it is intuitive that the latter approach is more powerful , since complex problems may contain many sub - problems each requiring different experts . they also all ##ude in their conclusion to the potential to introduce spa ##rs ##ity , turning moe ##s into a vehicle for computational computation . our work builds on this use of moe ##s as a general purpose neural network component . while [ reference ] uses two stacked moe ##s allowing for two sets of ga ##ting decisions , our con ##vo ##lu ##tion ##al application of the moe allows for different ga ##ting decisions at each position in the text . we also realize sparse ga ##ting and demonstrate its use as a practical way to massive ##ly increase model capacity . section : the structure of the mixture - of - experts layer the mixture - of - experts ( moe ) layer consists of a set of n \" expert networks \" e 1 , \u00b7 \u00b7 \u00b7 , e n , and a \" ga ##ting network \" g whose output is a sparse n - dimensional vector . figure 1 shows an overview of the moe module . the experts are themselves neural networks , each with their own parameters . although in principle we only require that the experts accept the same sized inputs and produce the same - sized outputs , in our initial investigations in this paper , we restrict ourselves to the case where the models are feed - forward networks with identical architecture ##s , but with separate parameters . let us denote by g ( x ) and e i ( x ) the output of the ga ##ting network and the output of the i - th expert network for a given input x . the output y of the moe module can be written as follows : we save computation based on the spa ##rs ##ity of the output of g ( x ) . wherever g ( x ) i = 0 , we need not compute e i ( x ) . in our experiments , we have up to thousands of experts , but only need to evaluate a handful of them for every example . if the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical moe . in a hierarchical moe , a primary ga ##ting network chooses a sparse weighted combination of \" experts \" , each of which is itself a secondary mixture - of - experts with its own ga ##ting network . in the following we focus on ordinary moe ##s . we provide more details on hierarchical moe ##s in appendix b . our implementation is related to other models of conditional computation . a moe whose experts are simple weight matrices is similar to the parameter ##ized weight matrix proposed in . a moe whose experts have one hidden layer is similar to the block - wise drop ##out described in [ reference ] , where the dropped - out layer is sandwich ##ed between fully - activated layers . section : ga ##ting network soft ##max ga ##ting : a simple choice of non - sparse ga ##ting function [ reference ] is to multi ##ply the input by a train ##able weight matrix w g and then apply the so ##f t ##max function . noisy top - k ga ##ting : we add two components to the soft ##max ga ##ting network : spa ##rs ##ity and noise . before taking the soft ##max function , we add tuna ##ble ga ##uss ##ian noise , then keep only the top k values , setting the rest to ##\u2212 ##\u221e ( which causes the corresponding gate values to equal 0 ) . the spa ##rs ##ity serves to save computation , as described above . while this form of spa ##rs ##ity creates some theoretically scary disco ##nti ##nu ##ities in the output of ga ##ting function , we have not yet observed this to be a problem in practice . the noise term helps with load balancing , as will be discussed in appendix a . the amount of noise per component is controlled by a second train ##able weight matrix w noise . training the ga ##ting network we train the ga ##ting network by simple back - propagation , along with the rest of the model . if we choose k > 1 , the gate values for the top k experts have non ##zer ##o derivatives with respect to the weights of the ga ##ting network . this type of occasionally - sensitive behavior is described in [ reference ] ) with respect to noisy rec ##ti ##fi ##ers . gradient ##s also back ##pro ##pa ##gate through the ga ##ting network to its inputs . our method differs here from [ reference ] who use boo ##lean gates and a reinforce - style approach to train the ga ##ting network . section : addressing performance challenges section : the shrinking batch problem on modern cpu ##s and gp ##us , large batch sizes are necessary for computational efficiency , so as to amor ##ti ##ze the overhead of parameter loads and updates . if the ga ##ting network chooses k out of n experts for each example , then for a batch of b examples , each expert receives a much smaller batch of approximately kb n b examples . this causes a naive moe implementation to become very in ##ef ##fi ##cie ##nt as the number of experts increases . the solution to this shrinking batch problem is to make the original batch size as large as possible . however , batch size tends to be limited by the memory necessary to store activation ##s between the forwards and backwards passes . we propose the following techniques for increasing the batch size : mixing data parallel ##ism and model parallel ##ism : in a conventional distributed training setting , multiple copies of the model on different devices as ##yn ##ch ##ron ##ously process distinct batch ##es of data , and parameters are synchronized through a set of parameter servers . in our technique , these different batch ##es run sync ##hr ##ono ##usly so that they can be combined for the moe layer . we distribute the standard layers of the model and the ga ##ting network according to conventional data - parallel schemes , but keep only one shared copy of each expert . each expert in the moe layer receives a combined batch consisting of the relevant examples from all of the data - parallel input batch ##es . the same set of devices function as data - parallel replica ##s ( for the standard layers and the ga ##ting networks ) and as model - parallel shards ( each hosting a subset of the experts ) . if the model is distributed over d devices , and each device processes a batch of size b , each expert receives a batch of approximately kb ##d n examples . thus , we achieve a factor of d improvement in expert batch size . in the case of a hierarchical moe ( section b ) , the primary ga ##ting network employs data parallel ##ism , and the secondary moe ##s employ model parallel ##ism . each secondary moe resides on one device . this technique allows us to increase the number of experts ( and hence the number of parameters ) by proportional ##ly increasing the number of devices in the training cluster . the total batch size increases , keeping the batch size per expert constant . the memory and bandwidth requirements per device also remain constant , as do the step times , as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model . it is our goal to train a trillion ##para ##meter model on a trillion - word corpus . we have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware . taking advantage of con ##vo ##lu ##tion ##ality : in our language models , we apply the same moe to each time step of the previous layer . if we wait for the previous layer to finish , we can apply the moe to all the time steps together as one big batch . doing so increases the size of the input batch to the moe layer by a factor of the number of un ##roll ##ed time steps . increasing batch size for a rec ##urrent moe : we suspect that even more powerful models may involve applying a moe rec ##urrent ##ly . for example , the weight matrices of a l ##st ##m or other rn ##n could be replaced by a moe . sadly , such models break the con ##vo ##lu ##tion ##al trick from the last paragraph , since the input to the moe at one times ##te ##p depends on the output of the moe at the previous times ##te ##p . [ reference ] describe a technique for drastically reducing the number of stored activation ##s in an un ##roll ##ed rn ##n , at the cost of rec ##omp ##uting forward activation ##s . this would allow for a large increase in batch size . section : network bandwidth another major performance concern in distributed computing is network bandwidth . since the experts are stationary ( see above ) and the number of ga ##ting parameters is small , most of the communication involves sending the inputs and outputs of the experts across the network . to maintain computational efficiency , the ratio of an expert ' s computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device . for gp ##us , this may be thousands to one . in our experiments , we use experts with one hidden layer containing thousands of re ##lu - activated units . since the weight matrices in the expert have sizes input _ size ##\u00d7 ##hid ##den _ size and hidden _ size ##\u00d7 output _ size , the ratio of computation to input and output is equal to the size of the hidden layer . convenient ##ly , we can increase computational efficiency simply by using a larger hidden layer , or more hidden layers . section : balancing expert utilization we have observed that the ga ##ting network tends to converge to a state where it always produces large weights for the same few experts . this im ##balance is self - rein ##for ##cing , as the favored experts are trained more rapidly and thus are selected even more by the ga ##ting network . [ reference ] describe the same phenomenon , and use a hard constraint at the beginning of training to avoid this local minimum . [ reference ] include a soft constraint on the batch - wise average of each gate . [ reference ] we take a soft constraint approach . we define the importance of an expert relative to a batch of training examples to be the batch ##wise sum of the gate values for that expert . we define an additional loss l importance , which is added to the overall loss function for the model . this loss is equal to the square of the coefficient of variation of the set of importance values , multiplied by a hand - tuned scaling factor w importance . this additional loss encourages all experts to have equal importance . 1 [ reference ] also include two additional losses . one controls per - example spa ##rs ##ity , which we do not need since it is enforced by the fixed value of k . a third loss encourages diversity of gate values . in our experiments , we find that the gate values naturally divers ##ify as the experts special ##ize ( in a vi ##rt ##uous cycle ) , and we do not need to enforce diversity of gate values . while this loss function can ensure equal importance , experts may still receive very different numbers of examples . for example , one expert may receive a few examples with large weights , and another may receive many examples with small weights . this can cause memory and performance problems on distributed hardware . to solve this problem , we introduce a second loss function , l load , which ensures balanced loads . appendix a contains the definition of this function , along with experimental results . section : experiments section : 1 billion word language modeling bench ##mark data ##set : this data ##set , introduced by [ reference ] consists of shuffled unique sentences from news articles , totaling approximately 82 ##9 million words , with a vocabulary of 79 ##3 , 47 ##1 words . previous state - of - the - art : the best previously published results [ reference ] use models consisting of one or more stacked long short - term memory ( l ##st ##m ) layers [ reference ] [ reference ] . the number of parameters in the l ##st ##m layers of these models vary from 2 million to 151 million . quality increases greatly with parameter count , as do computational costs . results for these models form the top line of figure 2 - right . moe models : our models consist of two stacked l ##st ##m layers with a moe layer between them ( see figure 1 ) . we vary the sizes of the layers and the number of experts . for full details on model architecture , training regime ##n , additional baseline ##s and results , see appendix c . low computation , varied capacity : to investigate the effects of adding capacity , we trained a series of moe models all with roughly equal computational costs : about 8 million multi ##ply - and ##ad ##ds per training example per times ##te ##p in the forwards pass , excluding the soft ##max layer . we call this metric ( ops / times ##te ##p ) . we trained models with flat moe ##s containing 4 , 32 , and 256 experts , and models with hierarchical moe ##s containing 256 , 102 ##4 , and 40 ##9 ##6 experts . each expert had about 1 million parameters . for all the moe layers , 4 experts were active per input . the results of these models are shown in figure 2 - left . the model with 4 always - active experts performed ( un ##sur ##pr ##ising ##ly ) similarly to the computational ##ly - matched baseline models , while the largest of the models ( 40 ##9 ##6 experts ) achieved an impressive 24 % lower per ##plex ##ity on the test set . varied computation , high capacity : in addition to the largest model from the previous section , we trained two more moe models with similarly high capacity ( 4 billion parameters ) , but higher computation budgets . these models had larger l ##st ##ms , and fewer but larger and experts . details can be found in appendix c . 2 . results of these three models form the bottom line of figure 2 - right . table 1 compares the results of these models to the best previously - published result on this data ##set . even the fastest of these models beats the best published result ( when controlling for the number of training epoch ##s ) , despite requiring only 6 % of the computation . computational efficiency : we trained our models using tensor ##flow [ reference ] on clusters containing 16 - 32 tesla k ##40 gp ##us . for each of our models , we determine computational efficiency in t ##fl ##ops / gp ##u by dividing the number of floating point operations required to process one training batch by the observed step time and the number of gp ##us in the cluster . the operation counts used here are higher than the ones we report in our ops / times ##te ##p numbers in that we include the backwards pass , we include the importance - sampling - based training of the soft ##max layer , and we count a multi ##ply - and - add as two separate operations . for all of our moe models , the floating point operations involved in the experts represent between 37 % and 46 % of the total . for our baseline models w ##ti ##h no moe , observed computational efficiency ranged from 1 . 07 - 1 . 29 t ##fl ##ops / gp ##u . for our low - computation moe models , computation efficiency ranged from 0 . 74 - 0 . 90 t ##fl ##ops / gp ##u , except for the 4 - expert model which did not make full use of the available parallel ##ism . our highest - computation moe model was more efficient at 1 . 56 t ##fl ##ops / gp ##u , likely due to the larger matrices . these numbers represent a significant fraction of the theoretical maximum of 4 . 29 t ##fl ##ops / gp ##u claimed by n ##vid ##ia . detailed results are in appendix c , table 7 . section : 100 billion word google news corpus figure 3 : language modeling on a 100 billion word corpus . models have similar computational budgets ( 8 million ops / times ##te ##p ) . on the 1 - billion - word corpus , adding additional capacity seems to produce dim ##ini ##shing returns as the number of parameters in the moe layer exceeds 1 billion , as can be seen in figure 2 - left . we h ##yp ##oth ##es ##ized that for a larger training set , even higher capacities would produce significant quality improvements . we constructed a similar training set consisting of shuffled unique sentences from google ' s internal news corpus , total ##ling roughly 100 billion words . similarly to the previous section , we tested a series of models with similar computational costs of about 8 million ops / times ##te ##p . in addition to a baseline l ##st ##m model , we trained models augmented with moe layers containing [ reference ] experts . this corresponds to up to 137 billion parameters in the moe layer . details on architecture , training , and results are given in appendix d . results : figure 3 shows test per ##plex ##ity as a function of capacity after training on 10 billion words ( top line ) and 100 billion words ( bottom line ) . when training over the full 100 billion words , test per ##plex ##ity improves significantly up to 65 ##53 ##6 experts ( 68 billion parameters ) , dropping 39 % lower than the computational ##ly matched baseline , but de ##grade ##s at 131 ##0 ##7 ##2 experts , possibly a result of too much spa ##rs ##ity . the widening gap between the two lines demonstrates ( un ##sur ##pr ##ising ##ly ) that increased model capacity helps more on larger training sets . even at 65 ##53 ##6 experts ( 99 . 99 ##4 % layer spa ##rs ##ity ) , computational efficiency for the model stays at a respectable 0 . 72 t ##fl ##ops / gp ##u . section : machine translation ( single language pair ) model architecture : our model was a modified version of the g ##n ##mt model described in [ reference ] . to reduce computation , we decreased the number of l ##st ##m layers in the en ##code ##r and deco ##der from 9 and 8 to 3 and 2 respectively . we inserted moe layers in both the en ##code ##r ( between layers 2 and 3 ) and the deco ##der ( between layers 1 and 2 ) . each moe layer contained up to 204 ##8 experts each with about two million parameters , adding a total of about 8 billion parameters to the models . further details on model architecture , testing procedure and results can be found in appendix e . data ##set ##s : we bench ##mark ##ed our method on the w ##mt ' 14 en ##\u2192 ##fr and en ##\u2192 ##de corp ##ora , whose training sets have 36 m sentence pairs and 5 m sentence pairs , respectively . the experimental protocols were also similar to those in [ reference ] : news ##test ##20 ##14 was used as the test set to compare against previous work [ reference ] [ reference ] [ reference ] , while the combination of news ##test ##20 ##12 and news ##test ##20 ##13 was used as the development set . we also tested the same model on a google ' s production english to french data . [ reference ] 2 . 79 39 . 22 214 m 278 m 6 days / 96 k ##80 ##s g ##n ##mt + r ##l [ reference ] 2 . 96 39 . 92 214 m 278 m 6 days / 96 k ##80 ##s p ##bm ##t [ reference ] 37 . 0 l ##st ##m ( 6 - layer ) [ reference ] 31 . 5 l ##st ##m ( 6 - layer + po ##sun ##k ) [ reference ] 33 . 1 deep ##att [ reference ] 37 . 7 deep ##att + po ##sun ##k [ reference ] 39 . 2 [ reference ] 5 . 25 24 . 91 214 m 278 m 1 day / 96 k ##80 ##s g ##n ##mt + r ##l [ reference ] 8 . 08 24 . 66 214 m 278 m 1 day / 96 k ##80 ##s p ##bm ##t [ reference ] 20 . 7 deep ##att [ reference ] 20 . 6 results : tables 2 , 3 , and 4 show the results of our largest models , compared with published results . our approach achieved b ##le ##u scores of 40 . 56 and 26 . 03 on the w ##mt ' 14 en ##\u2192 ##fr and en ##\u2192 ##de bench ##marks . as our models did not use r ##l ref ##ine ##ment , these results constitute significant gains of 1 . 34 and 1 . 12 b ##le ##u score on top of the strong baseline ##s in [ reference ] . the per ##plex ##ity scores are also better . 2 on the google production data ##set , our model achieved 1 . 01 higher test b ##le ##u score even after training for only one sixth of the time . section : multi ##ling ##ual machine translation data ##set : ( johnson et al . , 2016 ) train a single g ##n ##mt [ reference ] ) model on a very large combined data ##set of twelve language pairs . results are somewhat worse than those for 12 separately trained single - pair g ##n ##mt models . this is not surprising , given that the twelve models have 12 times the capacity and twelve times the aggregate training of the one model . we repeat this experiment with a single moe - augmented model . see appendix e for details on model architecture . we train our model on the same data ##set as [ reference ] and process the same number of training examples ( about 3 billion sentence pairs ) . our training time was shorter due to the lower computational budget of our model . section : results : results for the single - pair g ##n ##mt models , the multi ##ling ##ual g ##n ##mt model and the multi ##ling ##ual moe model are given in table 5 . the moe model achieve ##s 19 % lower per ##plex ##ity on the dev set than the multi ##ling ##ual g ##n ##mt model . on b ##le ##u score , the moe model significantly beats the multi ##ling ##ual g ##n ##mt model on 11 of the 12 language pairs ( by as much as 5 . 84 points ) , and even beats the mono ##ling ##ual g ##n ##mt models on 8 of 12 language pairs . the poor performance on english ##\u2192 korean seems to be a result of severe over ##train ##ing , as for the rare ##r language pairs a small number of real examples were highly overs ##amp ##led in the training corpus . section : conclusion this work is the first to demonstrate major wins from conditional computation in deep networks . we carefully identified the design considerations and challenges of conditional computing and addressed them with a combination of algorithm ##ic and engineering solutions . while we focused on text , conditional computation may help in other domains as well , provided sufficiently large training sets . we look forward to seeing many novel implementations and applications of conditional computation in the years to come . section : ac ##k ##now ##led ##gm ##ents we would like to thank all of the members of the google brain and google translate teams who helped us with this project , in particular z ##hi ##feng chen , yong ##hui wu , and melvin johnson . thanks also to our anonymous ic ##lr reviewers for the helpful suggestions on making this paper better . section : app ##end ##ices a load - balancing loss as discussed in section 4 , for load - balancing purposes , we want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples . unfortunately , the number of examples received by an expert is a discrete quantity , so it can not be used in back ##pro ##pa ##gation . instead , we define a smooth est ##ima ##tor load ( x ) of the number of examples assigned to each expert for a batch x of inputs . the smooth ##ness allows us to back - prop ##aga ##te gradient ##s through the est ##ima ##tor . this is the purpose of the noise term in the ga ##ting function . we define p ( x , i ) as the probability that g ( x ) i is non ##zer ##o , given a new random choice of noise on element i , but keeping the already - sampled choices of noise on the other elements . to compute p ( x , i ) , we note that the g ( x ) i is non ##zer ##o if and only if h ( x ) i is greater than the k th - greatest element of h ( x ) excluding itself . the probability works out to be : where k ##th _ excluding ( v , k , i ) means the k ##th highest component of v , excluding component i . sim ##plify ##ing , we get : where ##\u03c6 is the cd ##f of the standard normal distribution . we can now define the load loss to be the square of the coefficient of variation of the load vector , multiplied by a hand - tuned scaling factor w load . initial load im ##balance : to avoid out - of - memory errors , we need to initial ##ize the network in a state of approximately equal expert load ( since the soft constraints need some time to work ) . to accomplish this , we initial ##ize the matrices w g and w noise to all zero ##s , which yields no signal and some noise . section : experiments : we trained a set of models with identical architecture ( the moe - 256 model described in appendix c ) , using different values of w importance and w load . we trained each model for 10 epoch ##s , then measured per ##plex ##ity on the test set . we also measured the coefficients of variation in importance and load , as well as ratio of the load on the most over ##loaded expert to the average load . this last value is significant for load balancing purposes on distributed hardware . all of these metric ##s were averaged over several training batch ##es . results : results are reported in table 6 . all the combinations containing at least one the two losses led to very similar model quality , where having no loss was much worse . models with higher values of w load had lower loads on the most over ##loaded expert . section : b hi ##era ##chi ##cal mixture of experts if the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical moe . in a hierarchical moe , a primary ga ##ting network chooses a sparse weighted combination of \" experts \" , each of which is itself a secondary mixture - of - experts with its own ga ##ting network . 3 if the hierarchical moe consists of a groups of b experts each , we denote the primary ga ##ting network by g primary , the secondary ga ##ting networks by ( g 1 , g 2 . . g a ) , and the expert networks by ( e 0 , 0 , e 0 , 1 . . e a , b ) . the output of the moe is given by : our metric ##s of expert utilization change to the following : load primary and load i de ##onte the load functions for the primary ga ##ting network and i th secondary ga ##ting network respectively . x ( i ) denotes the subset of x for which g primary ( x ) i > 0 . it would seem simpler to let load h ( x ) i , j = load i ( x i ) j , but this would not have a gradient with respect to the primary ga ##ting network , so we use the formulation above . section : c 1 billion word language modeling bench ##mark - experimental details c . 1 8 - million - operations - per - times ##te ##p models model architecture : our model consists of five layers : a word em ##bed ##ding layer , a rec ##urrent long short - term memory ( l ##st ##m ) layer [ reference ] [ reference ] , a moe layer , a second l ##st ##m layer , and a soft ##max layer . the dimensional ##ity of the em ##bed ##ding layer , the number of units in each l ##st ##m layer , and the input and output dimensional ##ity of the moe layer are all equal to 512 . for every layer other than the soft ##max , we apply dr ##ou ##put [ reference ] to the layer output , dropping each activation with probability drop ##p rob , otherwise dividing by ( 1 ##\u2212 drop ##p rob ) . after drop ##out , the output of the previous layer is added to the layer output . this residual connection encourages gradient flow [ reference ] . for the hierarchical moe layers , the first level branching factor was 16 , corresponding to the number of gp ##us in our cluster . we use noisy - top - k ga ##ting ( see section 2 . 1 ) with k = 4 for the ordinary moe layers and k = 2 at each level of the hierarchical moe layers . thus , each example is processed by exactly 4 experts for a total of 4 m ops / times ##te ##p . the two l ##st ##m layers contribute 2 m ops / times ##te ##p each for the desired total of 8 ##m . section : computational ##ly - matched baseline ##s : the moe - 4 model does not employ spa ##rs ##ity , since all 4 experts are always used . in addition , we trained four more computational ##ly - matched baseline models with no spa ##rs ##ity : \u2022 moe - 1 - wide : the moe layer consists of a single \" expert \" containing one re ##lu - activated hidden layer of size 40 ##9 ##6 . \u2022 moe - 1 - deep : the moe layer consists of a single \" expert \" containing four re ##lu - activated hidden layers , each with size 102 ##4 . \u2022 4 ##x ##ls ##tm - 512 : we replace the moe layer with two additional 512 - unit l ##st ##m layers . \u2022 l ##st ##m - 204 ##8 - 512 : the model contains one 204 ##8 - unit l ##st ##m layer ( and no moe ) . the output of the l ##st ##m is projected down to 512 dimensions [ reference ] . the next times ##te ##p of the l ##st ##m receives the projected output . this is identical to one of the models published in [ reference ] . we re - ran it to account for differences in training regime ##n , and obtained results very similar to the published ones . training : the models were trained on a cluster of 16 k ##40 gp ##us using the sync ##hr ##ono ##us method described in section 3 . each batch consisted of a set of sentences totaling roughly 300 , 000 words . in the interest of time , we limited training to 10 epoch ##s , ( 27 , 000 steps ) . training took 12 - 16 hours for all models , except for moe - 4 , which took 18 hours ( since all the expert computation was performed on only 4 of 16 gp ##us ) . we used the adam opt ##imi ##zer [ reference ] . the base learning rate was increased linear ##ly for the first 1000 training steps , and decreased after that so as to be proportional to the inverse square root of the step number . the soft ##max output layer was trained efficiently using importance sampling similarly to the models in [ reference ] . for each model , we performed a hyper - par ##meter search to find the best drop ##out probability , in inc ##rem ##ents of 0 . 1 . to ensure balanced expert utilization we set w importance = 0 . 1 and w load = 0 . 1 , as described in section 4 and appendix a . results : we evaluate our model using per ##plex ##ity on the hold ##out data ##set , used by [ reference ] [ reference ] . we follow the standard procedure and sum over all the words including the end of sentence symbol . results are reported in table 7 . for each model , we report the test per ##plex ##ity , the computational budget , the parameter counts , the value of drop ##p rob , and the computational efficiency . section : c . 2 more expensive models we ran two additional models ( moe - 34 m and moe - 143 m ) to investigate the effects of adding more computation in the presence of a large moe layer . these models have computation budgets of 34 m and 143 m ops / times ##te ##p . similar to the models above , these models use a moe layer between two l ##st ##m layers . the dimensional ##ity of the em ##bed ##ding layer , and the input and output dimensional ##ity of the moe layer are set to 102 ##4 instead of 512 . for moe - 34 m , the l ##st ##m layers have 102 ##4 units . for moe - 143 m , the l ##st ##m layers have 40 ##9 ##6 units and an output projection of size 102 ##4 [ reference ] . moe - 34 m uses a hierarchical moe layer with 102 ##4 experts , each with a hidden layer of size 204 ##8 . moe - 143 m uses a hierarchical moe layer with 256 experts , each with a hidden layer of size 81 ##9 ##2 . both models have 4 ##b parameters in the moe layers . we searched for the best drop ##p rob for each model , and trained each model for 10 epoch ##s . the two models achieved test per ##plex ##ity of 31 . 3 and 28 . 0 respectively , showing that even in the presence of a large moe , more computation is still useful . results are reported at the bottom of table 7 . the larger of the two models has a similar computational budget to the best published model from the literature , and training times are similar . comparing after 10 epoch ##s , our model has a lower test per ##plex ##ity by 18 % . section : d 100 billion word google news corpus - experimental details model architecture : the models are similar in structure to the 8 - million - operations - per - times ##te ##p models described in the previous section . we vary the number of experts between models , using an ordinary moe layer with 32 experts and hierarchical moe layers with 256 , 102 ##4 , 40 ##9 ##6 , 1638 ##4 , 65 ##53 ##6 and 131 ##0 ##7 ##2 experts . for the hierarchical moe layers , the first level branching factors are [ reference ] training : models are trained on a cluster of 32 tesla k ##40 gp ##us , except for the last two models , which are trained on clusters of 64 and 128 gp ##us so as to have enough memory for all the parameters . for all models , training batch sizes are approximately 2 . 5 million words . models are trained once - through over about 100 billion words . we implement several memory optimization ##s in order to fit up to 1 billion parameters per gp ##u . first , we do not store the activation ##s of the hidden layers of the experts , but instead rec ##omp ##ute them on the backwards pass . secondly , we modify the opt ##imi ##zer on the expert parameters to require less auxiliary storage : the adam opt ##imi ##zer [ reference ] keeps first and second moment estimates of the per ##para ##meter gradient ##s . this triple ##s the required memory . to avoid keeping a first - moment est ##ima ##tor , we set ##\u03b2 1 = 0 . to reduce the size of the second moment est ##ima ##tor , we replace it with a factor ##ed approximation . for a matrix of parameters , instead of maintaining a full matrix of second - moment est ##ima ##tors , we maintain vectors of row - wise and column - wise averages of that matrix . at each step , the matrix of est ##ima ##tors is taken to be the outer product of those two vectors divided by the mean of either one . this technique could similarly be applied to ada ##grad [ reference ] . results : we evaluate our model using per ##plex ##ity on a hold ##out data ##set . results are reported in table 8 . per ##plex ##ity after 100 billion training words is 39 % lower for the 68 - billion - parameter moe model than for the baseline model . it is notable that the measured computational efficiency of the largest model ( 0 . 30 t ##fl ##ops / gp ##u ) is very low compared to the other models . this is likely a result of the fact that , for purposes of comparison to the other models , we did not increase the training batch size proportional ##ly to the number of gp ##us . for comparison , we include results for a computational ##ly matched baseline model consisting of 4 l ##st ##ms , and for an un ##pr ##une ##d 5 - gram model with kn ##ese ##r - ne ##y smoothing [ reference ] . section : e machine translation - experimental details model architecture for single language pair moe models : our model is a modified version of the g ##n ##mt model described in [ reference ] . to reduce computation , we decrease the number of l ##st ##m layers in the en ##code ##r and deco ##der from 9 and 8 to 3 and 2 respectively . we insert moe layers in both the en ##code ##r ( between layers 2 and 3 ) and the deco ##der ( between layers 1 and 2 ) . we use an attention mechanism between the en ##code ##r and deco ##der , with the first deco ##der l ##st ##m receiving output from and providing input for the attention 5 . all of the layers in our model have input and output dimensional ##ity of 512 . our l ##st ##m layers have 204 ##8 hidden units , with a 512 - dimensional output projection . we add residual connections around all l ##st ##m and moe layers to encourage gradient flow [ reference ] . similar to g ##n ##mt , to effectively deal with rare words , we used sub ##word units ( also known as \" word ##piece ##s \" ) ( schuster & nak ##aj ##ima , 2012 ) for inputs and outputs in our system . we use a shared source and target vocabulary of 32 k word ##piece ##s . we also used the same beam search technique as proposed in [ reference ] . we train models with different numbers of experts in the moe layers . in addition to a baseline model with no moe layers , we train models with flat moe layers containing 32 experts , and models with hierarchical moe layers containing 512 and 204 ##8 experts . the flat moe layers use k = 4 and the hierarchical moe models use k = 2 at each level of the ga ##ting network . thus , each input is processed by exactly 4 experts in each moe layer . each expert in the moe layer is a feed forward network with one hidden layer of size 204 ##8 and re ##lu activation . thus , each expert contains [ 512 * 204 ##8 ] + [ 204 ##8 * 512 ] = 2 m parameters . the output of the moe layer is passed through a si ##gm ##oid function . we use the strictly - balanced ga ##ting function described in appendix f . section : model architecture for multi ##ling ##ual moe model : we used the same model architecture as for the single - language - pair models , with the following exceptions : we used noisy - top - k ga ##ting as described in section 2 . 1 , not the scheme from appendix f . the moe layers in the en ##code ##r and deco ##der are non - hierarchical moe ##s with n = 512 experts , and k = 2 . each expert has a larger hidden layer of size 81 ##9 ##2 . this doubles the amount of computation in the moe layers , raising the computational budget of the entire model from 85 m to 102 m ops / times ##te ##p . training : we trained our networks using the adam opt ##imi ##zer [ reference ] . the base learning rate was increased linear ##ly for the first 2000 training steps , held constant for an additional 800 ##0 steps , and decreased after that so as to be proportional to the inverse square root of the step number . for the single - language - pair models , similarly to [ reference ] , we applied drop ##out [ reference ] to the output of all em ##bed ##ding , l ##st ##m and moe layers , using drop ##p rob = 0 . 4 . training was done sync ##hr ##ono ##usly on a cluster of up to 64 gp ##us as described in section 3 . each training batch consisted of a set of sentence pairs containing roughly 1600 ##0 words per gp ##u . to ensure balanced expert utilization we set w importance = 0 . 01 and w load = 0 . 01 , as described in section 4 and appendix a . section : metric ##s : we evaluated our models using the per ##plex ##ity and the standard b ##le ##u score metric . we reported token ##ized b ##le ##u score as computed by the multi - b ##le ##u . pl script , downloaded from the public implementation of moses ( on gi ##th ##ub ) , which was also used in [ reference ] . tables 2 , 3 and 4 in section 5 . 3 show comparisons of our results to other published methods . figure 4 shows test per ##plex ##ity as a function of number of words in the ( training data ' s ) source sentences processed for models with different numbers of experts . as can be seen from the figure , as we increased the number of experts to approach 204 ##8 , the test per ##plex ##ity of our model continued to improve . figure 4 : per ##plex ##ity on w ##mt ' 14 en ##\u2192 fr ( left ) and google production en ##\u2192 fr ( right ) data ##set ##s as a function of number of words processed . the large differences between models at the beginning of training are due to different batch sizes . all models inc ##ur the same computational budget ( 85 m ops / times ##te ##p ) except the one with no experts . section : results : we found that the experts indeed become highly specialized by syntax and / or semantics , as can be seen in table 9 . for example , one expert is used when the indefinite article \" a \" introduces the direct object in a verb phrase indicating importance or leadership . section : f strictly balanced ga ##ting due to some peculiar ##ities in our infrastructure which have since been fixed , at the time we ran some of the machine translation experiments , our models ran faster if every expert received exactly the same batch size . to accommodate this , we used a different ga ##ting function which we describe below . recall that we define the soft ##max ga ##ting function to be : sparse ga ##ting ( alternate formulation ) : to obtain a sparse ga ##ting vector , we multi ##ply g ##\u03c3 ( x ) component - wise with a sparse mask m ( g ##\u03c3 ( x ) ) and normal ##ize the output . the mask itself is a function of g ##\u03c3 ( x ) and specifies which experts are assigned to each input example : as our experiments suggest and also observed in [ reference ] , using a batch ##wise function during training ( such as m batch ##wise ) requires modifications to the inference when we may not have a large batch of examples . our solution to this is to train a vector t of per - expert threshold values to approximate the effects of the batch ##wise mask . we use the following mask at inference time : to learn the threshold values , we apply an additional loss at training time which is minimize ##d when the batch ##wise mask and the threshold mask are identical . g attention function the attention mechanism described in g ##n ##mt [ reference ] ) involves a learned \" attention function \" a ( x i , y j ) which takes a \" source vector \" x i and a \" target vector \" y j , and must be computed for every source time step i and target time step j . in g ##n ##mt , the attention function is implemented as a feed forward neural network with a hidden layer of size n . it can be expressed as : where u and w are train ##able weight matrices and v is a train ##able weight vector . for performance reasons , in our models , we used a slightly different attention function : with our attention function , we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using opt ##imi ##zed matrix multiplication ##s . we found little difference in quality between the two functions . section :",
        "pred_seq": "[SEP] sparsely layer [SEP] accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy [CLS]",
        "pred_templates": [],
        "gold_templates": [
            {
                "Material": [
                    [
                        "trillionword corpus",
                        "billion word corpus",
                        "1billionword corpus",
                        "100 billion word"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "parameter count"
                    ]
                ],
                "Task": [
                    [
                        "language modeling",
                        "billion word language modeling benchmark",
                        "singlelanguagepair models"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "trillionword corpus",
                        "billion word corpus",
                        "1billionword corpus",
                        "100 billion word"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "perplexity scores",
                        "test perplexity",
                        "perplexity"
                    ]
                ],
                "Task": [
                    [
                        "language modeling",
                        "billion word language modeling benchmark",
                        "singlelanguagepair models"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "wmt14 en\u2192fr",
                        "googles production english to french data"
                    ]
                ],
                "Method": [
                    [
                        "sparselygated mixtureofexperts layer",
                        "mixtureofexperts layer",
                        "moe",
                        "model architectures",
                        "deep learning models",
                        "model capacity",
                        "mixtureofexperts approach",
                        "ensemble model",
                        "mixture of experts",
                        "ensemble nmt model",
                        "moes",
                        "deep model",
                        "stacked moes",
                        "level hierarchical moe",
                        "trainable weight matrix w noise",
                        "modelparallel shards",
                        "secondary moes",
                        "model parallelism",
                        "trillionparameter model",
                        "language models",
                        "moe",
                        "language modeling",
                        "hierachical mixture of experts",
                        "8millionoperationspertimestep models",
                        "softmax output layer",
                        "importance sampling",
                        "m",
                        "singlelanguagepair models",
                        "hierarchical moes"
                    ]
                ],
                "Metric": [
                    [
                        "bleu scores",
                        "bleu score"
                    ]
                ],
                "Task": [
                    [
                        "machine translation",
                        "multilingual machine translation"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "en\u2192de corpora",
                        "en\u2192de"
                    ]
                ],
                "Method": [
                    [
                        "sparselygated mixtureofexperts layer",
                        "mixtureofexperts layer",
                        "moe",
                        "model architectures",
                        "deep learning models",
                        "model capacity",
                        "mixtureofexperts approach",
                        "ensemble model",
                        "mixture of experts",
                        "ensemble nmt model",
                        "moes",
                        "deep model",
                        "stacked moes",
                        "level hierarchical moe",
                        "trainable weight matrix w noise",
                        "modelparallel shards",
                        "secondary moes",
                        "model parallelism",
                        "trillionparameter model",
                        "language models",
                        "moe",
                        "language modeling",
                        "hierachical mixture of experts",
                        "8millionoperationspertimestep models",
                        "softmax output layer",
                        "importance sampling",
                        "m",
                        "singlelanguagepair models",
                        "hierarchical moes"
                    ]
                ],
                "Metric": [
                    [
                        "bleu scores",
                        "bleu score"
                    ]
                ],
                "Task": [
                    [
                        "machine translation",
                        "multilingual machine translation"
                    ]
                ]
            }
        ]
    },
    "63": {
        "doctext": "document : triple ##t pro ##ba ##bilis ##tic em ##bed ##ding for face verification and cluster ##ing despite significant progress made over the past twenty five years , un ##con ##stra ##ined face verification remains a challenging problem . this paper proposes an approach that couples a deep cnn - based approach with a low - dimensional disc ##rim ##ina ##tive em ##bed ##ding step , learned using triple ##t probability constraints to address the un ##con ##stra ##ined face verification problem . aside from yielding performance improvements , this em ##bed ##ding provides significant advantages in terms of memory and for post - processing operations like subject specific cluster ##ing . experiments on the challenging i ##j ##b - a data ##set show that the proposed algorithm performs close to the state of the art methods in verification and identification metric ##s , while requiring much less training data and training / test time . the superior performance of the proposed method on the cf ##p data ##set shows that the representation learned by our deep cnn is robust to large pose variation . furthermore , we demonstrate the robust ##ness of deep features to challenges including age , pose , blur and cl ##utter by performing simple cluster ##ing experiments on both i ##j ##b - a and l ##f ##w data ##set ##s . section : introduction recently , with the advent of curated face data ##set ##s like labeled faces in the wild ( l ##f ##w ) and advances in learning algorithms like deep neural nets , there is more hope that the un ##con ##stra ##ined face verification problem can be solved . a face verification algorithm compares two given template ##s that are typically not seen during training . research in face verification has progressed well over the past few years , resulting in the sat ##uration of performance on the l ##f ##w data ##set , yet the problem of un ##con ##stra ##ined face verification remains a challenge . this is evident by the performance of traditional algorithms on the publicly available i ##j ##b - a data ##set ( , ) that was released recently . moreover , despite the superb performance of cnn - based approaches compared to traditional methods , a draw ##back of such methods is the long training time needed . in this work , we present a deep cnn ( dc ##nn ) architecture that ensures faster training , and investigate how much the performance can be improved if we are provided domain specific data . specifically , our contributions are as follows : we propose a deep network architecture and a training scheme that ensures faster training time . we formula ##te a triple ##t probability em ##bed ##ding learning method to improve the performance of deep features for face verification and subject cluster ##ing . during training , we use a publicly available face data ##set to train our deep architecture . each image is pre - processed and aligned to a canonical view before passing it to the deep network whose features are used to represent the image . in the case of i ##j ##b - a data ##set , the data is divided into 10 splits , each split containing a training set and a test set . hence , to further improve performance , we learn the proposed triple ##t probability em ##bed ##ding using the training set provided with each split over the features extracted from our dc ##nn model . during the deployment phase , given a face template , we extract the deep features using the raw cnn model after implementing automatic pre - processing steps such as face detection and fi ##du ##cial extraction . the deep features are projected onto a low - dimensional space using the em ##bed ##ding matrix learned during training ( note that the projection involves only matrix multiplication ) . we use the 128 - dimensional feature as the final representation of the given face template . this paper is organized as follows : section [ reference ] places our work among the recently proposed approaches for face verification . section [ reference ] details the network architecture and the training scheme . the triple ##t pro ##ba ##bilis ##tic em ##bed ##ding learning method is described in section [ reference ] followed by results on i ##j ##b - a and cf ##p data ##set ##s and a brief discussion in section [ reference ] . in section [ reference ] , we demonstrate the ability of the proposed method to cluster a media collection from l ##f ##w and i ##j ##b - a data ##set ##s . section : related work in the past few years , there have been numerous works in using deep features for tasks related to face verification . the deep ##face approach uses a carefully crafted 3d alignment procedure to prep ##ro ##ces ##s face images and feeds them to a deep network that is trained using a large training set . more recently , face ##net uses a large private data ##set to train several deep network models using a triple ##t distance loss function . the training time for this network is of the order of few weeks . since the release of the i ##j ##b - a data ##set , there have been several works that have published verification results for this data ##set . previous approaches presented in and train deep networks using the cas ##ia - web ##face data ##set and the v ##gg - face data ##set respectively , requiring substantial training time . this paper proposes a network architecture and a training scheme that needs shorter training time and a small query time . the idea of learning a compact and disc ##rim ##ina ##tive representation has been around for decades . wei ##nberger et al . used a semi definite programming ( sd ##p ) - based formulation to learn a metric satisfying pair ##wise and triple ##t distance constraints in a large margin framework . more recently , this idea has been successfully applied to face verification by integrating the loss function within the deep network architecture ( , ) . joint bay ##esian metric learning is also another popular metric used for face verification ( , ) . these methods either require a large data ##set for convergence or learn a metric directly and therefore are not am ##ena ##ble to subsequent operations like disc ##rim ##ina ##tive cluster ##ing or hash ##ing . classic methods like t - s ##ne , t - ste and crowd kernel learning ( ck ##l ) perform extremely well when used to visual ##ize or cluster a given data collection . they either operate on the data matrix directly or the distance matrix generated from data by generating a large set of pair ##wise or triple ##t constraints . while these methods perform very well on a given set of data points , they do not general ##ize to out - of - sample data . in the current work , we aim to general ##ize such formulation ##s , to a more traditional classification setting , where domain specific training and testing data is provided . we formula ##te an optimization problem based on triple ##t pro ##ba ##bilities that performs dimensional ##ity reduction aside from improving the disc ##rim ##ina ##tive ability of the test data . the em ##bed ##ding scheme described in this work is a more general framework that can be applied to any setting where labeled training data is available . section : network architecture this section details the architecture and training algorithm for the deep network used in our work . our architecture consists of 7 con ##vo ##lu ##tion ##al layers with varying kernel sizes . the initial layers have a larger size rapidly sub ##sam ##pling the image and reducing the parameters while subsequent layers consist of small filter sizes , which has proved to be very useful in face recognition tasks ( , ) . furthermore , we use the para ##metric rec ##ti ##fi ##er linear units ( pre ##lus ) instead of re ##lus , since they allow a negative value for the output based on a learned threshold and have been shown to improve the convergence rate . table ##dee ##p network architecture details the top three con ##vo ##lu ##tion ##al layers ( con ##v ##1 - con ##v ##3 ) are initial ##ized with the weights from the alex ##net model trained on the image ##net challenge data ##set . several recent works ( , ) have empirical ##ly shown that this transfer of knowledge across different networks , albeit for a different objective , improves performance and more significantly reduces the need to train over a large number of iteration ##s . the compared methods either learn their deep models from scratch ( , ) or fine ##tu ##ne only the last layer of fully pre - trained models . the former results in large training time and the latter does not general ##ize well to the task at hand ( face verification ) and hence resulting in sub optimal performance . in the current work , even though we use a pre - trained model ( alex ##net ) to initial ##ize the proposed deep network , we do so only for the first three con ##vo ##lu ##tion ##al layers , since they retain more generic information ( ) . subsequent layers learn representations which are more specific to the task at hand . thus , to learn more task specific information , we add 4 con ##vo ##lu ##tion ##al layers each consisting of 512 kernel ##s of size . the layers con ##v ##4 - con ##v ##7 do not downs ##amp ##le the input thereby learning more complex higher dimensional representations . this hybrid architecture proves to be extremely effective as our raw cnn representation out ##per ##forms some very deep cnn models on the i ##j ##b - a data ##set ( table 2 in results ) . in addition , we achieve that performance by training the proposed deep network using the relatively smaller cas ##ia - web ##face data ##set . the architecture of our network is shown in table [ reference ] . layers con ##v ##4 - con ##v ##7 and the fully connected layers fc ##6 - fc ##8 are initial ##ized from scratch using random ga ##uss ##ian distributions . pre ##lu activation functions are added between each layer . since the network is used as a feature extract ##or , the last layer fc ##8 is removed during deployment , thus reducing the number of parameters to 29 ##m . the inputs to the network are 227 ##x ##22 ##7 ##x ##3 r ##gb images . when the network is deployed , the features are extracted from the fc ##7 layer resulting in a dimensional ##ity of 512 . the network is trained using the soft ##max loss function for multi ##class classification using the caf ##fe deep learning platform . section : learning a disc ##rim ##ina ##tive em ##bed ##ding in this section , we describe our algorithm for learning a low - dimensional em ##bed ##ding such that the resulting projections are more disc ##rim ##ina ##tive . aside from an improved performance , this em ##bed ##ding provides significant advantages in terms of memory and enables post - processing operations like visual ##ization and cluster ##ing . consider a triple ##t , where ( anchor ) and ( positive ) are from the same class , but ( negative ) belongs to a different class . consider a function that is parameter ##ized by the matrix , that measures the similarity between two vectors . ideally , for all triple ##ts that exist in the training set , we would like the following constraint to be satisfied : thus , the probability of a given triple ##t satisfying ( [ reference ] ) can be written as : the specific form of the similarity function is given as : . in our case , and are deep features normal ##ized to unit length . to learn the em ##bed ##ding from a given set of triple ##ts , we solve the following optimization : ( [ reference ] ) can be interpreted as maxim ##izing the likelihood ( [ reference ] ) or mini ##mi ##zing the negative log - likelihood ( nl ##l ) over the triple ##t set . in practice , the above problem is solved in a large - margin framework using st ##och ##astic gradient descent ( sg ##d ) and the triple ##ts are sampled online . the gradient update for is given as : where is the estimate at iteration , is the updated estimate , is the triple ##t sampled at the current iteration and is the learning rate . by choosing the dimension of as with , we achieve dimensional ##ity reduction in addition to improved performance . for our work , we fix based on cross validation and is the dimensional ##ity of our deep features . is initial ##ized with the first principal components of the training data . at each iteration , a random anchor and a random positive data point are chosen . to choose the negative , we perform hard negative mining , ie . we choose the data point that has the least likelihood ( [ reference ] ) among the randomly chosen 2000 negative instances at each iteration . since we compute the em ##bed ##ding matrix by opt ##imi ##zing over triple ##t pro ##ba ##bilities , we call this method triple ##t probability em ##bed ##ding ( t ##pe ) . the technique closest to the one presented in this section , which is used in recent works ( , ) compute ##s the em ##bed ##ding based on satisfying a hi ##nge loss constraint : acts a margin parameter for the loss function . to be consistent with the terminology used in this paper , we call it triple ##t distance em ##bed ##ding ( td ##e ) . to appreciate the difference between the two approaches , figure [ reference ] shows the case where the gradient update for the td ##e method ( [ reference ] ) occurs . if the value of is not appropriately chosen , a triple ##t is considered good even if the positive and negative are very close to one another . but under the proposed formulation , both cases referred to in figure [ reference ] will update the gradient but their contribution to the gradient will be mod ##ulated by the probability with which they violate the constraint in ( [ reference ] ) . this modulation factor is specified by the term in the gradient update for t ##pe in ( [ reference ] ) implying that if the likelihood of a sampled triple ##t satisfying ( [ reference ] ) is high , then the gradient update is given a lower weight and vice - versa . thus , in our method , the margin parameter ( ) is automatically set based on the likelihood . to compare the relative performances of the raw features before projection , with td ##e and with t ##pe ( proposed method ) , we plot the traditional roc curve ( tar ( vs ) far ) for split 1 of the i ##j ##b - a verify protocol for the three methods in figure [ reference ] . the equal error rate ( ee ##r ) metric is specified for each method . the performance improvement due to t ##pe is significant , especially at regions of far . we observed a similar behaviour for all the ten splits of the i ##j ##b - a data ##set . section : experimental setup and results in this section we evaluate the proposed method on two challenging data ##set ##s : ia ##rp ##a jan ##us bench ##mark - a ( i ##j ##b - a ) : this data ##set contains 500 subjects with a total of 25 , 81 ##3 images ( 5 , 39 ##9 still images and 20 , 41 ##4 video frames sampled at a rate of 1 in 60 ) . the faces in the i ##j ##b - a data ##set contain extreme poses and illumination ##s , more challenging than l ##f ##w . some sample images from the i ##j ##b - a data ##set are shown in figure [ reference ] . an additional challenge of the i ##j ##b - a verification protocol is that the template comparisons include image to image , image to set and set to set comparisons . in this work , for a given test template of the i ##j ##b - a data we perform two kinds of pool ##ing to produce its final representation : average pool ##ing ( cnn ) : the deep features of the images and / or frames present in the template are combined by taking a component ##wise average to produce one feature vector . thus each feature equally contributes to the final representation . media pool ##ing ( cnn ) : the deep features are combined keeping in mind the media source they come from . the metadata provided with i ##j ##b - a gives us the media i d for each item of the template . thus to get the final feature vector , we first take an intra - media average and then combine these by taking the inter - media average . thus each feature ' s contribution to the final representation is weighted based on its source . celebrities in frontal - profile ( cf ##p ) [ ] : this data ##set contains 700 ##0 images of 500 subjects . the data ##set is used for evaluating how face verification approaches handle pose variation . hence , it consists of 5000 images in frontal view and 2000 images in extreme profile . the data is organized into 10 splits , each containing equal number of frontal - frontal and frontal - profile comparisons . sample comparison pairs of the cf ##p data ##set are shown in figure [ reference ] . . 5 . 25 . 25 sub ##section : pre - processing in the training phase , given an input image , we use the hyper ##face method for face detection and fi ##du ##cial point extraction . the hyper ##face detector automatically extracts many faces from a given image . for the i ##j ##b - a data ##set , since most images contain more than one face , we use the bound ##ing boxes provided along with the data ##set to select the person of interest from the list of automatic detection ##s . we select the detection that has the maximum area overlap with the manually provided bound ##ing box . in the i ##j ##b - a data ##set , there are few images for which the hyper ##face detector can not find the relevant face . for the missed cases , we crop the face using the bound ##ing box information provided with the data ##set and pass it to hyper ##face to extract the fi ##du ##cial ##s . we use six fi ##du ##cial points ( eyes and mouth corners ) to align the detected image to a canonical view using the similarity transform . for the cf ##p data ##set , since the six key ##points can not be computed for profile faces we only use three key ##points on one side of the face for align ##ing them . table ##ide ##nti ##fication and verification results on the i ##j ##b - a data ##set . for identification , the scores reported are t ##pi ##r values at the indicated points . the results are averages over 10 splits and the standard deviation is given in the brackets for methods which have reported them . implies that the result is not reported for that method . the best results are given in bold . table ##res ##ult ##s on the cf ##p data ##set . the numbers are averaged over ten test splits and the numbers in brackets indicate standard deviation ##s of those runs . the best results are given in bold . sub ##section : parameters and training times the training of the proposed deep architecture is done using sg ##d with momentum , which is set to 0 . 9 and the learning rate is set to 1 ##e - 3 and decreased uniformly by a factor of 10 every 50 k iteration ##s . the weight decay is set to 5 ##e - 4 for all layers . the training batch size is set to 256 . the training time for our deep network is 24 hours on a single n ##vid ##ia titan ##x gp ##u . for the i ##j ##b - a data ##set , we use the training data provided with each split to obtain the triple ##t em ##bed ##ding which takes 3 min ##s per split . this is the only additional split ##wise processing that is done by the proposed approach . during deployment , the average enrollment time per image after pre - processing , including alignment and feature extraction is 8 ##ms . sub ##section : evaluation pipeline given an image , we pre - process it as described in section 5 . 1 . the deep features are computed as an average of the image and its flip . given two deep features to compare , we compute their co ##sin ##e similarity score . more specifically , for the i ##j ##b - a data ##set , given a template containing multiple faces , we flat ##ten the template features by average pool ##ing or media pool ##ing to obtain a vector representation . for each split , we learn the t ##pe projection using the provided training data . given two template ##s for comparison , we compute the co ##sin ##e similarity score using the projected 128 - dimensional representations . matrix . sub ##section : evaluation metric ##s we report two types of results for the i ##j ##b - a data ##set : verification and identification . for the verification protocol , we report the false non - match rate ( f ##n ##m ##r ) values at several false match rates ( fm ##r ) . for the identification results , we report open set and closed set metric ##s . for the open set metric ##s , the true positive identification rate quan ##ti ##fies the fraction of subjects that are classified correctly among the ones that exist in probe but not in gallery . for the closed set metric ##s , we report the cm ##c numbers at different values of false positive identification rates ( f ##pi ##rs ) and ranks . more details on the evaluation metric ##s for the i ##j ##b - a protocol can be found in . for the cf ##p data ##set , following the protocol set in , we report the area under the curve ( au ##c ) and equal error rate ( ee ##r ) values as averages across splits , in addition to the classification accuracy . to obtain the accuracy for each split , we threshold our cnn similarity scores where the threshold is set to the value that provides the highest classification accuracy over the training data for each split . sub ##section : discussion sub ##su ##bs ##ection : performance on i ##j ##b - a table [ reference ] presents the results for the proposed methods compared to existing results for the i ##j ##b - a verification and identification protocol . the compared methods are described below : government - of - the - shelf ( got ##s ) is the baseline performance provided along with the i ##j ##b - a data ##set . park ##hi et al . train a very deep network ( 22 layers ) over the v ##gg - face data ##set which contains 2 . 6 m images from 262 ##2 subjects . the neural aggregation network ( nan ) is trained over large amount of videos from the ce ##le ##b - 1000 data ##set starting from the google ##net architecture . mas ##i et al . use a deep cnn based approach that includes a combination of in - plane aligned images , 3d rendered images to aug ##ment their performance . the 3d rendered images are also generated during test time per template comparison . it should be noted that many test images of the i ##j ##b - a data ##set contain extreme poses , harsh illumination conditions and significant blur . cross ##w ##hit ##e et al . use template adaptation to tune the performance of their raw features specifically to the i ##j ##b - a data ##set . compared to these methods , the proposed method trains a single cnn model on the cas ##ia - web ##face data ##set which consists of about 500 k images and requires much shorter training time and has a very fast query time ( 0 . 08 ##s after face detection per image pair ) . as shown in table [ reference ] , our raw cnn features after media pool ##ing perform better than most compared methods across both the verification and identification protocols of the i ##j ##b - a data ##set , with the exception of the template adaptation method by cross ##w ##hit ##e et al . which is discussed below . the t ##pe method provides significant improvement for both identification and verification tasks as shown in table [ reference ] . the method by cross ##w ##hit ##e et al . uses the v ##gg - face network des ##cript ##ors ( 40 ##9 ##6 - d ) as the raw features . they use the concept of template adaptation to improve their performance as follows : when pool ##ing multiple faces of a given template , they train a linear sv ##m with the features of this template as positive and a fixed set of negative ##s extracted from the training data of the i ##j ##b - a splits . let ' s denote the poole ##d template feature and class ##ifier pair as . then , at query time when comparing two template ##s and , the similarity score is computed as : . even when using a carefully engineered fast linear class ##ifier training algorithm , this procedure increases the run time of the pool ##ing procedure . the query time per template comparison is also higher due to the high dimensional ##ity of the input features . in contrast , the proposed approach requires a matrix multiplication and a vector dot product per comparison . by using a simple neural network architecture , a relatively smaller training data ##set and a fast em ##bed ##ding method we have realized a faster and more efficient end - to - end system . to improve our performance further , we are currently incorporating the use of video data into our approach . sub ##su ##bs ##ection : performance on cf ##p on the cf ##p data ##set , we achieve a new state - of - art on both frontal - frontal and frontal - profile comparisons , the latter by a large margin . more specifically , for the frontal - profile case , we manage to reduce the error rate by 40 . 8 % . it should be noted that for a fair comparison we have used our raw cnn features without performing t ##pe . this shows that the raw cnn features we learn are effective even at extreme pose variations . section : cluster ##ing faces . 5 . 5 . 5 . 5 this section illustrates how the proposed t ##pe method can be used to cluster a given data collection . we perform two cluster ##ing experiments : we perform cluster ##ing on the entire l ##f ##w data ##set that consists of 132 ##33 images of 57 ##49 subjects . it should be noted that about 41 ##6 ##9 subjects have only one image . we use the i ##j ##b - a data ##set and cluster the template ##s corresponding to the query set for each split in the i ##j ##b - a verify protocol . for evaluating the cluster ##ing results , we use the metric ##s defined in . these are summarized below : pair ##wise precision ( pp ##air ) : the fraction of pairs of samples within a cluster among all possible pairs which are of the same class , over the total number of same cluster pairs . pair ##wise recall ( r ##pa ##ir ) : the fraction of pairs of samples within a class among all possible pairs which are placed in the same cluster , over the total number of same - class pairs . using these metric ##s , the f - score is computed as : the simplest way we found to demonstrate the effectiveness of our deep features and the proposed t ##pe method , is to use the standard mat ##lab implementation of the ag ##gl ##ome ##rative cluster ##ing algorithm with the average link ##age metric . we use the co ##sin ##e similarity as our basic cluster ##ing metric . the simple cluster ##ing algorithm that we have used here has computational complexity of . in its current form , this does not scale to large data ##set ##s with millions of images . we are currently working on a more efficient and scala ##ble ( yet approximate ) version of this algorithm . paragraph : cluster ##ing l ##f ##w : - the images in the l ##f ##w data ##set are pre - processed as described in section 5 . 1 . for each image and its flip , the deep features are extracted using the proposed architecture , averaged and normal ##ized to unit norm . we run the cluster ##ing algorithm over the entire data in a single shot . the cluster ##ing algorithm takes as input a cut - off parameter which acts as a distance threshold ( below which any two clusters will not be merged ) . in our experiments , we vary this cut - off parameter over a small range and evaluate the resulting cluster ##ing using the - score . we pick the result that yields the best - score . table [ reference ] shows the result of our approach and compares it to a recently released cluster ##ing approach based on approximate rank - order cluster ##ing . it should be noted that , in the case of , the cluster ##ing result is chosen by varying the number of clusters and picking the one with the best - score . in our approach , we vary the cut - off threshold which is the property of deep features and hence is a more intuitive parameter to tune . we see from table [ reference ] that aside from better performance , our total cluster estimate is closer to the ground truth value of 57 ##49 than . table - score for comparison of the two cluster ##ing schemes on the l ##f ##w data ##set . the ground truth cluster number is 57 ##49 . table ##cl ##ust ##ering metric ##s over the i ##j ##b - a 1 : 1 protocol . the standard deviation is indicated in brackets . the ground truth subjects per each split is 167 . paragraph : cluster ##ing i ##j ##b - a : - the i ##j ##b - a data ##set is processed as described in section 5 . in this section , we aim to cluster the query template ##s provided with each split for the verify protocol . we report the results of two experiments : with the raw cnn features ( cnn in table 2 ) and with the projected cnn features , where the projection matrix is learned through the proposed t ##pe method ( cnn + t ##pe in table 2 ) . the cut - off threshold required for our cluster ##ing algorithm is learned automatically based on the training data , i . e . we choose the threshold that gives the maximum - score over the training data . the scores reported in table [ reference ] are average values over ten splits . as expected , the t ##pe method improves the cluster ##ing performance of raw features . the subject estimate is the number of clusters produced as a direct result of our cluster ##ing algorithm . the pr ##une ##d estimate is obtained by ignoring clusters that have fewer than 3 images . for a more complete evaluation of our performance over varying threshold values , we plot the precision - recall ( pr ) curve for the i ##j ##b - a cluster ##ing experiment in figure [ reference ] . as can be observed , the pr curve for cluster ##ing the i ##j ##b - a data using embedded features exhibits a better performance at all operating points . this is a more transparent evaluation than reporting only the - score since the latter effectively fix ##es the operating point but the pr curve reveals the performance at all operating points . section : conclusion and future work in this paper , we proposed a deep cnn - based approach coupled with a low - dimensional disc ##rim ##ina ##tive em ##bed ##ding learned using triple ##t probability constraints in a large margin fashion . the proposed pipeline enables a faster training time and improves face verification performance especially at low fm ##rs . we demonstrated the effectiveness of the proposed method on two challenging data ##set ##s : i ##j ##b - a and cf ##p and achieved performance close to the state of the art while using a deep model which is more compact and trained using a moderately sized data ##set . we demonstrated the robust ##ness of our features using a simple cluster ##ing algorithm on the l ##f ##w and i ##j ##b - a data ##set ##s . for future work , we plan to use videos directly during training and also em ##bed our t ##pe approach into training the deep network . we intend to scale our cluster ##ing algorithm to handle large scale scenarios such as large imp ##ost ##or sets of the order of millions . section : acknowledge ##ment this research is based upon work supported by the office of the director of national intelligence ( o ##d ##ni ) , intelligence advanced research projects activity ( ia ##rp ##a ) , via ia ##rp ##a r & d contract no . 2014 - 140 ##7 ##16 ##00 ##01 ##2 . the views and conclusions contained here ##in are those of the authors and should not be interpreted as necessarily representing the official policies or endorsement ##s , either expressed or implied , of the o ##d ##ni , ia ##rp ##a , or the u . s . government . the u . s . government is authorized to reproduce and distribute reprint ##s for governmental purposes notwithstanding any copyright ann ##ota ##tion there ##on . bibliography : references",
        "pred_seq": "i ##set [SEP] triple ##ding [SEP] [SEP] face verification [SEP] [unused0] i a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a [CLS]",
        "pred_templates": [
            {
                "Material": [
                    [
                        "ijba dataset"
                    ]
                ],
                "Method": [
                    [
                        "triplet probabilistic embedding"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "face verification"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "ijba dataset",
                        "ijba",
                        "ijba datasets",
                        "iarpa janus benchmarka",
                        "ijba"
                    ]
                ],
                "Method": [
                    [
                        "triplet probabilistic embedding",
                        "triplet probability embedding learning method",
                        "triplet probability embedding",
                        "triplet probabilistic embedding learning method",
                        "tpe",
                        "triplet embedding",
                        "tpe method",
                        "tpe approach"
                    ]
                ],
                "Metric": [
                    [
                        "tar",
                        "far"
                    ]
                ],
                "Task": [
                    [
                        "face verification",
                        "unconstrained face verification",
                        "unconstrained face verification problem",
                        "verification",
                        "identification",
                        "verification protocol",
                        "verification and identification protocols",
                        "identification and verification tasks"
                    ]
                ]
            }
        ]
    },
    "64": {
        "doctext": "document : high - resolution image synthesis and semantic manipulation with conditional gan ##s we present a new method for synth ##es ##izing high - resolution photo - realistic images from semantic label maps using conditional genera ##tive ad ##vers ##aria ##l networks ( conditional gan ##s ) . conditional gan ##s have enabled a variety of applications , but the results are often limited to low - resolution and still far from realistic . in this work , we generate visually appealing results with a novel ad ##vers ##aria ##l loss , as well as new multi - scale generator and disc ##rim ##inator architecture ##s . furthermore , we extend our framework to interactive visual manipulation with two additional features . first , we incorporate object instance segment ##ation information , which enables object manipulation ##s such as removing / adding objects and changing the object category . second , we propose a method to generate diverse results given the same input , allowing users to edit the object appearance interactive ##ly . human opinion studies demonstrate that our method significantly out ##per ##forms existing methods , advancing both the quality and the resolution of deep image synthesis and editing . section : introduction photo - realistic image rendering using standard graphics techniques is involved , since geometry , materials , and light transport must be simulated explicitly . although existing graphics algorithms excel at the task , building and editing virtual environments is expensive and time - consuming . that is because we have to model every aspect of the world explicitly . if we were able to render photo - realistic images using a model learned from data , we could turn the process of graphics rendering into a model learning and inference problem . then , we could sim ##plify the process of creating new virtual worlds by training models on new data ##set ##s . we could even make it easier to custom ##ize environments by allowing users to simply specify overall semantic structure rather than modeling geometry , materials , or lighting . in this paper , we discuss a new approach that produces high - resolution images from semantic label maps . this method has a wide range of applications . for example , we can use it to create synthetic training data for training visual recognition algorithms , since it is much easier to create semantic labels for desired scenarios than to generate training images . using semantic segment ##ation methods , we can transform images into a semantic label domain , edit the objects in the label domain , and then transform them back to the image domain . this method also gives us new tools for higher - level image editing , e . g . , adding objects to images or changing the appearance of existing objects . to synth ##es ##ize images from semantic labels , one can use the pi ##x ##2 ##pi ##x method , an image - to - image translation framework which leverage ##s genera ##tive ad ##vers ##aria ##l networks ( gan ##s ) in a conditional setting . recently , chen and ko ##lt ##un suggest that ad ##vers ##aria ##l training might be unstable and prone to failure for high - resolution image generation tasks . instead , they adopt a modified per ##ce ##pt ##ual loss to synth ##es ##ize images , which are high - resolution but often lack fine details and realistic textures . here we address two main issues of the above state - of - the - art methods : ( 1 ) the difficulty of generating high - resolution images with gan ##s and ( 2 ) the lack of details and realistic textures in the previous high - resolution results . we show that through a new , robust ad ##vers ##aria ##l learning objective together with new multi - scale generator and disc ##rim ##inator architecture ##s , we can synth ##es ##ize photo - realistic images at resolution , which are more visually appealing than those computed by previous methods . we first obtain our results with ad ##vers ##aria ##l training only , without relying on any hand - crafted losses or pre - trained networks ( e . g . v ##gg ##net ) for per ##ce ##pt ##ual losses ( fig ##s . [ reference ] c , [ reference ] b ) . then we show that adding per ##ce ##pt ##ual losses from pre - trained networks can slightly improve the results in some circumstances ( fig ##s . [ reference ] d , [ reference ] c ) , if a pre - trained network is available . both results out ##per ##form previous works substantially in terms of image quality . furthermore , to support interactive semantic manipulation , we extend our method in two directions . first , we use instance - level object segment ##ation information , which can separate different object instances within the same category . this enables flexible object manipulation ##s , such as adding / removing objects and changing object types . second , we propose a method to generate diverse results given the same input label map , allowing the user to edit the appearance of the same object interactive ##ly . we compare against state - of - the - art visual synthesis systems , and show that our method out ##per ##forms these approaches regarding both quantitative evaluation ##s and human perception studies . we also perform an ab ##lation study regarding the training objectives and the importance of instance - level segment ##ation information . in addition to semantic manipulation , we test our method on edge ##2 ##ph ##oto applications ( fig ##s . [ reference ] , [ reference ] ) , which shows the general ##iza ##bility of our approach . code and data are available at our . section : related work paragraph : genera ##tive ad ##vers ##aria ##l networks genera ##tive ad ##vers ##aria ##l networks ( gan ##s ) aim to model the natural image distribution by forcing the generated samples to be ind ##ist ##ing ##uis ##hab ##le from natural images . gan ##s enable a wide variety of applications such as image generation , representation learning , image manipulation , object detection , and video applications . various coarse - to - fine schemes have been proposed to synth ##es ##ize larger images ( e . g . ) in an un ##con ##ditional setting . inspired by their successes , we propose a new coarse - to - fine generator and multi - scale disc ##rim ##inator architecture ##s suitable for conditional image generation at a much higher resolution . paragraph : image - to - image translation many researchers have leverage ##d ad ##vers ##aria ##l learning for image - to - image translation , whose goal is to translate an input image from one domain to another domain given input - output image pairs as training data . compared to loss , which often leads to blur ##ry images , the ad ##vers ##aria ##l loss has become a popular choice for many image - to - image tasks . the reason is that the disc ##rim ##inator can learn a train ##able loss function and automatically adapt to the differences between the generated and real images in the target domain . for example , the recent pi ##x ##2 ##pi ##x framework used image - conditional gan ##s for different applications , such as transforming google maps to satellite views and generating cats from user sketches . various methods have also been proposed to learn an image - to - image translation in the absence of training pairs . recently , chen and ko ##lt ##un suggest that it might be hard for conditional gan ##s to generate high - resolution images due to the training instability and optimization issues . to avoid this difficulty , they use a direct regression objective based on a per ##ce ##pt ##ual loss and produce the first model that can synth ##es ##ize images . the generated results are high - resolution but often lack fine details and realistic textures . motivated by their success , we show that using our new objective function as well as novel multi - scale generators and disc ##rim ##inator ##s , we not only largely stabilize the training of conditional gan ##s on high - resolution images , but also achieve significantly better results compared to chen and ko ##lt ##un . side - by - side comparisons clearly show our advantage ( fig ##s . [ reference ] , [ reference ] , [ reference ] , [ reference ] ) . paragraph : deep visual manipulation recently , deep neural networks have obtained promising results in various image processing tasks , such as style transfer , in ##pa ##int ##ing , color ##ization , and restoration . however , most of these works lack an interface for users to adjust the current result or explore the output space . to address this issue , zhu developed an optimization method for editing the object appearance based on the prior ##s learned by gan ##s . recent works also provide user interfaces for creating novel imagery from low - level cues such as color and sketch . all of the prior works report results on low - resolution images . our system shares the same spirit as this past work , but we focus on object - level semantic editing , allowing users to interact with the entire scene and manipulate individual objects in the image . as a result , users can quickly create a new scene with minimal effort . our interface is inspired by prior data - driven graphics systems . but our system allows more flexible manipulation ##s and produces high - res results in real - time . section : instance - level image synthesis we propose a conditional ad ##vers ##aria ##l framework for generating high - resolution photo - realistic images from semantic label maps . we first review our baseline model pi ##x ##2 ##pi ##x ( sec . [ reference ] ) . we then describe how we increase the photo - realism and resolution of the results with our improved objective function and network design ( sec . [ reference ] ) . next , we use additional instance - level object semantic information to further improve the image quality ( sec . [ reference ] ) . finally , we introduce an instance - level feature em ##bed ##ding scheme to better handle the multi - mod ##al nature of image synthesis , which enables interactive object editing ( sec . [ reference ] ) . sub ##section : the pi ##x ##2 ##pi ##x baseline the pi ##x ##2 ##pi ##x method is a conditional gan framework for image - to - image translation . it consists of a generator and a disc ##rim ##inator . for our task , the objective of the generator is to translate semantic label maps to realistic - looking images , while the disc ##rim ##inator aims to distinguish real images from the translated ones . the framework operates in a supervised setting . in other words , the training data ##set is given as a set of pairs of corresponding images , where is a semantic label map and is a corresponding natural photo . conditional gan ##s aim to model the conditional distribution of real images given the input semantic label maps via the following mini ##max game : where the objective function is given by the pi ##x ##2 ##pi ##x method adopt ##s u - net as the generator and a patch - based fully con ##vo ##lu ##tion ##al network as the disc ##rim ##inator . the input to the disc ##rim ##inator is a channel - wise con ##cate ##nation of the semantic label map and the corresponding image . however , the resolution of the generated images on city ##sca ##pes is up to . we tested directly applying the pi ##x ##2 ##pi ##x framework to generate high - resolution images but found the training unstable and the quality of generated images un ##sat ##is ##factory . therefore , we describe how we improve the pi ##x ##2 ##pi ##x framework in the next sub ##section . sub ##section : improving photo ##real ##ism and resolution we improve the pi ##x ##2 ##pi ##x framework by using a coarse - to - fine generator , a multi - scale disc ##rim ##inator architecture , and a robust ad ##vers ##aria ##l learning objective function . coarse - to - fine generator we deco ##mp ##ose the generator into two sub - networks : and . we term as the global generator network and as the local enhance ##r network . the generator is then given by the tu ##ple as visual ##ized in fig . [ reference ] . the global generator network operates at a resolution of , and the local enhance ##r network outputs an image with a resolution that is the output size of the previous one ( along each image dimension ) . for synth ##es ##izing images at an even higher resolution , additional local enhance ##r networks could be utilized . for example , the output image resolution of the generator is , and the output image resolution of is . our global generator is built on the architecture proposed by johnson , which has been proven successful for neural style transfer on images up to . it consists of components : a con ##vo ##lu ##tion ##al front - end , a set of residual blocks , and a trans ##posed con ##vo ##lu ##tion ##al back - end . a semantic label map of resolution is passed through the 3 components sequential ##ly to output an image of resolution . the local enhance ##r network also consists of 3 components : a con ##vo ##lu ##tion ##al front - end , a set of residual blocks , and a trans ##posed con ##vo ##lu ##tion ##al back - end . the resolution of the input label map to is . different from the global generator network , the input to the residual block is the element - wise sum of two feature maps : the output feature map of , and the last feature map of the back - end of the global generator network . this helps integrating the global information from to . during training , we first train the global generator and then train the local enhance ##r in the order of their resolutions . we then jointly fine - tune all the networks together . we use this generator design to effectively aggregate global and local information for the image synthesis task . we note that such a multi - resolution pipeline is a well - established practice in computer vision and two - scale is often enough . similar ideas but different architecture ##s could be found in recent un ##con ##ditional gan ##s and conditional image generation . multi - scale disc ##rim ##inator ##s high - resolution image synthesis poses a significant challenge to the gan disc ##rim ##inator design . to differentiate high - resolution real and synthesized images , the disc ##rim ##inator needs to have a large rec ##eptive field . this would require either a deeper network or larger con ##vo ##lu ##tion ##al kernel ##s , both of which would increase the network capacity and potentially cause over ##fi ##tting . also , both choices demand a larger memory footprint for training , which is already a scarce resource for high - resolution image generation . to address the issue , we propose using multi - scale disc ##rim ##inator ##s . we use disc ##rim ##inator ##s that have an identical network structure but operate at different image scales . we will refer to the disc ##rim ##inator ##s as , and . specifically , we downs ##amp ##le the real and synthesized high - resolution images by a factor of and to create an image pyramid of 3 scales . the disc ##rim ##inator ##s , and are then trained to differentiate real and synthesized images at the different scales , respectively . although the disc ##rim ##inator ##s have an identical architecture , the one that operates at the coarse ##st scale has the largest rec ##eptive field . it has a more global view of the image and can guide the generator to generate globally consistent images . on the other hand , the disc ##rim ##inator at the finest scale encourages the generator to produce finer details . this also makes training the coarse - to - fine generator easier , since extending a low - resolution model to a higher resolution only requires adding a disc ##rim ##inator at the finest level , rather than re ##train ##ing from scratch . without the multi - scale disc ##rim ##inator ##s , we observe that many repeated patterns often appear in the generated images . with the disc ##rim ##inator ##s , the learning problem in e ##q . ( [ reference ] ) then becomes a multi - task learning problem of using multiple gan disc ##rim ##inator ##s at the same image scale has been proposed in un ##con ##ditional gan ##s . ii ##zuka et al . add a global image class ##ifier to conditional gan ##s to synth ##es ##ize globally coherent content for in ##pa ##int ##ing . here we extend the design to multiple disc ##rim ##inator ##s at different image scales for modeling high - resolution images . improved ad ##vers ##aria ##l loss we improve the gan loss in e ##q . ( [ reference ] ) by incorporating a feature matching loss based on the disc ##rim ##inator . this loss stabilize ##s the training as the generator has to produce natural statistics at multiple scales . specifically , we extract features from multiple layers of the disc ##rim ##inator and learn to match these intermediate representations from the real and the synthesized image . for ease of presentation , we denote the th - layer feature extract ##or of disc ##rim ##inator as ( from input to the th layer of ) . the feature matching loss is then calculated as : where is the total number of layers and denotes the number of elements in each layer . our gan disc ##rim ##inator feature matching loss is related to the per ##ce ##pt ##ual loss , which has been shown to be useful for image super - resolution and style transfer . in our experiments , we discuss how the disc ##rim ##inator feature matching loss and the per ##ce ##pt ##ual loss can be jointly used for further improving the performance . we note that a similar loss is used in va ##e - gan ##s . our full objective combines both gan loss and feature matching loss as : where controls the importance of the two terms . note that for the feature matching loss , only serves as a feature extract ##or and does not maximize the loss . sub ##section : using instance maps existing image synthesis methods only utilize semantic label maps , an image where each pixel value represents the object class of the pixel . this map does not differentiate objects of the same category . on the other hand , an instance - level semantic label map contains a unique object id for each individual object . to incorporate the instance map , one can directly pass it into the network , or en ##code it into a one - hot vector . however , both approaches are difficult to implement in practice , since different images may contain different numbers of objects of the same category . alternatively , one can pre - all ##oca ##te a fixed number of channels ( e . g . , ) for each class , but this method fails when the number is set too small , and waste ##s memory when the number is too large . instead , we argue that the most critical information the instance map provides , which is not available in the semantic label map , is the object boundary . for example , when objects of the same class are next to one another , looking at the semantic label map alone can not tell them apart . this is especially true for the street scene since many parked cars or walking pedestrians are often next to one another , as shown in fig . [ reference ] a . however , with the instance map , separating these objects becomes an easier task . therefore , to extract this information , we first compute the instance boundary map ( fig . [ reference ] b ) . in our implementation , a pixel in the instance boundary map is if its object id is different from any of its - neighbors , and otherwise . the instance boundary map is then con ##cate ##nated with the one - hot vector representation of the semantic label map , and fed into the generator network . similarly , the input to the disc ##rim ##inator is the channel - wise con ##cate ##nation of instance boundary map , semantic label map , and the real / synthesized image . figure [ reference ] b shows an example demonstrating the improvement by using object boundaries . our user study in sec . [ reference ] also shows the model trained with instance boundary maps render ##s more photo - realistic object boundaries . sub ##section : learning an instance - level feature em ##bed ##ding image synthesis from semantic label maps is a one - to - many mapping problem . an ideal image synthesis algorithm should be able to generate diverse , realistic images using the same semantic label map . recently , several works learn to produce a fixed number of discrete outputs given the same input or synth ##es ##ize diverse modes controlled by a late ##nt code that en ##codes the entire image . although these approaches tackle the multi - mod ##al image synthesis problem , they are unsuitable for our image manipulation task mainly for two reasons . first , the user has no intuitive control over which kinds of images the model would produce . second , these methods focus on global color and texture changes and allow no object - level control on the generated contents . to generate diverse images and allow instance - level control , we propose adding additional low - dimensional feature channels as the input to the generator network . we show that , by manipulating these features , we can have flexible control over the image synthesis process . furthermore , note that since the feature channels are continuous quantities , our model is , in principle , capable of generating infinitely many images . to generate the low - dimensional features , we train an en ##code ##r network to find a low - dimensional feature vector that corresponds to the ground truth target for each instance in the image . our feature en ##code ##r architecture is a standard en ##code ##r - deco ##der network . to ensure the features are consistent within each instance , we add an instance - wise average pool ##ing layer to the output of the en ##code ##r to compute the average feature for the object instance . the average feature is then broadcast to all the pixel locations of the instance . figure [ reference ] visual ##izes an example of the encoded features . we replace with in e ##q . ( [ reference ] ) and train the en ##code ##r jointly with the generators and disc ##rim ##inator ##s . after the en ##code ##r is trained , we run it on all instances in the training images and record the obtained features . then we perform a - means cluster ##ing on these features for each semantic category . each cluster thus en ##codes the features for a specific style , for example , the asphalt or cobb ##les ##tone texture for a road . at inference time , we randomly pick one of the cluster centers and use it as the encoded features . these features are con ##cate ##nated with the label map and used as the input to our generator . we tried to enforce the ku ##ll ##back - lei ##bler loss on the feature space for better test - time sampling as used in the recent work but found it quite involved for users to adjust the late ##nt vectors for each object directly . instead , for each object instance , we present modes for users to choose from . section : results we first provide a quantitative comparison against leading methods in sec . [ reference ] . we then report a subjective human per ##ce ##pt ##ual study in sec . [ reference ] . finally , we show a few examples of interactive object editing results in sec . [ reference ] . implementation details we use l ##sg ##ans for stable training . in all experiments , we set the weight ( e ##q . ( [ reference ] ) ) and for k - means . we use - dimensional vectors to en ##code features for each object instance . we experimented with adding a per ##ce ##pt ##ual loss to our objective ( e ##q . ( [ reference ] ) ) , where and denotes the - th layer with elements of the v ##gg network . we observe that this loss slightly improves the results . we name these two variants as ours and ours ( w / o v ##gg loss ) . please find more training and architecture details in the appendix . data ##set ##s we conduct extensive comparisons and ab ##lation studies on city ##sca ##pes data ##set and nyu indoor r ##gb ##d data ##set . we report additional qu ##ali ##tative results on ad ##e ##20 k data ##set and helen face data ##set . baseline ##s we compare our method with two state - of - the - art algorithms : pi ##x ##2 ##pi ##x and cr ##n . we train pi ##x ##2 ##pi ##x models on high - res images with the default setting . we produce the high - res cr ##n images via the authors ' publicly available model . sub ##section : quantitative comparisons we adopt the same evaluation protocol from previous image - to - image translation works . to quan ##tify the quality of our results , we perform semantic segment ##ation on the synthesized images and compare how well the predicted segments match the input . the intuition is that if we can produce realistic images that correspond to the input label map , an off - the - shelf semantic segment ##ation model ( e . g . , ps ##p ##net that we use ) should be able to predict the ground truth label . table [ reference ] reports the calculated segment ##ation accuracy . as can be seen , for both pixel - wise accuracy and mean intersection - over - union ( io ##u ) , our method out ##per ##forms the other methods by a large margin . moreover , our result is very close to the result of the original images , the theoretical \" upper bound \" of the realism we can achieve . this just ##ifies the superiority of our algorithm . sub ##section : human per ##ce ##pt ##ual study we further evaluate our algorithm via a human subjective study . we perform pair ##wise a / b tests deployed on the amazon mechanical turk ( mt ##ur ##k ) platform on the city ##sca ##pes data ##set . we follow the same experimental procedure as described in chen and ko ##lt ##un . more specifically , two different kinds of experiments are conducted : unlimited time and limited time , as explained below . unlimited time for this task , workers are given two images at once , each of which is synthesized by a different method for the same label map . we give them unlimited time to select which image looks more natural . the left - right order and the image order are random ##ized to ensure fair comparisons . all city ##sca ##pes test images are compared times , resulting in human judgments for each method . in this experiment , we use the model trained on labels only ( without instance maps ) to ensure a fair comparison . table [ reference ] shows that both variants of our method out ##per ##form the other methods significantly . limited time next , for the limited time experiment , we compare our result with cr ##n and the original image ( ground truth ) . in each comparison , we show results of two methods for a short period of time . we randomly select a duration between seconds and seconds , as adopted by prior work . this evaluate ##s how quickly the difference between the images can be perceived . fig . [ reference ] shows the comparison results at different time intervals . as the given time becomes longer and longer , the differences between these three types of images become more apparent and easier to observe . figures [ reference ] and [ reference ] show some example results . analysis of the loss function we also study the importance of each term in our objective function using the unlimited time experiment . specifically , our final loss contains three components : gan loss , disc ##rim ##inator - based feature matching loss , and v ##gg per ##ce ##pt ##ual loss . we compare our final implementation to the results using ( ) only gan loss , and ( ) gan feature matching loss ( i . e . , without v ##gg loss ) . the obtained preference rates are and , respectively . as can be seen , adding the feature matching loss substantially improves the performance , while adding per ##ce ##pt ##ual loss further enhance ##s the results . however , note that using the per ##ce ##pt ##ual loss is not critical , and we are still able to generate visually appealing results even without it ( e . g . , fig ##s . [ reference ] c , [ reference ] b ) . using instance maps we compare the results using instance maps to results without using them . we highlight the car regions in the images and ask the participants to choose which region looks more realistic . we obtain a preference rate of , which indicates that using instance maps improves the realism of our results , especially around the object boundaries . analysis of the generator we compare results of different generators with all the other components fixed . in particular , we compare our generator with two state - of - the - art generator architecture ##s : u - net and cr ##n . we evaluate the performance regarding both semantic segment ##ation scores and human per ##ce ##pt ##ual study results . table [ reference ] and table [ reference ] show that our coarse - to - fine generator out ##per ##forms other networks by a large margin . analysis of the disc ##rim ##inator next , we also compare results using our multi - scale disc ##rim ##inator ##s and results using only one disc ##rim ##inator while we keep the generator and the loss function fixed . the segment ##ation scores on city ##sca ##pes ( table [ reference ] ) demonstrate that using multi - scale disc ##rim ##inator ##s helps produce higher quality results as well as stabilize the ad ##vers ##aria ##l training . we also perform pair ##wise a / b tests on the amazon mechanical turk platform . of the participants prefer our results with multi - scale disc ##rim ##inator ##s over the results trained with a single - scale disc ##rim ##inator ( chance is ) . additional data ##set ##s to further evaluate our method , we perform unlimited time comparisons on the nyu data ##set . we obtain and against pi ##x ##2 ##pi ##x and cr ##n , respectively . fig . [ reference ] show some example images . finally , we show results on the ad ##e ##20 k data ##set ( fig . [ reference ] ) . sub ##section : interactive object editing our feature en ##code ##r allows us to perform interactive instance editing on the resulting images . for example , we can change the object labels in the image to quickly create novel scenes , such as replacing trees with buildings ( fig . [ reference ] b ) . we can also change the colors of individual cars or the textures of the road ( fig . [ reference ] c ) . please check out our interactive demos on our website . besides , we implement our interactive object editing feature on the helen face data ##set where labels for different facial parts are available ( fig . [ reference ] ) . this makes it easy to edit human portraits , e . g . , changing the face color to mimic different make - up effects or adding beard to a face . section : discussion and conclusion the results in this paper suggest that conditional gan ##s can synth ##es ##ize high - resolution photo - realistic imagery without any hand - crafted losses or pre - trained networks . we have observed that incorporating a per ##ce ##pt ##ual loss can slightly improve the results . our method allows many applications and will be potentially useful for domains where high - resolution results are in demand but pre - trained networks are not available ( e . g . , medical imaging and biology ) . this paper also shows that an image - to - image synthesis pipeline can be extended to produce diverse outputs , and enable interactive image manipulation given appropriate training input - output pairs ( e . g . , instance maps in our case ) . without ever been told what a \" texture \" is , our model learns to st ##yl ##ize different objects , which may be generalized to other data ##set ##s as well ( i . e . , using textures in one data ##set to synth ##es ##ize images in another data ##set ) . we believe these extensions can be potentially applied to other image synthesis problems . acknowledge ##ments we thank tae ##sun ##g park , phillip iso ##la , ting ##hui zhou , richard zhang , rafael valle and alexei a . e ##fr ##os for helpful comments . we also thank chen and ko ##lt ##un and iso ##la et al . for sharing their code . j ##y ##z is supported by a facebook graduate fellowship . bibliography : references appendix : training details all the networks were trained from scratch , using the adam solve ##r and a learning rate of . we keep the same learning rate for the first epoch ##s and linear ##ly decay the rate to zero over the next epoch ##s . weights were initial ##ized from a ga ##uss ##ian distribution with mean and standard deviation . we train all our models on an n ##vid ##ia quad ##ro m ##60 ##00 gp ##u with gb gp ##u memory . the inference time is between mill ##ise ##con ##ds per input image on an n ##vid ##ia 108 ##0 ##ti gp ##u with gb gp ##u memory . this real - time performance allows us to develop interactive image editing applications . below we discuss the details of the data ##set ##s we used . city ##sca ##pes data ##set [ ] : training images from the city ##sca ##pes training set with image size . we use the city ##sca ##pes validation set for testing , which consists of 500 images . nyu indoor r ##gb ##d data ##set [ ] : training images and test images , all at resolution of . ad ##e ##20 k data ##set [ ] : training images and test images with varying image sizes . we scale the width of all images to before training and inference . helen face data ##set [ ] : training images and test images with varying image sizes . we res ##ize all images to before training and inference . appendix : generator architecture ##s our generator consists of a global generator network and a local enhance ##r network . we follow the naming convention used in johnson el al . and cycle ##gan . let c ##7 ##s ##1 - k denote a con ##vo ##lu ##tion - instance ##nor ##m - re ##lu layer with filters and stride . d ##k denotes a con ##vo ##lu ##tion - instance ##nor ##m - re ##lu layer with filters , and stride . we use reflection pad ##ding to reduce boundary artifacts . r ##k denotes a residual block that contains two con ##vo ##lu ##tion ##al layers with the same number of filters on both layers . uk denotes a fraction ##al - stride ##d - con ##vo ##lu ##tion - instance ##nor ##m - re ##lu layer with filters , and stride . recall that we have two generators : the global generator and the local enhance ##r . our global network : c ##7 ##s ##1 - 64 , d ##12 ##8 , d ##25 ##6 , d ##51 ##2 , d ##10 ##24 , r ##10 ##24 , r ##10 ##24 , r ##10 ##24 , r ##10 ##24 , r ##10 ##24 , r ##10 ##24 , r ##10 ##24 , r ##10 ##24 , r ##10 ##24 , u ##51 ##2 , u2 ##56 , u ##12 ##8 , u ##64 , c ##7 ##s ##1 - 3 our local enhance ##r : c ##7 ##s ##1 - 32 , d ##64 ##we add the last feature map ( u ##64 ) in our global network to the output of this layer . , r ##64 , r ##64 , r ##64 , u ##32 , c ##7 ##s ##1 - 3 appendix : disc ##rim ##inator architecture ##s for disc ##rim ##inator networks , we use patch ##gan . let ck denote a con ##vo ##lu ##tion - instance ##nor ##m - leak ##yre ##lu layer with k filters and stride . after the last layer , we apply a con ##vo ##lu ##tion to produce a dimensional output . we do not use instance ##nor ##m for the first c ##64 layer . we use leak ##y re ##lus with slope . all our three disc ##rim ##inator ##s have the identical architecture as follows : c ##64 - c1 ##28 - c2 ##56 - c ##51 ##2 appendix : change log paragraph : v ##1 initial prep ##rin ##t release paragraph : v ##2 cv ##pr camera ready , adding more results for edge - to - photo examples .",
        "pred_seq": "[SEP] conditional ##s [SEP] [SEP] high synthesis [SEP] [unused0] [CLS]",
        "pred_templates": [
            {
                "Material": [],
                "Method": [
                    [
                        "conditional gans"
                    ]
                ],
                "Metric": [],
                "Task": [
                    [
                        "highresolution image synthesis"
                    ]
                ]
            }
        ],
        "gold_templates": [
            {
                "Material": [
                    [
                        "ade20 k dataset",
                        "ade20"
                    ]
                ],
                "Method": [
                    [
                        "pix2pix method"
                    ]
                ],
                "Metric": [
                    [
                        "segmentation accuracy",
                        "pixelwise accuracy"
                    ]
                ],
                "Task": [
                    [
                        "imagetoimage translation"
                    ]
                ]
            },
            {
                "Material": [
                    [
                        "cityscapes",
                        "cityscapes dataset"
                    ]
                ],
                "Method": [
                    [
                        "pix2pix method"
                    ]
                ],
                "Metric": [
                    [
                        "intersectionoverunion",
                        "iou"
                    ]
                ],
                "Task": [
                    [
                        "imagetoimage translation"
                    ]
                ]
            }
        ]
    },
    "65": {
        "doctext": "document : spatial pyramid pool ##ing in deep con ##vo ##lu ##tion ##al networks for visual recognition existing deep con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) require a fixed - size ( , 224 224 ) input image . this requirement is \" artificial \" and may reduce the recognition accuracy for the images or sub - images of an arbitrary size / scale . in this work , we e ##qui ##p the networks with another pool ##ing strategy , \" spatial pyramid pool ##ing \" , to eliminate the above requirement . the new network structure , called spp - net , can generate a fixed - length representation regardless of image size / scale . pyramid pool ##ing is also robust to object deformation ##s . with these advantages , spp - net should in general improve all cnn - based image classification methods . on the image ##net 2012 data ##set , we demonstrate that spp - net boost ##s the accuracy of a variety of cnn architecture ##s despite their different designs . on the pascal vo ##c 2007 and cal ##tech ##10 ##1 data ##set ##s , spp - net achieve ##s state - of - the - art classification results using a single full - image representation and no fine - tuning . the power of spp - net is also significant in object detection . using spp - net , we compute the feature maps from the entire image only once , and then pool features in arbitrary regions ( sub - images ) to generate fixed - length representations for training the detectors . this method avoids repeatedly computing the con ##vo ##lu ##tion ##al features . in processing test images , our method is 24 - 102 faster than the r - cnn method , while achieving better or comparable accuracy on pascal vo ##c 2007 . in image ##net large scale visual recognition challenge ( il ##s ##vr ##c ) 2014 , our methods rank # 2 in object detection and # 3 in image classification among all 38 teams . this manuscript also introduces the improvement made for this competition . on ##vo ##lu ##tion ##al neural networks , spatial pyramid pool ##ing , image classification , object detection section : introduction we are witnessing a rapid , revolutionary change in our vision community , mainly caused by deep con ##vo ##lu ##tion ##al neural networks ( cnn ##s ) and the availability of large scale training data . deep - networks - based approaches have recently been substantially improving upon the state of the art in image classification , object detection , many other recognition tasks , and even non - recognition tasks . however , there is a technical issue in the training and testing of the cnn ##s : the prevalent cnn ##s require a fixed input image size ( , 224 224 ) , which limits both the aspect ratio and the scale of the input image . when applied to images of arbitrary sizes , current methods mostly fit the input image to the fixed size , either via crop ##ping or via warp ##ing , as shown in figure [ reference ] ( top ) . but the crop ##ped region may not contain the entire object , while the warped content may result in unwanted geometric distortion . recognition accuracy can be compromised due to the content loss or distortion . besides , a pre - defined scale may not be suitable when object scales vary . fixing input sizes overlook ##s the issues involving scales . so why do cnn ##s require a fixed input size ? a cnn mainly consists of two parts : con ##vo ##lu ##tion ##al layers , and fully - connected layers that follow . the con ##vo ##lu ##tion ##al layers operate in a sliding - window manner and output feature maps which represent the spatial arrangement of the activation ##s ( figure [ reference ] ) . in fact , con ##vo ##lu ##tion ##al layers do not require a fixed image size and can generate feature maps of any sizes . on the other hand , the fully - connected layers need to have fixed - size / length input by their definition . hence , the fixed - size constraint comes only from the fully - connected layers , which exist at a deeper stage of the network . in this paper , we introduce a spatial pyramid pool ##ing ( spp ) layer to remove the fixed - size constraint of the network . specifically , we add an spp layer on top of the last con ##vo ##lu ##tion ##al layer . the spp layer pools the features and generates fixed - length outputs , which are then fed into the fully - connected layers ( or other class ##ifiers ) . in other words , we perform some information \" aggregation \" at a deeper stage of the network hierarchy ( between con ##vo ##lu ##tion ##al layers and fully - connected layers ) to avoid the need for crop ##ping or warp ##ing at the beginning . figure [ reference ] ( bottom ) shows the change of the network architecture by introducing the spp layer . we call the new network structure spp - net . spatial pyramid pool ##ing ( popularly known as spatial pyramid matching or sp ##m ) , as an extension of the bag - of - words ( bow ) model , is one of the most successful methods in computer vision . it partition ##s the image into divisions from finer to coarse ##r levels , and aggregate ##s local features in them . spp has long been a key component in the leading and competition - winning systems for classification ( , ) and detection ( , ) before the recent prevalence of cnn ##s . nevertheless , spp has not been considered in the context of cnn ##s . we note that spp has several remarkable properties for deep cnn ##s : 1 ) spp is able to generate a fixed - length output regardless of the input size , while the sliding window pool ##ing used in the previous deep networks can not ; 2 ) spp uses multi - level spatial bin ##s , while the sliding window pool ##ing uses only a single window size . multi - level pool ##ing has been shown to be robust to object deformation ##s ; 3 ) spp can pool features extracted at variable scales thanks to the flexibility of input scales . through experiments we show that all these factors el ##eva ##te the recognition accuracy of deep networks . spp - net not only makes it possible to generate representations from ar ##bit ##rar ##ily sized images / windows for testing , but also allows us to feed images with varying sizes or scales during training . training with variable - size images increases scale - in ##var ##iance and reduces over - fitting . we develop a simple multi - size training method . for a single network to accept variable input sizes , we approximate it by multiple networks that share all parameters , while each of these networks is trained using a fixed input size . in each epoch we train the network with a given input size , and switch to another input size for the next epoch . experiments show that this multi - size training converge ##s just as the traditional single - size training , and leads to better testing accuracy . the advantages of spp are orthogonal to the specific cnn designs . in a series of controlled experiments on the image ##net 2012 data ##set , we demonstrate that spp improves four different cnn architecture ##s in existing publications ( or their modifications ) , over the no - spp counterparts . these architecture ##s have various filter numbers / sizes , strides , depths , or other designs . it is thus reasonable for us to conjecture that spp should improve more sophisticated ( deeper and larger ) con ##vo ##lu ##tion ##al architecture ##s . spp - net also shows state - of - the - art classification results on cal ##tech ##10 ##1 and pascal vo ##c 2007 using only a single full - image representation and no fine - tuning . spp - net also shows great strength in object detection . in the leading object detection method r - cnn , the features from candidate windows are extracted via deep con ##vo ##lu ##tion ##al networks . this method shows remarkable detection accuracy on both the vo ##c and image ##net data ##set ##s . but the feature computation in r - cnn is time - consuming , because it repeatedly applies the deep con ##vo ##lu ##tion ##al networks to the raw pixels of thousands of warped regions per image . in this paper , we show that we can run the con ##vo ##lu ##tion ##al layers only once on the entire image ( regardless of the number of windows ) , and then extract features by spp - net on the feature maps . this method yields a speed ##up of over one hundred times over r - cnn . note that training / running a detector on the feature maps ( rather than image regions ) is actually a more popular idea . but spp - net inherit ##s the power of the deep cnn feature maps and also the flexibility of spp on arbitrary window sizes , which leads to outstanding accuracy and efficiency . in our experiment , the spp - net - based system ( built upon the r - cnn pipeline ) compute ##s features 24 - 102 faster than r - cnn , while has better or comparable accuracy . with the recent fast proposal method of edge ##box ##es , our system takes 0 . 5 seconds processing an image ( including all steps ) . this makes our method practical for real - world applications . a preliminary version of this manuscript has been published in ec ##c ##v 2014 . based on this work , we attended the competition of il ##s ##vr ##c 2014 , and ranked # 2 in object detection and # 3 in image classification ( both are provided - data - only tracks ) among all 38 teams . there are a few modifications made for il ##s ##vr ##c 2014 . we show that the spp - nets can boost various networks that are deeper and larger ( sec . [ reference ] - [ reference ] ) over the no - spp counterparts . further , driven by our detection framework , we find that multi - view testing on feature maps with flex ##ibly located / sized windows ( sec . [ reference ] ) can increase the classification accuracy . this manuscript also provides the details of these modifications . we have released the code to facilitate future research ( http : / / research . microsoft . com / en - us / um / people / ka ##he / ) . section : deep networks with spatial pyramid pool ##ing sub ##section : con ##vo ##lu ##tion ##al layers and feature maps consider the popular seven - layer architecture ##s . the first five layers are con ##vo ##lu ##tion ##al , some of which are followed by pool ##ing layers . these pool ##ing layers can also be considered as \" con ##vo ##lu ##tion ##al \" , in the sense that they are using sliding windows . the last two layers are fully connected , with an n - way soft ##max as the output , where n is the number of categories . the deep network described above needs a fixed image size . however , we notice that the requirement of fixed sizes is only due to the fully - connected layers that demand fixed - length vectors as inputs . on the other hand , the con ##vo ##lu ##tion ##al layers accept inputs of arbitrary sizes . the con ##vo ##lu ##tion ##al layers use sliding filters , and their outputs have roughly the same aspect ratio as the inputs . these outputs are known as feature maps - they involve not only the strength of the responses , but also their spatial positions . in figure [ reference ] , we visual ##ize some feature maps . they are generated by some filters of the con ##v layer . figure [ reference ] ( c ) shows the strongest activated images of these filters in the image ##net data ##set . we see a filter can be activated by some semantic content . for example , the 55 - th filter ( figure [ reference ] , bottom left ) is most activated by a circle shape ; the 66 - th filter ( figure [ reference ] , top right ) is most activated by a - shape ; and the 118 - th filter ( figure [ reference ] , bottom right ) is most activated by a - shape . these shapes in the input images ( figure [ reference ] ( a ) ) activate the feature maps at the corresponding positions ( the arrows in figure [ reference ] ) . it is worth noticing that we generate the feature maps in figure [ reference ] without fixing the input size . these feature maps generated by deep con ##vo ##lu ##tion ##al layers are analogous to the feature maps in traditional methods . in those methods , si ##ft vectors or image patches are densely extracted and then encoded , , by vector quan ##ti ##zation , sparse coding , or fisher kernel ##s . these encoded features consist of the feature maps , and are then poole ##d by bag - of - words ( bow ) or spatial pyramid ##s . analogous ##ly , the deep con ##vo ##lu ##tion ##al features can be poole ##d in a similar way . sub ##section : the spatial pyramid pool ##ing layer the con ##vo ##lu ##tion ##al layers accept arbitrary input sizes , but they produce outputs of variable sizes . the class ##ifiers ( sv ##m / soft ##max ) or fully - connected layers require fixed - length vectors . such vectors can be generated by the bag - of - words ( bow ) approach that pools the features together . spatial pyramid pool ##ing improves bow in that it can maintain spatial information by pool ##ing in local spatial bin ##s . these spatial bin ##s have sizes proportional to the image size , so the number of bin ##s is fixed regardless of the image size . this is in contrast to the sliding window pool ##ing of the previous deep networks , where the number of sliding windows depends on the input size . to adopt the deep network for images of arbitrary sizes , we replace the last pool ##ing layer ( , pool , after the last con ##vo ##lu ##tion ##al layer ) with a spatial pyramid pool ##ing layer . figure [ reference ] illustrates our method . in each spatial bin , we pool the responses of each filter ( throughout this paper we use max pool ##ing ) . the outputs of the spatial pyramid pool ##ing are - dimensional vectors with the number of bin ##s denoted as ( is the number of filters in the last con ##vo ##lu ##tion ##al layer ) . the fixed - dimensional vectors are the input to the fully - connected layer . with spatial pyramid pool ##ing , the input image can be of any sizes . this not only allows arbitrary aspect ratios , but also allows arbitrary scales . we can res ##ize the input image to any scale ( , = 180 , 224 , \u2026 ) and apply the same deep network . when the input image is at different scales , the network ( with the same filter sizes ) will extract features at different scales . the scales play important roles in traditional methods , , the si ##ft vectors are often extracted at multiple scales ( determined by the sizes of the patches and ga ##uss ##ian filters ) . we will show that the scales are also important for the accuracy of deep networks . interesting ##ly , the coarse ##st pyramid level has a single bin that covers the entire image . this is in fact a \" global pool ##ing \" operation , which is also investigated in several concurrent works . in a global average pool ##ing is used to reduce the model size and also reduce over ##fi ##tting ; in , a global average pool ##ing is used on the testing stage after all fc layers to improve accuracy ; in , a global max pool ##ing is used for weakly supervised object recognition . the global pool ##ing operation corresponds to the traditional bag - of - words method . sub ##section : training the network theoretically , the above network structure can be trained with standard back - propagation , regardless of the input image size . but in practice the gp ##u implementations ( such as cu ##da - con ##vn ##et and caf ##fe ) are prefer ##ably run on fixed input images . next we describe our training solution that takes advantage of these gp ##u implementations while still preserving the spatial pyramid pool ##ing behaviors . sub ##su ##bs ##ection : single - size training as in previous works , we first consider a network taking a fixed - size input ( 224 224 ) crop ##ped from images . the crop ##ping is for the purpose of data aug ##ment ##ation . for an image with a given size , we can pre - compute the bin sizes needed for spatial pyramid pool ##ing . consider the feature maps after con ##v that have a size of ( , 13 13 ) . with a pyramid level of bin ##s , we implement this pool ##ing level as a sliding window pool ##ing , where the window size and stride with and den ##oting ceiling and floor operations . with an - level pyramid , we implement such layers . the next fully - connected layer ( fc ) will con ##cate ##nate the outputs . figure [ reference ] shows an example configuration of 3 - level pyramid pool ##ing ( 3 3 , 2 2 , 1 1 ) in the cu ##da - con ##vn ##et style . the main purpose of our single - size training is to enable the multi - level pool ##ing behavior . experiments show that this is one reason for the gain of accuracy . sub ##su ##bs ##ection : multi - size training our network with spp is expected to be applied on images of any sizes . to address the issue of varying image sizes in training , we consider a set of pre - defined sizes . we consider two sizes : 180 180 in addition to 224 224 . rather than crop a smaller 180 180 region , we res ##ize the aforementioned 224 224 region to 180 180 . so the regions at both scales differ only in resolution but not in content / layout . for the network to accept 180 180 inputs , we implement another fixed - size - input ( 180 180 ) network . the feature map size after con ##v is 10 in this case . then we still use and to implement each pyramid pool ##ing level . the output of the spatial pyramid pool ##ing layer of this 180 - network has the same fixed length as the 224 - network . as such , this 180 - network has exactly the same parameters as the 224 - network in each layer . in other words , during training we implement the varying - input - size spp - net by two fixed - size networks that share parameters . to reduce the overhead to switch from one network ( , 224 ) to the other ( , 180 ) , we train each full epoch on one network , and then switch to the other one ( keeping all weights ) for the next full epoch . this is it ##erated . in experiments , we find the convergence rate of this multi - size training to be similar to the above single - size training . the main purpose of our multi - size training is to simulate the varying input sizes while still lever ##aging the existing well - opt ##imi ##zed fixed - size implementations . besides the above two - scale implementation , we have also tested a variant using as input where is randomly and uniformly sampled from at each epoch . we report the results of both variants in the experiment section . note that the above single / multi - size solutions are for training only . at the testing stage , it is straightforward to apply spp - net on images of any sizes . section : spp - net for image classification sub ##section : experiments on image ##net 2012 classification we train the networks on the 1000 - category training set of image ##net 2012 . our training algorithm follows the practices of previous work . the images are res ##ized so that the smaller dimension is 256 , and a 224 224 crop is picked from the center or the four corners from the entire image . the data are augmented by horizontal flipping and color altering . drop ##out is used on the two fully - connected layers . the learning rate starts from 0 . 01 , and is divided by 10 ( twice ) when the error plateau ##s . our implementation is based on the publicly available code of cu ##da - con ##vn ##et and caf ##fe . all networks in this paper can be trained on a single ge ##force gt ##x titan gp ##u ( 6 gb memory ) within two to four weeks . sub ##su ##bs ##ection : baseline network architecture ##s the advantages of spp are independent of the con ##vo ##lu ##tion ##al network architecture ##s used . we investigate four different network architecture ##s in existing publications ( or their modifications ) , and we show spp improves the accuracy of all these architecture ##s . these baseline architecture ##s are in table [ reference ] and briefly introduced below : z ##f - 5 : this architecture is based on ze ##ile ##r and fergus ' s ( z ##f ) \" fast \" ( smaller ) model . the number indicates five con ##vo ##lu ##tion ##al layers . con ##vn ##et * - 5 : this is a modification on k ##riz ##he ##vsky ' s network . we put the two pool ##ing layers after con ##v and con ##v ( instead of after con ##v and con ##v ) . as a result , the feature maps after each layer have the same size as z ##f - 5 . over ##fe ##at - 5 / 7 : this architecture is based on the over ##fe ##at paper , with some modifications as in . in contrast to z ##f - 5 / con ##vn ##et * - 5 , this architecture produces a larger feature map ( instead of ) before the last pool ##ing layer . a larger filter number ( 512 ) is used in con ##v and the following con ##vo ##lu ##tion ##al layers . we also investigate a deeper architecture with 7 con ##vo ##lu ##tion ##al layers , where con ##v to con ##v have the same structures . in the baseline models , the pool ##ing layer after the last con ##vo ##lu ##tion ##al layer generates feature maps , with two 40 ##9 ##6 - d fc layers and a 1000 - way soft ##max layer following . our replication ##s of these baseline networks are in table [ reference ] ( a ) . we train 70 epoch ##s for z ##f - 5 and 90 epoch ##s for the others . our replication of z ##f - 5 is better than the one reported in . this gain is because the corner crops are from the entire image , as is also reported in . sub ##su ##bs ##ection : multi - level pool ##ing improves accuracy in table [ reference ] ( b ) we show the results using single - size training . the training and testing sizes are both 224 224 . in these networks , the con ##vo ##lu ##tion ##al layers have the same structures as the corresponding baseline models , whereas the pool ##ing layer after the final con ##vo ##lu ##tion ##al layer is replaced with the spp layer . for the results in table [ reference ] , we use a 4 - level pyramid . the pyramid is { 6 6 , 3 3 , 2 2 , 1 1 } ( totally 50 bin ##s ) . for fair comparison , we still use the standard 10 - view prediction with each view a 224 224 crop . our results in table [ reference ] ( b ) show considerable improvement over the no - spp baseline ##s in table [ reference ] ( a ) . interesting ##ly , the largest gain of top - 1 error ( 1 . 65 % ) is given by the most accurate architecture . since we are still using the same 10 crop ##ped views as in ( a ) , these gains are solely because of multi - level pool ##ing . it is worth noticing that the gain of multi - level pool ##ing is not simply due to more parameters ; rather , it is because the multi - level pool ##ing is robust to the variance in object deformation ##s and spatial layout . to show this , we train another z ##f - 5 network with a different 4 - level pyramid : { 4 4 , 3 3 , 2 2 , 1 1 } ( totally 30 bin ##s ) . this network has fewer parameters than its no - spp counterpart , because its fc layer has 30 256 - d inputs instead of 36 256 - d . the top - 1 / top - 5 errors of this network are 35 . 06 / 14 . 04 . this result is similar to the 50 - bin pyramid above ( 34 . 98 / 14 . 14 ) , but considerably better than the no - spp counterpart ( 35 . 99 / 14 . 76 ) . sub ##su ##bs ##ection : multi - size training improves accuracy table [ reference ] ( c ) shows our results using multi - size training . the training sizes are 224 and 180 , while the testing size is still 224 . we still use the standard 10 - view prediction . the top - 1 / top - 5 errors of all architecture ##s further drop . the top - 1 error of spp - net ( over ##fe ##at - 7 ) drops to 29 . 68 % , which is 2 . 33 % better than its no - spp counterpart and 0 . 68 % better than its single - size trained counterpart . besides using the two discrete sizes of 180 and 224 , we have also evaluated using a random size uniformly sampled from . the top - 1 / 5 error of spp - net ( over ##fe ##at - 7 ) is 30 . 06 % / 10 . 96 % . the top - 1 error is slightly worse than the two - size version , possibly because the size of 224 ( which is used for testing ) is visited less . but the results are still better the single - size version . there are previous cnn solutions that deal with various scales / sizes , but they are mostly based on testing . in over ##fe ##at and howard ' s method , the single network is applied at multiple scales in the testing stage , and the scores are averaged . howard further trains two different networks on low / high - resolution image regions and averages the scores . to our knowledge , our method is the first one that trains a single network with input images of multiple sizes . sub ##su ##bs ##ection : full - image representations improve accuracy next we investigate the accuracy of the full - image views . we res ##ize the image so that = 256 while maintaining its aspect ratio . the spp - net is applied on this full image to compute the scores of the full view . for fair comparison , we also evaluate the accuracy of the single view in the center 224 224 crop ( which is used in the above evaluation ##s ) . the comparisons of single - view testing accuracy are in table [ reference ] . here we evaluate z ##f - 5 / over ##fe ##at - 7 . the top - 1 error rates are all reduced by the full - view representation . this shows the importance of maintaining the complete content . even though our network is trained using square images only , it general ##izes well to other aspect ratios . comparing table [ reference ] and table [ reference ] , we find that the combination of multiple views is substantially better than the single full - image view . however , the full - image representations are still of good merits . first , we empirical ##ly find that ( discussed in the next sub ##section ) even for the combination of dozens of views , the additional two full - image views ( with flipping ) can still boost the accuracy by about 0 . 2 % . second , the full - image view is method ##ological ##ly consistent with the traditional methods where the encoded si ##ft vectors of the entire image are poole ##d together . third , in other applications such as image retrieval , an image representation , rather than a classification score , is required for similarity ranking . a full - image representation can be preferred . sub ##su ##bs ##ection : multi - view testing on feature maps inspired by our detection algorithm ( described in the next section ) , we further propose a multi - view testing method on the feature maps . thanks to the flexibility of spp , we can easily extract the features from windows ( views ) of arbitrary sizes from the con ##vo ##lu ##tion ##al feature maps . on the testing stage , we res ##ize an image so where represents a pre ##de ##fine ##d scale ( like 256 ) . then we compute the con ##vo ##lu ##tion ##al feature maps from the entire image . for the usage of flipped views , we also compute the feature maps of the flipped image . given any view ( window ) in the image , we map this window to the feature maps ( the way of mapping is in appendix ) , and then use spp to pool the features from this window ( see figure [ reference ] ) . the poole ##d features are then fed into the fc layers to compute the soft ##max score of this window . these scores are averaged for the final prediction . for the standard 10 - view , we use and the views are 224 224 windows on the corners or center . experiments show that the top - 5 error of the 10 - view prediction on feature maps is within 0 . 1 % around the original 10 - view prediction on image crops . we further apply this method to extract multiple views from multiple scales . we res ##ize the image to six scales and compute the feature maps on the entire image for each scale . we use as the view size for any scale , so these views have different relative sizes on the original image for different scales . we use 18 views for each scale : one at the center , four at the corners , and four on the middle of each side , with / without flipping ( when = 224 there are 6 different views ) . the combination of these 96 views reduces the top - 5 error from 10 . 95 % to 9 . 36 % . combining the two full - image views ( with flipping ) further reduces the top - 5 error to 9 . 14 % . in the over ##fe ##at paper , the views are also extracted from the con ##vo ##lu ##tion ##al feature maps instead of image crops . however , their views can not have arbitrary sizes ; rather , the windows are those where the poole ##d features match the desired dimensional ##ity . we empirical ##ly find that these restricted windows are less beneficial than our flex ##ibly located / sized windows . sub ##su ##bs ##ection : summary and results for il ##s ##vr ##c 2014 in table [ reference ] we compare with previous state - of - the - art methods . k ##riz ##he ##vsky ' s is the winning method in il ##s ##vr ##c 2012 ; over ##fe ##at , howard ' s , and ze ##ile ##r and fergus ' s are the leading methods in il ##s ##vr ##c 2013 . we only consider single - network performance for manage ##able comparisons . our best single network achieve ##s 9 . 14 % top - 5 error on the validation set . this is exactly the single - model entry we submitted to il ##s ##vr ##c 2014 . the top - 5 error is 9 . 08 % on the testing set ( il ##s ##vr ##c 2014 has the same training / validation / testing data as il ##s ##vr ##c 2012 ) . after combining eleven models , our team ' s result ( 8 . 06 % ) is ranked # 3 among all 38 teams attending il ##s ##vr ##c 2014 ( table [ reference ] ) . since the advantages of spp - net should be in general independent of architecture ##s , we expect that it will further improve the deeper and larger con ##vo ##lu ##tion ##al architecture ##s . sub ##section : experiments on vo ##c 2007 classification our method can generate a full - view image representation . with the above networks pre - trained on image ##net , we extract these representations from the images in the target data ##set ##s and re - train sv ##m class ##ifiers . in the sv ##m training , we intentionally do not use any data aug ##ment ##ation ( flip / multi - view ) . we l - normal ##ize the features for sv ##m training . the classification task in pascal vo ##c 2007 involves 9 , 96 ##3 images in 20 categories . 5 , 01 ##1 images are for training , and the rest are for testing . the performance is evaluated by mean average precision ( map ) . table [ reference ] sum ##mar ##izes the results . we start from a baseline in table [ reference ] ( a ) . the model is z ##f - 5 without spp . to apply this model , we res ##ize the image so that its smaller dimension is 224 , and crop the center 224 224 region . the sv ##m is trained via the features of a layer . on this data ##set , the deeper the layer is , the better the result is . in table [ reference ] ( b ) , we replace the no - spp net with our spp - net . as a first - step comparison , we still apply the spp - net on the center 224 224 crop . the results of the fc layers improve . this gain is mainly due to multi - level pool ##ing . table [ reference ] ( c ) shows our results on full images , where the images are res ##ized so that the shorter side is 224 . we find that the results are considerably improved ( 78 . 39 % 76 . 45 % ) . this is due to the full - image representation that maintains the complete content . because the usage of our network does not depend on scale , we res ##ize the images so that the smaller dimension is and use the same network to extract features . we find that gives the best results ( table [ reference ] ( d ) ) based on the validation set . this is mainly because the objects occupy smaller regions in vo ##c 2007 but larger regions in image ##net , so the relative object scales are different between the two sets . these results indicate scale matters in the classification tasks , and spp - net can partially address this \" scale mis ##mat ##ch \" issue . in table [ reference ] ( e ) the network architecture is replaced with our best model ( over ##fe ##at - 7 , multi - size trained ) , and the map increases to 82 . 44 % . table [ reference ] sum ##mar ##izes our results and the comparisons with the state - of - the - art methods . among these methods , v ##q , lc ##c , and fk are all based on spatial pyramid ##s matching , and are based on deep networks . in these results , o ##qua ##b ' s ( 77 . 7 % ) and chat ##field ' s ( 82 . 42 % ) are obtained by network fine - tuning and multi - view testing . our result is comparable with the state of the art , using only a single full - image representation and without fine - tuning . sub ##section : experiments on cal ##tech ##10 ##1 the cal ##tech ##10 ##1 data ##set contains 9 , 144 images in 102 categories ( one background ) . we randomly sample 30 images per category for training and up to 50 images per category for testing . we repeat 10 random splits and average the accuracy . table [ reference ] sum ##mar ##izes our results . there are some common observations in the pascal vo ##c 2007 and cal ##tech ##10 ##1 results : spp - net is better than the no - spp net ( table [ reference ] ( b ) ( a ) ) , and the full - view representation is better than the crop ( ( c ) ( b ) ) . but the results in cal ##tech ##10 ##1 have some differences with pascal vo ##c . the fully - connected layers are less accurate , and the spp layers are better . this is possibly because the object categories in cal ##tech ##10 ##1 are less related to those in image ##net , and the deeper layers are more category - specialized . further , we find that the scale 224 has the best performance among the scales we tested on this data ##set . this is mainly because the objects in cal ##tech ##10 ##1 also occupy large regions of the images , as is the case of image ##net . besides crop ##ping , we also evaluate warp ##ing the image to fit the 224 224 size . this solution maintains the complete content , but introduces distortion . on the spp ( z ##f - 5 ) model , the accuracy is 89 . 91 % using the spp layer as features - lower than 91 . 44 % which uses the same model on the und ##isto ##rted full image . table [ reference ] sum ##mar ##izes our results compared with the state - of - the - art methods on cal ##tech ##10 ##1 . our result ( 93 . 42 % ) exceeds the previous record ( 88 . 54 % ) by a substantial margin ( 4 . 88 % ) . section : spp - net for object detection deep networks have been used for object detection . we briefly review the recent state - of - the - art r - cnn method . r - cnn first extracts about 2 , 000 candidate windows from each image via selective search . then the image region in each window is warped to a fixed size ( 227 227 ) . a pre - trained deep network is used to extract the feature of each window . a binary sv ##m class ##ifier is then trained on these features for detection . r - cnn generates results of compelling quality and substantially out ##per ##forms previous methods . however , because r - cnn repeatedly applies the deep con ##vo ##lu ##tion ##al network to about 2 , 000 windows per image , it is time - consuming . feature extraction is the major timing bottle ##neck in testing . our spp - net can also be used for object detection . we extract the feature maps from the entire image only once ( possibly at multiple scales ) . then we apply the spatial pyramid pool ##ing on each candidate window of the feature maps to pool a fixed - length representation of this window ( see figure [ reference ] ) . because the time - consuming con ##vo ##lu ##tions are only applied once , our method can run orders of magnitude faster . our method extracts window - wise features from regions of the feature maps , while r - cnn extracts directly from image regions . in previous works , the def ##or ##mable part model ( d ##pm ) extracts features from windows in hog feature maps , and the selective search ( ss ) method extracts from windows in encoded si ##ft feature maps . the over ##fe ##at detection method also extracts from windows of deep con ##vo ##lu ##tion ##al feature maps , but needs to pre - define the window size . on the contrary , our method enables feature extraction in arbitrary windows from the deep con ##vo ##lu ##tion ##al feature maps . sub ##section : detection algorithm we use the \" fast \" mode of selective search to generate about 2 , 000 candidate windows per image . then we res ##ize the image such that , and extract the feature maps from the entire image . we use the spp - net model of z ##f - 5 ( single - size trained ) for the time being . in each candidate window , we use a 4 - level spatial pyramid ( 1 1 , 2 2 , 3 3 , 6 6 , totally 50 bin ##s ) to pool the features . this generates a 12 , 800 - d ( 256 50 ) representation for each window . these representations are provided to the fully - connected layers of the network . then we train a binary linear sv ##m class ##ifier for each category on these features . our implementation of the sv ##m training follows . we use the ground - truth windows to generate the positive samples . the negative samples are those overlapping a positive window by at most 30 % ( measured by the intersection - over - union ( io ##u ) ratio ) . any negative sample is removed if it overlap ##s another negative sample by more than 70 % . we apply the standard hard negative mining to train the sv ##m . this step is it ##erated once . it takes less than 1 hour to train sv ##ms for all 20 categories . in testing , the class ##ifier is used to score the candidate windows . then we use non - maximum suppression ( threshold of 30 % ) on the scored windows . our method can be improved by multi - scale feature extraction . we res ##ize the image such that , and compute the feature maps of con ##v for each scale . one strategy of combining the features from these scales is to pool them channel - by - channel . but we empirical ##ly find that another strategy provides better results . for each candidate window , we choose a single scale such that the scaled candidate window has a number of pixels closest to 224 224 . then we only use the feature maps extracted from this scale to compute the feature of this window . if the pre - defined scales are dense enough and the window is approximately square , our method is roughly equivalent to res ##izing the window to 224 224 and then extract ##ing features from it . nevertheless , our method only requires computing the feature maps once ( at each scale ) from the entire image , regardless of the number of candidate windows . we also fine - tune our pre - trained network , following . since our features are poole ##d from the con ##v feature maps from windows of any sizes , for simplicity we only fine - tune the fully - connected layers . in this case , the data layer accepts the fixed - length poole ##d features after con ##v , and the fc layers and a new 21 - way ( one extra negative category ) fc layer follow . the fc weights are initial ##ized with a ga ##uss ##ian distribution of = 0 . 01 . we fix all the learning rates to 1 ##e - 4 and then adjust to 1 ##e - 5 for all three layers . during fine - tuning , the positive samples are those overlapping with a ground - truth window by , and the negative samples by . in each mini - batch , 25 % of the samples are positive . we train 250 ##k mini - batch ##es using the learning rate 1 ##e - 4 , and then 50 ##k mini - batch ##es using 1 ##e - 5 . because we only fine - tune the fc layers , the training is very fast and takes about 2 hours on the gp ##u ( excluding pre - ca ##ching feature maps which takes about 1 hour ) . also following , we use bound ##ing box regression to post - process the prediction windows . the features used for regression are the poole ##d features from con ##v ( as a counterpart of the pool features used in ) . the windows used for the regression training are those overlapping with a ground - truth window by at least 50 % . sub ##section : detection results we evaluate our method on the detection task of the pascal vo ##c 2007 data ##set . table [ reference ] shows our results on various layers , by using 1 - scale ( = 68 ##8 ) or 5 - scale . here the r - cnn results are as reported in using the alex ##net with 5 con ##v layers . using the pool layers ( in our case the poole ##d features ) , our result ( 44 . 9 % ) is comparable with r - cnn ' s result ( 44 . 2 % ) . but using the non - fine - tuned fc layers , our results are inferior . an explanation is that our fc layers are pre - trained using image regions , while in the detection case they are used on the feature map regions . the feature map regions can have strong activation ##s near the window boundaries , while the image regions may not . this difference of usage ##s can be addressed by fine - tuning . using the fine - tuned fc layers ( ft ##fc ) , our results are comparable with or slightly better than the fine - tuned results of r - cnn . after bound ##ing box regression , our 5 - scale result ( 59 . 2 % ) is 0 . 7 % better than r - cnn ( 58 . 5 % ) , and our 1 - scale result ( 58 . 0 % ) is 0 . 5 % worse . in table [ reference ] we further compare with r - cnn using the same pre - trained model of spp ##net ( z ##f - 5 ) . in this case , our method and r - cnn have comparable averaged scores . the r - cnn result is boosted by this pre - trained model . this is because of the better architecture of z ##f - 5 than alex ##net , and also because of the multi - level pool ##ing of spp ##net ( if using the no - spp z ##f - 5 , the r - cnn result drops ) . table [ reference ] shows the results for each category . table [ reference ] also includes additional methods . selective search ( ss ) applies spatial pyramid matching on si ##ft feature maps . d ##pm and region ##let are based on hog features . the region ##let method improves to 46 . 1 % by combining various features including con ##v . detector ##net trains a deep network that outputs pixel - wise object masks . this method only needs to apply the deep network once to the entire image , as is the case for our method . but this method has lower map ( 30 . 5 % ) . sub ##section : complexity and running time despite having comparable accuracy , our method is much faster than r - cnn . the complexity of the con ##vo ##lu ##tion ##al feature computation in r - cnn is with the window number ( 2000 ) . this complexity of our method is at a scale , where is the aspect ratio . assume is about 4 / 3 . in the single - scale version when , this complexity is about 1 / 160 of r - cnn ' s ; in the 5 - scale version , this complexity is about 1 / 24 of r - cnn ' s . in table [ reference ] , we provide a fair comparison on the running time of the feature computation using the same spp ( z ##f - 5 ) model . the implementation of r - cnn is from the code published by the authors implemented in caf ##fe . we also implement our feature computation in caf ##fe . in table [ reference ] we evaluate the average time of 100 random vo ##c images using gp ##u . r - cnn takes 14 . 37 ##s per image for con ##vo ##lu ##tions , while our 1 - scale version takes only 0 . 05 ##3 ##s per image . so ours is 270 faster than r - cnn . our 5 - scale version takes 0 . 293 ##s per image for con ##vo ##lu ##tions , so is 49 faster than r - cnn . our con ##vo ##lu ##tion ##al feature computation is so fast that the computational time of fc layers takes a considerable portion . table [ reference ] shows that the gp ##u time of computing the 4 , 09 ##6 - d fc features is 0 . 08 ##9 ##s per image . considering both con ##vo ##lu ##tion ##al and fully - connected features , our 1 - scale version is 102 ##\u00d7 faster than r - cnn and is 1 . 2 % inferior ; our 5 - scale version is 38 ##\u00d7 faster and has comparable results . we also compares the running time in table [ reference ] where r - cnn uses alex ##net as is in the original paper . our method is 24 to 64 faster . note that the alex ##net has the same number of filters as our z ##f - 5 on each con ##v layer . the alex ##net is faster because it uses splitting on some layers , which was designed for two gp ##us in . we further achieve an efficient full system with the help of the recent window proposal method . the selective search ( ss ) proposal takes about 1 - 2 seconds per image on a cpu . the method of edge ##box ##es only takes 0 . 2 ##s . note that it is sufficient to use a fast proposal method during testing only . using the same model trained as above ( using ss ) , we test proposals generated by edge ##box ##es only . the map is 52 . 8 without bound ##ing box regression . this is reasonable considering that edge ##box ##es are not used for training . then we use both ss and edge ##box as proposals in the training stage , and adopt only edge ##box ##es in the testing stage . the map is 56 . 3 without bound ##ing box regression , which is better than 55 . 2 ( table [ reference ] ) due to additional training samples . in this case , the overall testing time is 0 . 5 ##s per image including all steps ( proposal and recognition ) . this makes our method practical for real - world applications . sub ##section : model combination for detection model combination is an important strategy for boost ##ing cnn - based classification accuracy . we propose a simple combination method for detection . we pre - train another network in image ##net , using the same structure but different random initial ##izations . then we repeat the above detection algorithm . table [ reference ] ( spp - net ( 2 ) ) shows the results of this network . its map is comparable with the first network ( 59 . 1 % 59 . 2 % ) , and out ##per ##forms the first network in 11 categories . given the two models , we first use either model to score all candidate windows on the test image . then we perform non - maximum suppression on the union of the two sets of candidate windows ( with their scores ) . a more confident window given by one method can suppress those less confident given by the other method . after combination , the map is boosted to 60 . 9 % ( table [ reference ] ) . in 17 out of all 20 categories the combination performs better than either individual model . this indicates that the two models are complementary . we further find that the complement ##ari ##ty is mainly because of the con ##vo ##lu ##tion ##al layers . we have tried to combine two randomly initial ##ized fine - tuned results of the same con ##vo ##lu ##tion ##al model , and found no gain . sub ##section : il ##s ##vr ##c 2014 detection the il ##s ##vr ##c 2014 detection task involves 200 categories . there are 450 ##k / 20 ##k / 40 ##k images in the training / validation / testing sets . we focus on the task of the provided - data - only track ( the 1000 - category cl ##s training data is not allowed to use ) . there are three major differences between the detection ( det ) and classification ( cl ##s ) training data ##set ##s , which greatly impacts the pre - training quality . first , the det training data is merely 1 / 3 of the cl ##s training data . this seems to be a fundamental challenge of the provided - data - only det task . second , the category number of det is 1 / 5 of cl ##s . to overcome this problem , we harness the provided sub ##cate ##gor ##y labels for pre - training . there are totally 49 ##9 non - overlapping sub ##cate ##gor ##ies ( , the leaf nodes in the provided category hierarchy ) . so we pre - train a 49 ##9 - category network on the det training set . third , the distributions of object scales are different between det / cl ##s training sets . the dominant object scale in cl ##s is about 0 . 8 of the image length , but in det is about 0 . 5 . to address the scale difference , we res ##ize each training image to ( instead of ) , and randomly crop views for training . a crop is only used when it overlap ##s with a ground truth object by at least 50 % . we verify the effect of pre - training on pascal vo ##c 2007 . for a cl ##s - pre - training baseline , we consider the pool features ( map 43 . 0 % in table [ reference ] ) . replaced with a 200 - category network pre - trained on det , the map significantly drops to 32 . 7 % . a 49 ##9 - category pre - trained network improves the result to 35 . 9 % . interesting ##ly , even if the amount of training data do not increase , training a network of more categories boost ##s the feature quality . finally , training with instead of further improves the map to 37 . 8 % . even so , we see that there is still a considerable gap to the cl ##s - pre - training result . this indicates the importance of big data to deep learning . for il ##s ##vr ##c 2014 , we train a 49 ##9 - category over ##fe ##at - 7 spp - net . the remaining steps are similar to the vo ##c 2007 case . following , we use the validation set to generate the positive / negative samples , with windows proposed by the selective search fast mode . the training set only contributes positive samples using the ground truth windows . we fine - tune the fc layers and then train the sv ##ms using the samples in both validation and training sets . the bound ##ing box regression is trained on the validation set . our single model leads to 31 . 84 % map in the il ##s ##vr ##c 2014 testing set . we combine six similar models using the strategy introduced in this paper . the map is 35 . 11 % in the testing set . this result ranks # 2 in the provided - data - only track of il ##s ##vr ##c 2014 ( table [ reference ] ) . the winning result is 37 . 21 % from nu ##s , which uses context ##ual information . our system still shows great advantages on speed for this data ##set . it takes our single model 0 . 6 seconds ( 0 . 5 for con ##v , 0 . 1 for fc , excluding proposals ) per testing image on a gp ##u extract ##ing con ##vo ##lu ##tion ##al features from all 5 scales . using the same model , it takes 32 seconds per image in the way of rc ##nn . for the 40 ##k testing images , our method requires 8 gp ##u hours to compute con ##vo ##lu ##tion ##al features , while rc ##nn would require 15 gp ##u days . section : conclusion spp is a flexible solution for handling different scales , sizes , and aspect ratios . these issues are important in visual recognition , but received little consideration in the context of deep networks . we have suggested a solution to train a deep network with a spatial pyramid pool ##ing layer . the resulting spp - net shows outstanding accuracy in classification / detection tasks and greatly accelerate ##s d ##nn - based detection . our studies also show that many time - proven techniques / insights in computer vision can still play important roles in deep - networks - based recognition . section : in the appendix , we describe some implementation details : mean sub ##tra ##ction . the 224 224 crop ##ped training / testing images are often pre - processed by sub ##tra ##cting the per - pixel mean . when input images are in any sizes , the fixed - size mean image is not directly applicable . in the image ##net data ##set , we warp the 224 224 mean image to the desired size and then sub ##tra ##ct it . in pascal vo ##c 2007 and cal ##tech ##10 ##1 , we use the constant mean ( 128 ) in all the experiments . implementation of pool ##ing bin ##s . we use the following implementation to handle all bin ##s when applying the network . denote the width and height of the con ##v feature maps ( can be the full image or a window ) as and . for a pyramid level with bin ##s , the - th bin is in the range of . intuitive ##ly , if rounding is needed , we take the floor operation on the left / top boundary and ceiling on the right / bottom boundary . mapping a window to feature maps . in the detection algorithm ( and multi - view testing on feature maps ) , a window is given in the image domain , and we use it to crop the con ##vo ##lu ##tion ##al feature maps ( , con ##v ) which have been sub - sampled several times . so we need to align the window on the feature maps . in our implementation , we project the corner point of a window onto a pixel in the feature maps , such that this corner point in the image domain is closest to the center of the rec ##eptive field of that feature map pixel . the mapping is complicated by the pad ##ding of all con ##vo ##lu ##tion ##al and pool ##ing layers . to sim ##plify the implementation , during deployment we pad pixels for a layer with a filter size of . as such , for a response centered at , its effective rec ##eptive field in the image domain is centered at where is the product of all previous strides . in our models , for z ##f - 5 on con ##v , and for over ##fe ##at - 5 / 7 on con ##v . given a window in the image domain , we project the left ( top ) boundary by : and the right ( bottom ) boundary . if the pad ##ding is not , we need to add a proper offset to . bibliography : references section : change ##log ar ##xi ##v v ##1 . initial technical report for ec ##c ##v 2014 paper . ar ##xi ##v v ##2 . submitted version for t ##pa ##mi . includes extra experiments of spp on various architecture ##s . includes details for il ##s ##vr ##c 2014 . ar ##xi ##v v ##3 . accepted version for t ##pa ##mi . includes comparisons with r - cnn using the same architecture . includes detection experiments using edge ##box ##es . ar ##xi ##v v ##4 . revised \" mapping a window to feature maps \" in appendix for easier implementation .",
        "pred_seq": "pascal 2007 [SEP] spp net [SEP] accuracy accuracy accuracy [SEP] object object object object object object object object object object object object object object object object detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection detection [CLS]",
        "pred_templates": [],
        "gold_templates": [
            {
                "Material": [
                    [
                        "pascal voc 2007",
                        "voc"
                    ]
                ],
                "Method": [],
                "Metric": [
                    [
                        "mean average precision",
                        "map"
                    ]
                ],
                "Task": [
                    [
                        "object detection",
                        "detection",
                        "detection task",
                        "detection case",
                        "detection det"
                    ]
                ]
            }
        ]
    }
}