{
    "docid": "007ff2ca5f297b04636699ce4d01ca6d6f21dc77-0",
    "doctext": "document: attention boosted sequential inference model attention mechanism has been proven effective on natural language processing. this paper proposes an attention boosted natural language inference model named aesim by adding word attention and adaptive direction-oriented attention mechanisms to the traditional bi-lstm layer of natural language inference models, e.g. esim. this makes the inference model aesim has the ability to effectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis. the empirical studies on the snli, multinli and quora benchmarks manifest that aesim is superior to the original esim model. acmlicensed [dapa'19], 2019melbourne, australia 2019 2019 printacmref= false 1234-5678-9012 1234-5678-9012 1234-5678-9012 section: introduction natural language inference (nli) is an important and significant task in natural language processing (nlp). it concerns whether a hypothesis can be inferred from a premise, requiring understanding of the semantic similarity between the hypothesis and the premise to discriminate their relation. table [reference] shows several samples of natural language inference from snli (stanford natural language inference) corpus. in the literature, the task of nli is usually viewed as a relation classification. it learns the relation between a premise and a hypothesis in a large training set, then predicts the relation between a new pair of premise and hypothesis. the existing methods of nli can be roughly partitioned into two categories: feature-based models and neural network-based models. feature-based models represent a premise and a hypothesis by their unlexicalized and lexicalized features, such as-gram length and the real-valued feature of length difference, then train a classifier to perform relation classification. recently, end-to-end neural network-based models have drawn worldwide attention since they have demonstrated excellent performance on quite a few nlp tasks including machine translation, natural language inference, etc. on the basis of their model structures, we can divide neural network-based models for nli into two classes, sentence encoding models and sentence interaction-aggregation models. the architectures of the two types of models are shown in figure [reference]. sentence encoding models (their main architecture is shown in figure [reference].a) independently encode a pair of sentences, a premise and a hypothesis using pre-trained word embedding vectors, then learn semantic relation between two sentences with a multi-layer perceptron (mlp). in these models, lstm (long short-term memory networks), its variants gru (gated recurrent units) and bi-lstm, are usually utilized to encode the sentences since they were capable of learning long-term dependencies inside sentences. for example, conneau et al. proposed a generic nli training scheme and compared several sentence encoding architectures: lstm or gru, bi-lstm with mean/ max pooling, self-attention network and hierarchical convolutional networks. the experimental results demonstrated that the bi-lstm with max pooling achieved the best performance. talman et al. designed a hierarchical bi-lstm max pooling (hbmp) model to encode sentences. this model applied parameters of one bi-lstm to initialize the next bi-lstm to convey information, which shown better results than the model with a single bi-lstm. besides lstm, attention mechanisms could also be used to boost the effectiveness of sentence encoding. the model developed by ghaeini et al. added self-attention to lstm model, and achieved better performance. sentence interaction-aggregation models (their main architecture is shown in figure [reference].b) learn vector representations of pairs of sentences in the way similar to sentence encoding models and calculate pairwise word interaction matrix between two sentences using the newly updated word vectors, and then the matching results are aggregated into a vector to make the final decision. compared with sentence encoding model, sentence interaction-aggregation models aggregate word similarities between a pair of sentences, are capable of capturing the relevant information between two sentences, a premise and a hypothesis. bahdanau et al. translated and aligned text simultaneously in machine translation task, innovatively introducing attention mechanism to natural language process (nlp). he et al. designed a pairwise word interaction model (pwim), which made full use of word-level fine-grained information. wang et al. put forward a bilateral multi-perspective matching (bimpm) model, focusing on various matching strategies that could be seen as different types of attention. the empirical studies of lan et al. and chen et al. concluded that sentence interation-aggregation models, especially esim (enhanced sequential inference model), a carefully designed sequential inference model based on chain lstms, outperformed all previous sentence encoding models. although esim has achieved excellent achievements, this model does n't consider the attention along the words in a sentence in its bi-lstm layer. word attention can characterize the different contribution of each word. therefore, it will be beneficial to put word attention into the bi-ltsm layer. moreover, the orientation of the words represents the direction of the information flow, either forward or backward, should not be ignored. in traditional bi-lstm model, the forward and the backward vectors learnt by bi-lstm are simply jointed. it's necessary to consider whether each orientation (forward or backward) has different importance on word encoding, thus adaptively joint the two orientation vectors together with different weights. therefore, in this study, using esim model as the baseline, we add an attention layer behind each bi-lstm layer, then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors. we name this attention boosted bi-lstm as bi-alstm, and denote the modified esim as aesim. experimental results on snli, multinli and quora benchmarks have demonstrated better performance of aesim model than that of the baseline esim and the other state-of-the-art models. we believe that the architecture of bi-alstm has potentially to be used in other nlp tasks such as text classification, machine translation and so on. this paper is organized as follows. we introduce the general frameworks of esim and aesim in section 2. we describe the datasets and the experiment settings, and analyze our experimental results in section 3. we then draw conclusions in section 4. section: attention boosted sequential inference model supposed that we have two sentences and, where represents premise and represents hypothesis. the goal is to predict the label meaning for their relation. subsection: esim model enhanced sequential inference model (esim) is composed of four main components: input encoding layer, local inference modeling layer, inference composition layer and classification layer. in the input encoding layer, esim first uses bi-lstm layer to encode input sentence pairs (equations 1-2), which can be initialized using pre-trained word embeddings (e.g. glove 840b vectors), where is the word embedding vector of the-th word in, is that of word in. secondly, esim implements the local inference layer for enhancing the sentence information. first it calculates a similarity matrix based on and. it then gets the new expression for and with the equation below: where and represent the weighted summation of and. it further enhances the local inference information collected as below. after the enhancement of local inference, another bi-lstm layer is used to capture local inference information and their context for inference composition. instead of summation adopted by parikh et al., esim proposes to compute both max and average pooling and feeds the concatenate fixed length vector to the final classifier: a fully connected multi-layer perceptron. figure [reference] shows a high-level view of the esim architecture, where the bottom lstm1 layer of figure [reference] is the input encoding layer, the middle part with lstm2 layer is the local inference layer, the upper part is the inference composition layer. subsection: aesim model the overall architecture of our newly proposed attention boosted sequential inference model (named aesim) based on esim is similar to esim. in detail, aesim also consists of four main parts: encoding layer, local inference modeling layer, decoding layer and classification layer. the only difference between esim and aesim is that we substitute the two bi-lstm layers (lstm1 and lstm2) in esim with two bi-alstm layers in aesim. therefore, as illustrated in figure [reference], the layers with red-dotted circles in esim will be replaced by the bi-alstm layers shown in the right upper corner of the figure [reference] and the details of bi-alstm can be found in figure [reference]. given the word vector of the-th word in sentence, which can be obtained by pre-trained word embeddings such as glove 840b vectors in the first bi-alstm layer or obtained from the local inference modeling layer in the second bi-alstm layer. we utilize a forward lstm layer and a backward lstm layer to collect both direction information and. as described in introduction section, in the following newly proposed bi-alstm, we add word attention and additive operation on both orientations of traditional bi-lstm layer. word attention layer it's obvious that not all words contribute equally to the representation of a sentence. attention mechanism, which is introduced in, is extremely effective to extract vital words from the whole sentence, and is particularly beneficial to generate the sentence vector. therefore, we use the following attention mechanism after we get and. suppose, we then have where is obtained after one-layer mlp for the input, is the importance of word, is calculated by the softmax unit on the context vector of the sentence which is randomly initialized and modified during the training, is the attention enhanced vector through multiplying the weight and original vector, where correspond to the forward vector and the backward vector, respectively. adaptive word direction layer in traditional bi-lstm model, the forward and the backward vectors of a word are considered to have equal importance on the word representation. the model simply connects the forward and backward vectors head and tail without weighing their importance. for a word in different direction or orientation, the former and the latter words are reversed. thus, different direction vectors of a word make different contribution to the representation, especially the words in a long sentence. therefore, we propose a new adaptive direction layer to learn the contribution of different directions for a single word. formally, given two direction word vectors and, the whole word vector can be expressed as: where, and denote weight matrix and the bias, denotes the nonlinear function, denotes the concentration. all the parameters can be learned during training. then we can get the whole sentence vector as below: this word and orientation enhanced bi-lstm is called bi-alstm. its whole architecture is shown in the figure [reference], is applied in esim model to replace the two bi-lstm layers for the task of natural language inference. besides, this bi-alstm can be used to other natural language processing tasks and our preliminary experiments have demonstrated that bi-alstm is capable of improving the performance of bi-lstm models on sentimental classification task (for space limitation, this results will not be shown in the paper). section: experiment setup subsection: datasets we evaluated our model on three datasets: the stanford natural language inference (snli) corpus, the multi-genre natural language inference (multinli) corpus, and quora duplicate question dataset. we selected these three relatively large corpora out of eight corpora in since deep learning models usually show better generalization ability on large training sets and produce more convincing results than on small training sets. snli the stanford natural language inference (snli) corpus contains 570, 152 sentence pairs, including 549 k training pairs, 10 k validation pairs and 10 k testing pairs. each pair has one of relation classes (entailment, neutral, contradiction and '-'). the '-' class indicates there is no conclusion between the two sentences. consequently, we remove all pairs with relation' -' during training, validating and testing processes. multinli this corpus is a crowd-sourced collection of 433 k sentence pairs annotated with textual entailment information. the corpus is modeled on the snli corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generation evaluation. quora the quora dataset contains 400, 000 question pairs. the task of this corpus is to judge whether the two sentences means the same affair. subsection: setting we use the validation set to select models for testing. the hyper-parameters of aesim model are listed as follows. we use the adam method for optimization. the first momentum is set to be 0.9 and the second 0.999. the initial learning rate is set to 0.0005, and the batch size is 128. the dimensions of all hidden states of bi-alstm and word embedding are 300. we employ non-linearity function replacing rectified linear unit on account of its faster convergence rate. dropout rate is set to 0.2 during training. we use pre-trained 300-d glove 840b vectors to initialize word embeddings. out-of-vocabulary (oov) words are initialized randomly with gaussian samples. all vectors are updated during training. subsection: experiment results except for comparing our method aesim with esim, we listed the experimental results of methods with their references in table [reference] on snil. in table [reference], the method in the first block is a traditional feature engineering method, those in the second are the sentence vector-based models, those in the third are attention-based models, and esim and our aesim are shown in the fourth block. where the results of esim and aesim are implemented by ourselves on keras, the results of the others are taken from their original publications. we then compare the baseline models, cbow, bi-lstm with esim and our aesim on multinli corpus shown in table [reference], where the results of the baselines are taken from. finally, we compare several types of cnn and rnn models on quroa corpus shown in table [reference], the results of theses cnn and rnn models are taken from. the accuracy (acc) of each method is measured by the commonly used precision score, and the methods with the best accuracy are marked in bold. according to the results in tables 2-4, aesim model achieved 88.1% on snli corpus, elevating 0.8 percent higher than esim model. it promoted almost 0.5 percent accuracy and outperformed the baselines on multinli. it also achieved 88.01% on quora. therefore, we concluded that aesim with further word attention and word orientation operation was superior to esim model. subsection: attention visualization we selected three types of sentence pairs from a premise and its three hypothesis sentences in the test set of snli corpus as shown in figure [reference], where the premise sentence is 'a woman with a green headscarf, blue shirt and a very big grin', and three hypothesis sentences are 'the woman has been shot', 'the woman is very happy' and 'the woman is young' with relation labels 'contradiction', 'entailment', and 'neutral', respectively. each pair of sentences has their key word pairs: grin-shot, grin-happy and grin-young, which determines whether the premise can entail the hypothesis. figures 4.a-4.c are the visualization of the attention layer between sentence pairs after the bi-lstm layer in esim model and that after bi-alstm layer in aesim model for contrasting esim and aesim. by doing so, we could understand how the models judge the relation between two sentences. in each figure, the brighter the color, the higher the weight is. we could conclude that our aesim model had the higher weight than esim model on each key word pair, especially in figure [reference].b, where the similarity of 'happy' and 'grin' in aesim model is much higher than that in esim model. therefore, our aesim model was able to capture the most important word pair in each pair of sentences. section: conclusion in this study, we propose an improved version of esim named aesim for nli. it modifies the bi-lstm layer to collect more information. we evaluate our aesim model on three nli corpora. experimental results show that aesim model achieves better performance than esim model. in the future, we will evaluate how attention mechanisms can be applied on other tasks and explore a way to use less time and space with guaranteed accuracy. section: acknowledgement this work is supported in part by the national nature science foundation of china (no. 61876016 and no. 61632004), the fundamental research funds for the central universities (no. 2018jbz006). bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "multinli",
                        601
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "attention boosted sequential inference model",
                        10
                    ],
                    [
                        "attention boosted natural language inference model",
                        156
                    ],
                    [
                        "aesim",
                        213
                    ],
                    [
                        "inference model aesim",
                        394
                    ],
                    [
                        "aesim model",
                        6175
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "natural language inference",
                        835
                    ],
                    [
                        "nli",
                        864
                    ],
                    [
                        "nli training scheme",
                        2901
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "quora",
                        614
                    ],
                    [
                        "quroa corpus",
                        14645
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "attention boosted sequential inference model",
                        10
                    ],
                    [
                        "attention boosted natural language inference model",
                        156
                    ],
                    [
                        "aesim",
                        213
                    ],
                    [
                        "inference model aesim",
                        394
                    ],
                    [
                        "aesim model",
                        6175
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        14747
                    ],
                    [
                        "acc",
                        14758
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "natural language inference",
                        835
                    ],
                    [
                        "nli",
                        864
                    ],
                    [
                        "nli training scheme",
                        2901
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "snli",
                        595
                    ],
                    [
                        "snil",
                        14006
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "attention boosted sequential inference model",
                        10
                    ],
                    [
                        "attention boosted natural language inference model",
                        156
                    ],
                    [
                        "aesim",
                        213
                    ],
                    [
                        "inference model aesim",
                        394
                    ],
                    [
                        "aesim model",
                        6175
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "natural language inference",
                        835
                    ],
                    [
                        "nli",
                        864
                    ],
                    [
                        "nli training scheme",
                        2901
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "027d6c52be01b583b9a0d9eb8c9364c6b701c656-1",
    "doctext": "document: all you need is a good init layer-sequential unit-variance (lsuv) initialization- a simple method for weight initialization for deep net learning- is proposed. the method consists of the two steps. first, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. experiment with different activation functions (maxout, relu-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as fitnets () and highway (). performance is evaluated on googlenet, caffenet, fitnets and residual nets and the state-of-the-art, or very close to it, is achieved on the mnist, cifar-10/ 100 and imagenet datasets. update section: introduction deep nets have demonstrated impressive results on a number of computer vision and natural language processing problems. at present, state-of-the-art results in image classification () and speech recognition (), etc., have been achieved with very deep (layer) cnns. thin deep nets are of particular interest, since they are accurate and at the same inference-time efficient (). one of the main obstacles preventing the wide adoption of very deep nets is the absence of a general, repeatable and efficient procedure for their end-to-end training. for example, vggnet () was optimized by a four stage procedure that started by training a network with moderate depth, adding progressively more layers. stated that deep and thin networks are very hard to train by backpropagation if deeper than five layers, especially with uniform initialization. on the other hand, showed that it is possible to train the vggnet in a single optimization run if the network weights are initialized with a specific relu-aware initialization. the procedure generalizes to the relu non-linearity the idea of filter-size dependent initialization, introduced for the linear case by (). batch normalization (), a technique that inserts layers into the the deep net that transform the output for the batch to be zero mean unit variance, has successfully facilitated training of the twenty-two layer googlenet (). however, batch normalization adds a 30% computational overhead to each iteration. the main contribution of the paper is a proposal of a simple initialization procedure that, in connection with standard stochastic gradient descent (sgd), leads to state-of-the-art thin and very deep neural nets. the result highlights the importance of initialization in very deep nets. we review the history of cnn initialization in section [reference], which is followed by a detailed description of the novel initialization method in section [reference]. the method is experimentally validated in section [reference]. section: initialization in neural networks after the success of cnns in ivsrc 2012 (), initialization with gaussian noise with mean equal to zero and standard deviation set to 0.01 and adding bias equal to one for some layers become very popular. but, as mentioned before, it is not possible to train very deep network from scratch with it (). the problem is caused by the activation (and/ or) gradient magnitude in final layers (). if each layer, not properly initialized, scales input by, the final scale would be, where is a number of layers. values of lead to extremely large values of output layers, leads to a diminishing signal and gradient. proposed a formula for estimating the standard deviation on the basis of the number of input and output channels of the layers under assumption of no non-linearity between layers. despite invalidity of the assumption, glorot initialization works well in many applications. extended this formula to the relu () non-linearity and showed its superior performance for relu-based nets. figure [reference] shows why scaling is important. large weights lead to divergence via updates larger than the initial values, small initial weights do not allow the network to learn since the updates are of the order of 0.0001 per iteration. the optimal scaling for relu-net is around 1.4, which is in line with the theoretically derived by. proposed the so called random walk initialization, rwi, which keeps constant the log of the norms of the backpropagated errors. in our experiments, we have not been able to obtain good results with our implementation of rwi, that is why this method is not evaluated in experimental section. and take another approach to initialization and formulate training as mimicking teacher network predictions (so called knowledge distillation) and internal representations (so called hints initialization) rather than minimizing the softmax loss. proposed a lstm-inspired gating scheme to control information and gradient flow through the network. they trained a 1000-layers mlp network on mnist. basically, this kind of networks implicitly learns the depth needed for the given task. independently, showed that orthonormal matrix initialization works much better for linear networks than gaussian noise, which is only approximate orthogonal. it also work for networks with non-linearities. the approach of layer-wise pre-training () which is still useful for multi-layer-perceptron, is not popular for training discriminative convolution networks. section: layer-sequential unit-variance initialization to the best of our knowledge, there have been no attempts to generalize formulas to non-linearities other than relu, such as tanh, maxout, etc. also, the formula does not cover max-pooling, local normalization layers and other types of layers which influences activations variance. instead of theoretical derivation for all possible layer types, or doing extensive parameters search as in figure [reference], we propose a data-driven weights initialization. we thus extend the orthonormal initialization to an iterative procedure, described in algorithm [reference]. could be implemented in two steps. first, fill the weights with gaussian noise with unit variance. second, decompose them to orthonormal basis with qr or svd-decomposition and replace weights with one of the components. the lsuv process then estimates output variance of each convolution and inner product layer and scales the weight to make variance equal to one. the influence of selected mini-batch size on estimated variance is negligible in wide margins, see appendix. the proposed scheme can be viewed as an orthonormal initialization combined with batch normalization performed only on the first mini-batch. the similarity to batch normalization is the unit variance normalization procedure, while initial ortho-normalization of weights matrices efficiently de-correlates layer activations, which is not done in. experiments show that such normalization is sufficient and computationally highly efficient in comparison with full batch normalization. [b] layer-sequential unit-variance orthogonal initialization. l- convolution or full-connected layer, wl-its weights, bl-its output blob., tolvar-variance tolerance, ti- current trial, tmax- max number of trials. pre-initialize network with orthonormal matrices as in [] layer l|var (bl)- 1.0|\u2265tolvar and (ti< tmax) forward pass with a mini-batch\u2062var (bl)= wl/\u2062var (bl) the lsuv algorithm is summarized in algorithm [reference]. the single parameter influences convergence of the initialization procedure, not the properties of the trained network. its value does not noticeably influence the performance in a broad range of 0.01 to 0.1. because of data variations, it is often not possible to normalize variance with the desired precision. to eliminate the possibility of an infinite loop, we restricted number of trials to. however, in experiments described in paper, the was never reached. the desired variance was achieved in 1-5 iterations. we tested a variant lsuv initialization which was normalizing input activations of the each layer instead of output ones. normalizing the input or output is identical for standard feed-forward nets, but normalizing input is much more complicated for networks with maxout () or for networks like googlenet () which use the output of multiple layers as input. input normalization brought no improvement of results when tested against the lsuv algorithm [reference], lsuv was also tested with pre-initialization of weights with gaussian noise instead of orthonormal matrices. the gaussian initialization led to small, but consistent, decrease in performance. section: experimental validation here we show that very deep and thin nets could be trained in a single stage. network architectures are exactly as proposed by. the architectures are presented in table [reference]. subsection: mnist first, as a\" sanity check\", we performed an experiment on the mnist dataset (). it consists of 60, 000 28x28 grayscale images of handwritten digits 0 to 9. we selected the fitnet-mnist architecture (see table [reference]) of and trained it with the proposed initialization strategy, without data augmentation. recognition results are shown in table [reference], right block. lsuv outperforms orthonormal initialization and both lsuv and orthonormal outperform hints initialization. the error rates of the deeply-supervised nets (dsn,) and maxout networks, the current state-of-art, are provided for reference. since the widely cited dsn error rate of 0.39%, the state-of-the-art (until recently) was obtained after replacing the softmax classifier with svm, we do the same and also observe improved results (line fitnet-lsuv-svm in table [reference]). subsection: cifar-10/ 100 we validated the proposed initialization lsuv strategy on the cifar-10/ 100 () dataset. it contains 60, 000 32x32 rgb images, which are divided into 10 and 100 classes, respectively. the fitnets are trained with the stochastic gradient descent with momentum set to 0.9, the initial learning rate set to 0.01 and reduced by a factor of 10 after the 100th, 150th and 200th epoch, finishing at 230th epoch. and trained their networks for 500 epochs. of course, training time is a trade-off dependent on the desired accuracy; one could train a slightly less accurate network much faster. like in the mnist experiment, lsuv and orthonormal initialized nets outperformed hints-trained fitnets, leading to the new state-of-art when using commonly used augmentation- mirroring and random shifts. the gain on the fine-grained cifar-100 is much larger than on cifar-10. also, note that fitnets with lsuv initialization outperform even much larger networks like large-all-cnn and fractional max-pooling trained with affine and color dataset augmentation on cifar-100. the results of lsuv are virtually identical to the orthonormal initialization. section: analysis of empirical results subsection: initialization strategies and non-linearities for the fitnet-1 architecture, we have not experienced any difficulties training the network with any of the activation functions (relu, maxout, tanh), optimizers (sgd, rmsprop) or initialization (xavier, msra, ortho, lsuv), unlike the uniform initialization used in. the most probable cause is that cnns tolerate a wide variety of mediocre initialization, only the learning time increases. the differences in the final accuracy between the different initialization methods for the fitnet-1 architecture is rather small and are therefore not presented here. the fitnet-4 architecture is much more difficult to optimize and thus we focus on it in the experiments presented in this section. we have explored the initializations with different activation functions in very deep networks. more specifically, relu, hyperbolic tangent, sigmoid, maxout and the vlrelu- very leaky relu ()- a variant of leaky relu (, with a large value of the negative slope 0.333, instead of the originally proposed 0.01) which is popular in kaggle competitions,). testing was performed on cifar-10 and results are in table [reference] and figure [reference]. performance of orthonormal-based methods is superior to the scaled gaussian-noise approaches for all tested types of activation functions, except tanh. proposed lsuv strategy outperforms orthonormal initialization by smaller margin, but still consistently (see table [reference]). all the methods failed to train sigmoid-based very deep network. figure [reference] shows that lsuv method not only leads to better generalization error, but also converges faster for all tested activation functions, except tanh. we have also tested how the different initializations work\" out-of-the-box\" with the residual net training; a residual net won the ilsvrc-2015 challenge. the original paper proposed different implementations of residual learning. we adopted the simplest one, showed in table [reference], fitresnet-4. the output of each even convolutional layer is summed with the output of the previous non-linearity layer and then fed into the next non-linearity. results are shown in table [reference]. lsuv is the only initialization algorithm which leads nets to convergence with all tested non-linearities without any additional tuning, except, again, sigmoid. it is worth nothing that the residual training improves results for relu and maxout, but does not help tanh-based network. subsection: comparison to batch normalization (bn) lsuv procedure could be viewed as batch normalization of layer output done only before the start of training. therefore, it is natural to compare lsuv against a batch-normalized network, initialized with the standard method. subsubsection: where to put bn- before or after non-linearity? it is not clear from the paper where to put the batch-normalization layer- before input of each layer as stated in section 3.1, or before non-linearity, as stated in section 3.2, so we have conducted an experiment with fitnet4 on cifar-10 to clarify this. results are shown in table [reference]. exact numbers vary from run to run, but in the most cases, batch normalization put after non-linearity performs better. in the next experiment we compare bn-fitnet4, initialized with xavier and lsuv-initialized fitnet4. batch-normalization reduces training time in terms of needed number of iterations, but each iteration becomes slower because of extra computations. the accuracy versus wall-clock-time graphs are shown in figure [reference]. lsuv-initialized network is as good as batch-normalized one. however, we are not claiming that batch normalization can always be replaced by proper initialization, especially in large datasets like imagenet. subsection: imagenet training we trained caffenet () and googlenet () on the imagenet-1000 dataset () with the original initialization and lsuv. caffenet is a variant of alexnet with the nearly identical performance, where the order of pooling and normalization layers is switched to reduce the memory footprint. lsuv initialization reduces the starting flat-loss time from 0.5 epochs to 0.05 for caffenet, and starts to converge faster, but it is overtaken by a standard caffenet at the 30-th epoch (see figure [reference]) and its final precision is 1.3% lower. we have no explanation for this empirical phenomenon. on the contrary, the lsuv-initialized googlenet learns faster than hen then original one and shows better test accuracy all the time- see figure [reference]. the final accuracy is 0.680 vs. 0.672 respectively. subsection: timings a significant part of lsuv initialization is svd-decomposition of the weight matrices, e.g. for the fc6 layer of caffenet, an svd of a 9216x4096 matrix is required. the computational overhead on top of generating almost instantly the scaled random gaussian samples is shown in table [reference]. in the slowest case- caffenet- lsuv initialization takes 3.5 minutes, which is negligible in comparison the training time. section: conclusions lsuv, layer sequential uniform variance, a simple strategy for weight initialization for deep net learning, is proposed. we have showed that the lsuv initialization, described fully in six lines of pseudocode, is as good as complex learning schemes which need, for instance, auxiliary nets. the lsuv initialization allows learning of very deep nets via standard sgd, is fast, and leads to (near) state-of-the-art results on mnist, cifar, imagenet datasets, outperforming the sophisticated systems designed specifically for very deep nets such as fitnets () and highway (). the proposed initialization works well with different activation functions. our experiments confirm the finding of that very thin, thus fast and low in parameters, but deep networks obtain comparable or even better performance than wider, but shallower ones. subsubsection: acknowledgments the authors were supported by the czech science foundation project gacr p103/ 12/ g084 and ctu student grant sgs15/ 155/ ohk3/ 2t/ 13. bibliography: references appendix: technical details subsection: influence of mini-batch size to lsuv initialization we have selected tanh activation as one, where lsuv initialization shows the worst performance and tested the influence of mini-batch size to training process. note, that training mini-batch is the same for all initializations, the only difference is mini-batch used for variance estimation. one can see from table [reference] that there is no difference between small or large mini-batch, except extreme cases, where only two sample are used. subsection: lsuv weight standard deviations in different networks tables [reference] and [reference] show the standard deviations of the filter weights, found by the lsuv procedure and by other initialization schemes. subsection: gradients to check how the activation variance normalization influences the variance of the gradient, we measure the average variance of the gradient at all layers after 10 mini-batches. the variance is close to for all convolutional layers. it is much more stable than for the reference methods, except msra; see table [reference].",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar-10",
                        939
                    ],
                    [
                        "cifar",
                        16457
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "fitnets",
                        764
                    ],
                    [
                        "lsuv algorithm",
                        8467
                    ],
                    [
                        "fitresnet-4",
                        12966
                    ],
                    [
                        "fitnet4",
                        14009
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "image classification",
                        1165
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "100",
                        949
                    ],
                    [
                        "fine-grained cifar-100",
                        10602
                    ],
                    [
                        "cifar-100",
                        10844
                    ],
                    [
                        "cifar",
                        16457
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "fitnets",
                        764
                    ],
                    [
                        "lsuv algorithm",
                        8467
                    ],
                    [
                        "fitresnet-4",
                        12966
                    ],
                    [
                        "fitnet4",
                        14009
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "image classification",
                        1165
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "mnist",
                        932
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "fitnets",
                        764
                    ],
                    [
                        "lsuv algorithm",
                        8467
                    ],
                    [
                        "fitnet-1 architecture",
                        11039
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "error rates",
                        9406
                    ],
                    [
                        "dsn error rate",
                        9553
                    ],
                    [
                        "generalization error",
                        12580
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "image classification",
                        1165
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "0496691c6d54fd746c111d1004349acf7654145e-2",
    "doctext": "calibrating energy-based generative adver-sarial networks section: abstract in this paper we propose equipping generative adversarial networks with the ability to produce direct energy estimates for samples. specifically, we develop a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimum. we derive the analytic form of the induced solution, and analyze its properties. in order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. empirically, the experiment results closely match our theoretical analysis, verifying that the discriminator is able to recover the energy of data distribution. section: introduction generative adversarial networks (gans) [reference] represent an important milestone on the path towards more effective generative models. gans cast generative model training as a minimax game between a generative network (generator), which maps a random vector into the data space, and a discriminative network (discriminator), whose objective is to distinguish generated samples from real samples. multiple researchers [reference]; [reference]; [reference] have shown that the adversarial interaction with the discriminator can result in a generator that produces compelling samples. the empirical successes of the gan framework were also supported by the theoretical analysis of [reference] showed that, under certain conditions, the distribution produced by the generator converges to the true data distribution, while the discriminator converges to a degenerate uniform solution. while gans have excelled as compelling sample generators, their use as general purpose probabilistic generative models has been limited by the difficulty in using them to provide density estimates or even unnormalized energy values for sample evaluation. it is tempting to consider the gan discriminator as a candidate for providing this sort of scoring function. conceptually, it is a trainable sample evaluation mechanism that-owing to gan training paradigm-could be closely calibrated to the distribution modeled by the generator. if the discriminator could retain fine-grained information of the relative quality of samples, measured for instance by probability density or unnormalized energy, it could be used as an evaluation metric. such data-driven evaluators would be highly desirable for problems where it is difficult to define evaluation criteria that correlate well with human judgment. indeed, the real-valued discriminator of the recently introduced energy-based gans [reference] might seem like an ideal candidate energy function. unfortunately, as we will show, the degenerate fate of the gan discriminator at the optimum equally afflicts the energy-based gan of [reference] in this paper we consider the questions: (i) does there exists an adversarial framework that induces a non-degenerate discriminator, and (ii) if so, what form will the resulting discriminator take? we introduce a novel adversarial learning formulation, which leads to a non-degenerate discriminator while ensuring the generator distribution matches the data distribution at the global optimum. we derive a general analytic form of the optimal discriminator, and discuss its properties and their following the original work on gans [reference], our analysis focuses on the nonparametric case, where all models are assumed to have infinite capacities. while many of the nonparametric intuitions can directly transfer to the parametric case, we will point out cases where this transfer fails. we assume a finite data space throughout the analysis, to avoid technical machinery out of the scope of this paper. our results, however, can be extended to continuous data spaces, and our experiments are indeed performed on continuous data. let x be the data space under consideration, and p= {p| p (x)\u2265 0,\u2200x\u2208 x, x\u2208x p (x)= 1} be the set of all proper distributions defined on x. then, p data\u2208 p: x\u2192 r and p gen\u2208 p: x\u2192 r will denote the true data distribution and the generator distribution. e x\u223cp f (x) denotes the expectation of the quantity f (x) w.r.t. x drawn from p. finally, the term\" discriminator\" will refer to any structure that provides training signals to the generator based on some measure of difference between the generator distribution and the real data distribution, which which includes but is not limited to f-divergence. section: proposed formulation in order to understand the motivation of the proposed approach, it is helpful to analyze the optimization dynamics near convergence in gans first. when the generator distribution matches the data distribution, the training signal (gradient) w.r.t. the discriminator vanishes. at this point, assume the discriminator still retains density information, and views some samples as more real and others as less. this discriminator will produce a training signal (gradient) w.r.t. the generator, pushing the generator to generate samples that appear more real to the discriminator. critically, this training signal is the sole driver of the generator's training. hence, the generator distribution will diverge from the data distribution. in other words, as long as the discriminator retains relative density information, the generator distribution can not stably match the data distribution. thus, in order to keep the generator stationary as the data distribution, the discriminator must assign flat (exactly the same) density to all samples at the optimal. from the analysis above, the fundamental difficulty is that the generator only receives a single training signal (gradient) from the discriminator, which it has to follow. to keep the generator stationary, this single training signal (gradient) must vanish, which requires a degenerate discriminator. in this work, we propose to tackle this single training signal constraint directly. specifically, we introduce a novel adversarial learning formulation which incorporates an additional training signal to the generator, such that this additional signal can\u2022 balance (cancel out) the discriminator signal at the optimum, so that the generator can stay stationary even if the discriminator assigns non-flat density to samples\u2022 cooperate with the discriminator signal to make sure the generator converges to the data distribution, and the discriminator retains the correct relative density information the proposed formulation can be written as the following minimax training objective, where c (x): x\u2192 r is the discriminator that assigns each data point an unbounded scalar cost, and k (p gen): p\u2192 r is some (functionally) differentiable, convex function of p gen. compared to the original gan, despite the similar minimax surface form, the proposed fomulation has two crucial distinctions. firstly, while the gan discriminator tries to distinguish\" fake\" samples from real ones using binary classification, the proposed discriminator achieves that by assigning lower cost to real samples and higher cost to\" fake\" one. this distinction can be seen from the first two terms of eqn. (1), where the discriminator c (x) is trained to widen the expected cost gap between\" fake\" and real samples, while the generator is adversarially trained to minimize it. in addition to the different adversarial mechanism, a calibrating term k (p gen) is introduced to provide a countervailing source of training signal for p gen as we motivated above. for now, the form of k (p gen) has not been specified. but as we will see later, its choice will directly decide the form of the optimal discriminator c* (x). with the specific optimization objective, we next provide theoretical characterization of both the generator and the discriminator at the global optimum. is the lagrange dual function of the following optimization problem where c (x),\u2200x appears in l (p gen, c) as the dual variables introduced for the equality constraints. this duality relationship has been observed previously in [reference]) under the adversarial imitation learning setting. however, in their case, the focus was fully on the generator side (induced policy), and no analysis was provided for the discriminator (reward function). in order to characterize c*, we first expand the set constraint on p gen into explicit equality and inequality constraints: min notice that k (p gen) is a convex function of p gen (x) by definition, and both the equality and inequality constraints are affine functions of p gen (x). thus, problem (2) is a convex optimization problem. what's more, since (i) dom k is open, and (ii) there exists a feasible solution p gen= p data to (3), by the refined slater's condition [reference], page 226), we can further verify that strong duality holds for (3). with strong duality, a typical approach to characterizing the optimal solution is to apply the karush-kuhn-tucker (kkt) conditions, which gives rise to this theorem: proposition 3.1. by the kkt conditions of the convex problem (3), at the global optimum, the optimal generator distribution p* gen matches the true data distribution p data, and the optimal discriminator c* (x) has the following form: is an under-determined real number independent of x, u x\u2208 r+, is an under-determined non-negative real number. the detailed proof of proposition 3.1 is provided in appendix a.1. from (4), we can see the exact form of the optimal discriminator depends on the term k (p gen), or more specifically its gradient. but, before we instantiate k (p gen) with specific choices and show the corresponding forms of c* (x), we first discuss some general properties of c* (x) that do not depend on the choice of k. weak support discriminator. as part of the optimal discriminator function, the term\u00b5* (x) plays the role of support discriminator. that is, it tries to distinguish the support of the data distribution, i.e. supp (p data)= {x\u2208 x| p data (x)> 0}, from its complement set with zeroprobability, i.e. supp (p data)= {x\u2208 x| p data (x)= 0}. specifically, for any x\u2208 supp (p data) and x\u2208 supp (p data), it is guaranteed that\u00b5* (x)\u2264\u00b5* (x). however, because\u00b5* (\u00b7) is underdetermined, there is nothing preventing the inequality from degenerating into an equality. therefore, we name it the weak support discriminator. but, in all cases,\u00b5* (\u00b7) assigns zero cost to all data points within the support. as a result, it does not possess any fine-grained density information inside of the data support. it is worth pointing out that, in the parametric case, because of the smoothness and the generalization properties of the parametric model, the learned discriminator may generalize beyond the data support. global bias. in (4), the term\u03bb* is a scalar value shared for all x. as a result, it does not affect the relative cost among data points, and only serves as a global bias for the discriminator function. having discussed general properties, we now consider some specific cases of the convex function k, and analyze the resulting optimal discriminator c* (x) in detail. 1. first, let us consider the case where k is the negative entropy of the generator distribution, i.e. k (p gen)=\u2212h (p gen). taking the derivative of the negative entropy w.r.t. p gen (x), we have where\u00b5* (x) and\u03bb* have the same definitions as in (4). up to a constant, this form of c* ent (x) is exactly the energy function of the data distribution p data (x). this elegant result has deep connections to several existing formulations, which include max-entropy imitation learning [reference] and the directed-generator-trained energybased model [reference]. the core difference is that these previous formulations are originally derived from maximum-likelihood estimation, and thus the minimax optimization is only implicit. in contrast, with an explicit minimax formulation we can develop a better understanding of the induced solution. for example, the global bias\u03bb* suggests that there exists more than one stable equilibrium the optimal discriminator can actually reach. further,\u00b5* (x) can be understood as a support discriminator that poses extra cost on generator samples which fall in zero-probability regions of data space., which can be understood as posing 2 regularization on p gen, we have with\u00b5* (x),\u03bb* similarly defined as in (4). surprisingly, the result suggests that the optimal discriminator c* 2 (x) directly recovers the negative probability\u2212p data (x), shifted by a constant. thus, similar to the entropy solution (5), it fully retains the relative density information of data points within the support. however, because of the under-determined term\u00b5* (x), we can not recover the distribution density p data exactly from either c* 2 or c* ent if the data support is finite. whether this ambiguity can be resolved is beyond the scope of this paper, but poses an interesting research problem. 3. finally, let's consider consider a degenerate case, where k (p gen) is a constant. that is, we do nt provide any additional training signal for pgen at all. with k (p gen)= const, we simply have whose discriminative power is fully controlled by the weak support discriminator\u00b5* (x). thus, it follows that c* cst (x) wo n't be able to discriminate data points within the support of p data, and its power to distinguish data from supp (p data) and supp (p data) is weak. this closely matches the intuitive argument in the beginning of this section. note that when k (p gen) is a constant, the objective function (1) simplifies to: which is very similar to the ebgan objective [reference] (2) and (4)). as we show in appendix a.2, compared to the objective in (8), the ebgan objective puts extra constraints on the allowed discriminator function. in spite of that, the ebgan objective suffers from the single-training-signal problem and does not guarantee that the discriminator will recover the real energy function (see appendix a.2 for detailed analysis). as we finish the theoretical analysis of the proposed formulation, we want to point out that simply adding the same term k (p gen) to the original gan formulation will not lead to both a generator that matches the data distribution, and a discriminator that retains the density information (see appendix a.3 for detailed analysis). section: parametric instantiation with entropy approximation while the discussion in previous sections focused on the non-parametric case, in practice we are limited to a finite amount of data, and the actual problem involves high dimensional continuous spaces. thus, we resort to parametric representations for both the generator and the discriminator. in order to train the generator using standard back-propagation, we do not parametrize the generator distribution directly. instead, we parametrize a directed generator network that transforms random noise z\u223c p z (z) to samples from a continuous data space r n. consequently, we do n't have analytical access to the generator distribution, which is defined implicitly by the generator network's noise\u2192data mapping. however, the regularization term k (p gen) in the training objective (1) requires the generator distribution. faced with this problem, we focus on the max-entropy formulation, and exploit two different approximations of the regularization term k (p gen)=\u2212h (p gen). section: nearest-neighbor entropy gradient approximation the first proposed solution is built upon an intuitive interpretation of the entropy gradient. firstly, since we construct p gen by applying a deterministic, differentiable transform g\u03b8 to samples z from a fixed distribution p z, we can write the gradient of h (p gen) with respect to the generator parameters\u03b8 as follows: where the first equality relies on the\" reparametrization trick\". equation 9 implies that, if we can compute the gradient of the generator log-density log p gen (x) w.r.t. any x= g\u03b8 (z), then we can directly construct the monte-carlo estimation of the entropy gradient\u2207\u03b8 h (p gen) using samples from the generator. intuitively, for any generated data x= g\u03b8 (z), the term essentially describes the direction of local change in the sample space that will increase the log-density. motivated by this intuition, we propose to form a local gaussian approximation p i gen of p gen around each point x i in a batch of samples {x 1,..., x n} from the generator, and then compute the gradient\u2202 log pgen (xi)\u2202xi based on the gaussian approximation. specifically, each local gaussian approximation p i gen is formed by finding the k nearest neighbors of x i in the batch {x 1,..., x n}, and then placing an isotropic gaussian distribution at their mean (i.e. maximimum likelihood). based on the isotropic gaussian approximation, the resulting gradient has the following form x is the mean of the gaussian (10) finally, note the scale of this gradient approximation may not be reliable. to fix this problem, we normalize the approximated gradient into unit norm, and use a single hyper-parameter to model the scale for all x, leading to the following entropy gradient approximation where\u03b1 is the hyper-parameter and\u00b5 i is defined as in equation (10). an obvious weakness of this approximation is that it relies on euclidean distance to find the k nearest neighbors. however, euclidean distance is usually not the proper metric to use when the effective dimension is very high. as the problem is highly challenging, we leave it for future work. section: variational lower bound on the entropy another approach we consider relies on defining and maximizing a variational lower bound on the entropy h (p gen (x)) of the generator distribution. we can define the joint distribution over observed data and the noise variables as is a fixed prior. using the joint, we can also define the marginal p gen (x) and the posterior p gen (z| x). we can also write the mutual information between the observed data and noise variables as: where h (p gen (.|.)) denotes the conditional entropy. by reorganizing terms in this definition, we can write the entropy h (p gen (x)) as: we can think of p gen (x| z) as a peaked gaussian with a fixed, diagonal covariance, and hence its conditional entropy is constant and can be dropped. furthermore, h (p gen (z)) is also assumed to be fixed a priori. hence, we can maximize h (p gen (x)) by minimizing the conditional entropy: optimizing this term is still problematic, because (i) we do not have access to the posterior p gen (z| x), and (ii) we can not sample from it. therefore, we instead minimize a variational upper bound defined by an approximate posterior q gen (z| x): we can also rewrite the variational upper bound as: which can be optimized efficiently with standard back-propagation and monte carlo integration of the relevant expectations based on independent samples drawn from the joint p gen (x, z). by minimizing this upper bound on the conditional entropy h (p gen (z| x)), we are effectively maximizing a variational lower bound on the entropy h (p gen (x)). section: experiments in this section, we verify our theoretical results empirically on several synthetic and real datasets. in particular, we evaluate whether the discriminator obtained from the entropy-regularized adversarial training can capture the density information (in the form of energy), while making sure the generator distribution matches the data distribution. for convenience, we refer to the obtained models as egan-ent. our experimental setting follows closely recommendations from [reference], except in sec. 5.1 where we use fully-connected models (see appendix b.1 for details). gaussians arranged as two spirals (100 components each spiral), and (iii) mixture of 2 gaussians with highly biased mixture weights, p (c 1)= 0.9, p (c 2)= 0.1. we visualize the ground-truth energy of these distributions along with 100 k training samples in figure 1. since the data lies in 2-dimensional space, we can easily visualize both the learned generator (by drawing samples) and the discriminator for direct comparison and evaluation. we evaluate here our egan-ent model using both approximations: the nearest-neighbor based approximation (egan-ent-nn) and the variational-inference based approximation (egan-ent-vi), and compare them with two baselines: the original gan and the energy based gan with no regularization (egan-const). experiment results are summarized in figure 2 for baseline models, and figure 3 for the proposed models. as we can see, all four models can generate perfect samples. however, for the discriminator, both gan and egan-const lead to degenerate solution, assigning flat energy inside the empirical data support. in comparison, egan-ent-vi and egan-ent-nn clearly capture the density information, though to different degrees. specifically, on the equally weighted gaussian mixture and the two-spiral mixture datasets, egan-ent-nn tends to give more accurate and fine-grained solutions compared to egan-ent-vi. however, on the biased weighted gaussian mixture dataset, egan-ent-vi actually fails to captures the correct mixture weights of the two modes, incorrectly assigning lower energy to the mode with lower probability (smaller weight). in contrast, eganent-nn perfectly captures the bias in mixture weight, and obtains a contour very close to the ground truth. to better quantify these differences, we present detailed comparison based on kl divergence in appendix b.2. what's more, the performance difference between egan-ent-vi and egan-ent-nn on biased gaussian mixture reveals the limitations of the variational inference based approximation, i.e. providing inaccurate gradients. due to space consideratiosn, we refer interested readers to the appendix b.3 for a detailed discussion. section: ranking nist digits in this experiment, we verify that the results in synthetic datasets can translate into data with higher dimensions. while visualizing the learned energy function is not feasible in high-dimensional space, we can verify whether the learned energy function learns relative densities by inspecting the ranking of samples according to their assigned energies. we train on 28\u00d7 28 images of a single handwritten figure 2: learned energies and samples from baseline models whose discriminator can not retain density information at the optimal. in the sample plots, blue dots indicate generated samples, and red dots indicate real ones. section: (a) entropy regularized energy gan with variational inference approximation (egan-ent-vi) (b) entropy regularized energy gan with nearest neighbor approximation (egan-ent-nn) figure 3: learned energies and samples from proposed models whose discriminator can retain density information at the optimal. blue dots are generated samples, and red dots are real ones. digit from the nist dataset. [reference] we compare the ability of egan-ent-nn with both egan-const and gan on ranking a set of 1, 000 images, half of which are generated samples and the rest are real test images. figures 4 and 5 show the top-100 and bottom-100 ranked images respectively for each model, after training them on digit 1. we also show in figure 7 the mean of all training samples, so we can get a sense of what is the most common style (highest density) of digit 1 in nist. we can notice that all of the top-ranked images by egan-ent-nn look similar to the mean sample. in addition, the lowest-ranked images are clearly different from the mean image, with either high (clockwise or counter-clockwise) rotation degrees from the mean, or an extreme thickness level. we do not see such clear distinction in other models. we provide in the appendix b.4 the ranking of the full set of images. section: sample quality on natural image datasets in this last set of experiments, we evaluate the visual quality of samples generated by our model in two datasets of natural images, namely cifar-10 and celeba. we employ here the variationalbased approximation for entropy regularization, which can scale well to high-dimensional data. figure 6 shows samples generated by egan-ent-vi. we can see that despite the noisy gradients provided by the variational approximation, our model is able to generate high-quality samples. we futher validate the quality of our model's samples on cifar-10 using the inception score proposed by [reference] 3. table 1 shows the scores of our egan-ent-vi, the best gan model from [reference] which uses only unlabeled data, and an egan-const model which has the same architecture as our model. we notice that even without employing suggested techniques in [reference], energy-based models perform quite similarly to the gan model. furthermore, the fact that our model scores higher than egan-const highlights the importance of entropy regularization in obtaining good quality samples. section: conclusion in this paper we have addressed a fundamental limitation in adversarial learning approaches, which is their inability of providing sensible energy estimates for samples. we proposed a novel adversarial learning formulation which results in a discriminator function that recovers the true data energy. we provided a rigorous characterization of the learned discriminator in the non-parametric setting, and proposed two methods for instantiating it in the typical parametric setting. our experimental results verify our theoretical analysis about the discriminator properties, and show that we can also obtain samples of state-of-the-art quality. figure 6: samples generated from our model. section: model our model improved gan\u2020 egan-const score\u00b1 std. 7.07\u00b1.10 6.86\u00b1.06 6.7447\u00b1 0.09 table 1: inception scores on cifar-10.\u2020 as reported in salimans et al. (2016) without using labeled data. (17) where c (x)\u2208 r,\u2200x,\u00b5 (x)\u2208 r+,\u2200x, and\u03bb\u2208 r are the dual variables. the kkt conditions for the optimal primal and dual variables are as follows rearranging the conditions above, we get p* gen (x)= p data (x),\u2200x\u2208 x as well as equation (4), which concludes the proof. section: a.2 optimal conditions of ebgan in [reference], the training objectives of the generator and the discriminator can not be written as a single minimax optimization problem since the margin structure is only applied to the objective of the discriminator. in addition, the discriminator is designed to produce the mean squared reconstruction error of an auto-encoder structure. this restricted the range of the discriminator output to be non-negative, which is equivalent to posing a set constraint on the discriminator under the non-parametric setting. thus, to characterize the optimal generator and discriminator, we adapt the same analyzing logic used in the proof sketch of the original gan [reference]. specifically, given a specific generator distribution p gen, the optimal discriminator function given the generator distribution c* (x; p gen) can be derived by examining the objective of the discriminator. then, the conditional optimal discriminator function is substituted into the training objective of p gen, simplifying the\" adversarial\" training as a minimizing problem only w.r.t. p gen, which can be well analyzed. firstly, given any generator distribution p gen, the ebgan training objective for the discriminator can be written as the following form where c= {c: c (x)\u2265 0,\u2200x\u2208 x} is the set of allowed non-negative discriminator functions. note this set constraint comes from the fact the mean squared reconstruction error as discussed above. since the problem (19) is independent w.r.t. each x, the optimal solution can be easily derived as where\u03b1 x\u2208 [0, m] is an under-determined number, a\u03b2 x\u2208 [0,\u221e) is another under-determined nonnegative real number, and the subscripts in m,\u03b1 x,\u03b2 x reflect that fact that these under-determined values can be distinct for different x. this way, the overall training objective can be cast into a minimization problem w.r.t. p gen, where the second term of the first line is implicitly defined as the problem is an adversarial game between p gen and c. proposition a.1. the global optimal of the ebgan training objective is achieved if and only if p gen= p data. at that point, c* (x) is fully under-determined. proof. the proof is established by showing contradiction. firstly, assume the optimal p* gen= p data. thus, there must exist a non-equal set x== {x| p data (x)= p* gen (x)}, which can be further splitted into two subsets, the greater-than set x>= {x| p* gen (x)> p data (x)}, and the less-than set x<= {x| p* gen (x)< p data (x)}. similarly, we define the equal set, substituting the results from equation (20) into (21), the l (p gen)* can be written as however, when p gen= p data, we have which contradicts the optimal (miminum) assumption of p* gen. hence, the contradiction concludes that at the global optimal, p* gen= p data. by equation (20), it directly follows that c* (x; p* gen)=\u03b1 x, which completes the proof. section: a.3 analysis of adding additional training signal to gan formulation to show that simply adding the same training signal to gan will not lead to the same result, it is more convenient to directly work with the formulation of f-gan [reference]) family, which include the original gan formulation as a special case. specifically, the general f-gan formulation takes the following form where the f (\u00b7) denotes the convex conjugate [reference] of the f-divergence function. the optimal condition of the discriminator can be found by taking the variation w.r.t. c, which gives the optimal discriminator where f (\u00b7) is the first-order derivative of f (\u00b7). note that, even when we add an extra term l (p gen) to equation (24), since the term k (p gen) is a constant w.r.t. the discriminator, it does not change the result given by equation (25) about the optimal discriminator. as a consequence, for the optimal discriminator to retain the density information, it effectively means p gen= p data. hence, there will be a contradiction if both c* (x) retains the density information, and the generator matches the data distribution. intuitively, this problem roots in the fact that f-divergence is quite\" rigid\" in the sense that given the p gen (x) it only allows one fixed point for the discriminator. in comparison, the divergence used in our proposed formulation, which is the expected cost gap, is much more flexible. by the expected cost gap itself, i.e. without the k (p gen) term, the optimal discriminator is actually under-determined. section: b supplementary materials for section 5 b.1 experiment setting here, we specify the neural architectures used for experiements presented in section 5. firstly, for the egan-ent-vi model, we parameterize the approximate posterior distribution q gen (z| x) with a diagonal gaussian distribution, whose mean and covariance matrix are the output of a trainable inference network, i.e. where f infer denotes the inference network, and i is the identity matrix. note that the inference network only appears in the egan-ent-vi model. for experiments with the synthetic datasets, the following fully-connected feed forward neural networks are employed where fc and bn denote fully-connected layer and batch normalization layer respectively. note that since the input noise to the generator has dimension 4, the inference net output has dimension 4* 2, where the first 4 elements correspond the inferred mean, and the last 4 elements correspond to the inferred diagonal covariance matrix in log scale. for the handwritten digit experiment, we closely follow the dcgan [reference] architecture with the following configuration here, lrec is the leaky rectified non-linearity recommended by [reference]. in addition, cv (128, 256, 4c2s) denotes a convolutional layer with 128 input channels, 256 output channels, and kernel size 4 with stride 2. similarly, dc (256, 128, 4c2s) denotes a corresponding transposed convolutional operation. compared to the original dcgan architecture, the discriminator under our formulation does not have the last sigmoid layer which squashes a scalar value into a probability in [reference][reference]. for celeba experiment with 64\u00d7 64 color images, we use the following architecture given the chosen architectures, we follow [reference] and use adam as the optimization algorithm. for more detailed hyper-parameters, please refer to the code. in order to quantify the quality of recovered distributions, we compute the pairwise kl divergence of the following four distributions: section: b.2 quantitative comparison of different models\u2022 the real data distribution with analytic form, denoted as p data\u2022 the empirical data distribution approximated from the 100 k training data, denoted as p emp\u2022 the generator distribution approximated from 100 k generated data, denoted as p gen\u2022 the discriminator distribution re-normalized from the learned energy, denoted as p disc since the synthetic datasets are two dimensional, we approximate both the empirical data distribution and the generator distribution using the simple histogram estimation. specifically, we divide the canvas into a 100-by-100 grid, and assign each sample into its nearest grid cell based on euclidean distance. then, we normalize the number of samples in each cell into a proper distribution. when recovering the discriminator distribution from the learned energy, we assume that\u00b5* (x)= 0 (i.e. infinite data support), and discretize the distribution into the same grid cells,\u2200x\u2208 grid based on these approximation, table 2 summarizes the results. for all measures related to the discriminator distribution, egan-ent-vi and egan-ent-nn significantly outperform the other two baseline models, which matches our visual assessment in figure 2 and 3. meanwhile, the generator distributions learned from our proposed framework also achieve relatively lower divergence to both the empirical data distribution and the true data distribution. section: b.3 comparison of the entropy (gradient) approximation methods in order to understand the performance difference between egan-ent-vi and egan-ent-nn, we analyze the quality of the entropy gradient approximation during training. to do that, we visualize some detailed training information in figure 8. fig. (1, 2): frequency map of generated samples in the current batch. fig. (1, 3): frequency map of real samples in the current batch. fig -(1, 4): frequency difference between real and generated samples. fig. (2, 1) comparison between more generated from current model and real sample. fig. (2, 2): the discriminator gradient w.r.t. each training sample. fig. (2, 3): the entropy gradient w.r.t. each training samples. fig. (2, 4): all gradient (discriminator+ entropy) w.r.t. each training sample. as we can see in figure 8a, the viarational entropy gradient approximation w.r.t. samples is not accurate:\u2022 it is inaccurate in terms of gradient direction. ideally, the direction of the entropy gradient should be pointing from the center of its closest mode towards the surroundings, with the direction orthogonal to the implicit contour in fig. (1, 2). however, the direction of gradients in the fig. (2, 3) does not match this.\u2022 it is inaccurate in magnitude. as we can see, the entropy approximation gradient (fig. (2, 3)) has much larger norm than the discriminator gradient (fig. (2, 2)). as a result, the total gradient (fig. (2, 4)) is fully dominated by the entropy approximation gradient. thus, it usually takes much longer for the generator to learn to generate rare samples, and the training also proceeds much slower compared to the nearest neighbor based approximation. in comparison, the nearest neighbor based gradient approximation is much more accurate as shown in 8b. as a result, it leads to more accurate energy contour, as well as faster training. what's more, from figure 8b fig. (2, 4), we can see the entropy gradient does have the cancel-out effect on the discriminator gradient, which again matches our theory. b.4 ranking nist digits figure 9 shows the ranking of all 1000 generated and real images (from the test set) for three models: egan-ent-nn, egan-const, and gan. we can clearly notice that in egan-ent-nn the topranked digits look very similar to the mean digit. from the upper-left corner to the lower-right corner, the transition trend is: the rotation degree increases, and the digits become increasingly thick or thin compared to the mean. in addition, samples in the last few rows do diverge away from the mean image: either highly diagonal to the right or left, or have different shape: very thin or thick, or typewriter script. other models are not able to achieve a similar clear distinction for high versus low probability images. finally, we consistently observe the same trend in modeling other digits, which are not shown in this paper due to space constraint. section: b.5 classifier performance as a proxy measure as mentioned in section 5, evaluating the proposed formulation quantitatively on high-dimensional data is extremely challenging. here, in order to provide more quantitative intuitions on the learned discriminator at convergence, we adopt a proxy measure. specifically, we take the last-layer activation of the converged discriminator network as fixed pretrained feature, and build a linear classifier upon it. hypothetically, if the discriminator does not degenerate, the extracted last-layer feature should maintain more information about the data points, especially compared to features from degenerated discriminators. following this idea, we first train egan-ent-nn, egan-const, and gan on the mnist till convergence, and then extract the last-layer activation from their discriminator networks as fixed feature input. based on fixed feature, a randomly initialized linear classifier is trained to do classification on mnist. based on 10 runs (with different initialization) of each of the three models, the test classification performance is summarized in table 3. for comparison purpose, we also include a baseline where the input features are extracted from a discriminator network with random weights. table 3: test performance of linear classifiers based on last-layer discriminator features. based on the proxy measure, egan-ent-nn seems to maintain more information of data, which suggests that the discriminator from our proposed formulation is more informative. despite the positive result, it is important to point out that maintaining information about categories does not necessarily mean maintaining information about the energy (density). thus, this proxy measure should be understood cautiously. section: section: acknowledgements we would like to thank the developers of theano (theano development team, 2016) for developing such a powerful tool for scientific computing. amjad almahairi was supported by funding from maluuba research. section:",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar-10",
                        23912
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "calibrating energy-based generative adver-sarial networks",
                        0
                    ],
                    [
                        "egan-ent-vi",
                        20271
                    ],
                    [
                        "egan-ent-vi model",
                        30631
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "inception score",
                        24322
                    ],
                    [
                        "inception scores",
                        25650
                    ]
                ]
            ],
            "Task": []
        }
    ]
}
{
    "docid": "0680f04750b1e257ffdd161e85382031dc73ea7f-3",
    "doctext": "document: end-to-end answer chunk extraction and ranking for reading comprehension this paper proposes dynamic chunk reader (dcr), an end-to-end neural reading comprehension (rc) model that is able to extract and rank a set of answer candidates from a given document to answer questions. dcr is able to predict answers of variable lengths, whereas previous neural rc models primarily focused on predicting single tokens or entities. dcr encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. experimental results show that dcr achieves state-of-the-art exact match and f1 scores on the squad dataset. section: introduction reading comprehension-based question answering (rcqa) is the task of answering a question with a chunk of text taken from related document (s). a variety of neural models have been proposed recently either for extracting a single entity or a single token as an answer from a given text; or for selecting the correct answer by ranking a small set of human-provided candidates. in both cases, an answer boundary is either easy to determine or already given. different from the above two assumptions for rcqa, in the real-world qa scenario, people may ask questions about both entities (factoid) and non-entities such as explanations and reasons (non-factoid) (see table [reference] for examples). in this regard, rcqa has the potential to complement other qa approaches that leverage structured data (e.g., knowledge bases) for both the above question types. this is because rcqa can exploit the textual evidences to ensure increased answer coverage, which is particularly helpful for non-factoid answers. however, it is also challenging for rcqa to identify answer in arbitrary position in the passage with arbitrary length, especially for non-factoid answers which might be clauses or sentences. as a result, apart from a few exceptions, this research direction has not been fully explored yet. compared to the relatively easier rc task of predicting single tokens/ entities, predicting answers of arbitrary lengths and positions significantly increase the search space complexity: the number of possible candidates to consider is in the order of, where is the number of passage words. in contrast, for previous works in which answers are single tokens/ entities or from candidate lists, the complexity is in or the size of candidate lists (usually 5), respectively. to address the above complexity, rajpurkar et al. rajpurkar2016squad used a two-step chunk-and-rank approach that employs a rule-based algorithm to extract answer candidates from a passage, followed by a ranking approach with hand-crafted features to select the best answer. the rule-based chunking approach suffered from low coverage (70% recall of answer chunks) that can not be improved during training; and candidate ranking performance depends greatly on the quality of the hand-crafted features. more recently, wang and jiang wang2016machine proposed two end-to-end neural network models, one of which chunks a candidate answer by predicting the answer's two boundary indices and the other classifies each passage word into answer/ not-answer. both models improved significantly over the method proposed by rajpurkar et al. rajpurkar2016squad. our proposed model, called dynamic chunk reader (dcr), not only significantly differs from both the above systems in the way that answer candidates are generated and ranked, but also shares merits with both works. first, our model uses deep networks to learn better representations for candidate answer chunks, instead of using fixed feature representations as in. second, it represents answer candidates as chunks, as in, instead of word-level representations, to make the model aware of the subtle differences among candidates (importantly, overlapping candidates). the contributions of this paper are three-fold. (1) we propose a novel neural network model for joint candidate answer chunking and ranking, where the candidate answer chunks are dynamically constructed and ranked in an end-to-end manner. (2) we propose a new question-attention mechanism to enhance passage word representation, which is subsequently used to construct chunk representations. (3) we also propose several simple but effective features to strengthen the attention mechanism, which fundamentally improves candidate ranking, with the by-product of higher exact boundary match accuracy. the experiments on the stanford question answering dataset (squad), which contains a variety of human-generated factoid and non-factoid questions, have shown the effectiveness of above three contributions. our paper is organized as follows. we formally define the rcqa problem first. next, we describe our baseline with a neural network component. we present the end-to-end dynamic chunk reader model next. finally, we analyze our experimental results and discuss the related work. section: problem definition table [reference] shows an example of our rc setting where the goal is to answer a question, factoid (q1) or non-factoid (q2 and q3), based on a supporting passage, by selecting a continuous sequence of text as answer.,, and are all word sequences, where each word is drawn from a vocabulary,. the-th instance in the training set is a triple in the form of, where,, and (). owing to the disagreement among annotators, there could be more than one correct answer for the same question; and the-th answer to is denoted by. an answer candidate for the-th training example is defined as, a sub-sequence in, that spans from position to (). the ground truth answer could be included in the set of all candidates, where is the constraint put on the candidate chunk for, such as,\" can have at most 10 tokens\", or\" must have a pre-defined pos pattern\". to evaluate a system's performance, its top answer to a question is matched against the corresponding gold standard answer (s). paragraph: remark: categories of rc tasks other simpler variants of the aforementioned rc task were explored in the past. for example, quiz-style datasets (e.g., mctest, movieqa) have multiple-choice questions with answer options. cloze-style datesets, usually automatically generated, have factoid\" question\"s created by replacing the answer in a sentence from the text with blank. for the answer selection task this paper focuses on, several datasets exist, e.g. trec-qa for factoid answer extraction from multiple given passages, babi designed for inference purpose, and the squad dataset used in this paper. to the best of our knowledge, the squad dataset is the only one for both factoid and non-factoid answer extraction with a question distribution more close to real-world applications. section: baseline: chunk-and-rank pipeline with neural rc in this section we modified a state-of-the-art rc system for cloze-style tasks for our answer extraction purpose, to see how much gap we have for the two type of tasks, and to inspire our end-to-end system in the next section. in order to make the cloze-style rc system to make chunk-level decision, we use the rc model to generate features for chunks, which are further used in a feature-based ranker like in. as a result, this baseline can be viewed as a deep learning based counterpart of the system in. it has two main components: 1) a stand-alone answer chunker, which is trained to produce overlapping candidate chunks, and 2) a neural rc model, which is used to score each word in a given passage to be used thereafter for generating chunk scores. answer chunking to reduce the errors generated by the rule-based chunker in, first, we capture the part-of-speech (pos) pattern of all answer sub-sequences in the training dataset to form a pos pattern trie tree, and then apply the answer pos patterns to passage to acquire a collection of all subsequences (chunk candidates) whose pos patterns can be matched to the pos pattern trie. this is equivalent to putting an constraint to candidate answer chunk generation process that only choose the chunk with a pos pattern seen for answers in the training data. then the sub-sequences are used as answer candidates for. note that overlapping chunks could be generated for a passage, and we rely on the ranker to choose the best candidate based on features from the cloze-style rc system. experiments showed that for of the questions on the development set, the ground truth answer is included in the candidate set constructed in such manner. feature extraction and ranking for chunk ranking, we (1) use neural rcqa model to annotate each in passage to get score, then (2) for every chunk in passage, collect scores for all the contained within, and (3) extract features on the sequence of scores to characterize its scale and distribution information, which serves as the feature representation of. in step (1) to acquire we train and apply a word-level single-layer gated attention reader, which has state-of-the-art performance on cnn/ dailymail cloze-style rc task. in step (3) for chunk, we designed 5 features, including 4 statistics on: maximum, minimum, average and sum; as well as the count of matched pos pattern within the chunk, which serves as an answer prior. we use these 5 features in a state-of-the-art ranker. section: dynamic chunk reader the dynamic chunk reader (dcr) model is presented in figure [reference]. inspired by the baseline we built, dcr is deemed to be superior to the baseline for 3 reasons. first, each chunk has a representation constructed dynamically, instead of having a set of pre-defined feature values. second, each passage word's representation is enhanced by word-by-word attention that evaluates the relevance of the passage word to the question. third, these components are all within a single, end-to-end model that can be trained in a joint manner. dcr works in four steps. first, the encoder layer encodes passage and question separately, by using bidirectional recurrent neural networks (rnn). second, the attention layer calculates the relevance of each passage word to the question. third, the chunk representation layer dynamically extracts the candidate chunks from the given passage, and create chunk representation that encodes the contextual information of each chunk. fourth, the ranker layer scores the relevance between the representations of a chunk and the given question, and ranks all candidate chunks using a softmax layer. we describe each step in details below. encoder layer we use bi-directional rnn encoder to encode and of example, and get hidden state for each word position and. as rnn input, a word is represented by a row vector. can be the concatenation of word embedding and word features (see fig. [reference]). the word vector for the-th word is. a word sequence is processed using an rnn encoder with gated recurrent units (gru), which was proved to be effective in rc and neural machine translation tasks. for each position, gru computes with input and previous state, as: where,, and are d-dimensional hidden state, reset gate, and update gate, respectively;, and, are the parameters of the gru; is the sigmoid function, and denotes element-wise production. for a word at, we use the hidden state from the forward rnn as a representation of the preceding context, and the from a backward rnn that encodes text reversely, to incorporate the context after. next,, the bi-directional contextual encoding of, is formed. is the concatenation operator. to distinguish hidden states from different sources, we denote the of-th word in and the of-th word in as and respectively. attention layer attention mechanism in previous rc tasks enables question-aware passage representations. we propose a novel attention mechanism inspired by word-by-word style attention methods. for each, a question-attended representation is computed as follows (example index is omitted for simplicity): where and are hidden states from the bi-directional rnn encoders (see figure [reference]). an inner product,, is calculated between and every question word. it indicates how well the passage word matches with every question word. is a weighted pooling of question hidden states, which serves as a-aware question representation. the concatenation of and leads to a passage-question joint representation,. next, we apply a second bi-gru layer taking the s as inputs, and obtain forward and backward representations and, and in turn their concatenation,. chunk representation layer a candidate answer chunk representation is dynamically created given attention layer output. we first decide the text boundary for the candidate chunk, and then form a chunk representation using all or part of those outputs inside the chunk. to decide a candidate chunk (boundary): we tried two ways: (1) adopt the pos trie-based approach used in our baseline, and (2) enumerate all possible chunks up to a maximum number of tokens. for (2), we create up to (max chunk length) chunks starting from any position in. approach (1) can generate candidates with arbitrary lengths, but fails to recall candidates whose pos pattern is unseen in training set; whereas approach (2) considers all possible candidates within a window and is more flexible, but over-generates invalid candidates. for a candidate answer chunk spanning from position to inclusively, we construct chunk representation using every within range, with a function. formally, we experimented with several pooling functions (e.g., max, average) for, and found out that, instead of pooling, the best function is to concatenate the hidden state of the first word in a chunk in forward rnn and that of the last word in backward rnn. formally, we hypothesize that the hidden states at that two ends can better represent the chunk's contexts, which is critical for this task, than the states within the chunk. this observation also agrees with. ranker layer each chunk is evaluated on its context similarity to the question, by taking the cosine similarity between the chunk context representation acquired from chunk representation layer, and the question representation which is the concatenation of the last hidden state in forward rnn and the first hidden state in backward rnn. thus, for training example, we have the probability of the chunk as where denotes representation of the chunk, or is the-th hidden state output from question's forward and backward rnn encoder, respectively. in runtime, the chunk with the highest probability is taken as the answer. in training, the following negative log likelihood is minimized: note that the-th training instance is only used when is included in the corresponding candidate chunk set, i.e.. the softmax in the final layer serves as the list-wise ranking module similar in spirit to. section: experiments paragraph: dataset we used the stanford question answering dataset (squad) for the experiment. squad came into our sight because it is a mix of factoid and non-factoid questions, a real-world data (crowd-sourced), and of large scale (over 100 k question-answer pairs collected from 536 wikipedia articles). answers range from single words to long, variable-length phrase/ clauses. it is a relaxation of assumptions by the cloze-style and quiz-style rc datasets in the problem definition section. features the input vector representation of each word to encoder rnns has six parts including a pre-trained 300-dimensional glove embedding and five features (see figure [reference]): (1) a one-hot encoding (46 dimensions) for the part-of-speech (pos) tag of; (2) a one-hot encoding (14 dimensions) for named entity (ne) tag of; (3) a binary value indicating whether's surface form is the same to any word in the quesiton; (4) if the lemma form of is the same to any word in the question; and (5) if is caplitalized. feature (3) and (4) are designed to help the model align the passage text with question. note that some types of questions (e.g.,\" who\",\" when\" questions) have answers that have a specific pos/ ne tag pattern. for instance,\" who\" questions mostly have proper nouns/ persons as answers and\" when\" questions may frequently have numbers/ dates (e.g., a year) as answers. thus, we believe that the model could exploit the co-relation between question types and answer pos/ ne patterns easier with pos and ne tag features. implementation details we pre-processed the squad dataset using stanford corenlp tool with its default setting to tokenize the text and obtain the pos and ne annotations. to train our model, we used stochastic gradient descent with the adam optimizer, with an initial learning rate of 0.001. all gru weights were initialized from a uniform distribution between (- 0.01, 0.01). the hidden state size,, was set to 300 for all grus. the question bi-gru shared parameters with the passage bi-gru, while the attention-based passage bi-gru had its own parameters. we shuffled all training examples at the beginning of each epoch and adopted a curriculum learning approach, by sorting training instances by length in every 10 batches, to enable the model start learning from relatively easier instances and to harder ones. we also applied dropout of rate 0.2 to the embedding layer of input bi-gru encoder, and gradient clipping when the norm of gradients exceeded 10. we trained in mini-batch style (mini-batch size is 180) and applied zero-padding to the passage and question inputs in each batch. we also set the maximum passage length to be 300 tokens, and pruned all the tokens after the 300-th token in the training set to save memory and speed up the training process. this step reduced the training set size by about 1.6%. during test, we test on the full length of passage, so that we do n't prune out the potential candidates. we trained the model for at most 30 epochs, and in case the accuracy did not improve for 10 epochs, we stopped training. for the feature ranking-based system, we used jforest ranker with lambdamart-regressiontree algorithm and the ranking metric was ndcg@10. for the gated attention reader in baseline system, we replicated the method and use the same configurations as in. results table [reference] shows our main results on the squad dataset. compared to the scores reported in, our exact match (em) and f1 on the development set and em score on the test set are better, and f1 on the test set is comparable. we also studied how each component in our model contributes to the overall performance. table [reference] shows the details as well as the results of the baseline ranker. as the first row of table [reference] shows, our baseline system improves 10% (em) over rajpurkar et al. rajpurkar2016squad (table [reference], row 1), the feature-based ranking system. however when compared to our dcr model (table [reference], row 2), the baseline (row 1) is more than 12% (em) behind even though it is based on the state-of-the-art model for cloze-style rc tasks. this can be attributed to the advanced model structure and end-to-end manner of dcr. we also did ablation tests on our dcr model. first, replacing the word-by-word attention with attentive reader style attention decreases the em score by about 4.5%, showing the strength of our proposed attention mechanism. second, we remove the features in input to see the contribution of each feature. the result shows that pos feature (1) and question-word feature (3) are the two most important features. finally, combining the dcr model with the proposed pos-trie constraints yields a score similar to the one obtained using the dcr model with all possible-gram chunks. the result shows that (1) our chunk representations are powerful enough to differentiate even a huge amount of chunks when no constraints are applied; and (2) the proposed pos-trie reduces the search space at the cost of a small drop in performance. analysis to better understand our system, we calculated the accuracy of the attention mechanism of the gated attention reader used in our deep learning-based baseline. we found that it is 72% accurate i.e., 72% of the times a word with the highest attention score is inside the correct answer span. this means that, if we could accurately detect the boundary around the word with the highest attention score to form the answer span, we could achieve an accuracy close to 72%. in addition, we checked the answer recall of our candidate chunking approach. when we use a window size of 10, 92% of the time, the ground truth answer will be included in the extracted candidate chunk set. thus the upper bound of the exact match score of our baseline system is around 66% (92% (the answer recall) 72%). from the results, we see our dcr system's exact match score is at 62%. this shows that dcr is proficient at differentiating answer spans dynamically. [] [] to further analyze the system's performance while predicting answers of different lengths, we show the exact match (em) and f1 scores for answers with lengths up to 10 tokens in figure 2 (a). from the graph, we can see that, with the increase of answer length, both em and f1 drops, but in different speed. the gap between f1 and exact match also widens as answer length increases. however, the model still yields a decent accuracy when the answer is longer than a single word. additionally, figure 2 (b) shows that the system is better at\" when\" and\" who\" questions, but performs poorly on\" why\" questions. the large gap between exact match and f1 on\" why\" questions means that perfectly identifying the span is harder than locating the core of the answer span. since\" what\",\" which\", and\" how\" questions contain a broad range of question types, we split them further based on the bigram a question starts with, and figure [reference] shows the breakdown for\" what\" questions. we can see that\" what\" questions asking for explanations such as\" what happens\" and\" what happened\" have lower em and f1 scores. in contrast,\" what\" questions asking for year and numbers have much higher scores and, for these questions, exact match scores are close to f1 scores, which means chunking for these questions are easier for dcr. section: related work attentive reader was the first neural model for factoid rcqa. it uses bidirectional rnn (cho et al., 2014; chung et al., 2014) to encode document and query respectively, and use query representation to match with every token from the document. attention sum reader simplifies the model to just predicting positions of correct answer in the document and the training speed and test accuracy are both greatly improved on the cnn/ daily mail dataset. also simplified attentive reader and reported higher accuracy. window-based memory networks (memn2n) is introduced along with the cbt dataset, which does not use rnn encoders, but embeds contexts as memory and matches questions with embedded contexts. those models' mechanism is to learn the match between answer context with question/ query representation. in contrast, memory enhanced neural networks like neural turing machines and its variants were also potential candidates for the task, and gulcehre et al. gulcehre2016dynamic reported results on the babi task, which is worse than memory networks. similarly, sequence-to-sequence models were also used, but they did not yield better results either. recently, several models have been proposed to enable more complex inference for rc task. for instance, gated attention model employs a multi-layer architecture, where each layer encodes the same document, but the attention is updated from layer to layer. epireader adopted a joint training model for answer extractor and reasoner, where the extractor proposes top candidates, and the reasoner weighs each candidate by examining entailment relationship between question-answer representation and the document. an iterative alternating attention mechanism and gating strategies were proposed in to optimize the attention through several hops. in contrast, cui et al. cui2016aoa, cui2016consensus introduced fine-grained document attention from each question word and then aggregated those attentions from each question token by summation with or without weights. this system achieved the state-of-the-art score on the cnn dataset. those different variations all result in roughly 3-5% improvement over attention sum reader, but none of those could achieve higher than that. other methods include using dynamic entity representation with max-pooling that aims to change entity representation with context, and weissenborn's weissenborn2016separating system, which tries to separate entity from the context and then matches the question to context, scoring an accuracy around 70% on the cnn dataset. however, all of those models assume that the answers are single tokens. this limits the type of questions the models can answer. wang and jiang wang2016machine proposed a match-lstm and achieved good results on squad. however, this approach predicts a chunk boundary or whether a word is part of a chunk or not. in contrast, our approach explicitly constructs the chunk representations and similar chunks are compared directly to determine correct answer boundaries. section: conclusion in this paper we proposed a novel neural reading comprehension model for question answering. different from the previously proposed models for factoid rcqa, the proposed model, dynamic chunk reader, is not restricted to predicting a single named entity as an answer or selecting an answer from a small, pre-defined candidate list. instead, it is capable of answering both factoid and non-factoid questions as it learns to select answer chunks that are suitable for an input question. dcr achieves this goal with a joint deep learning model enhanced with a novel attention mechanism and five simple yet effective features. error analysis shows that the dcr model achieves good performance, but still needs to improve on predicting longer answers, which are usually non-factoid in nature. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "squad dataset",
                        836
                    ],
                    [
                        "stanford question answering dataset",
                        4679
                    ],
                    [
                        "squad",
                        4717
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "dynamic chunk reader",
                        103
                    ],
                    [
                        "dcr",
                        126
                    ],
                    [
                        "dcr model",
                        18951
                    ],
                    [
                        "dcr system",
                        20855
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "exact match",
                        803
                    ],
                    [
                        "exact boundary match accuracy",
                        4625
                    ],
                    [
                        "em",
                        18453
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "reading comprehension-based question answering",
                        873
                    ],
                    [
                        "rcqa",
                        922
                    ],
                    [
                        "rcqa problem",
                        4920
                    ],
                    [
                        "factoid rcqa",
                        22371
                    ],
                    [
                        "question answering",
                        25434
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "squad dataset",
                        836
                    ],
                    [
                        "stanford question answering dataset",
                        4679
                    ],
                    [
                        "squad",
                        4717
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "dynamic chunk reader",
                        103
                    ],
                    [
                        "dcr",
                        126
                    ],
                    [
                        "dcr model",
                        18951
                    ],
                    [
                        "dcr system",
                        20855
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "f1 scores",
                        819
                    ],
                    [
                        "f1",
                        18460
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "reading comprehension-based question answering",
                        873
                    ],
                    [
                        "rcqa",
                        922
                    ],
                    [
                        "rcqa problem",
                        4920
                    ],
                    [
                        "factoid rcqa",
                        22371
                    ],
                    [
                        "question answering",
                        25434
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "0690ba31424310a90028533218d0afd25a829c8d-4",
    "doctext": "document: semantic image segmentation with deep convolutional nets and fully connected crfs deep convolutional neural networks (dcnns) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. this work brings together methods from dcnns and probabilistic graphical models for addressing the task of pixel-level classification (also called\" semantic image segmentation\"). we show that responses at the final layer of dcnns are not sufficiently localized for accurate object segmentation. this is due to the very invariance properties that make dcnns good for high level tasks. we overcome this poor localization property of deep networks by combining the responses at the final dcnn layer with a fully connected conditional random field (crf). qualitatively, our\" deeplab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. quantitatively, our method sets the new state-of-art at the pascal voc-2012 semantic image segmentation task, reaching 71.6% iou accuracy in the test set. we show how these results can be obtained efficiently: careful network re-purposing and a novel application of the' hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern gpu. section: introduction deep convolutional neural networks (dcnns) had been the method of choice for document recognition since lecun1998, but have only recently become the mainstream of high-level vision research. over the past two years dcnns have pushed the performance of computer vision systems to soaring heights on a broad array of high-level problems, including image classification krizhevskynips2013, sermanet2013overfeat, simonyan2014very, szegedy2014going, papandreou2014untangling, object detection girshick2014rcnn, fine-grained categorization zhang2014part, among others. a common theme in these works is that dcnns trained in an end-to-end manner deliver strikingly better results than systems relying on carefully engineered representations, such as sift or hog features. this success can be partially attributed to the built-in invariance of dcnns to local image transformations, which underpins their ability to learn hierarchical abstractions of data zeiler2014visualizing. while this invariance is clearly desirable for high-level vision tasks, it can hamper low-level tasks, such as pose estimation chen2014articulated, tompson2014joint and semantic segmentation-where we want precise localization, rather than abstraction of spatial details. there are two technical hurdles in the application of dcnns to image labeling tasks: signal downsampling, and spatial 'insensitivity' (invariance). the first problem relates to the reduction of signal resolution incurred by the repeated combination of max-pooling and downsampling ('striding') performed at every layer of standard dcnns krizhevskynips2013, simonyan2014very, szegedy2014going. instead, as in papandreou2014untangling, we employ the 'atrous' (with holes) algorithm originally developed for efficiently computing the undecimated discrete wavelet transform mall99. this allows efficient dense computation of dcnn responses in a scheme substantially simpler than earlier solutions to this problem gcmg+ 13, sermanet2013overfeat. the second problem relates to the fact that obtaining object-centric decisions from a classifier requires invariance to spatial transformations, inherently limiting the spatial accuracy of the dcnn model. we boost our model's ability to capture fine details by employing a fully-connected conditional random field (crf). conditional random fields have been broadly used in semantic segmentation to combine class scores computed by multi-way classifiers with the low-level information captured by the local interactions of pixels and edges rother2004grabcut, shotton2009textonboost or superpixels lucchi2011spatial. even though works of increased sophistication have been proposed to model the hierarchical dependency he2004multiscale, ladicky2009associative, lempitsky2011pylon and/ or high-order dependencies of segments delong2012fast, gonfaus2010harmony, kohli2009robust, cpy13, wang15, we use the fully connected pairwise crf proposed by krahenbuhl2011efficient for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies. that model was shown in krahenbuhl2011efficient to largely improve the performance of a boosting-based pixel-level classifier, and in our work we demonstrate that it leads to state-of-the-art results when coupled with a dcnn-based pixel-level classifier. the three main advantages of our\" deeplab\" system are (i) speed: by virtue of the 'atrous' algorithm, our dense dcnn operates at 8 fps, while mean field inference for the fully-connected crf requires 0.5 second, (ii) accuracy: we obtain state-of-the-art results on the pascal semantic segmentation challenge, outperforming the second-best approach of mostajabi2014feedforward by a margin of 7.2 and (iii) simplicity: our system is composed of a cascade of two fairly well-established modules, dcnns and crfs. section: related work our system works directly on the pixel representation, similarly to long2014fully. this is in contrast to the two-stage approaches that are now most common in semantic segmentation with dcnns: such techniques typically use a cascade of bottom-up image segmentation and dcnn-based region classification, which makes the system commit to potential errors of the front-end segmentation system. for instance, the bounding box proposals and masked regions delivered by arbelaez2014multiscale, uijlings13 are used in girshick2014rcnn and hariharan2014simultaneous as inputs to a dcnn to introduce shape information into the classification process. similarly, the authors of mostajabi2014feedforward rely on a superpixel representation. a celebrated non-dcnn precursor to these works is the second order pooling method of carreira2012semantic which also assigns labels to the regions proposals delivered by carreira2012cpmc. understanding the perils of committing to a single segmentation, the authors of cogswell2014combining build on to explore a diverse set of crf-based segmentation proposals, computed also by carreira2012cpmc. these segmentation proposals are then re-ranked according to a dcnn trained in particular for this reranking task. even though this approach explicitly tries to handle the temperamental nature of a front-end segmentation algorithm, there is still no explicit exploitation of the dcnn scores in the crf-based segmentation algorithm: the dcnn is only applied post-hoc, while it would make sense to directly try to use its results during segmentation. moving towards works that lie closer to our approach, several other researchers have considered the use of convolutionally computed dcnn features for dense image labeling. among the first have been farabet2013learning who apply dcnns at multiple image resolutions and then employ a segmentation tree to smooth the prediction results; more recently, hariharan2014hypercolumns propose to concatenate the computed inter-mediate feature maps within the dcnns for pixel classification, and dai2014convolutional propose to pool the inter-mediate feature maps by region proposals. even though these works still employ segmentation algorithms that are decoupled from the dcnn classifier's results, we believe it is advantageous that segmentation is only used at a later stage, avoiding the commitment to premature decisions. more recently, the segmentation-free techniques of long2014fully, eigen2014predicting directly apply dcnns to the whole image in a sliding window fashion, replacing the last fully connected layers of a dcnn by convolutional layers. in order to deal with the spatial localization issues outlined in the beginning of the introduction, long2014fully upsample and concatenate the scores from inter-mediate feature maps, while eigen2014predicting refine the prediction result from coarse to fine by propagating the coarse results to another dcnn. the main difference between our model and other state-of-the-art models is the combination of pixel-level crfs and dcnn-based 'unary terms'. focusing on the closest works in this direction, cogswell2014combining use crfs as a proposal mechanism for a dcnn-based reranking system, while farabet2013learning treat superpixels as nodes for a local pairwise crf and use graph-cuts for discrete inference; as such their results can be limited by errors in superpixel computations, while ignoring long-range superpixel dependencies. our approach instead treats every pixel as a crf node, exploits long-range dependencies, and uses crf inference to directly optimize a dcnn-driven cost function. we note that mean field had been extensively studied for traditional image segmentation/ edge detection tasks, e.g., geiger1991parallel, geiger1991common, kokkinos2008computational, but recently krahenbuhl2011efficient showed that the inference can be very efficient for fully connected crf and particularly effective in the context of semantic segmentation. after the first version of our manuscript was made publicly available, it came to our attention that two other groups have independently and concurrently pursued a very similar direction, combining dcnns and densely connected crfs bell2014material, zheng2015crfrnn. there are several differences in technical aspects of the respective models. bell2014material focus on the problem of material classification, while zheng2015crfrnn unroll the crf mean-field inference steps to convert the whole system into an end-to-end trainable feed-forward network. we have updated our proposed\" deeplab\" system with much improved methods and results in our latest work chen2016deeplab. we refer the interested reader to the paper for details. section: convolutional neural networks for dense image labeling herein we describe how we have re-purposed and finetuned the publicly available imagenet-pretrained state-of-art 16-layer classification network of simonyan2014very (vgg-16) into an efficient and effective dense feature extractor for our dense semantic image segmentation system. subsection: efficient dense sliding window feature extraction with the hole algorithm dense spatial score evaluation is instrumental in the success of our dense cnn feature extractor. as a first step to implement this, we convert the fully-connected layers of vgg-16 into convolutional ones and run the network in a convolutional fashion on the image at its original resolution. however this is not enough as it yields very sparsely computed detection scores (with a stride of 32 pixels). to compute scores more densely at our target stride of 8 pixels, we develop a variation of the method previously employed by gcmg+ 13, sermanet2013overfeat. we skip subsampling after the last two max-pooling layers in the network of simonyan2014very and modify the convolutional filters in the layers that follow them by introducing zeros to increase their length (in the last three convolutional layers and in the first fully connected layer). we can implement this more efficiently by keeping the filters intact and instead sparsely sample the feature maps on which they are applied on using an input stride of 2 or 4 pixels, respectively. this approach, illustrated in fig. [reference] is known as the 'hole algorithm' ('atrous algorithm') and has been developed before for efficient computation of the undecimated wavelet transform mall99. we have implemented this within the caffe framework jia2014caffe by adding to the im2col function (it converts multi-channel feature maps to vectorized patches) the option to sparsely sample the underlying feature map. this approach is generally applicable and allows us to efficiently compute dense cnn feature maps at any target subsampling rate without introducing any approximations. we finetune the model weights of the imagenet-pretrained vgg-16 network to adapt it to the image classification task in a straightforward fashion, following the procedure of long2014fully. we replace the 1000-way imagenet classifier in the last layer of vgg-16 with a 21-way one. our loss function is the sum of cross-entropy terms for each spatial position in the cnn output map (subsampled by 8 compared to the original image). all positions and labels are equally weighted in the overall loss function. our targets are the ground truth labels (subsampled by 8). we optimize the objective function with respect to the weights at all network layers by the standard sgd procedure of krizhevskynips2013. during testing, we need class score maps at the original image resolution. as illustrated in figure [reference] and further elaborated in section [reference], the class score maps (corresponding to log-probabilities) are quite smooth, which allows us to use simple bilinear interpolation to increase their resolution by a factor of 8 at a negligible computational cost. note that the method of long2014fully does not use the hole algorithm and produces very coarse scores (subsampled by a factor of 32) at the cnn output. this forced them to use learned upsampling layers, significantly increasing the complexity and training time of their system: fine-tuning our network on pascal voc 2012 takes about 10 hours, while they report a training time of several days (both timings on a modern gpu). subsection: controlling the receptive field size and accelerating dense computation with convolutional nets another key ingredient in re-purposing our network for dense score computation is explicitly controlling the network's receptive field size. most recent dcnn-based image recognition methods rely on networks pre-trained on the imagenet large-scale classification task. these networks typically have large receptive field size: in the case of the vgg-16 net we consider, its receptive field is (with zero-padding) and pixels if the net is applied convolutionally. after converting the network to a fully convolutional one, the first fully connected layer has 4, 096 filters of large spatial size and becomes the computational bottleneck in our dense score map computation. we have addressed this practical problem by spatially subsampling (by simple decimation) the first fc layer to (or) spatial size. this has reduced the receptive field of the network down to (with zero-padding) or (in convolutional mode) and has reduced computation time for the first fc layer by times. using our caffe-based implementation and a titan gpu, the resulting vgg-derived network is very efficient: given a input image, it produces dense raw feature scores at the top of the network at a rate of about 8 frames/ sec during testing. the speed during training is 3 frames/ sec. we have also successfully experimented with reducing the number of channels at the fully connected layers from 4, 096 down to 1, 024, considerably further decreasing computation time and memory footprint without sacrificing performance, as detailed in section [reference]. using smaller networks such as krizhevskynips2013 could allow video-rate test-time dense feature computation even on light-weight gpus. section: detailed boundary recovery: fully-connected conditional random fields and multi-scale prediction subsection: deep convolutional networks and the localization challenge as illustrated in figure [reference], dcnn score maps can reliably predict the presence and rough position of objects in an image but are less well suited for pin-pointing their exact outline. there is a natural trade-off between classification accuracy and localization accuracy with convolutional networks: deeper models with multiple max-pooling layers have proven most successful in classification tasks, however their increased invariance and large receptive fields make the problem of inferring position from the scores at their top output levels more challenging. recent work has pursued two directions to address this localization challenge. the first approach is to harness information from multiple layers in the convolutional network in order to better estimate the object boundaries long2014fully, eigen2014predicting. the second approach is to employ a super-pixel representation, essentially delegating the localization task to a low-level segmentation method. this route is followed by the very successful recent method of mostajabi2014feedforward. in section [reference], we pursue a novel alternative direction based on coupling the recognition capacity of dcnns and the fine-grained localization accuracy of fully connected crfs and show that it is remarkably successful in addressing the localization challenge, producing accurate semantic segmentation results and recovering object boundaries at a level of detail that is well beyond the reach of existing methods. subsection: fully-connected conditional random fields for accurate localization traditionally, conditional random fields (crfs) have been employed to smooth noisy segmentation maps rother2004grabcut, kohli2009robust. typically these models contain energy terms that couple neighboring nodes, favoring same-label assignments to spatially proximal pixels. qualitatively, the primary function of these short-range crfs has been to clean up the spurious predictions of weak classifiers built on top of local hand-engineered features. compared to these weaker classifiers, modern dcnn architectures such as the one we use in this work produce score maps and semantic label predictions which are qualitatively different. as illustrated in figure [reference], the score maps are typically quite smooth and produce homogeneous classification results. in this regime, using short-range crfs can be detrimental, as our goal should be to recover detailed local structure rather than further smooth it. using contrast-sensitive potentials rother2004grabcut in conjunction to local-range crfs can potentially improve localization but still miss thin-structures and typically requires solving an expensive discrete optimization problem. to overcome these limitations of short-range crfs, we integrate into our system the fully connected crf model of krahenbuhl2011efficient. the model employs the energy function where is the label assignment for pixels. we use as unary potential, where is the label assignment probability at pixel as computed by dcnn. the pairwise potential is, where, and zero otherwise (i.e., potts model). there is one pairwise term for each pair of pixels and in the image no matter how far from each other they lie, i.e. the model's factor graph is fully connected. each is the gaussian kernel depends on features (denoted as) extracted for pixel and and is weighted by parameter. we adopt bilateral position and color terms, specifically, the kernels are where the first kernel depends on both pixel positions (denoted as) and pixel color intensities (denoted as), and the second kernel only depends on pixel positions. the hyper parameters, and control the\" scale\" of the gaussian kernels. crucially, this model is amenable to efficient approximate probabilistic inference krahenbuhl2011efficient. the message passing updates under a fully decomposable mean field approximation can be expressed as convolutions with a gaussian kernel in feature space. high-dimensional filtering algorithms adams2010fast significantly speed-up this computation resulting in an algorithm that is very fast in practice, less that 0.5 sec on average for pascal voc images using the publicly available implementation of krahenbuhl2011efficient. subsection: multi-scale prediction following the promising recent results of hariharan2014hypercolumns, long2014fully we have also explored a multi-scale prediction method to increase the boundary localization accuracy. specifically, we attach to the input image and the output of each of the first four max pooling layers a two-layer mlp (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional filters) whose feature map is concatenated to the main network's last layer feature map. the aggregate feature map fed into the softmax layer is thus enhanced by 5* 128= 640 channels. we only adjust the newly added weights, keeping the other network parameters to the values learned by the method of section [reference]. as discussed in the experimental section, introducing these extra direct connections from fine-resolution layers improves localization performance, yet the effect is not as dramatic as the one obtained with the fully-connected crf. section: experimental evaluation paragraph: dataset we test our deeplab model on the pascal voc 2012 segmentation benchmark everingham2014pascal, consisting of 20 foreground object classes and one background class. the original dataset contains,, and images for training, validation, and testing, respectively. the dataset is augmented by the extra annotations provided by hariharan2011semantic, resulting in training images. the performance is measured in terms of pixel intersection-over-union (iou) averaged across the 21 classes. paragraph: training we adopt the simplest form of piecewise training, decoupling the dcnn and crf training stages, assuming the unary terms provided by the dcnn are fixed during crf training. for dcnn training we employ the vgg-16 network which has been pre-trained on imagenet. we fine-tuned the vgg-16 network on the voc 21-way pixel-classification task by stochastic gradient descent on the cross-entropy loss function, as described in section [reference]. we use a mini-batch of 20 images and initial learning rate of (for the final classifier layer), multiplying the learning rate by 0.1 at every 2000 iterations. we use momentum of and a weight decay of. after the dcnn has been fine-tuned, we cross-validate the parameters of the fully connected crf model in eq. ([reference]) along the lines of krahenbuhl2011efficient. we use the default values of and and we search for the best values of,, and by cross-validation on a small subset of the validation set (we use 100 images). we employ coarse-to-fine search scheme. specifically, the initial search range of the parameters are, and (matlab notation), and then we refine the search step sizes around the first round's best values. we fix the number of mean field iterations to 10 for all reported experiments. paragraph: evaluation on validation set we conduct the majority of our evaluations on the pascal 'val' set, training our model on the augmented pascal 'train' set. as shown in tab. [reference] (a), incorporating the fully connected crf to our model (denoted by deeplab-crf) yields a substantial performance boost, about 4% improvement over deeplab. we note that the work of krahenbuhl2011efficient improved the result of textonboost shotton2009textonboost to, which makes the improvement we report here (from to) all the more impressive. turning to qualitative results, we provide visual comparisons between deeplab and deeplab-crf in fig. [reference]. employing a fully connected crf significantly improves the results, allowing the model to accurately capture intricate object boundaries. paragraph: multi-scale features we also exploit the features from the intermediate layers, similar to hariharan2014hypercolumns, long2014fully. as shown in tab. [reference] (a), adding the multi-scale features to our deeplab model (denoted as deeplab-msc) improves about performance, and further incorporating the fully connected crf (denoted as deeplab-msc-crf) yields about 4% improvement. the qualitative comparisons between deeplab and deeplab-msc are shown in fig. [reference]. leveraging the multi-scale features can slightly refine the object boundaries. paragraph: field of view the 'atrous algorithm' we employed allows us to arbitrarily control the field-of-view (fov) of the models by adjusting the input stride, as illustrated in fig. [reference]. in tab. [reference], we experiment with several kernel sizes and input strides at the first fully connected layer. the method, deeplab-crf-7x7, is the direct modification from vgg-16 net, where the kernel size= and input stride= 4. this model yields performance of on the 'val' set, but it is relatively slow (images per second during training). we have improved model speed to images per second by reducing the kernel size to. we have experimented with two such network variants with different fov sizes, deeplab-crf and deeplab-crf-4x4; the latter has large fov (i.e., large input stride) and attains better performance. finally, we employ kernel size and input stride= 12, and further change the filter sizes from 4096 to 1024 for the last two layers. interestingly, the resulting model, deeplab-crf-largefov, matches the performance of the expensive deeplab-crf-7x7. at the same time, it is times faster to run and has significantly fewer parameters (20.5 m instead of 134.3 m). the performance of several model variants is summarized in tab. [reference], showing the benefit of exploiting multi-scale features and large fov. paragraph: mean pixel iou along object boundaries to quantify the accuracy of the proposed model near object boundaries, we evaluate the segmentation accuracy with an experiment similar to kohli2009robust, krahenbuhl2011efficient. specifically, we use the 'void' label annotated in val set, which usually occurs around object boundaries. we compute the mean iou for those pixels that are located within a narrow band (called trimap) of 'void' labels. as shown in fig. [reference], exploiting the multi-scale features from the intermediate layers and refining the segmentation results by a fully connected crf significantly improve the results around object boundaries. paragraph: comparison with state-of-art in fig. [reference], we qualitatively compare our proposed model, deeplab-crf, with two state-of-art models: fcn-8s long2014fully and tti-zoomout-16 mostajabi2014feedforward on the 'val' set (the results are extracted from their papers). our model is able to capture the intricate object boundaries. paragraph: reproducibility we have implemented the proposed methods by extending the excellent caffe framework jia2014caffe. we share our source code, configuration files, and trained models that allow reproducing the results in this paper at a companion web site. paragraph: test set results having set our model choices on the validation set, we evaluate our model variants on the pascal voc 2012 official 'test' set. as shown in tab. [reference], our deeplab-crf and deeplab-msc-crf models achieve performance of and mean iou, respectively. our models outperform all the other state-of-the-art models (specifically, tti-zoomout-16 mostajabi2014feedforward, fcn-8s long2014fully, and msra-cfm dai2014convolutional). when we increase the fov of the models, deeplab-crf-largefov yields performance of, the same as deeplab-crf-7x7, while its training speed is faster. furthermore, our best model, deeplab-msc-crf-largefov, attains the best performance of by employing both multi-scale features and large fov. section: discussion our work combines ideas from deep convolutional neural networks and fully-connected conditional random fields, yielding a novel method able to produce semantically accurate predictions and detailed segmentation maps, while being computationally efficient. our experimental results show that the proposed method significantly advances the state-of-art in the challenging pascal voc 2012 semantic image segmentation task. there are multiple aspects in our model that we intend to refine, such as fully integrating its two main components (cnn and crf) and train the whole system in an end-to-end fashion, similar to koltun13, chen2014learning, zheng2015crfrnn. we also plan to experiment with more datasets and apply our method to other sources of data such as depth maps or videos. recently, we have pursued model training with weakly supervised annotations, in the form of bounding boxes or image-level labels papandreou15weak. at a higher level, our work lies in the intersection of convolutional neural networks and probabilistic graphical models. we plan to further investigate the interplay of these two powerful classes of methods and explore their synergistic potential for solving challenging computer vision tasks. subsection: acknowledgments this work was partly supported by aro 62250-cs, nih grant 5r01ey022247-03, eu project reconfig fp7-ict-600825 and eu project mobot fp7-ict-2011-600796. we also gratefully acknowledge the support of nvidia corporation with the donation of gpus used for this research. we would like to thank the anonymous reviewers for their detailed comments and constructive feedback. subsection: paper revisions here we present the list of major paper revisions for the convenience of the readers. paragraph: v1 submission to iclr 2015. introduces the model deeplab-crf, which attains the performance of on pascal voc 2012 test set. paragraph: v2 rebuttal for iclr 2015. adds the model deeplab-msc-crf, which incorporates multi-scale features from the intermediate layers. deeplab-msc-crf yields the performance of on pascal voc 2012 test set. paragraph: v3 camera-ready for iclr 2015. experiments with large field-of-view. on pascal voc 2012 test set, deeplab-crf-largefov achieves the performance of. when exploiting both mutli-scale features and large fov, deeplab-msc-crf-largefov attains the performance of. paragraph: v4 reference to our updated\" deeplab\" system chen2016deeplab with much improved results. bibliography: references",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "deeplab\" system",
                        834
                    ],
                    [
                        "deeplab model",
                        20710
                    ],
                    [
                        "deeplab",
                        22788
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "frames per second",
                        1313
                    ],
                    [
                        "frames/ sec",
                        14794
                    ]
                ]
            ],
            "Task": []
        },
        {
            "Material": [],
            "Method": [
                [
                    [
                        "deeplab\" system",
                        834
                    ],
                    [
                        "deeplab model",
                        20710
                    ],
                    [
                        "deeplab",
                        22788
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "mean pixel iou",
                        25140
                    ],
                    [
                        "mean iou",
                        25482
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semantic image segmentation",
                        10
                    ],
                    [
                        "semantic segmentation",
                        2509
                    ],
                    [
                        "pascal semantic segmentation challenge",
                        4963
                    ]
                ]
            ]
        },
        {
            "Material": [],
            "Method": [
                [
                    [
                        "deeplab\" system",
                        834
                    ],
                    [
                        "deeplab model",
                        20710
                    ],
                    [
                        "deeplab",
                        22788
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "training time",
                        13322
                    ],
                    [
                        "computation time",
                        14532
                    ]
                ]
            ],
            "Task": []
        },
        {
            "Material": [
                [
                    [
                        "pascal voc 2012",
                        13380
                    ],
                    [
                        "pascal voc images",
                        19583
                    ],
                    [
                        "pascal",
                        22538
                    ],
                    [
                        "augmented pascal 'train' set",
                        22582
                    ],
                    [
                        "voc 2012",
                        26528
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "deeplab-msc-crf-largefov",
                        27034
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "mean pixel iou",
                        25140
                    ],
                    [
                        "mean iou",
                        25482
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semantic image segmentation",
                        10
                    ],
                    [
                        "semantic segmentation",
                        2509
                    ],
                    [
                        "pascal semantic segmentation challenge",
                        4963
                    ]
                ]
            ]
        },
        {
            "Material": [],
            "Method": [],
            "Metric": [
                [
                    [
                        "mean pixel iou",
                        25140
                    ],
                    [
                        "mean iou",
                        25482
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "accurate object segmentation",
                        528
                    ],
                    [
                        "segmentation",
                        6786
                    ],
                    [
                        "image segmentation",
                        8917
                    ],
                    [
                        "detailed segmentation maps",
                        27355
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "07a9478e87a8304fc3267fa16e83e9f3bbd98b27-5",
    "doctext": "document: a decomposable attention model for natural language inference we propose a simple neural architecture for natural language inference. our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. on the stanford natural language inference (snli) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. adding intra-sentence attention that takes a minimum amount of order into account yields further improvements. section: introduction natural language inference (nli) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis. nli is a central problem in language understanding and recently the large snli corpus of 570 k sentence pairs was created for this task. we present a new model for nli and leverage this corpus for comparison with prior work. a large body of work based on neural networks for text similarity tasks including nli has been published in recent years. the dominating trend in these models is to build complex, deep text representation models, for example, with convolutional networks or long short-term memory networks with the goal of deeper sentence comprehension. while these approaches have yielded impressive results, they are often computationally very expensive, and result in models having millions of parameters (excluding embeddings). here, we take a different approach, arguing that for natural language inference it can often suffice to simply align bits of local text substructure and then aggregate this information. for example, consider the following sentences: bob is in his room, but because of the thunder and lightning outside, he can not sleep. bob is awake. it is sunny outside. the first sentence is complex in structure and it is challenging to construct a compact representation that expresses its entire meaning. however, it is fairly easy to conclude that the second sentence follows from the first one, by simply aligning bob with bob and can not sleep with awake and recognizing that these are synonyms. similarly, one can conclude that it is sunny outside contradicts the first sentence, by aligning thunder and lightning with sunny and recognizing that these are most likely incompatible. we leverage this intuition to build a simpler and more lightweight approach to nli within a neural framework; with considerably fewer parameters, our model outperforms more complex existing neural architectures. in contrast to existing approaches, our approach only relies on alignment and is fully computationally decomposable with respect to the input text. an overview of our approach is given in figure [reference]. given two sentences, where each word is represented by an embedding vector, we first create a soft alignment matrix using neural attention. we then use the (soft) alignment to decompose the task into subproblems that are solved separately. finally, the results of these subproblems are merged to produce the final classification. in addition, we optionally apply intra-sentence attention to endow the model with a richer encoding of substructures prior to the alignment step. asymptotically our approach does the same total work as a vanilla lstm encoder, while being trivially parallelizable across sentence length, which can allow for considerable speedups in low-latency settings. empirical results on the snli corpus show that our approach achieves state-of-the-art results, while using almost an order of magnitude fewer parameters compared to complex lstm-based approaches. section: related work our method is motivated by the central role played by alignment in machine translation and previous approaches to sentence similarity modeling, natural language inference, and semantic parsing. the neural counterpart to alignment, attention, which is a key part of our approach, was originally proposed and has been predominantly used in conjunction with lstms and to a lesser extent with cnns. in contrast, our use of attention is purely based on word embeddings and our method essentially consists of feed-forward networks that operate largely independently of word order. section: approach let and be the two input sentences of length and, respectively. we assume that each, is a word embedding vector of dimension and that each sentence is prepended with a\" null\" token. our training data comes in the form of labeled pairs, where is an indicator vector encoding the label and is the number of output classes. at test time, we receive a pair of sentences and our goal is to predict the correct label. paragraph: input representation. let and denote the input representation of each fragment that is fed to subsequent steps of the algorithm. the vanilla version of our model simply defines and. with this input representation, our model does not make use of word order. however, we discuss an extension using intra-sentence attention in section [reference] that uses a minimal amount of sequence information. the core model consists of the following three components (see figure [reference]), which are trained jointly: paragraph: attend. first, soft-align the elements of and using a variant of neural attention and decompose the problem into the comparison of aligned subphrases. paragraph: compare. second, separately compare each aligned subphrase to produce a set of vectors for and for. each is a nonlinear combination of and its (softly) aligned subphrase in (and analogously for). paragraph: aggregate. finally, aggregate the sets and from the previous step and use the result to predict the label. subsection: attend we first obtain unnormalized attention weights, computed by a function, which decomposes as: this decomposition avoids the quadratic complexity that would be associated with separately applying times. instead, only applications of are needed. we take to be a feed-forward neural network with relu activations. these attention weights are normalized as follows: here is the subphrase in that is (softly) aligned to and vice versa for. subsection: compare next, we separately compare the aligned phrases and using a function, which in this work is again a feed-forward network: where the brackets denote concatenation. note that since there are only a linear number of terms in this case, we do not need to apply a decomposition as was done in the previous step. thus can jointly take into account both and. subsection: aggregate we now have two sets of comparison vectors and. we first aggregate over each set by summation: and feed the result through a final classifier, that is a feed forward network followed by a linear layer: where represents the predicted (unnormalized) scores for each class and consequently the predicted class is given by. for training, we use multi-class cross-entropy loss with dropout regularization: here denote the learnable parameters of the functions f, g and h, respectively. subsection: intra-sentence attention (optional) in the above model, the input representations are simple word embeddings. however, we can augment this input representation with intra-sentence attention to encode compositional relationships between words within each sentence, as proposed by cheng2016long. similar to eqs. [reference] and [reference], we define where is a feed-forward network. we then create the self-aligned phrases the distance-sensitive bias terms provides the model with a minimal amount of sequence information, while remaining parallelizable. these terms are bucketed such that all distances greater than 10 words share the same bias. the input representation for subsequent steps is then defined as and analogously. section: computational complexity we now discuss the asymptotic complexity of our approach and how it offers a higher degree of parallelism than lstm-based approaches. recall that denotes embedding dimension and means sentence length. for simplicity we assume that all hidden dimensions are and that the complexity of matrix ()-vector () multiplication is. a key assumption of our analysis is that, which we believe is reasonable and is true of the snli dataset where, whereas recent lstm-based approaches have used. this assumption allows us to bound the complexity of computing the attention weights. paragraph: complexity of lstms. the complexity of an lstm cell is, resulting in a complexity of to encode the sentence. adding attention as in rocktaschel2015reasoning increases this complexity to. paragraph: complexity of our approach. application of a feed-forward network requires steps. thus, the compare and aggregate steps have complexity and respectively. for the attend step, is evaluated times, giving a complexity of. each attention weight requires one dot product, resulting in a complexity of. thus the total complexity of the model is, which is equal to that of an lstm with attention. however, note that with the assumption that, this becomes which is the same complexity as a regular lstm. moreover, unlike the lstm, our approach has the advantage of being parallelizable over, which can be useful at test time. section: experiments we evaluate our approach on the stanford natural language inference (snli) dataset. given a sentences pair, the task is to predict whether is entailed by, contradicts, or whether their relationship is neutral. subsection: implementation details the method was implemented in tensorflow. data preprocessing: following bowman2015large, we remove examples labeled\"-\" (no gold label) from the dataset, which leaves 549, 367 pairs for training, 9, 842 for development, and 9, 824 for testing. we use the tokenized sentences from the non-binary parse provided in the dataset and prepend each sentence with a\" null\" token. during training, each sentence was padded up to the maximum length of the batch for efficient training (the padding was explicitly masked out so as not to affect the objective/ gradients). for efficient batching in tensorflow, we semi-sorted the training data to first contain examples where both sentences had length less than 20, followed by those with length less than 50, and then the rest. this ensured that most training batches contained examples of similar length. embeddings: we use 300 dimensional glove embeddings to represent words. each embedding vector was normalized to have norm of 1 and projected down to 200 dimensions, a number determined via hyperparameter tuning. out-of-vocabulary (oov) words are hashed to one of 100 random embeddings each initialized to mean 0 and standard deviation 1. all embeddings remain fixed during training, but the projection matrix is trained. all other parameter weights (hidden layers etc.) were initialized from random gaussians with mean 0 and standard deviation 0.01. each hyperparameter setting was run on a single machine with 10 asynchronous gradient-update threads, using adagrad for optimization with the default initial accumulator value of 0.1. dropout regularization was used for all relu layers, but not for the final linear layer. we additionally tuned the following hyperparameters and present their chosen values in parentheses: network size (2-layers, each with 200 neurons), batch size (4), dropout ratio (0.2) and learning rate (0.05-vanilla, 0.025-intra-attention). all settings were run for 50 million steps (each step indicates one batch) but model parameters were saved frequently as training progressed and we chose the model that did best on the development set. subsection: results results in terms of 3-class accuracy are shown in table [reference]. our vanilla approach achieves state-of-the-art results with almost an order of magnitude fewer parameters than the lstmn of cheng2016long. adding intra-sentence attention gives a considerable improvement of 0.5 percentage points over the existing state of the art. table [reference] gives a breakdown of accuracy on the development set showing that most of our gains stem from neutral, while most losses come from contradiction pairs. table [reference] shows some wins and losses. examples a-c are cases where both variants of our approach are correct while both spinn-pi and the mlstm are incorrect. in the first two cases, both sentences contain phrases that are either identical or highly lexically related (e.g.\" two kids\" and\" ocean/ beach\") and our approach correctly favors neutral in these cases. in example c, it is possible that relying on word-order may confuse spinn-pi and the mlstm due to how\" fountain\" is the object of a preposition in the first sentence but the subject of the second. the second set of examples (d-f) are cases where our vanilla approach is incorrect but mlstm and spinn-pi are correct. example f requires sequential information and neither variant of our approach can predict the correct class. examples d-e are interesting however, since they do n't require word order information, yet intra-attention seems to help. we suspect this may be because the word embeddings are not fine-grained enough for the algorithm to conclude that\" play/ watch\" is a contradiction, but intra-attention, by adding an extra layer of composition/ nonlinearity to incorporate context, compensates for this. finally, examples g-i are cases that all methods get wrong. the first is actually representative of many examples in this category where there is one critical word that separates the two sentences (close vs open in this case) and goes unnoticed by the algorithms. examples h requires inference about numbers and example i needs sequence information. section: conclusion we presented a simple attention-based approach to natural language inference that is trivially parallelizable. the approach outperforms considerably more complex neural methods aiming for text understanding. our results suggest that, at least for this task, pairwise comparisons are relatively more important than global sentence-level representations. section: acknowledgements we thank slav petrov, tom kwiatkowski, yoon kim, erick fonseca, mark neumann for useful discussion and sam bowman and shuohang wang for providing us their model outputs for error analysis. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "stanford natural language inference",
                        293
                    ],
                    [
                        "snli) dataset",
                        331
                    ],
                    [
                        "snli corpus",
                        857
                    ],
                    [
                        "snli dataset",
                        8247
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "decomposable attention model",
                        12
                    ],
                    [
                        "attention",
                        4139
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        11997
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "natural language inference",
                        45
                    ],
                    [
                        "nli",
                        663
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "087337fdad69caaab8ebd8ae68a731c5bf2e8b14-6",
    "doctext": "document: fully convolutional networks for semantic segmentation convolutional networks are powerful visual models that yield hierarchies of features. we show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. our key insight is to build\" fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. we define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. we adapt contemporary classification networks (alexnet, the vgg net, and googlenet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. we then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. our fully convolutional network achieves improved segmentation of pascal voc (30% relative improvement to 67.2% mean iu on 2012), nyudv2, sift flow, and pascal-context, while inference takes one tenth of a second for a typical image. semantic segmentation, convolutional networks, deep learning, transfer learning section: introduction convolutional networks are driving advances in recognition. convnets are not only improving for whole-image classification, but also making progress on local tasks with structured output. these include advances in bounding box object detection, part and keypoint prediction, and local correspondence. the natural next step in the progression from coarse to fine inference is to make a prediction at every pixel. prior approaches have used convnets for semantic segmentation, in which each pixel is labeled with the class of its enclosing object or region, but with shortcomings that this work addresses. we show that fully convolutional networks (fcns) trained end-to-end, pixels-to-pixels on semantic segmentation exceed the previous best results without further machinery. to our knowledge, this is the first work to train fcns end-to-end (1) for pixelwise prediction and (2) from supervised pre-training. fully convolutional versions of existing networks predict dense outputs from arbitrary-sized inputs. both learning and inference are performed whole-image-at-a-time by dense feedforward computation and backpropagation. in-network upsampling layers enable pixelwise prediction and learning in nets with subsampling. this method is efficient, both asymptotically and absolutely, and precludes the need for the complications in other works. patchwise training is common, but lacks the efficiency of fully convolutional training. our approach does not make use of pre-and post-processing complications, including superpixels, proposals, or post-hoc refinement by random fields or local classifiers. our model transfers recent success in classification to dense prediction by reinterpreting classification nets as fully convolutional and fine-tuning from their learned representations. in contrast, previous works have applied small convnets without supervised pre-training. semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where. what can be done to navigate this spectrum from location to semantics? how can local decisions respect global structure? it is not immediately clear that deep networks for image classification yield representations sufficient for accurate, pixelwise recognition. in the conference version of this paper, we cast pre-trained networks into fully convolutional form, and augment them with a skip architecture that takes advantage of the full feature spectrum. the skip architecture fuses the feature hierarchy to combine deep, coarse, semantic information and shallow, fine, appearance information (see section [reference] and figure [reference]). in this light, deep feature hierarchies encode location and semantics in a nonlinear local-to-global pyramid. this journal paper extends our earlier work through further tuning, analysis, and more results. alternative choices, ablations, and implementation details better cover the space of fcns. tuning optimization leads to more accurate networks and a means to learn skip architectures all-at-once instead of in stages. experiments that mask foreground and background investigate the role of context and shape. results on the object and scene labeling of pascal-context reinforce merging object segmentation and scene parsing as unified pixelwise prediction. in the next section, we review related work on deep classification nets, fcns, recent approaches to semantic segmentation using convnets, and extensions to fcns. the following sections explain fcn design, introduce our architecture with in-network upsampling and skip layers, and describe our experimental framework. next, we demonstrate improved accuracy on pascal voc 2011-2, nyudv2, sift flow, and pascal-context. finally, we analyze design choices, examine what cues can be learned by an fcn, and calculate recognition bounds for semantic segmentation. section: related work our approach draws on recent successes of deep nets for image classification and transfer learning. transfer was first demonstrated on various visual recognition tasks, then on detection, and on both instance and semantic segmentation in hybrid proposal-classifier models. we now re-architect and fine-tune classification nets to direct, dense prediction of semantic segmentation. we chart the space of fcns and relate prior models both historical and recent. fully convolutional networks to our knowledge, the idea of extending a convnet to arbitrary-sized inputs first appeared in matan et al., which extended the classic lenet to recognize strings of digits. because their net was limited to one-dimensional input strings, matan et al. used viterbi decoding to obtain their outputs. wolf and platt expand convnet outputs to 2-dimensional maps of detection scores for the four corners of postal address blocks. both of these historical works do inference and learning fully convolutionally for detection. ning et al. define a convnet for coarse multiclass segmentation of c. elegans tissues with fully convolutional inference. fully convolutional computation has also been exploited in the present era of many-layered nets. sliding window detection by sermanet et al., semantic segmentation by pinheiro and collobert, and image restoration by eigen et al. do fully convolutional inference. fully convolutional training is rare, but used effectively by tompson et al. to learn an end-to-end part detector and spatial model for pose estimation, although they do not exposit on or analyze this method. dense prediction with convnets several recent works have applied convnets to dense prediction problems, including semantic segmentation by ning et al., farabet et al., and pinheiro and collobert; boundary prediction for electron microscopy by ciresan et al. and for natural images by a hybrid convnet/ nearest neighbor model by ganin and lempitsky; and image restoration and depth estimation by eigen et al.. common elements of these approaches include small models restricting capacity and receptive fields; patchwise training; refinement by superpixel projection, random field regularization, filtering, or local classification;\" interlacing\" to obtain dense output; multi-scale pyramid processing; saturating nonlinearities; and ensembles, whereas our method does without this machinery. however, we do study patchwise training (section [reference]) and\" shift-and-stitch\" dense output (section [reference]) from the perspective of fcns. we also discuss in-network upsampling (section [reference]), of which the fully connected prediction by eigen et al. is a special case. unlike these existing methods, we adapt and extend deep classification architectures, using image classification as supervised pre-training, and fine-tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths. hariharan et al. and gupta et al. likewise adapt deep classification nets to semantic segmentation, but do so in hybrid proposal-classifier models. these approaches fine-tune an r-cnn system by sampling bounding boxes and/ or region proposals for detection, semantic segmentation, and instance segmentation. neither method is learned end-to-end. they achieve the previous best segmentation results on pascal voc and nyudv2 respectively, so we directly compare our standalone, end-to-end fcn to their semantic segmentation results in section [reference]. combining feature hierarchies we fuse features across layers to define a nonlinear local-to-global representation that we tune end-to-end. the laplacian pyramid is a classic multi-scale representation made of fixed smoothing and differencing. the jet of koenderink and van doorn is a rich, local feature defined by compositions of partial derivatives. in the context of deep networks, sermanet et al. fuse intermediate layers but discard resolution in doing so. in contemporary work hariharan et al. and mostajabi et al. also fuse multiple layers but do not learn end-to-end and rely on fixed bottom-up grouping. fcn extensions following the conference version of this paper, fcns have been extended to new tasks and data. tasks include region proposals, contour detection, depth regression, optical flow, and weakly-supervised semantic segmentation. in addition, new works have improved the fcns presented here to further advance the state-of-the-art in semantic segmentation. the deeplab models raise output resolution by dilated convolution and dense crf inference. the joint crfasrnn model is an end-to-end integration of the crf for further improvement. parsenet normalizes features for fusion and captures context with global pooling. the\" deconvolutional network\" approach of restores resolution by proposals, stacks of learned deconvolution, and unpooling. u-net combines skip layers and learned deconvolution for pixel labeling of microscopy images. the dilation architecture of makes thorough use of dilated convolution for pixel-precise output without a random field or skip layers. section: fully convolutional networks each layer output in a convnet is a three-dimensional array of size, where and are spatial dimensions, and is the feature or channel dimension. the first layer is the image, with pixel size, and channels. locations in higher layers correspond to the locations in the image they are path-connected to, which are called their receptive fields. convnets are inherently translation invariant. their basic components (convolution, pooling, and activation functions) operate on local input regions, and depend only on relative spatial coordinates. writing for the data vector at location in a particular layer, and for the following layer, these functions compute outputs by where is called the kernel size, is the stride or subsampling factor, and determines the layer type: a matrix multiplication for convolution or average pooling, a spatial max for max pooling, or an elementwise nonlinearity for an activation function, and so on for other types of layers. this functional form is maintained under composition, with kernel size and stride obeying the transformation rule while a general net computes a general nonlinear function, a net with only layers of this form computes a nonlinear filter, which we call a deep filter or fully convolutional network. an fcn naturally operates on an input of any size, and produces an output of corresponding (possibly resampled) spatial dimensions. a real-valued loss function composed with an fcn defines a task. if the loss function is a sum over the spatial dimensions of the final layer,, its parameter gradient will be a sum over the parameter gradients of each of its spatial components. thus stochastic gradient descent on computed on whole images will be the same as stochastic gradient descent on, taking all of the final layer receptive fields as a minibatch. when these receptive fields overlap significantly, both feedforward computation and backpropagation are much more efficient when computed layer-by-layer over an entire image instead of independently patch-by-patch. we next explain how to convert classification nets into fully convolutional nets that produce coarse output maps. for pixelwise prediction, we need to connect these coarse outputs back to the pixels. section [reference] describes a trick used for this purpose (e.g., by\" fast scanning\"). we explain this trick in terms of network modification. as an efficient, effective alternative, we upsample in section [reference], reusing our implementation of convolution. in section [reference] we consider training by patchwise sampling, and give evidence in section [reference] that our whole image training is faster and equally effective. subsection: adapting classifiers for dense prediction typical recognition nets, including lenet, alexnet, and its deeper successors, ostensibly take fixed-sized inputs and produce non-spatial outputs. the fully connected layers of these nets have fixed dimensions and throw away spatial coordinates. however, fully connected layers can also be viewed as convolutions with kernels that cover their entire input regions. doing so casts these nets into fully convolutional networks that take input of any size and make spatial output maps. this transformation is illustrated in figure [reference]. furthermore, while the resulting maps are equivalent to the evaluation of the original net on particular input patches, the computation is highly amortized over the overlapping regions of those patches. for example, while alexnet takes ms (on a typical gpu) to infer the classification scores of a image, the fully convolutional net takes ms to produce a grid of outputs from a image, which is more than times faster than the na\u00efve approach. the spatial output maps of these convolutionalized models make them a natural choice for dense problems like semantic segmentation. with ground truth available at every output cell, both the forward and backward passes are straightforward, and both take advantage of the inherent computational efficiency (and aggressive optimization) of convolution. the corresponding backward times for the alexnet example are ms for a single image and ms for a fully convolutional output map, resulting in a speedup similar to that of the forward pass. while our reinterpretation of classification nets as fully convolutional yields output maps for inputs of any size, the output dimensions are typically reduced by subsampling. the classification nets subsample to keep filters small and computational requirements reasonable. this coarsens the output of a fully convolutional version of these nets, reducing it from the size of the input by a factor equal to the pixel stride of the receptive fields of the output units. subsection: shift-and-stitch is filter dilation dense predictions can be obtained from coarse outputs by stitching together outputs from shifted versions of the input. if the output is downsampled by a factor of, shift the input pixels to the right and pixels down, once for every such that. process each of these inputs, and interlace the outputs so that the predictions correspond to the pixels at the centers of their receptive fields. although this transformation na\u00efvely increases the cost by a factor of, there is a well-known trick for efficiently producing identical results. (this trick is also used in the algorithme\u00e0 trous for wavelet transforms and related to the noble identities from signal processing.) consider a layer (convolution or pooling) with input stride, and a subsequent convolution layer with filter weights (eliding the irrelevant feature dimensions). setting the earlier layer's input stride to one upsamples its output by a factor of. however, convolving the original filter with the upsampled output does not produce the same result as shift-and-stitch, because the original filter only sees a reduced portion of its (now upsampled) input. to produce the same result, dilate (or\" rarefy\") the filter by forming (with and zero-based). reproducing the full net output of shift-and-stitch involves repeating this filter enlargement layer-by-layer until all subsampling is removed. (in practice, this can be done efficiently by processing subsampled versions of the upsampled input.) simply decreasing subsampling within a net is a tradeoff: the filters see finer information, but have smaller receptive fields and take longer to compute. this dilation trick is another kind of tradeoff: the output is denser without decreasing the receptive field sizes of the filters, but the filters are prohibited from accessing information at a finer scale than their original design. although we have done preliminary experiments with dilation, we do not use it in our model. we find learning through upsampling, as described in the next section, to be effective and efficient, especially when combined with the skip layer fusion described later on. for further detail regarding dilation, refer to the dilated fcn of. subsection: upsampling is (fractionally strided) convolution another way to connect coarse outputs to dense pixels is interpolation. for instance, simple bilinear interpolation computes each output from the nearest four inputs by a linear map that depends only on the relative positions of the input and output cells: where is the upsampling factor, and denotes the fractional part. in a sense, upsampling with factor is convolution with a fractional input stride of. so long as is integral, it's natural to implement upsampling through\" backward convolution\" by reversing the forward and backward passes of more typical input-strided convolution. thus upsampling is performed in-network for end-to-end learning by backpropagation from the pixelwise loss. per their use in deconvolution networks (esp.), these (convolution) layers are sometimes referred to as deconvolution layers. note that the convolution filter in such a layer need not be fixed (e.g., to bilinear upsampling), but can be learned. a stack of deconvolution layers and activation functions can even learn a nonlinear upsampling. in our experiments, we find that in-network upsampling is fast and effective for learning dense prediction. subsection: patchwise training is loss sampling in stochastic optimization, gradient computation is driven by the training distribution. both patchwise training and fully convolutional training can be made to produce any distribution of the inputs, although their relative computational efficiency depends on overlap and minibatch size. whole image fully convolutional training is identical to patchwise training where each batch consists of all the receptive fields of the output units for an image (or collection of images). while this is more efficient than uniform sampling of patches, it reduces the number of possible batches. however, random sampling of patches within an image may be easily recovered. restricting the loss to a randomly sampled subset of its spatial terms (or, equivalently applying a dropconnect mask between the output and the loss) excludes patches from the gradient. if the kept patches still have significant overlap, fully convolutional computation will still speed up training. if gradients are accumulated over multiple backward passes, batches can include patches from several images. if inputs are shifted by values up to the output stride, random selection of all possible patches is possible even though the output units lie on a fixed, strided grid. sampling in patchwise training can correct class imbalance and mitigate the spatial correlation of dense patches. in fully convolutional training, class balance can also be achieved by weighting the loss, and loss sampling can be used to address spatial correlation. we explore training with sampling in section [reference], and do not find that it yields faster or better convergence for dense prediction. whole image training is effective and efficient. section: segmentation architecture we cast ilsvrc classifiers into fcns and augment them for dense prediction with in-network upsampling and a pixelwise loss. we train for segmentation by fine-tuning. next, we add skips between layers to fuse coarse, semantic and local, appearance information. this skip architecture is learned end-to-end to refine the semantics and spatial precision of the output. for this investigation, we train and validate on the pascal voc 2011 segmentation challenge. we train with a per-pixel softmax loss and validate with the standard metric of mean pixel intersection over union, with the mean taken over all classes, including background. the training ignores pixels that are masked out (as ambiguous or difficult) in the ground truth. subsection: from classifier to dense fcn we begin by convolutionalizing proven classification architectures as in section [reference]. we consider the alexnet architecture that won ilsvrc12, as well as the vgg nets and the googlenet which did exceptionally well in ilsvrc14. we pick the vgg 16-layer net, which we found to be equivalent to the 19-layer net on this task. for googlenet, we use only the final loss layer, and improve performance by discarding the final average pooling layer. we decapitate each net by discarding the final classifier layer, and convert all fully connected layers to convolutions. we append a convolution with channel dimension to predict scores for each of the pascal classes (including background) at each of the coarse output locations, followed by a (backward) convolution layer to bilinearly upsample the coarse outputs to pixelwise outputs as described in section [reference]. table [reference] compares the preliminary validation results along with the basic characteristics of each net. we report the best results achieved after convergence at a fixed learning rate (at least 175 epochs). our training for this comparison follows the practices for classification networks. we train by sgd with momentum. gradients are accumulated over 20 images. we set fixed learning rates of,, and for fcn-alexnet, fcn-vgg16, and fcn-googlenet, respectively, chosen by line search. we use momentum, weight decay of or, and doubled learning rate for biases. we zero-initialize the class scoring layer, as random initialization yielded neither better performance nor faster convergence. dropout is included where used in the original classifier nets (however, training without it made little to no difference). fine-tuning from classification to segmentation gives reasonable predictions from each net. even the worst model achieved of the previous best performance. fcn-vgg16 already appears to be better than previous methods at 56.0 mean iu on val, compared to 52.6 on test. although vgg and googlenet are similarly accurate as classifiers, our fcn-googlenet did not match fcn-vgg16. we select fcn-vgg16 as our base network. subsection: image-to-image learning batch size pixel acc. mean acc. mean iu f.w. iu the image-to-image learning setting includes high effective batch size and correlated inputs. this optimization requires some attention to properly tune fcns. we begin with the loss. we do not normalize the loss, so that every pixel has the same weight regardless of the batch and image dimensions. thus we use a small learning rate since the loss is summed spatially over all pixels. we consider two regimes for batch size. in the first, gradients are accumulated over 20 images. accumulation reduces the memory required and respects the different dimensions of each input by reshaping the network. we picked this batch size empirically to result in reasonable convergence. learning in this way is similar to standard classification training: each minibatch contains several images and has a varied distribution of class labels. the nets compared in table [reference] are optimized in this fashion. however, batching is not the only way to do image-wise learning. in the second regime, batch size one is used for online learning. properly tuned, online learning achieves higher accuracy and faster convergence in both number of iterations and wall clock time. additionally, we try a higher momentum of, which increases the weight on recent gradients in a similar way to batching. see table [reference] for the comparison of accumulation, online, and high momentum or\" heavy\" learning (discussed further in section [reference]). subsection: combining what and where we define a new fully convolutional net for segmentation that combines layers of the feature hierarchy and refines the spatial precision of the output. see figure [reference]. while fully convolutionalized classifiers fine-tuned to semantic segmentation both recognize and localize, as shown in section [reference], these networks can be improved to make direct use of shallower, more local features. even though these base networks score highly on the standard metrics, their output is dissatisfyingly coarse (see figure [reference]). the stride of the network prediction limits the scale of detail in the upsampled output. we address this by adding skips that fuse layer outputs, in particular to include shallower layers with finer strides in prediction. this turns a line topology into a dag: edges skip ahead from shallower to deeper layers. it is natural to make more local predictions from shallower layers since their receptive fields are smaller and see fewer pixels. once augmented with skips, the network makes and fuses predictions from several streams that are learned jointly and end-to-end. combining fine layers and coarse layers lets the model make local predictions that respect global structure. this crossing of layers and resolutions is a learned, nonlinear counterpart to the multi-scale representation of the laplacian pyramid. by analogy to the jet of koenderick and van doorn, we call our feature hierarchy the deep jet. layer fusion is essentially an elementwise operation. however, the correspondence of elements across layers is complicated by resampling and padding. thus, in general, layers to be fused must be aligned by scaling and cropping. we bring two layers into scale agreement by upsampling the lower-resolution layer, doing so in-network as explained in section [reference]. cropping removes any portion of the upsampled layer which extends beyond the other layer due to padding. this results in layers of equal dimensions in exact alignment. the offset of the cropped region depends on the resampling and padding parameters of all intermediate layers. determining the crop that results in exact correspondence can be intricate, but it follows automatically from the network definition (and we include code for it in caffe). having spatially aligned the layers, we next pick a fusion operation. we fuse features by concatenation, and immediately follow with classification by a\" score layer\" consisting of a convolution. rather than storing concatenated features in memory, we commute the concatenation and subsequent classification (as both are linear). thus, our skips are implemented by first scoring each layer to be fused by convolution, carrying out any necessary interpolation and alignment, and then summing the scores. we also considered max fusion, but found learning to be difficult due to gradient switching. the score layer parameters are zero-initialized when a skip is added, so that they do not interfere with existing predictions of other streams. once all layers have been fused, the final prediction is then upsampled back to image resolution. skip architectures for segmentation we define a skip architecture to extend fcn-vgg16 to a three-stream net with eight pixel stride shown in figure [reference]. adding a skip from pool4 halves the stride by scoring from this stride sixteen layer. the interpolation layer of the skip is initialized to bilinear interpolation, but is not fixed so that it can be learned as described in section [reference]. we call this two-stream net fcn-16s, and likewise define fcn-8s by adding a further skip from pool3 to make stride eight predictions. (note that predicting at stride eight does not significantly limit the maximum achievable mean iu; see section [reference].) we experiment with both staged training and all-at-once training. in the staged version, we learn the single-stream fcn-32s, then upgrade to the two-stream fcn-16s and continue learning, and finally upgrade to the three-stream fcn-8s and finish learning. at each stage the net is learned end-to-end, initialized with the parameters of the earlier net. the learning rate is dropped from fcn-32s to fcn-16s and more from fcn-16s to fcn-8s, which we found to be necessary for continued improvements. learning all-at-once rather than in stages gives nearly equivalent results, while training is faster and less tedious. however, disparate feature scales make na\u00efve training prone to divergence. to remedy this we scale each stream by a fixed constant, for a similar in-network effect to the staged learning rate adjustments. these constants are picked to approximately equalize average feature norms across streams. (other normalization schemes should have similar effect.) with fcn-16s validation score improves to 65.0 mean iu, and fcn-8s brings a minor improvement to 65.5. at this point our fusion improvements have met diminishing returns, so we do not continue fusing even shallower layers. to identify the contribution of the skips we compare scoring from the intermediate layers in isolation, which results in poor performance, or dropping the learning rate without adding skips, which gives negligible improvement in score without refining the visual quality of output. all skip comparisons are reported in table [reference]. figure [reference] shows the progressively finer structure of the output. pixel acc. mean acc. mean iu f.w. iu subsection: experimental framework fine-tuning we fine-tune all layers by backpropagation through the whole net. fine-tuning the output classifier alone yields only 73% of the full fine-tuning performance as compared in table [reference]. fine-tuning in stages takes 36 hours on a single gpu. learning fcn-8s all-at-once takes half the time to reach comparable accuracy. training from scratch gives substantially lower accuracy. more training data the pascal voc 2011 segmentation training set labels 1, 112 images. hariharan et al. collected labels for a larger set of 8, 498 pascal training images, which was used to train the previous best system, sds. this training data improves the fcn-32s validation score from 57.7 to 63.6 mean iu and improves the fcn-alexnet score from 39.8 to 48.0 mean iu. loss the per-pixel, unnormalized softmax loss is a natural choice for segmenting images of any size into disjoint classes, so we train our nets with it. the softmax operation induces competition between classes and promotes the most confident prediction, but it is not clear that this is necessary or helpful. for comparison, we train with the sigmoid cross-entropy loss and find that it gives similar results, even though it normalizes each class prediction independently. patch sampling as explained in section [reference], our whole image training effectively batches each image into a regular grid of large, overlapping patches. by contrast, prior work randomly samples patches over a full dataset, potentially resulting in higher variance batches that may accelerate convergence. we study this tradeoff by spatially sampling the loss in the manner described earlier, making an independent choice to ignore each final layer cell with some probability. to avoid changing the effective batch size, we simultaneously increase the number of images per batch by a factor. note that due to the efficiency of convolution, this form of rejection sampling is still faster than patchwise training for large enough values of (e.g., at least for according to the numbers in section [reference]). figure [reference] shows the effect of this form of sampling on convergence. we find that sampling does not have a significant effect on convergence rate compared to whole image training, but takes significantly more time due to the larger number of images that need to be considered per batch. we therefore choose unsampled, whole image training in our other experiments. class balancing fully convolutional training can balance classes by weighting or sampling the loss. although our labels are mildly unbalanced (about are background), we find class balancing unnecessary. dense prediction the scores are upsampled to the input dimensions by backward convolution layers within the net. final layer backward convolution weights are fixed to bilinear interpolation, while intermediate upsampling layers are initialized to bilinear interpolation, and then learned. this simple, end-to-end method is accurate and fast. augmentation we tried augmenting the training data by randomly mirroring and\" jittering\" the images by translating them up to 32 pixels (the coarsest scale of prediction) in each direction. this yielded no noticeable improvement. implementation all models are trained and tested with caffe on a single nvidia titan x. our models and code are publicly available at. section: results we test our fcn on semantic segmentation and scene parsing, exploring pascal voc, nyudv2, sift flow, and pascal-context. although these tasks have historically distinguished between objects and regions, we treat both uniformly as pixel prediction. we evaluate our fcn skip architecture on each of these datasets, and then extend it to multi-modal input for nyudv2 and multi-task prediction for the semantic and geometric labels of sift flow. all experiments follow the same network architecture and optimization settings decided on in section [reference]. metrics we report metrics from common semantic segmentation and scene parsing evaluations that are variations on pixel accuracy and region intersection over union (iu): pixel accuracy: mean accuraccy: mean iu: frequency weighted iu: where is the number of pixels of class predicted to belong to class, there are different classes, and is the total number of pixels of class. pascal voc table [reference] gives the performance of our fcn-8s on the test sets of pascal voc 2011 and 2012, and compares it to the previous best, sds, and the well-known r-cnn. we achieve the best results on mean iu by 30% relative. inference time is reduced (convnet only, ignoring proposals and refinement) or (overall). nyudv2 is an rgb-d dataset collected using the microsoft kinect. it has 1, 449 rgb-d images, with pixelwise labels that have been coalesced into a 40 class semantic segmentation task by gupta et al.. we report results on the standard split of 795 training images and 654 testing images. table [reference] gives the performance of several net variations. first we train our unmodified coarse model (fcn-32s) on rgb images. to add depth information, we train on a model upgraded to take four-channel rgb-d input (early fusion). this provides little benefit, perhaps due to similar number of parameters or the difficulty of propagating meaningful gradients all the way through the net. following the success of gupta et al., we try the three-dimensional hha encoding of depth and train a net on just this information. to effectively combine color and depth, we define a\" late fusion\" of rgb and hha that averages the final layer scores from both nets and learn the resulting two-stream net end-to-end. this late fusion rgb-hha net is the most accurate. sift flow is a dataset of 2, 688 images with pixel labels for 33 semantic classes (\" bridge\",\" mountain\",\" sun\"), as well as three geometric classes (\" horizontal\",\" vertical\", and\" sky\"). an fcn can naturally learn a joint representation that simultaneously predicts both types of labels. we learn a two-headed version of fcn-32/ 16/ 8s with semantic and geometric prediction layers and losses. this net performs as well on both tasks as two independently trained nets, while learning and inference are essentially as fast as each independent net by itself. the results in table [reference], computed on the standard split into 2, 488 training and 200 test images, show better performance on both tasks. pixel acc. mean acc. mean iu f.w. iu pixel acc. mean acc. mean iu f.w. iu geom. acc. pixel acc. mean acc. mean iu f.w. iu pascal-context provides whole scene annotations of pascal voc 2010. while there are 400+ classes, we follow the 59 class task defined by that picks the most frequent classes. we train and evaluate on the training and val sets respectively. in table [reference] we compare to the previous best result on this task. fcn-8s scores 39.1 mean iu for a relative improvement of more than 10%. section: analysis we examine the learning and inference of fully convolutional networks. masking experiments investigate the role of context and shape by reducing the input to only foreground, only background, or shape alone. defining a\" null\" background model checks the necessity of learning a background classifier for semantic segmentation. we detail an approximation between momentum and batch size to further tune whole image learning. finally, we measure bounds on task accuracy for given output resolutions to show there is still much to improve. subsection: cues given the large receptive field size of an fcn, it is natural to wonder about the relative importance of foreground and background pixels in the prediction. is foreground appearance sufficient for inference, or does the context influence the output? conversely, can a network learn to recognize a class by its shape and context alone? masking to explore these issues we experiment with masked versions of the standard pascal voc segmentation challenge. we both mask input to networks trained on normal pascal, and learn new networks on the masked pascal. see table [reference] for masked results. masking the foreground at inference time is catastrophic. however, masking the foreground during learning yields a network capable of recognizing object segments without observing a single pixel of the labeled class. masking the background has little effect overall but does lead to class confusion in certain cases. when the background is masked during both learning and inference, the network unsurprisingly achieves nearly perfect background accuracy; however certain classes are more confused. all-in-all this suggests that fcns do incorporate context even though decisions are driven by foreground pixels. to separate the contribution of shape, we learn a net restricted to the simple input of foreground/ background masks. the accuracy in this shape-only condition is lower than when only the foreground is masked, suggesting that the net is capable of learning context to boost recognition. nonetheless, it is surprisingly accurate. see figure [reference]. background modeling it is standard in detection and semantic segmentation to have a background model. this model usually takes the same form as the models for the classes of interest, but is supervised by negative instances. in our experiments we have followed the same approach, learning parameters to score all classes including background. is this actually necessary, or do class models suffice? to investigate, we define a net with a\" null\" background model that gives a constant score of zero. instead of training with the softmax loss, which induces competition by normalizing across classes, we train with the sigmoid cross-entropy loss, which independently normalizes each score. for inference each pixel is assigned the highest scoring class. in all other respects the experiment is identical to our fcn-32s on pascal voc. the null background net scores 1 point lower than the reference fcn-32s and a control fcn-32s trained on all classes including background with the sigmoid cross-entropy loss. to put this drop in perspective, note that discarding the background model in this way reduces the total number of parameters by less than 0.1%. nonetheless, this result suggests that learning a dedicated background model for semantic segmentation is not vital. image ground truth output input subsection: momentum and batch size in comparing optimization schemes for fcns, we find that\" heavy\" online learning with high momentum trains more accurate models in less wall clock time (see section [reference]). here we detail a relationship between momentum and batch size that motivates heavy learning. by writing the updates computed by gradient accumulation as a non-recursive sum, we will see that momentum and batch size can be approximately traded off, which suggests alternative training parameters. let be the step taken by minibatch sgd with momentum at time, where is the loss for example and parameters, is the momentum, is the batch size, and is the learning rate. expanding this recurrence as an infinite sum with geometric coefficients, we have in other words, each example is included in the sum with coefficient, where the index orders the examples from most recently considered to least recently considered. approximating this expression by dropping the floor, we see that learning with momentum and batch size appears to be similar to learning with momentum and batch size if. note that this is not an exact equivalence: a smaller batch size results in more frequent weight updates, and may make more learning progress for the same number of gradient computations. for typical fcn values of momentum and a batch size of 20 images, an approximately equivalent training regime uses momentum and a batch size of one, resulting in online learning. in practice, we find that online learning works well and yields better fcn models in less wall clock time. subsection: upper bounds on iu fcns achieve good performance on the mean iu segmentation metric even with spatially coarse semantic prediction. to better understand this metric and the limits of this approach with respect to it, we compute approximate upper bounds on performance with prediction at various resolutions. we do this by downsampling ground truth images and then upsampling back to simulate the best results obtainable with a particular downsampling factor. the following table gives the mean iu on a subset of pascal 2011 val for various downsampling factors. pixel-perfect prediction is clearly not necessary to achieve mean iu well above state-of-the-art, and, conversely, mean iu is a not a good measure of fine-scale accuracy. the gaps between oracle and state-of-the-art accuracy at every stride suggest that recognition and not resolution is the bottleneck for this metric. section: conclusion fully convolutional networks are a rich class of models that address many pixelwise tasks. fcns for semantic segmentation dramatically improve accuracy by transferring pre-trained classifier weights, fusing different layer representations, and learning end-to-end on whole images. end-to-end, pixel-to-pixel operation simultaneously simplifies and speeds up learning and inference. all code for this paper is open source in caffe, and all models are freely available in the caffe model zoo. further works have demonstrated the generality of fully convolutional networks for a variety of image-to-image tasks. section: acknowledgements this work was supported in part by darpa's msee and smisc programs, nsf awards iis-1427425, iis-1212798, iis-1116411, and the nsf grfp, toyota, and the berkeley vision and learning center. we gratefully acknowledge nvidia for gpu donation. we thank bharath hariharan and saurabh gupta for their advice and dataset tools. we thank sergio guadarrama for reproducing googlenet in caffe. we thank jitendra malik for his helpful comments. thanks to wei liu for pointing out an issue wth our sift flow mean iu computation and an error in our frequency weighted mean iu formula. bibliography: references",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "fully convolutional networks",
                        10
                    ],
                    [
                        "classification networks",
                        662
                    ],
                    [
                        "fully convolutional versions",
                        2285
                    ],
                    [
                        "fcn",
                        5207
                    ],
                    [
                        "stacks of learned deconvolution",
                        10108
                    ],
                    [
                        "real-valued loss function",
                        11812
                    ],
                    [
                        "classification architectures",
                        21154
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "val",
                        23044
                    ],
                    [
                        "region intersection over union",
                        34260
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semantic segmentation",
                        43
                    ]
                ]
            ]
        },
        {
            "Material": [],
            "Method": [
                [
                    [
                        "fully convolutional networks",
                        10
                    ],
                    [
                        "classification networks",
                        662
                    ],
                    [
                        "fully convolutional versions",
                        2285
                    ],
                    [
                        "fcn",
                        5207
                    ],
                    [
                        "stacks of learned deconvolution",
                        10108
                    ],
                    [
                        "real-valued loss function",
                        11812
                    ],
                    [
                        "classification architectures",
                        21154
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "ms",
                        13911
                    ],
                    [
                        "wall clock time",
                        24453
                    ],
                    [
                        "inference time",
                        34739
                    ]
                ]
            ],
            "Task": []
        },
        {
            "Material": [
                [
                    [
                        "pascal-context",
                        1194
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "fcns",
                        2025
                    ],
                    [
                        "fcn-8s",
                        28339
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "mean iu",
                        1153
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semantic segmentation",
                        43
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "pascal voc",
                        1107
                    ],
                    [
                        "pascal classes",
                        21768
                    ],
                    [
                        "pascal training images",
                        30760
                    ],
                    [
                        "2012",
                        34608
                    ],
                    [
                        "normal pascal",
                        38159
                    ],
                    [
                        "masked pascal",
                        38204
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "fully convolutional networks",
                        10
                    ],
                    [
                        "classification networks",
                        662
                    ],
                    [
                        "fully convolutional versions",
                        2285
                    ],
                    [
                        "fcn",
                        5207
                    ],
                    [
                        "stacks of learned deconvolution",
                        10108
                    ],
                    [
                        "real-valued loss function",
                        11812
                    ],
                    [
                        "classification architectures",
                        21154
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "mean iu",
                        1153
                    ],
                    [
                        "mean pixel intersection over union",
                        20882
                    ],
                    [
                        "mean acc",
                        23283
                    ],
                    [
                        "iu",
                        23306
                    ],
                    [
                        "mean accuraccy",
                        34313
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semantic segmentation",
                        43
                    ]
                ]
            ]
        },
        {
            "Material": [],
            "Method": [
                [
                    [
                        "fully convolutional networks",
                        10
                    ],
                    [
                        "classification networks",
                        662
                    ],
                    [
                        "fully convolutional versions",
                        2285
                    ],
                    [
                        "fcn",
                        5207
                    ],
                    [
                        "stacks of learned deconvolution",
                        10108
                    ],
                    [
                        "real-valued loss function",
                        11812
                    ],
                    [
                        "classification architectures",
                        21154
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "mean iu",
                        1153
                    ],
                    [
                        "mean pixel intersection over union",
                        20882
                    ],
                    [
                        "mean acc",
                        23283
                    ],
                    [
                        "iu",
                        23306
                    ],
                    [
                        "mean accuraccy",
                        34313
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "segmentation task",
                        823
                    ],
                    [
                        "segmentation",
                        1091
                    ],
                    [
                        "merging object segmentation",
                        4636
                    ],
                    [
                        "instance segmentation",
                        8522
                    ],
                    [
                        "augmentation",
                        33190
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "088b649415378c4dde666f614ba24d08a36b6652-7",
    "doctext": "document: long short-term memory-networks for machine reading in this paper we address the question of how to render sequence-level networks better at handling structured input. we propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. the reader extends the long short-term memory architecture with a memory network in place of a single memory cell. this enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. the system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art. section: introduction how can a sequence-level network induce relations which are presumed latent during text processing? how can a recurrent network attentively memorize longer sequences in a way that humans do? in this paper we design a machine reader that automatically learns to understand text. the term machine reading is related to a wide range of tasks from answering reading comprehension questions, to fact and relation extraction, ontology learning, and textual entailment. rather than focusing on a specific task, we develop a general-purpose reading simulator, drawing inspiration from human language processing and the fact language comprehension is incremental with readers continuously extracting the meaning of utterances on a word-by-word basis. in order to understand texts, our machine reader should provide facilities for extracting and representing meaning from natural language text, storing meanings internally, and working with stored meanings to derive further consequences. ideally, such a system should be robust, open-domain, and degrade gracefully in the presence of semantic representations which may be incomplete, inaccurate, or incomprehensible. it would also be desirable to simulate the behavior of english speakers who process text sequentially, from left to right, fixating nearly every word while they read and creating partial representations for sentence prefixes. language modeling tools such as recurrent neural networks (rnn) bode well with human reading behavior. rnns treat each sentence as a sequence of words and recursively compose each word with its previous memory, until the meaning of the whole sentence has been derived. in practice, however, sequence-level networks are met with at least three challenges. the first one concerns model training problems associated with vanishing and exploding gradients, which can be partially ameliorated with gated activation functions, such as the long short-term memory (lstm), and gradient clipping. the second issue relates to memory compression problems. as the input sequence gets compressed and blended into a single dense vector, sufficiently large memory capacity is required to store past information. as a result, the network generalizes poorly to long sequences while wasting memory on shorter ones. finally, it should be acknowledged that sequence-level networks lack a mechanism for handling the structure of the input. this imposes an inductive bias which is at odds with the fact that language has inherent structure. in this paper, we develop a text processing system which addresses these limitations while maintaining the incremental, generative property of a recurrent language model. the the the the fbi fbi the the fbi fbi is is the the fbi fbi is is chasing chasing the the fbi fbi is is chasing chasing a a the the fbi fbi is is chasing chasing a a criminal criminal the the fbi fbi is is chasing chasing a a criminal criminal on on the the fbi fbi is is chasing chasing a a criminal criminal on on the the the the fbi fbi is is chasing chasing a a criminal criminal on on the the run run recent attempts to render neural networks more structure aware have seen the incorporation of external memories in the context of recurrent neural networks. the idea is to use multiple memory slots outside the recurrence to piece-wise store representations of the input; read and write operations for each slot can be modeled as an attention mechanism with a recurrent controller. we also leverage memory and attention to empower a recurrent network with stronger memorization capability and more importantly the ability to discover relations among tokens. this is realized by inserting a memory network module in the update of a recurrent network together with attention for memory addressing. the attention acts as a weak inductive module discovering relations between input tokens, and is trained without direct supervision. as a point of departure from previous work, the memory network we employ is internal to the recurrence, thus strengthening the interaction of the two and leading to a representation learner which is able to reason over shallow structures. the resulting model, which we term long short-term memory-network (lstmn), is a reading simulator that can be used for sequence processing tasks. figure [reference] illustrates the reading behavior of the lstmn. the model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed. as a result, the model induces undirected relations among tokens as an intermediate step of learning representations. we validate the performance of the lstmn in language modeling, sentiment analysis, and natural language inference. in all cases, we train lstmn models end-to-end with task-specific supervision signals, achieving performance comparable or better to state-of-the-art models and superior to vanilla lstms. section: related work our machine reader is a recurrent neural network exhibiting two important properties: it is incremental, simulating human behavior, and performs shallow structure reasoning over input streams. recurrent neural network (rnns) have been successfully applied to various sequence modeling and sequence-to-sequence transduction tasks. the latter have assumed several guises in the literature such as machine translation, sentence compression, and reading comprehension. a key contributing factor to their success has been the ability to handle well-known problems with exploding or vanishing gradients, leading to models with gated activation functions, and more advanced architectures that enhance the information flow within the network. a remaining practical bottleneck for rnns is memory compression: since the inputs are recursively combined into a single memory representation which is typically too small in terms of parameters, it becomes difficult to accurately memorize sequences. in the encoder-decoder architecture, this problem can be sidestepped with an attention mechanism which learns soft alignments between the decoding states and the encoded memories. in our model, memory and attention are added within a sequence encoder allowing the network to uncover lexical relations between tokens. the idea of introducing a structural bias to neural models is by no means new. for example, it is reflected in the work of socher-etal:2013:emnlp who apply recursive neural networks for learning natural language representations. in the context of recurrent neural networks, efforts to build modular, structured neural models date back to das1992learning who connect a recurrent neural network with an external memory stack for learning context free grammars. recently, weston2014memory propose memory networks to explicitly segregate memory storage from the computation of neural networks in general. their model is trained end-to-end with a memory addressing mechanism closely related to soft attention and has been applied to machine translation. grefenstette2015learning define a set of differentiable data structures (stacks, queues, and dequeues) as memories controlled by a recurrent neural network. ke2016memory combine the lstm with an external memory block component which interacts with its hidden state. kumar2015ask employ a structured neural network with episodic memory modules for natural language and also visual question answering. similar to the above work, we leverage memory and attention in a recurrent neural network for inducing relations between tokens as a module in a larger network responsible for representation learning. as a property of soft attention, all intermediate relations we aim to capture are soft and differentiable. this is in contrast to shift-reduce type neural models where the intermediate decisions are hard and induction is more difficult. finally, note that our model captures undirected lexical relations and is thus distinct from work on dependency grammar induction where the learned head-modifier relations are directed. section: the machine reader in this section we present our machine reader which is designed to process structured input while retaining the incrementality of a recurrent neural network. the core of our model is a long short-term memory (lstm) unit with an extended memory tape that explicitly simulates the human memory span. the model performs implicit relation analysis between tokens with an attention-based memory addressing mechanism at every time step. in the following, we first review the standard long short-term memory and then describe our model. subsection: long short-term memory a long short-term memory (lstm) recurrent neural network processes a variable-length sequence by incrementally adding new content into a single memory slot, with gates controlling the extent to which new content should be memorized, old content should be erased, and current content should be exposed. at time step, the memory and the hidden state are updated with the following equations: where,, and are gate activations. compared to the standard rnn, the lstm uses additive memory updates and it separates the memory from the hidden state, which interacts with the environment when making predictions. subsection: long short-term memory-network the first question that arises with lstms is the extent to which they are able to memorize sequences under recursive compression. lstms can produce a list of state representations during composition, however, the next state is always computed from the current state. that is to say, given the current state, the next state is conditionally independent of states and tokens. while the recursive state update is performed in a markov manner, it is assumed that lstms maintain unbounded memory (i.e., the current state alone summarizes well the tokens it has seen so far). this assumption may fail in practice, for example when the sequence is long or when the memory size is not large enough. another undesired property of lstms concerns modeling structured input. an lstm aggregates information on a token-by-token basis in sequential order, but there is no explicit mechanism for reasoning over structure and modeling relations between tokens. our model aims to address both limitations. our solution is to modify the standard lstm structure by replacing the memory cell with a memory network. the resulting long short-term memory-network (lstmn) stores the contextual representation of each input token with a unique memory slot and the size of the memory grows with time until an upper bound of the memory span is reached. this design enables the lstm to reason about relations between tokens with a neural attention layer and then perform non-markov state updates. although it is feasible to apply both write and read operations to the memories with attention, we concentrate on the latter. we conceptualize the read operation as attentively linking the current token to previous memories and selecting useful content when processing it. although not the focus of this work, the significance of the write operation can be analogously justified as a way of incrementally updating previous memories, e.g., to correct wrong interpretations when processing garden path sentences. the architecture of the lstmn is shown in figure [reference] and the formal definition is provided as follows. the model maintains two sets of vectors stored in a hidden state tape used to interact with the environment (e.g., computing attention), and a memory tape used to represent what is actually stored in memory. therefore, each token is associated with a hidden vector and a memory vector. let denote the current input; denotes the current memory tape, and the previous hidden tape. at time step, the model computes the relation between and through with an attention layer: this yields a probability distribution over the hidden state vectors of previous tokens. we can then compute an adaptive summary vector for the previous hidden tape and memory tape denoted by and, respectively: and use them for computing the values of and in the recurrent update as: where,, and are the new weight terms of the network. a key idea behind the lstmn is to use attention for inducing relations between tokens. these relations are soft and differentiable, and components of a larger representation learning network. although it is appealing to provide direct supervision for the attention layer, e.g., with evidence collected from a dependency treebank, we treat it as a submodule being optimized within the larger network in a downstream task. it is also possible to have a more structured relational reasoning module by stacking multiple memory and hidden layers in an alternating fashion, resembling a stacked lstm or a multi-hop memory network. this can be achieved by feeding the output of the lower layer as input to the upper layer. the attention at the th layer is computed as: skip-connections can be applied to feed to upper layers as well. [decoder with shallow attention fusion.] [decoder with deep attention fusion.] section: modeling two sequences with lstmn natural language processing tasks such as machine translation and textual entailment are concerned with modeling two sequences rather than a single one. a standard tool for modeling two sequences with recurrent networks is the encoder-decoder architecture where the second sequence (also known as the target) is being processed conditioned on the first one (also known as the source). in this section we explain how to combine the lstmn which applies attention for intra-relation reasoning, with the encoder-decoder network whose attention module learns the inter-alignment between two sequences. figures [reference] a and [reference] b illustrate two types of combination. we describe the models more formally below. paragraph: shallow attention fusion shallow fusion simply treats the lstmn as a separate module that can be readily used in an encoder-decoder architecture, in lieu of a standard rnn or lstm. as shown in figure [reference] a, both encoder and decoder are modeled as lstmns with intra-attention. meanwhile, inter-attention is triggered when the decoder reads a target token, similar to the inter-attention introduced in bahdanau2014neural. paragraph: deep attention fusion deep fusion combines inter-and intra-attention (initiated by the decoder) when computing state updates. we use different notation to represent the two sets of attention. following section [reference], and denote the target memory tape and hidden tape, which store representations of the target symbols that have been processed so far. the computation of intra-attention follows equations ([reference])- ([reference]). additionally, we use and to represent the source memory tape and hidden tape, with being the length of the source sequence conditioned upon. we compute inter-attention between the input at time step and tokens in the entire source sequence as follows: after that we compute the adaptive representation of the source memory tape and hidden tape as: we can then transfer the adaptive source representation to the target memory with another gating operation, analogous to the gates in equation ([reference]). the new target memory includes inter-alignment, intra-relation, and the new input information: as shown in the equations above and figure [reference] b, the major change of deep fusion lies in the recurrent storage of the inter-alignment vector in the target memory network, as a way to help the target network review source information. section: experiments in this section we present our experiments for evaluating the performance of the lstmn machine reader. we start with language modeling as it is a natural testbed for our model. we then assess the model's ability to extract meaning representations for generic sentence classification tasks such as sentiment analysis. finally, we examine whether the lstmn can recognize the semantic relationship between two sentences by applying it to a natural language inference task. our code is available at. subsection: language modeling our language modeling experiments were conducted on the english penn treebank dataset. following common practice, we trained on sections 0-20 (1 m words), used sections 21-22 for validation (80 k words), and sections 23-24 (90 k words for testing). the dataset contains approximately 1 million tokens and a vocabulary size of 10k. the average sentence length is 21. we use perplexity as our evaluation metric:, where denotes the negative log likelihood of the entire test set and the corresponding number of tokens. we used stochastic gradient descent for optimization with an initial learning rate of 0.65, which decays by a factor of 0.85 per epoch if no significant improvement has been observed on the validation set. we renormalize the gradient if its norm is greater than 5. the mini-batch size was set to 40. the dimensions of the word embeddings were set to 150 for all models. in this suite of experiments we compared the lstmn against a variety of baselines. the first one is a kneser-ney 5-gram language model (kn5) which generally serves as a non-neural baseline for the language modeling task. we also present perplexity results for the standard rnn and lstm models. we also implemented more sophisticated lstm architectures, such as a stacked lstm (slstm), a gated-feedback lstm (glstm; chung2015gated) and a depth-gated lstm (dlstm; yao2015depth). the gated-feedback lstm has feedback gates connecting the hidden states across multiple time steps as an adaptive control of the information flow. the depth-gated lstm uses a depth gate to connect memory cells of vertically adjacent layers. in general, both glstm and dlstm are able to capture long-term dependencies to some degree, but they do not explicitly keep past memories. we set the number of layers to 3 in this experiment, mainly to agree with the language modeling experiments of chung2015gated. also note that that there are no single-layer variants for glstm and dlstm; they have to be implemented as multi-layer systems. the hidden unit size of the lstmn and all comparison models (except kn5) was set to 300. the results of the language modeling task are shown in table [reference]. perplexity results for kn5 and rnn are taken from mikolov2015learning. as can be seen, the single-layer lstmn outperforms these two baselines and the lstm by a significant margin. amongst all deep architectures, the three-layer lstmn also performs best. we can study the memory activation mechanism of the machine reader by visualizing the attention scores. figure [reference] shows four sentences sampled from the penn treebank validation set. although we explicitly encourage the reader to attend to any memory slot, much attention focuses on recent memories. this agrees with the linguistic intuition that long-term dependencies are relatively rare. as illustrated in figure [reference] the model captures some valid lexical relations (e.g., the dependency between sits and at, sits and plays, everyone and is, is and watching). note that arcs here are undirected and are different from the directed arcs denoting head-modifier relations in dependency graphs. subsection: sentiment analysis our second task concerns the prediction of sentiment labels of sentences. we used the stanford sentiment treebank, which contains fine-grained sentiment labels (very positive, positive, neutral, negative, very negative) for 11, 855 sentences. following previous work on this dataset, we used 8, 544 sentences for training, 1, 101 for validation, and 2, 210 for testing. the average sentence length is 19.1. in addition, we also performed a binary classification task (positive, negative) after removing the neutral label. this resulted in 6, 920 sentences for training, 872 for validation and 1, 821 for testing. table [reference] reports results on both fine-grained and binary classification tasks. we experimented with 1-and 2-layer lstmns. for the latter model, we predict the sentiment label of the sentence based on the averaged hidden vector passed to a 2-layer neural network classifier with relu as the activation function. the memory size for both lstmn models was set to 168 to be compatible with previous lstm models applied to the same task. we used pre-trained 300-d glove 840b vectors to initialize the word embeddings. the gradient for words with glove embeddings, was scaled by 0.35 in the first epoch after which all word embeddings were updated normally. we used adam for optimization with the two momentum parameters set to 0.9 and 0.999 respectively. the initial learning rate was set to 2e-3. the regularization constant was 1e-4 and the mini-batch size was 5. a dropout rate of 0.5 was applied to the neural network classifier. we compared our model with a wide range of top-performing systems. most of these models (including ours) are lstm variants (third block in table [reference]), recursive neural networks (first block), or convolutional neural networks (cnns; second block). recursive models assume the input sentences are represented as parse trees and can take advantage of annotations at the phrase level. lstm-type models and cnns are trained on sequential input, with the exception of ct-lstm which operates over tree-structured network topologies such as constituent trees. for comparison, we also report the performance of the paragraph vector model (pv; le2014distributed; see table [reference], second block) which neither operates on trees nor sequences but learns distributed document representations parameterized directly. the results in table [reference] show that both 1-and 2-layer lstmns outperform the lstm baselines while achieving numbers comparable to state of the art. the number of layers for our models was set to be comparable to previously published results. on the fine-grained and binary classification tasks our 2-layer lstmn performs close to the best system t-cnn. figure [reference] shows examples of intra-attention for sentiment words. interestingly, the network learns to associate sentiment important words such as though and fantastic or not and good. subsection: natural language inference the ability to reason about the semantic relationship between two sentences is an integral part of text understanding. we therefore evaluate our model on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral. for this task we used the stanford natural language inference (snli) dataset, which contains premise-hypothesis pairs and target labels indicating their relation. after removing sentences with unknown labels, we end up with 549, 367 pairs for training, 9, 842 for development and 9, 824 for testing. the vocabulary size is 36, 809 and the average sentence length is 22. we performed lower-casing and tokenization for the entire dataset. recent approaches use two sequential lstms to encode the premise and the hypothesis respectively, and apply neural attention to reason about their logical relationship. furthermore, rocktaschel2015reasoning show that a non-standard encoder-decoder architecture which processes the hypothesis conditioned on the premise results significantly boosts performance. we use a similar approach to tackle this task with lstmns. specifically, we use two lstmns to read the premise and hypothesis, and then match them by comparing their hidden state tapes. we perform average pooling for the hidden state tape of each lstmn, and concatenate the two averages to form the input to a 2-layer neural network classifier with relu as the activation function. we used pre-trained 300-d glove 840b vectors to initialize the word embeddings. out-of-vocabulary (oov) words were initialized randomly with gaussian samples (= 0,= 1). we only updated oov vectors in the first epoch, after which all word embeddings were updated normally. the dropout rate was selected from [0.1, 0.2, 0.3, 0.4]. we used adam for optimization with the two momentum parameters set to 0.9 and 0.999 respectively, and the initial learning rate set to 1e-3. the mini-batch size was set to 16 or 32. for a fair comparison against previous work, we report results with different hidden/ memory dimensions (i.e., 100, 300, and 450). we compared variants of our model against different types of lstms (see the second block in table [reference]). specifically, these include a model which encodes the premise and hypothesis independently with two lstms, a shared lstm, a word-by-word attention model, and a matching lstm (mlstm; wang2015learning). this model sequentially processes the hypothesis, and at each position tries to match the current word with an attention-weighted representation of the premise (rather than basing its predictions on whole sentence embeddings). we also compared our models with a bag-of-words baseline which averages the pre-trained embeddings for the words in each sentence and concatenates them to create features for a logistic regression classifier (first block in table [reference]). lstmns achieve better performance compared to lstms (with and without attention; 2nd block in table [reference]). we also observe that fusion is generally beneficial, and that deep fusion slightly improves over shallow fusion. one explanation is that with deep fusion the inter-attention vectors are recurrently memorized by the decoder with a gating operation, which also improves the information flow of the network. with standard training, our deep fusion yields the state-of-the-art performance in this task. although encouraging, this result should be interpreted with caution since our model has substantially more parameters compared to related systems. we could compare different models using the same number of total parameters. however, this would inevitably introduce other biases, e.g., the number of hyper-parameters would become different. section: conclusions in this paper we proposed a machine reading simulator to address the limitations of recurrent neural networks when processing inherently structured input. our model is based on a long short-term memory architecture embedded with a memory network, explicitly storing contextual representations of input tokens without recursively compressing them. more importantly, an intra-attention mechanism is employed for memory addressing, as a way to induce undirected relations among tokens. the attention layer is not optimized with a direct supervision signal but with the entire network in downstream tasks. experimental results across three tasks show that our model yields performance comparable or superior to state of the art. although our experiments focused on lstms, the idea of building more structure aware neural models is general and can be applied to other types of networks. when direct supervision is provided, similar architectures can be adapted to tasks such as dependency parsing and relation extraction. in the future, we hope to develop more linguistically plausible neural architectures able to reason over nested structures and neural models that learn to discover compositionality with weak or indirect supervision. section: acknowledgments we thank members of the ilcc at the school of informatics and the anonymous reviewers for helpful comments. the support of the european research council under award number 681760\" translating multiple modalities into text\" is gratefully acknowledged. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "stanford natural language inference",
                        23423
                    ],
                    [
                        "snli",
                        23461
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "long short-term memory-networks",
                        10
                    ],
                    [
                        "attention",
                        4635
                    ],
                    [
                        "long short-term memory-network",
                        5075
                    ],
                    [
                        "lstmn",
                        5108
                    ],
                    [
                        "lstmn models",
                        5663
                    ],
                    [
                        "lstm",
                        8084
                    ],
                    [
                        "attention layer",
                        12710
                    ],
                    [
                        "deep attention fusion",
                        13946
                    ],
                    [
                        "lstmns with intra-attention",
                        14997
                    ],
                    [
                        "deep fusion",
                        15203
                    ],
                    [
                        "dlstm",
                        18354
                    ],
                    [
                        "2-layer lstmns",
                        20895
                    ],
                    [
                        "300-d glove 840b",
                        21242
                    ],
                    [
                        "2-layer lstmn",
                        22839
                    ],
                    [
                        "sequential lstms",
                        23860
                    ],
                    [
                        "lstmns",
                        24246
                    ],
                    [
                        "300",
                        25204
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "parameters",
                        6769
                    ],
                    [
                        "hyper-parameters",
                        26816
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "natural language inference",
                        778
                    ],
                    [
                        "natural language",
                        8249
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "stanford natural language inference",
                        23423
                    ],
                    [
                        "snli",
                        23461
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "long short-term memory-networks",
                        10
                    ],
                    [
                        "long short-term memory-network",
                        5075
                    ],
                    [
                        "lstmn",
                        5108
                    ],
                    [
                        "lstmn models",
                        5663
                    ],
                    [
                        "lstm",
                        8084
                    ],
                    [
                        "attention layer",
                        12710
                    ],
                    [
                        "2-layer lstmns",
                        20895
                    ],
                    [
                        "2-layer lstmn",
                        22839
                    ],
                    [
                        "sequential lstms",
                        23860
                    ],
                    [
                        "450",
                        25213
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "parameters",
                        6769
                    ],
                    [
                        "hyper-parameters",
                        26816
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "natural language inference",
                        778
                    ],
                    [
                        "natural language",
                        8249
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "0910a4c470a410fac446f4026f7c8ef512ae7427-8",
    "doctext": "document: hierarchical question-image co-attention for visual question answering a number of recent works have proposed attention models for visual question answering (vqa) that generate spatial maps highlighting image regions relevant to answering the question. in this paper, we argue that in addition to modeling ''where to look'' or visual attention, it is equally important to model ''what words to listen to'' or question attention. we present a novel co-attention model for vqa that jointly reasons about image and question attention. in addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (cnn). our model improves the state-of-the-art on the vqa dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the coco-qa dataset. by using resnet, the performance is further improved to 62.1% for vqa and 65.4% for coco-qa.. [itemize] leftmargin=20pt section: introduction visual question answering (vqa) has emerged as a prominent multi-discipline research problem in both academia and industry. to correctly answer visual questions about an image, the machine needs to understand both the image and question. recently, visual attention based models have been explored for vqa, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question. so far, all attention models for vqa in literature have focused on the problem of identifying ''where to look'' or visual attention. in this paper, we argue that the problem of identifying ''which words to listen to'' or question attention is equally important. consider the questions ''how many horses are in this image?'' and ''how many horses can you see in this image?\". they have the same meaning, essentially captured by the first three words. a machine that attends to the first three words would arguably be more robust to linguistic variations irrelevant to the meaning and answer of the question. motivated by this observation, in addition to reasoning about visual attention, we also address the problem of question attention. specifically, we present a novel multi-modal attention model for vqa with the following two unique features: co-attention: we propose a novel mechanism that jointly reasons about visual attention and question attention, which we refer to as co-attention. unlike previous works, which only focus on visual attention, our model has a natural symmetry between the image and question, in the sense that the image representation is used to guide the question attention and the question representation (s) are used to guide image attention. question hierarchy: we build a hierarchical architecture that co-attends to the image and question at three levels: (a) word level, (b) phrase level and (c) question level. at the word level, we embed the words to a vector space through an embedding matrix. at the phrase level, 1-dimensional convolution neural networks are used to capture the information contained in unigrams, bigrams and trigrams. specifically, we convolve word representations with temporal filters of varying support, and then combine the various n-gram responses by pooling them into a single phrase level representation. at the question level, we use recurrent neural networks to encode the entire question. for each level of the question representation in this hierarchy, we construct joint question and image co-attention maps, which are then combined recursively to ultimately predict a distribution over the answers. overall, the main contributions of our work are: we propose a novel co-attention mechanism for vqa that jointly performs question-guided visual attention and image-guided question attention. we explore this mechanism with two strategies, parallel and alternating co-attention, which are described in sec. [reference]; we propose a hierarchical architecture to represent the question, and consequently construct image-question co-attention maps at 3 different levels: word level, phrase level and question level. these co-attended features are then recursively combined from word level to question level for the final answer prediction; at the phrase level, we propose a novel convolution-pooling strategy to adaptively select the phrase sizes whose representations are passed to the question level representation; finally, we evaluate our proposed model on two large datasets, vqa and coco-qa. we also perform ablation studies to quantify the roles of different components in our model. section: related work many recent works have proposed models for vqa. we compare and relate our proposed co-attention mechanism to other vision and language attention mechanisms in literature. image attention. instead of directly using the holistic entire-image embedding from the fully connected layer of a deep cnn (as in), a number of recent works have explored image attention models for vqa. zhu add spatial attention to the standard lstm model for pointing and grounded qa. andreas propose a compositional scheme that consists of a language parser and a number of neural modules networks. the language parser predicts which neural module network should be instantiated to answer the question. some other works perform image attention multiple times in a stacked manner. in, the authors propose a stacked attention network, which runs multiple hops to infer the answer progressively. to capture fine-grained information from the question, xu propose a multi-hop image attention scheme. it aligns words to image patches in the first hop, and then refers to the entire question for obtaining image attention maps in the second hop. in, the authors generate image regions with object proposals and then select the regions relevant to the question and answer choice. xiong augments dynamic memory network with a new input fusion module and retrieves an answer from an attention based gru. in concurrent work, collected 'human attention maps' that are used to evaluate the attention maps generated by attention models for vqa. note that all of these approaches model visual attention alone, and do not model question attention. moreover, model attention sequentially, i.e., later attention is based on earlier attention, which is prone to error propagation. in contrast, we conduct co-attention at three levels independently. language attention. though no prior work has explored question attention in vqa, there are some related works in natural language processing (nlp) in general that have modeled language attention. in order to overcome difficulty in translation of long sentences, bahdanau propose rnnsearch to learn an alignment over the input sentences. in, the authors propose an attention model to circumvent the bottleneck caused by fixed width hidden vector in text reading and comprehension. a more fine-grained attention mechanism is proposed in. the authors employ a word-by-word neural attention mechanism to reason about the entailment in two sentences. also focused on modeling sentence pairs, the authors in propose an attention-based bigram cnn for jointly performing attention between two cnn hierarchies. in their work, three attention schemes are proposed and evaluated. in, the authors propose a two-way attention mechanism to project the paired inputs into a common representation space. section: method we begin by introducing the notation used in this paper. to ease understanding, our full model is described in parts. first, our hierarchical question representation is described in sec. [reference] and the proposed co-attention mechanism is then described in sec. [reference]. finally, sec. [reference] shows how to recursively combine the attended question and image features to output answers. subsection: notation given a question with words, its representation is denoted by, where is the feature vector for the-th word. we denote, and as word embedding, phrase embedding and question embedding at position, respectively. the image feature is denoted by, where is the feature vector at the spatial location. the co-attention features of image and question at each level in the hierarchy are denoted as and where. the weights in different modules/ layers are denoted with, with appropriate sub/ super-scripts as necessary. in the exposition that follows, we omit the bias term to avoid notational clutter. subsection: question hierarchy given the 1-hot encoding of the question words, we first embed the words to a vector space (learnt end-to-end) to get. to compute the phrase features, we apply 1-d convolution on the word embedding vectors. concretely, at each word location, we compute the inner product of the word vectors with filters of three window sizes: unigram, bigram and trigram. for the-th word, the convolution output with window size is given by where is the weight parameters. the word-level features are appropriately 0-padded before feeding into bigram and trigram convolutions to maintain the length of the sequence after convolution. given the convolution result, we then apply max-pooling across different n-grams at each word location to obtain phrase-level features our pooling method differs from those used in previous works in that it adaptively selects different gram features at each time step, while preserving the original sequence length and order. we use a lstm to encode the sequence after max-pooling. the corresponding question-level feature is the lstm hidden vector at time. our hierarchical representation of the question is depicted in fig. [reference] (a). subsection: co-attention we propose two co-attention mechanisms that differ in the order in which image and question attention maps are generated. the first mechanism, which we call parallel co-attention, generates image and question attention simultaneously. the second mechanism, which we call alternating co-attention, sequentially alternates between generating image and question attentions. see fig. [reference]. these co-attention mechanisms are executed at all three levels of the question hierarchy. parallel co-attention. parallel co-attention attends to the image and question simultaneously. similar to, we connect the image and question by calculating the similarity between image and question features at all pairs of image-locations and question-locations. specifically, given an image feature map, and the question representation, the affinity matrix is calculated by where contains the weights. after computing this affinity matrix, one possible way of computing the image (or question) attention is to simply maximize out the affinity over the locations of other modality, and. instead of choosing the max activation, we find that performance is improved if we consider this affinity matrix as a feature and learn to predict image and question attention maps via the following where, are the weight parameters. and are the attention probabilities of each image region and word respectively. the affinity matrix transforms question attention space to image attention space (vice versa for). based on the above attention weights, the image and question attention vectors are calculated as the weighted sum of the image features and question features, i.e., the parallel co-attention is done at each level in the hierarchy, leading to and where. alternating co-attention. in this attention mechanism, we sequentially alternate between generating image and question attention. briefly, this consists of three steps (marked in fig. [reference] b): 1) summarize the question into a single vector; 2) attend to the image based on the question summary; 3) attend to the question based on the attended image feature. concretely, we define an attention operation, which takes the image (or question) features and attention guidance derived from question (or image) as inputs, and outputs the attended image (or question) vector. the operation can be expressed in the following steps where is a vector with all elements to be 1. and are parameters. is the attention weight of feature. the alternating co-attention process is illustrated in fig. [reference] (b). at the first step of alternating co-attention,, and is; at the second step, where is the image features, and the guidance is intermediate attended question feature from the first step; finally, we use the attended image feature as the guidance to attend the question again, i.e., and. similar to the parallel co-attention, the alternating co-attention is also done at each level of the hierarchy. subsection: encoding for predicting answers following, we treat vqa as a classification task. we predict the answer based on the co-attended image and question features from all three levels. we use a multi-layer perceptron (mlp) to recursively encode the attention features as shown in fig. [reference] (b). where and are the weight parameters. is the concatenation operation on two vectors. is the probability of the final answer. section: experiment subsection: datasets we evaluate the proposed model on two datasets, the vqa dataset and the coco-qa dataset. vqa dataset is the largest dataset for this problem, containing human annotated questions and answers on microsoft coco dataset. the dataset contains 248, 349 training questions, 121, 512 validation questions, 244, 302 testing questions, and a total of 6, 141, 630 question-answers pairs. there are three sub-categories according to answer-types including yes/ no, number, and other. each question has 10 free-response answers. we use the top 1000 most frequent answers as the possible outputs similar to. this set of answers covers 86.54% of the train+ val answers. for testing, we train our model on vqa train+ val and report the test-dev and test-standard results from the vqa evaluation server. we use the evaluation protocol of in the experiment. coco-qa dataset is automatically generated from captions in the microsoft coco dataset. there are 78, 736 train questions and 38, 948 test questions in the dataset. these questions are based on 8, 000 and 4, 000 images respectively. there are four types of questions including object, number, color, and location. each type takes,,, and of the whole dataset, respectively. all answers in this data set are single word. as in, we report classification accuracy as well as wu-palmer similarity (wups) in table 2. subsection: setup we use torch to develop our model. we use the rmsprop optimizer with a base learning rate of 4e-4, momentum 0.99 and weight-decay 1e-8. we set batch size to be 300 and train for up to 256 epochs with early stopping if the validation accuracy has not improved in the last 5 epochs. for coco-qa, the size of hidden layer is set to 512 and 1024 for vqa since it is a much larger dataset. all the other word embedding and hidden layers were vectors of size 512. we apply dropout with probability on each layer. following, we rescale the image to, and then take the activation from the last pooling layer of vggnet or resnet as its feature. subsection: results and analysis there are two test scenarios on vqa: open-ended and multiple-choice. the best performing method deeper lstm q+ norm i from is used as our baseline. for open-ended test scenario, we compare our method with the recent proposed smem, san, fda and dmn+. for multiple choice, we compare with region sel. and fda. we compare with 2-vis+ blstm, img-cnn and san on coco-qa. we use to refer to our parallel co-attention, for alternating co-attention. table [reference] shows results on the vqa test sets for both open-ended and multiple-choice settings. we can see that our approach improves the state of art from 60.4% (dmn+) to 62.1% (+ resnet) on open-ended and from 64.2% (fda) to 66.1% (+ resnet) on multiple-choice. notably, for the question type other and num, we achieve 3.4% and 1.4% improvement on open-ended questions, and 4.0% and 1.1% on multiple-choice questions. as we can see, resnet features outperform or match vgg features in all cases. our improvements are not solely due to the use of a better cnn. specifically, fda also uses resnet, but+ resnet outperforms it by 1.8% on test-dev. smem uses googlenet and the rest all use vggnet, and ours+ vgg outperforms them by 0.2% on test-dev (dmn+). table [reference] shows results on the coco-qa test set. similar to the result on vqa, our model improves the state-of-the-art from 61.6% (san (2, cnn)) to 65.4% (+ resnet). we observe that parallel co-attention performs better than alternating co-attention in this setup. both attention mechanisms have their advantages and disadvantages: parallel co-attention is harder to train because of the dot product between image and text which compresses two vectors into a single value. on the other hand, alternating co-attention may suffer from errors being accumulated at each round. subsection: ablation study in this section, we perform ablation studies to quantify the role of each component in our model. specifically, we re-train our approach by ablating certain components: image attention alone, where in a manner similar to previous works, we do not use any question attention. the goal of this comparison is to verify that our improvements are not the result of orthogonal contributions. (say better optimization or better cnn features). question attention alone, where no image attention is performed. w/ o conv, where no convolution and pooling is performed to represent phrases. instead, we stack another word embedding layer on the top of word level outputs. w/ o w-atten, where no word level co-attention is performed. we replace the word level attention with a uniform distribution. phrase and question level co-attentions are still modeled. w/ o p-atten, where no phrase level co-attention is performed, and the phrase level attention is set to be uniform. word and question level co-attentions are still modeled. w/ o q-atten, where no question level co-attention is performed. we replace the question level attention with a uniform distribution. word and phrase level co-attentions are still modeled. table [reference] shows the comparison of our full approach w.r.t these ablations on the vqa validation set (test sets are not recommended to be used for such experiments). the deeper lstm q+ norm i baseline in antol2015vqa is also reported for comparison. we can see that image-attention-alone does improve performance over the holistic image feature (deeper lstm q+ norm i), which is consistent with findings of previous attention models for vqa. ablation study on the vqa dataset using oursa+ vgg. validationmethody/ nnumotheralllstm q+ i79.832.940.754.3image atten79.433.341.754.8w/ o q-atten79.632.142.955.3w/ o p-atten79.534.145.456.7w/ o w-atten79.634.445.656.8full model79.635.045.757.0 comparing the full model ablated versions without word, phrase, question level attentions reveals a clear interesting trend- the attention mechanisms closest to the 'top' of the hierarchy (question) matter most, with a drop of 1.7% in accuracy if not modeled; followed by the intermediate level (phrase), with a drop of 0.3%; finally followed by the 'bottom' of the hierarchy (word), with a drop of 0.2% in accuracy. we hypothesize that this is because the question level is the 'closest' to the answer prediction layers in our model. note that all levels are important, and our final model significantly outperforms not using any linguistic attention (1.1% difference between full model and image atten). the question attention alone model is better than lstm q+ i, with an improvement of 0.5% and worse than image attention alone, with a drop of 1.1%. further improves if we performed alternating co-attention for one more round, with an improvement of 0.3%. subsection: qualitative results we now visualize some co-attention maps generated by our method in fig. [reference]. at the word level, our model attends mostly to the object regions in an image, e.g., heads, bird. at the phrase level, the image attention has different patterns across images. for the first two images, the attention transfers from objects to background regions. for the third image, the attention becomes more focused on the objects. we suspect that this is caused by the different question types. on the question side, our model is capable of localizing the key phrases in the question, thus essentially discovering the question types in the dataset. for example, our model pays attention to the phrases ''what color'' and ''how many snowboarders''. our model successfully attends to the regions in images and phrases in the questions appropriate for answering the question, e.g., ''color of the bird'' and bird region. because our model performs co-attention at three levels, it often captures complementary information from each level, and then combines them to predict the answer. q: what is the man holding a snowboard on top of a snow covered? a: mountain what is the man holding a snowboard on top of a snow covered what is the man holding a snowboard on top of a snow covered? what is the man holding a snowboard on top of a snow covered? q: what is the color of the bird? a: white what is the color of the bird? what is the color of the bird? what is the color of the bird? q: how many snowboarders in formation in the snow, four is sitting? a: 5 how many snowboarders in formation in the snow, four is sitting? how many snowboarders in formation in the snow, four is sitting? how many snowboarders in formation in the snow, four is sitting? section: conclusion in this paper, we proposed a hierarchical co-attention model for visual question answering. co-attention allows our model to attend to different regions of the image as well as different fragments of the question. we model the question hierarchically at three levels to capture information from different granularities. the ablation studies further demonstrate the roles of co-attention and question hierarchy in our final performance. through visualizations, we can see that our model co-attends to interpretable regions of images and questions for predicting the answer. though our model was evaluated on visual question answering, it can be potentially applied to other tasks involving vision and language. acknowledgements this work was funded in part by nsf career awards to dp and db, an onr yip award to dp, onr grant n00014-14-1-0679 to db, a sloan fellowship to dp, aro yip awards to db and dp, a allen distinguished investigator award to dp from the paul g. allen family foundation, ictas junior faculty awards to db and dp, google faculty research awards to dp and db, aws in education research grant to db, and nvidia gpu donations to db. the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the u.s. government or any sponsor. q: what is the color of the kitten? a: black q: what are standing in tall dry grass look at the tourists? a: zebras q: where is the woman while her baby is sleeping? a: kitchen q: what seating area is on the right? a: park q: is the person dressed properly for this sport? a: yes what is the color of the kitten? what are standing in tall dry grass look at the tourists? where is the woman while her baby is sleeping? what seating area is on the right? is the person dressed properly for the sport? what is the color of the kitten? what are standing in tall dry grass look at the tourists? where is the woman while her baby is sleeping? what seating area is on the right? is the person dressed properly for the sport? what is the color of the kitten? what are standing in tall dry grass look at the tourists? where is the woman while her baby is sleeping? what seating area is on the right? is the person dressed properly for the sport? q: how many red motorcycles with riders in protective gear are on the street? a: two (three) q: what is the color of the bus? a: green (red) q: what is shown in different places? a: hydrant (toy) q: do the doors open in or out? a: open (in) q: is this an open market at night? a: yes (no) how many red motorcycles with riders in protective gear are on the street? what is the color of the bus? what is shown in different places? do the doors open in or out? is this an open market at night? how many red motorcycles with riders in protective gear are on the street? what is the color of the bus? what is shown in different places? do the doors open in or out? is this an open market at night? how many red motorcycles with riders in protective gear are on the street? what is the color of the bus? what is shown in different places? do the doors open in or out? is this an open market at night? bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "vqa dataset",
                        786
                    ],
                    [
                        "microsoft coco dataset",
                        13262
                    ],
                    [
                        "vqa train+ val",
                        13759
                    ],
                    [
                        "multiple-choice",
                        15159
                    ],
                    [
                        "vqa test sets",
                        15588
                    ],
                    [
                        "multiple-choice questions",
                        15949
                    ],
                    [
                        "vqa validation set",
                        18215
                    ]
                ]
            ],
            "Method": [],
            "Metric": [],
            "Task": [
                [
                    [
                        "visual question answering",
                        55
                    ],
                    [
                        "vqa",
                        169
                    ],
                    [
                        "vqa evaluation server",
                        13833
                    ],
                    [
                        "vision",
                        22260
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "vqa dataset",
                        786
                    ],
                    [
                        "microsoft coco dataset",
                        13262
                    ],
                    [
                        "vqa train+ val",
                        13759
                    ],
                    [
                        "open-ended",
                        15144
                    ],
                    [
                        "vqa test sets",
                        15588
                    ],
                    [
                        "open-ended questions",
                        15906
                    ],
                    [
                        "vqa validation set",
                        18215
                    ]
                ]
            ],
            "Method": [],
            "Metric": [],
            "Task": [
                [
                    [
                        "visual question answering",
                        55
                    ],
                    [
                        "vqa",
                        169
                    ],
                    [
                        "vqa evaluation server",
                        13833
                    ],
                    [
                        "vision",
                        22260
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "097851f362667a599208f4a06fa289dac2e91fbe-9",
    "doctext": "we propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. experiments on stl-10 show that the method presents state-of-the-art performance and provides discriminative features that generalize well. nomoremeta-section: introduction significant effort has been devoted to handcraft appropriate feature representations of data in several fields. in tasks such as image classification and object recognition, unsupervised learned features have shown to compete well or even outperform manually designed ones. unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures. in, the author claims that potentially interesting research involves pre-training algorithms, which\" [\u2026] would be proficient at extracting good features but involving an easier optimization problem.\" in addition to that, one of the main criticisms to state-of-the-art methods is that they require a significant amount of meta-parameters. as stated in, the tuning of these meta-parameters is a laborious task that requires expert knowledge, rules of thumb or extensive search and, whose setting can vary for different tasks. therefore, there is great interest for meta-parameter free methods and automatic approaches to optimize the performance of learning algorithms. nevertheless, little effort has been devoted to address this problem (see table [reference] for a comparison of meta-parameters required by unsupervised feature learning methods). to the best of our knowledge, work in this direction includes ica and sparse filtering. although ica provides good results at object recognition tasks, the method scales poorly to large datasets and high input dimensionality. computational complexity is also a major drawback of many state-of-the-art methods. ica requires an expensive orthogonalization to be computed at each iteration. sparse coding has an expensive inference, which requires a prohibitive iterative optimization. significant amount of work has been done in order to overcome this limitation. predictive sparse decomposition (psd) is a successful variant of sparse coding, which uses a predictor to approximate the sparse representation and solves the sparse coding computationally expensive encoding step. in this paper, we aim to solve some of the above-mentioned problems. we propose a meta-parameter free, off-the-shelf, simple and fast approach, which exploits a new way of optimizing for a sparsity, without explicitly modeling the data distribution. the method iteratively builds an ideally sparse target and optimizes the dictionary by minimizing the error between the system output and the ideally sparse target. defining sparsity concepts in terms of expected output allows to exploit a new strategy in unsupervised training. it is worth stressing that many optimization strategies can be used to minimize the above-mentioned error and that parameters of these optimization techniques must not be considered as belonging to our approach. experiments on stl-10 dataset show that the method outperforms state-of-the-art methods in single layer image classification, providing discriminative features that generalize well. linear feature extraction methods combined with sparse coding encodings are among best performers on object recognition datasets. the importance of properly combining training/ encoding and encoding/ pooling strategies has been argued in and respectively. since the goal of this paper is to propose a new method for unsupervised feature learning, dealing with all the possible combinations of encoding and pooling could mask the benefits of the method that we propose. however, for the sake of fair comparison with the state-of-the-art, we test the method with sparse coding and soft-threshold encodings combined with sum pooling, following the experimental pipeline of. section: state-of-the-art commonly used algorithms for unsupervised feature learning include restricted boltzmann machines (rbm), auto-encoders, sparse coding and hybrids such as psd. many other methods such as ica, reconstruction ica (rica), sparse filtering and methods related to vector quantization such as orthogonal matching pursuit (omp-k) have also been used in the literature to extract unsupervised feature representations. these algorithms could be divided into two categories: explicitly modeling or not the input distribution. sparse auto-encoders, sparse rbm, sparse coding, psd, omp-k and reconstruction ica (rica) explicitly model the data distribution by minimizing the reconstruction error. although learning a good approximation of the data distribution may be desirable, approaches such as sparse filtering show that this seems not so important if the goal is to have a discriminative sparse system. sparse filtering does not attempt to explicitly model the input distribution but focuses on the properties of the output distribution instead. sparsity is among the desirable properties of a good output representation. sparse features consist of a large amount of outputs, which respond rarely and provide high responses when they do respond. sparsity can be described in terms of population sparsity and lifetime sparsity. both lifetime and population sparsity are important properties of the output distribution. on one hand, lifetime sparsity plays an important role in preventing bad solutions such as numerous dead outputs. there seems to be a consensus to overcome such degenerate solutions, which is to ensure similar statistics among outputs. on the other hand, population sparsity helps providing a simple interpretation of the input data such as the ones found in early visual areas. to the best of our knowledge, the definition of population sparsity remains ambiguous. state-of-the-art methods optimize either for one or both sparsity forms in their objective function. the great majority seeks sparsity using the penalty and does not optimize for an explicit level of sparsity in their outputs. sparse auto-encoders optimize for a target activation allowing to deal with lifetime sparsity; nevertheless, the target activation requires tuning and does not explicitly control the level of population sparsity. omp-k defines the level of population sparsity by setting to the maximum expected number of non-zero elements per output code, whereas the methods in do not explicitly define the proportion of outputs expected to be active at the same time. section: method in this section, we describe how the proposed method learns a sparse feature representation of the data in terms of population and lifetime sparsity. the method iteratively builds an ideally sparse target and optimizes the dictionary by minimizing the error between the system output and the ideally sparse target. subsection [reference] highlights the algorithm to enforce lifetime and population sparsity in the ideally sparse target. subsection [reference] provides implementation details on the system and optimization strategies used to minimize the error between the system output and the ideally sparse target. subsection: enforcing population and lifetime sparsity by defining an ideal target we define population and lifetime sparsity as properties of an ideal sparse output. given training samples and an output of dimensionality, we define the first property of the output as: strong lifetime sparsity: the output vectors must be composed solely of active and inactive units (no intermediate values between two fixed scalars are allowed) and all outputs must activate for an equal number of inputs. activation is exactly distributed among the outputs. our strong lifetime sparsity definition is a more strict requirement than the high dispersal concept introduced in, since they only require that\" the mean squared activations of each feature (output) [\u2026] should be roughly the same for all features (outputs)\". while high dispersal attempts to diversify the learned bases, it does not guarantee the output distribution, in the lifetime sense, to be composed of only a few activations. furthermore, our definition ensures the absence of dead outputs. given our definition of strong lifetime sparsity, the population sparsity must require that, for each training sample, only one output element is active: strong population sparsity: for each training sample only one output must be active. the rationale of our approach is to appropriately generate an ideal output target that fulfils properties (1) and (2), and then learn the parameters of the system by minimizing the error between the output target and the output generated by the system during training. in this way, we seek a system optimized for both population and lifetime sparsity in an explicit way. the key component of our approach is how to define the ideal output target based on the above-mentioned properties. however, to ensure that the optimization of the system parameters converges, we add a third property: minimal perturbation: the ideal output target should be defined as the best approximation of the system output by means of error fulfilling properties (1)& (2). creating the output target that ensures the above-mentioned properties is analogous to solving an assignment problem. the hungarian method is a combinatorial optimization algorithm, which solves the assignment problem. however, its computational cost is prohibitive. therefore, in the next section we propose a simple and fast algorithm to generate the ideal output target, which ensures sparsity properties (1) and (2) and provides an approximate solution for minimal perturbation property (3). subsubsection: ideal target generation: the enforcing population and lifetime sparsity (epls) algorithm let us assume that we have a system, which produces a row output vector. we use the notation to refer to one element of. we define an output matrix composed of output vectors of dimensionality, such that. likewise, we define an ideal target output matrix of the same size. algorithm [reference] details the epls algorithm to generate the ideal target from. for the sake of simplicity, every step of the algorithm where the subscript appears must be applied. epls [1], a, n a t to active/ inactive values of the corresponding function. starting with no activation in (line 1), the algorithm proceeds as follows. a row vector from is processed at each iteration (line 3). the crucial step is performed in line 4: the output that has to be activated in the row of is selected as the one that has the maximal activation value minus the inhibitor. the inhibitor can be seen as an accumulator that\" counts\" the number of times an output has been selected, increasing its inhibition progressively by until reaching maximal inhibition. this prevents the selection of an output that has already been activated times. the rationale behind the equation in line 4 is that, while selecting the maximal responses in the matrix, we have to take care to distribute them evenly among all outputs (in order to ensure strong lifetime sparsity). using this strategy, it can be demonstrated that the resulting matrix perfectly fulfills properties (1) and (2). in line 5, the algorithm activates the element of row of the target matrix. by activating the\" relative\" maximum, we approximate property (3). finally, the inhibitor is updated in line 6. subsection: system and optimization strategies let us assume that we have a system parameterized by, with activation function, which takes as input a data vector and produces an output vector. we use the same notation as in section [reference] and define a data matrix composed of rows and columns, where is the input dimensionality. to compare our training strategy to previous well known systems, we tested our algorithm using where is a logistic non-linearity. subsubsection: optimization strategy the system might be trained by means of an off-the-shelf mini-batch stochastic gradient descent (sgd) method with adaptive learning rates such as variance-based sgd (vsgd). algorithm [reference] details the latter training process. the mini-batch size can be set to any value, in all the experiments we have set. starting with set to small random numbers as in (line 1), at each epoch we shuffle the samples of the training set (line 3), reset the epls inhibitor to a flat activation (line 4) and process all mini-batches. for each mini-batch, samples are selected (line 6). then, the output is computed (line 7) and the epls is invoked to compute and update (line 8). after that, the gradient of the error is computed (line 9) and the learning rate is estimated as in (line 10). the system parameters are then updated to minimize the error (line 11). finally, the bases in are limited to have unit norm to avoid degenerate solutions (line 13). this procedure is repeated until a stop condition is met; in our experiments, the training stops when the relative decrement error between epochs is small (). when updating the system parameters, we assume that does not depend on, thus; we carried out experiments that show that this approximation does not significantly influence the gradient descent convergence nor the quality of the minimization. moreover, this assumption makes the algorithm faster, since we remove the need of computing the numerical partial derivatives of. the mini-batch vsgd allows to scale the algorithm easily, especially with respect to the number of samples. standard epls training [1] small random values d randomly flat activation mini-batch samples d (b) learning rate\u03b7 as in () the bases w in\u03b3 to have unit norm condition verified section: experiments the performance of training and encoding strategies in single layer networks has been extensively analyzed in the literature on stl-10 dataset. stl-10 dataset consists of 96x96 pixels color images belonging to 10 different classes. the dataset is divided into a large unlabeled training set containing 100 k images and smaller labeled training and test sets, containing 5000 and 8000 images, respectively. it has to be considered that in stl-10, the primary challenge is to make use of the unlabeled data (100 k images), which is 100 times bigger than the labeled data used to train the classifier (1000 images per fold). in this case, the supervised training must strongly rely on the ability of the unsupervised method to learn discriminative features. moreover, since the unlabeled dataset contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set, the unsupervised method should be able to generalize well. to validate our method, we follow the experimental pipeline of. we first extract random patches and normalize them for local brightness and contrast. note that epls does not require any whitening of the input data, since it decorrelates the data during the training by means of the imposed strong sparsity properties of the output target. then, we apply the system to retrieve sparse features of patches covering the input image, pool them into 4 quadrants and finally train a svm for classification purposes. we tune the svm parameter using 5-fold cross-validation. as in, we use a receptive field of 10x10 pixels and a stride of 1. the number of outputs is set to for fair comparison with the other state-of-the-art methods. we also provide the results of our method with sign split (x, using and for encoding as in) and using the sparse coding (sc) encoder, which found to be the best when small number of labeled data is available. for this encoder, we searched over the same set of parameter values as, i.e.,. the parameter is tuned to consider the use of sparse coding as encoder after the training and, thus, does not belong to the method that we propose. table [reference] summarizes the results obtained on this dataset compared to other state-of-the-art methods. when pairing each training method with its associated natural encoding, epls outperforms all the other methods. when pairing the training methods with sparse coding, epls outperforms the state-of-the-art best performer in single layer networks as well, achieving accuracy. moreover, the standard deviation of the folds is lower than the one provided by omp-1 with sparse coding encoding. results are even more impressive if we compare them to meta-parameter free algorithms. figure [reference] shows a subset of 100 randomly selected bases learned by our method, 10x10 pixel receptive field and a system of outputs. as shown in the figure, the method learns not only common bases such as oriented edges/ ridges in many directions and colors but also corner detectors, tri-banded colored filters, center surrounds and laplacian of gaussians among others. this suggests that enforcing lifetime sparsity helps the system to learn a set of complex, rich and diversified bases. section: computational complexity the epls algorithm requires the computation of, which has cost, and therefore scales linearly on both and. since we can use vsgd for optimization, the method scales linearly on given a fixed number of epochs. finally, applying the activation function, the cost of computing the derivative is linear with, since we use a closed form for. the memory complexity is related to the mini-batch size. consequently, the method can scale gracefully to very large datasets: theoretically, it requires to store in memory the mini-batch input data (elements), output (elements), target (elements) and the system parameters to optimize (elements); a total amount of elements. section: discussion our results show that simultaneously enforcing both population and lifetime sparsity helps in learning discriminative dictionaries, which reflect in better performance, especially when compared to meta-parameter free methods. experiments suggest that our algorithm is able to extract features that generalize well on unseen data. when comparing the performance stl-10 dataset, our algorithm outperforms state-of-the-art best performers. results suggest that our algorithm helps the classifier in generalizing with a few training examples (of the dataset), gaining accuracy w.r.t. the state-of-the art best performer (omp-1 paired with sparse coding) with a lower standard deviation across folds, suggesting more robustness to variations in the training folds. it is important to highlight that omp-1 can be seen as a special case of our algorithm, where the activation function is and lifetime sparsity is not taken into account in the optimization process (potentially leading to dead outputs). our algorithm has several advantages over omp-1: (1) it can use any activation function; (2) by enforcing lifetime sparsity it does not suffer of the dead output problem, thus not requiring ad-hoc tricks to avoid it; (3) it does not require whitening, which can be a problem if the input dimensionality is large. with our proposal, we advance in the meta-parameter free line of ica and sparse filtering. it is clear that the advantage of sparse filtering over ica comes from removing the orthogonality constraint, and imposing some sort of\" competition\" between outputs, which also permits overcomplete representations. following this spirit, our algorithm imposes an even more strict form of competition to prevent dead outputs by means of strong lifetime sparsity and confirms the trend of that data reconstruction seems not so important if the goal is to have a discriminative sparse system. last and most importantly, it is worth highlighting five interesting properties of the epls algorithm. first, the method is meta-parameter free, which highly simplifies the training process for practitioners, especially when used as a greedy pre-training method in deep architectures. second, the method is fast and scales linearly with the number of training samples and the input/ output dimensionalities. third, epls is easy to implement. we implemented the epls in algorithm [reference] in less than 50 lines of c code. the mini-batch vsgd is a general purpose optimizer; our matlab implementation of vsgd plus the epls mex source will be publicly available after publication. fourth, the proposed learning strategy is not limited to perceptrons. fifth, there is an interest in the literature in avoiding redundancy in the image representation by using the algorithms in a convolutional fashion. for this purpose, the epls can be slightly modified to apply the procedure to a whole image at once and consider the mini-batch size to be the image divided into patches. this aspect is not considered in the paper and is left for future investigation. section: conclusion in this paper, we introduced the enforcing population and lifetime sparsity method. the algorithm provides a meta-parameter free, off-the-shelf, simple and computationally efficient approach for unsupervised sparse feature learning. it seeks both lifetime and population sparsity in an explicit way in order to learn discriminative features, thus preventing dead outputs. results show that the method significantly outperforms all state-of-the-art methods on stl-10 dataset with lower standard deviation across folds, suggesting more robustness across training sets. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "stl-10",
                        174
                    ],
                    [
                        "stl-10 dataset",
                        3101
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm",
                        13
                    ],
                    [
                        "unsupervised feature learning",
                        606
                    ],
                    [
                        "unsupervised feature learning methods",
                        1529
                    ],
                    [
                        "meta-parameter",
                        2427
                    ],
                    [
                        "unsupervised sparse feature learning",
                        20904
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "image classification",
                        461
                    ],
                    [
                        "classification purposes",
                        15168
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "0b0dc14b8a8dcccbfc62a38355fff2f6a361e9d2-10",
    "doctext": "document: effective lstms for target-dependent sentiment classification target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. different context words have different influences on determining the sentiment polarity of a sentence towards the target. therefore, it is desirable to integrate the connections between target word and context words when building a learning system. in this paper, we develop two target dependent long short-term memory (lstm) models, where target information is automatically taken into account. we evaluate our methods on a benchmark dataset from twitter. empirical results show that modeling sentence representation with standard lstm does not perform well. incorporating target information into lstm can significantly boost the classification accuracy. the target-dependent lstm models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons. section: introduction this work is licensed under a creative commons attribution 4.0 international license. license details: sentiment analysis, also known as opinion mining, is a fundamental task in natural language processing and computational linguistics. sentiment analysis is crucial to understanding user generated text in social networks or product reviews, and has drawn a lot of attentions from both industry and academic communities. in this paper, we focus on target-dependent sentiment classification, which is a fundamental and extensively studied task in the field of sentiment analysis. given a sentence and a target mention, the task calls for inferring the sentiment polarity (e.g. positive, negative, neutral) of the sentence towards the target. for example, let us consider the sentence:\" i bought a new camera. the picture quality is amazing but the battery life is too short\". if the target string is picture quality, the expected sentiment polarity is\" positive\" as the sentence expresses a positive opinion towards picture quality. if we consider the target as battery life, the correct sentiment polarity should be\" negative\". target-dependent sentiment classification is typically regarded as a kind of text classification problem in literature. majority of existing studies build sentiment classifiers with supervised machine learning approach, such as feature based supported vector machine or neural network approaches. despite the effectiveness of these approaches, we argue that target-dependent sentiment classification remains a challenge: how to effectively model the semantic relatedness of a target word with its context words in a sentence. one straight forward way to address this problem is to manually design a set of target-dependent features, and integrate them into existing feature-based svm. however, feature engineering is labor intensive and the\" sparse\" and\" discrete\" features are clumsy in encoding side information like target-context relatedness. in addition, a person asked to do this task will naturally\" look at\" parts of relevant context words which are helpful to determine the sentiment polarity of a sentence towards the target. these motivate us to develop a powerful neural network approach, which is capable of learning continuous features (representations) without feature engineering and meanwhile capturing the intricate relatedness between target and context words. in this paper, we present neural network models to deal with target-dependent sentiment classification. the approach is an extension on long short-term memory (lstm) by incorporating target information. such target-dependent lstm approach models the relatedness of a target word with its context words, and selects the relevant parts of contexts to infer the sentiment polarity towards the target. the model could be trained in an end-to-end way with standard backpropagation, where the loss function is cross-entropy error of supervised sentiment classification. we apply the neural model to target-dependent sentiment classification on a benchmark dataset. we compare with feature-based svm, adaptive recursive neural network and lexicon-enhanced neural network. empirical results show that the proposed approach without using syntactic parser or external sentiment lexicon obtains state-of-the-art classification accuracy. in addition, we find that modeling sentence with standard lstm does not perform well on this target-dependent task. integrating target information into lstm could significantly improve the classification accuracy. section: the approach we describe the proposed approach for target-dependent sentiment classification in this section. we first present a basic long short-term memory (lstm) approach, which models the semantic representation of a sentence without considering the target word being evaluated. afterwards, we extend lstm by considering the target word, obtaining the target-dependent long short-term memory (td-lstm) model. finally, we extend td-lstm with target connection, where the semantic relatedness of target with its context words are incorporated. subsection: long short-term memory (lstm) in this part, we describe a long short-term memory (lstm) model for target-dependent sentiment classification. it is a basic version of our approach. in this setting, the target to be evaluated is ignored so that the task is considered in a target independent way. we use lstm as it is a state-of-the-art performer for semantic composition in the area of sentiment analysis. it is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. the sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding. all the word vectors are stacked in a word embedding matrix, where is the dimension of word vector and is vocabulary size. in this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms to make better use of semantic and grammatical associations of words. we use lstm to compute the vector of a sentence from the vectors of words it contains, an illustration of the model is shown in figure [reference]. lstm is a kind of recurrent neural network (rnn), which is capable of mapping vectors of words with variable length to a fixed-length vector by recursively transforming current word vector with the output vector of the previous step. the transition function of standard rnn is a linear layer followed by a pointwise non-linear layer such as hyperbolic tangent function (). where,, is dimension of word vector. however, standard rnn suffers the problem of gradient vanishing or exploding, where gradients may grow or decay exponentially over long sequences. many researchers use a more sophisticated and powerful lstm cell as the transition function, so that long-distance semantic correlations in a sequence could be better modeled. compared with standard rnn, lstm cell contains three additional neural gates: an input gate, a forget gate and an output gate. these gates adaptively remember input vector, forget previous history and generate output vector. lstm cell is calculated as follows. where stands for element-wise multiplication, is sigmoid function,,,,,, are the parameters of input, forget and output gates. after calculating the hidden vector of each position, we regard the last hidden vector as the sentence representation. we feed it to a linear layer whose output length is class number, and add a layer to output the probability of classifying the sentence as positive, negative or neutral. softmax function is calculated as follows, where is the number of sentiment categories. subsection: target-dependent lstm (td-lstm) the aforementioned lstm model solves target-dependent sentiment classification in a target-independent way. that is to say, the feature representation used for sentiment classification remains the same without considering the target words. let us again take\" i bought a new camera. the picture quality is amazing but the battery life is too short\" as an example. the representations of this sentence with regard to picture quality and battery life are identical. this is evidently problematic as the sentiment polarity labels towards these two targets are different. to take into account of the target information, we make a slight modification on the aforementioned lstm model and introduce a target-dependent lstm (td-lstm) in this subsection. the basic idea is to model the preceding and following contexts surrounding the target string, so that contexts in both directions could be used as feature representations for sentiment classification. we believe that capturing such target-dependent context information could improve the accuracy of target-dependent sentiment classification. specifically, we use two lstm neural networks, a left one lstm and a right one lstm, to model the preceding and following contexts respectively. an illustration of the model is shown in figure [reference]. the input of lstm is the preceding contexts plus target string, and the input of lstm is the following contexts plus target string. we run lstm from left to right, and run lstm from right to left. we favor this strategy as we believe that regarding target string as the last unit could better utilize the semantics of target string when using the composed representation for sentiment classification. afterwards, we concatenate the last hidden vectors of lstm and lstm, and feed them to a layer to classify the sentiment polarity label. one could also try averaging or summing the last hidden vectors of lstm and lstm as alternatives. subsection: target-connection lstm (tc-lstm) compared with lstm model, target-dependent lstm (td-lstm) could make better use of the target information. however, we think td-lstm is still not good enough because it does not capture the interactions between target word and its contexts. furthermore, a person asked to do target-dependent sentiment classification will select the relevant context words which are helpful to determine the sentiment polarity of a sentence towards the target. based on the consideration mentioned above, we go one step further and develop a target-connection long short-term memory (tc-lstm). this model extends td-lstm by incorporating an target connection component, which explicitly utilizes the connections between target word and each context word when composing the representation of a sentence. an overview of tc-lstm is illustrated in figure [reference]. the input of tc-lstm is a sentence consisting of words and a target string occurs in the sentence. we represent target as because a target could be a word sequence of variable length, such as\" google\" or\" harry potter\". when processing a sentence, we split it into three components: target words, preceding context words and following context words. we obtain target vector by averaging the vectors of words it contains, which has been proven to be simple and effective in representing named entities. when compute the hidden vectors of preceding and following context words, we use two separate long short-term memory models, which are similar with the strategy used in td-lstm. the difference is that in tc-lstm the input at each position is the concatenation of word embedding and target vector, while in td-lstm the input at each position only includes the embedding of current word. we believe that tc-lstm could make better use of the connection between target and each context word when building the representation of a sentence. subsection: model training we train lstm, td-lstm and tc-lstm in an end-to-end way in a supervised learning framework. the loss function is the cross-entropy error of sentiment classification. where is the training data, is the number of sentiment categories, means a sentence, is the probability of predicting as class given by the layer, indicates whether class is the correct sentiment category, whose value is 1 or 0. we take the derivative of loss function through back-propagation with respect to all parameters, and update parameters with stochastic gradient descent. section: experiment we apply the proposed method to target-dependent sentiment classification to evaluate its effectiveness. we describe experimental setting and empirical results in this section. subsection: experimental settings we conduct experiment in a supervised setting on a benchmark dataset. each instance in the training/ test set has a manually labeled sentiment polarity. training set contains 6, 248 sentences and test set has 692 sentences. the percentages of positive, negative and neutral in training and test sets are both 25%, 25%, 50%. we train the model on training set, and evaluate the performance on test set. evaluation metrics are accuracy and macro-f1 score over positive, negative and neutral categories. subsection: comparison to other methods we compare with several baseline methods, including: in svm-indep, svm classifier is built with target-independent features, such as unigram, bigram, punctuations, emoticons, hashtags, the numbers of positive or negative words in general inquirer sentiment lexicon. in svm-dep, target-dependent features are also concatenated as the feature representation. in recursive nn, standard recursive neural network is used for feature learning over a transfered target-dependent dependency tree. adarnn-w/ oe, adarnn-w/ e and adarnn-comb are different variations of adaptive recursive neural network, whose composition functions are adaptively selected according to the inputs. in target-dep, svm classifier is built based on rich target-independent and target-dependent features. in target-dep+, sentiment lexicon features are further incorporated. the neural models developed in this paper are abbreviated as lstm, td-lstm and tc-lstm, which are described in the previous section. we use 100-dimensional glove vectors learned from twitter, randomize the parameters with uniform distribution, set the clipping threshold of softmax layer as 200 and set learning rate as 0.01. experimental results of baseline models and our methods are given in table [reference]. comparing between svm-indep and svm-dep, we can find that incorporating target information can improve the classification accuracy of a basic svm classifier. adarnn performs better than feature based svm by making use of dependency parsing information and tree-structured semantic composition. we can find that target-dep is a strong performer even without using lexicon features. it benefits from rich automatic features generated from word embeddings. among lstm based models described in this paper, the basic lstm approach performs worst. this is not surprising because this task requires understanding target-dependent text semantics, while the basic lstm model does not capture any target information so that it predicts the same result for different targets in a sentence. td-lstm obtains a big improvement over lstm when target signals are taken into consideration. this result demonstrates the importance of target information for target-dependent sentiment classification. by incorporating target-connection mechanism, tc-lstm obtains the best performances and outperforms all baseline methods in term of classification accuracy. comparing between target-dep and target-dep, we find that sentiment lexicon feature could further improve the classification accuracy. our final model tc-lstm without using sentiment lexicon information performs comparably with target-dep. we believe that incorporation lexicon information in tc-lstm could get further improvement. we leave this as a potential future work. subsection: effects of word embeddings it is well accepted that a good word embedding is crucial to composing a powerful text representation at higher level. we therefore study the effects of different word embeddings on lstm, td-lstm and tc-lstm in this part. since the benchmark dataset from comes from twitter, we compare between sentiment-specific word embedding (sswe) and glove vectors. all these word vectors are 50-dimensional and learned from twitter. sswe, sswe and sswe are different embedding learning algorithms introduced in. sswe and sswe learn word embeddings by only using sentiment of sentences. sswe takes into account of sentiment of sentences and contexts of words simultaneously. from figure [reference], we can find that sswe and sswe perform worse than sswe, which is consistent with the results reported on target-independent sentiment classification of tweets. this shows the importance of context information for word embedding learning as both sswe and sswe do not encode any word contexts. glove and sswe perform comparably, which indicates the importance of global context for estimating a good word representation. in addition, the target connection model tc-lstm performs best when considering a specific word embedding. we compare between glove vectors with different dimensions (50/ 100/ 200). classification accuracy and time cost are given in figure [reference] and table [reference], respectively. we can find that 100-dimensional word vectors perform better than 50-dimensional word vectors, while 200-dimensional word vectors do not show significant improvements. furthermore, td-lstm and lstm have similar time cost, while td-lstm gets higher classification accuracy as target information is incorporated. tc-lstm performs slightly better than td-lstm while at the cost of longer training time because the parameter number of tc-lstm is larger. subsection: case study in this section, we explore to what extent the target-dependent lstm models including td-lstm and tc-lstm improve the performance of a basic lstm model. in table [reference], we list some examples whose polarity labels are incorrectly inferred by lstm but correctly predicted by both td-lstm and tc-lstm. we observe that lstm model prefers to assigning the polarity of the entire sentence while ignoring the target to be evaluated. td-lstm and tc-lstm could take into account of target information to some extend. for example, in the 2nd example the opinion holder expresses a negative opinion about his work, but holds a neutral sentiment towards the target\" lindsay lohan\". in the last example, the whole sentence expresses a neutral sentiment while it holds a positive opinion towards\" google\". we analyse the error cases that both td-lstm and tc-lstm can not well handle, and find that 85.4% of the misclassified examples relate to neutral category. the positive instances are rarely misclassified as negative, and vice versa. a example of errors is:\" freaky friday on television reminding me to think wtf happened to lindsay lohan, she was such a terrific actress,+ my huge crush on haley hudson.\", which is incorrectly predicted as positive towards target\" indsay lohan\" in both td-lstm and tc-lstm. subsection: discussion in order to capture the semantic relatedness between target and context words, we extend td-lstm by adding a target connection component. one could also try other extensions to capture the connection between target and context words. for example, we also tried an attention-based lstm model, which is inspired by the recent success of attention-based neural network in machine translation and document encoding. we implement the soft-attention mechanism to enhance td-lstm. we incorporate two attention layers for preceding lstm and following lstm, respectively. the output vector for each attention layer is the weighted average among hidden vectors of lstm, where the weight of each hidden vector is calculated with a feedforward neural network. the outputs of preceding and following attention models are concatenated and fed to for sentiment classification. however, we can not obtain better result with such an attention model. the accuracy of this attention model is slightly lower than the standard lstm model (around 65%), which means that the attention component has a negative impact on the model. a potential reason might be that the attention based lstm has larger number of parameters, which can not be easily optimized with the small number of corpus. section: related work we briefly review existing studies on target-dependent sentiment classification and neural network approaches for sentiment classification in this section. subsection: target-dependent sentiment classification target-dependent sentiment classification is typically regarded as a kind of text classification problem in literature. therefore, standard text classification approach such as feature-based supported vector machine can be naturally employed to build a sentiment classifier. despite the effectiveness of feature engineering, it is labor intensive and unable to discover the discriminative or explanatory factors of data. to handle this problem, some recent studies use neural network methods and encode each sentence in continuous and low-dimensional vector space without feature engineering. dong et al. dong2014a transfer a dependency tree of a sentence into a target-specific recursive structure, and get higher level representation based on that structure. vo and zhang vo2015 use rich features including sentiment-specific word embedding and sentiment lexicons. different from previous studies, the lstm models developed in this work are purely data-driven, and do not rely on dependency parsing results or external sentiment lexicons. subsection: neural network for sentiment classification neural network approaches have shown promising results on many sentence/ document-level sentiment classification. the power of neural model lies in its ability in learning continuous text representation from data without any feature engineering. for sentence/ document level sentiment classification, previous studies mostly have two steps. they first learn continuous word vector embeddings from data. afterwards, semantic compositional approaches are used to compute the vector of a sentence/ document from the vectors of its constituents based on the principle of compositionality. representative compositional approaches to learn sentence representation include recursive neural networks, convolutional neural network, long short-term memory and tree-structured lstm. there also exists some studies focusing on learning continuous representation of documents. section: conclusion we develop target-specific long short term memory models for target-dependent sentiment classification. the approach captures the connection between target word and its contexts when generating the representation of a sentence. we train the model in an end-to-end way on a benchmark dataset, and show that incorporating target information could boost the performance of a long short-term memory model. the target-dependent lstm model obtains state-of-the-art classification accuracy. section: acknowledgements we greatly thank yaming sun for tremendously helpful discussions. this work was supported by the national high technology development 863 program of china (no. 2015aa015407), national natural science foundation of china (no. 61632011 and no.61273321). according to the meaning given to this role by harbin institute of technology, the contact author of this paper is bing qin. bibliography: references",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "target dependent long short-term memory",
                        498
                    ],
                    [
                        "target-dependent long short-term memory",
                        4945
                    ],
                    [
                        "td-lstm",
                        4987
                    ],
                    [
                        "target-dependent lstm",
                        7913
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "target-dependent sentiment classification",
                        30
                    ],
                    [
                        "sentiment analysis",
                        1134
                    ],
                    [
                        "target-independent sentiment classification",
                        16754
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "0b4b6932d5df74b366d9235b40334bc40d719c72-11",
    "doctext": "published as a conference paper at iclr 2017 temporal ensembling for semi-supervised learning section: abstract in this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. we introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. this ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in svhn with 500 labels and from 18.63% to 16.55% in cifar-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. we additionally obtain a clear improvement in cifar-100 classification accuracy by using random images from the tiny images dataset as unlabeled extra inputs during training. finally, we demonstrate good tolerance to incorrect labels. section: introduction it has long been known that an ensemble of multiple neural networks generally yields better predictions than a single network in the ensemble. this effect has also been indirectly exploited when training a single network through dropout [reference], dropconnect [reference], or stochastic depth [reference] regularization methods, and in swapout networks [reference], where training always focuses on a particular subset of the network, and thus the complete network can be seen as an implicit ensemble of such trained sub-networks. we extend this idea by forming ensemble predictions during training, using the outputs of a single network on different training epochs and under different regularization and input augmentation conditions. our training still operates on a single network, but the predictions made on different epochs correspond to an ensemble prediction of a large number of individual sub-networks because of dropout regularization. this ensemble prediction can be exploited for semi-supervised learning where only a small portion of training data is labeled. if we compare the ensemble prediction to the current output of the network being trained, the ensemble prediction is likely to be closer to the correct, unknown labels of the unlabeled inputs. therefore the labels inferred this way can be used as training targets for the unlabeled inputs. our method relies heavily on dropout regularization and versatile input augmentation. indeed, without neither, there would be much less reason to place confidence in whatever labels are inferred for the unlabeled training data. we describe two ways to implement self-ensembling,\u03c0-model and temporal ensembling. both approaches surpass prior state-of-the-art results in semi-supervised learning by a considerable margin. we furthermore observe that self-ensembling improves the classification accuracy in fully labeled cases as well, and provides tolerance against incorrect labels. the recently introduced transform/ stability loss of [reference] is based on the same principle as our work, and the\u03c0-model can be seen as a special case of it. the\u03c0-model can also be seen as a simplification of the\u03b3-model of the ladder network by [reference], a previously presented network architecture for semi-supervised learning. our temporal ensembling method has connections to the bootstrapping method of [reference] targeted for training with noisy labels. 1|b| i\u2208 (b\u2229l) log z i [y i] supervised loss component+ w (t) 1 c|b| i\u2208b||z i\u2212z i|| 2 unsupervised loss component update\u03b8 using, e.g., adam update network parameters end for end for return\u03b8 section: self-ensembling during training we present two implementations of self-ensembling during training. the first one,\u03c0-model, encourages consistent network output between two realizations of the same input stimulus, under two different dropout conditions. the second method, temporal ensembling, simplifies and extends this by taking into account the network predictions over multiple previous training epochs. we shall describe our methods in the context of traditional image classification networks. let the training data consist of total of n inputs, out of which m are labeled. the input stimuli, available for all training data, are denoted x i, where i\u2208 {1... n}. let set l contain the indices of the labeled inputs,|l|= m. for every i\u2208 l, we have a known correct label y i\u2208 {1... c}, where c is the number of different classes. section:\u03c0-model the structure of\u03c0-model is shown in figure 1 (top), and the pseudocode in algorithm 1. during training, we evaluate the network for each training input x i twice, resulting in prediction vectors z i andz i. our loss function consists of two components. the first component is the standard crossentropy loss, evaluated for labeled inputs only. the second component, evaluated for all inputs, penalizes different predictions for the same training input x i by taking the mean square difference between the prediction vectors z i andz i. 1 to combine the supervised and unsupervised loss terms, we scale the latter by time-dependent weighting function w (t). by comparing the entire output vectors z i andz i, we effectively ask the\" dark knowledge\" [reference] between the two evaluations to be close, which is a much stronger requirement compared to asking that only the final classification remains the same, which is what happens in traditional training. it is important to notice that, because of dropout regularization, the network output during training is a stochastic variable. thus two evaluations of the same input x i under same network weights\u03b8 yield different results. in addition, gaussian noise and augmentations such as random translation are evaluated twice, resulting in additional variation. the combination of these effects explains the difference between the prediction vectors z i andz i. this difference can be seen as an error in classification, given that the original input x i was the same, and thus minimizing it is a reasonable goal. in our implementation, the unsupervised loss weighting function w (t) ramps up, starting from zero, along a gaussian curve during the first 80 training epochs. see appendix a for further details about this and other training parameters. in the beginning the total loss and the learning gradients are thus dominated by the supervised loss component, i.e., the labeled data only. we have found it to be very important that the ramp-up of the unsupervised loss component is slow enough-otherwise, the network gets easily stuck in a degenerate solution where no meaningful classification of the data is obtained. our approach is somewhat similar to the\u03b3-model of the ladder network by [reference], but conceptually simpler. in the\u03c0-model, the comparison is done directly on network outputs, i.e., after softmax activation, and there is no auxiliary mapping between the two branches such as the learned denoising functions in the ladder network architecture. furthermore, instead of having one\" clean\" and one\" corrupted\" branch as in\u03b3-model, we apply equal augmentation and noise to the inputs for both branches. as shown in section 3, the\u03c0-model combined with a good convolutional network architecture provides a significant improvement over prior art in classification accuracy. section: temporal ensembling analyzing how the\u03c0-model works, we could equally well split the evaluation of the two branches in two separate phases: first classifying the training set once without updating the weights\u03b8, and then training the network on the same inputs under different augmentations and dropout, using the just obtained predictions as targets for the unsupervised loss component. as the training targets obtained this way are based on a single evaluation of the network, they can be expected to be noisy. temporal ensembling alleviates this by aggregating the predictions of multiple previous network evaluations into an ensemble prediction. it also lets us evaluate the network only once during training, gaining an approximate 2x speedup over the\u03c0-model. the structure of our temporal ensembling method is shown in figure 1 (bottom), and the pseudocode in algorithm 2. the main difference to the\u03c0-model is that the network and augmentations are evaluated only once per input per epoch, and the target vectorsz for the unsupervised loss component are based on prior network evaluations instead of a second evaluation of the network. after every training epoch, the network outputs z i are accumulated into ensemble outputs z i by updating z i\u2190\u03b1z i+ (1\u2212\u03b1) z i, where\u03b1 is a momentum term that controls how far the ensemble reaches into training history. because of dropout regularization and stochastic augmentation, z thus contains a weighted average of the outputs of an ensemble of networks f from previous training epochs, with recent epochs having larger weight than distant epochs. for generating the training targetsz, we need to correct for the startup bias in z by dividing by factor (1\u2212\u03b1 t). a similar bias correction has been used in, e.g., [reference] and mean-only batch normalization [reference]. on the first training epoch, z andz are zero as no data from previous epochs is available. for this reason, we specify the unsupervised weight ramp-up function w (t) to also be zero on the first training epoch. algorithm 2 temporal ensembling pseudocode. note that the updates of z andz could equally well be done inside the minibatch loop; in this pseudocode they occur between epochs for clarity. require: x i= training stimuli require: l= set of training input indices with known labels require: y i= labels for labeled inputs i\u2208 l require:\u03b1= ensembling momentum, 0\u2264\u03b1< 1 require: w (t)= unsupervised weight ramp-up function require: f\u03b8 (x)= stochastic neural network with trainable parameters\u03b8 require: g (x)= stochastic input augmentation function unsupervised loss component update\u03b8 using, e.g., adam update network parameters end for construct target vectors by bias correction end for return\u03b8 the benefits of temporal ensembling compared to\u03c0-model are twofold. first, the training is faster because the network is evaluated only once per input on each epoch. second, the training targets z can be expected to be less noisy than with\u03c0-model. as shown in section 3, we indeed obtain somewhat better results with temporal ensembling than with\u03c0-model in the same number of training epochs. the downside compared to\u03c0-model is the need to store auxiliary data across epochs, and the new hyperparameter\u03b1. while the matrix z can be fairly large when the dataset contains a large number of items and categories, its elements are accessed relatively infrequently. thus it can be stored, e.g., in a memory mapped file. an intriguing additional possibility of temporal ensembling is collecting other statistics from the network predictions z i besides the mean. for example, by tracking the second raw moment of the network outputs, we can estimate the variance of each output component z i, j. this makes it possible to reason about the uncertainty of network outputs in a principled way [reference]. based on this information, we could, e.g., place more weight on more certain predictions vs. uncertain ones in the unsupervised loss term. however, we leave the exploration of these avenues as future work. section: results our network structure is given in table 5, and the test setup and all training parameters are detailed in appendix a. we test the\u03c0-model and temporal ensembling in two image classification tasks, cifar-10 and svhn, and report the mean and standard deviation of 10 runs using different random seeds. although it is rarely stated explicitly, we believe that our comparison methods do not use input augmentation, i.e., are limited to dropout and other forms of permutation-invariant noise. therefore we report the error rates without augmentation, unless explicitly stated otherwise. given that the ability of an algorithm to extract benefit from augmentation is also an important property, we report the classification accuracy using a standard set of augmentations as well. in purely supervised training the de facto standard way of augmenting the cifar-10 dataset includes horizontal flips and random translations, while svhn is limited to random translations. by using these same augmentations we can compare against the best fully supervised results as well. after all, the fully supervised results should indicate the upper bound of obtainable accuracy. [reference] 20.40\u00b1 0.47 catgan [reference] 19.58\u00b1 0.58 gan of 18.63\u00b1 2.32\u03c0-model 16.55\u00b1 0.29 6.90\u00b1 0.07\u03c0-model with augmentation 12.36\u00b1 0.31 5.56\u00b1 0.10 temporal ensembling with augmentation 12.16\u00b1 0.24 5.60\u00b1 0.10 table 2: svhn results for 500 and 1000 labels, averages of 10 runs (4 runs for all labels). error rate (%) with# labels 36.02\u00b1 0.10 virtual adversarial [reference] 24.63 adgm [reference] 22.86 sdgm [reference] 16.61\u00b1 0.24 gan of 18.44\u00b1 4.8 8. section: cifar-10 cifar-10 is a dataset consisting of 32\u00d7 32 pixel rgb images from ten classes. table 1 shows a 2.1 percentage point reduction in classification error rate with 4000 labels (400 per class) compared to earlier methods for the non-augmented\u03c0-model. enabling the standard set of augmentations further reduces the error rate by 4.2 percentage points to 12.36%. temporal ensembling is slightly better still at 12.16%, while being twice as fast to train. this small improvement conceals the subtle fact that random horizontal flips need to be done independently for each epoch in temporal ensembling, while\u03c0-model can randomize once per a pair of evaluations, which according to our measurements is\u223c0.5 percentage points better than independent flips. a principled comparison with [reference] is difficult due to several reasons. they provide results only for a fairly extreme set of augmentations (translations, flipping, rotations, stretching, and shearing) on top of fractional max pooling [reference], which introduces random, local stretching inside the network, and is known to improve classification results substantially. they quote an error rate of only 13.60% for supervised-only training with 4000 labels, while our corresponding baseline is 34.85%. this gap indicates a huge benefit from versatile augmentations and fractional max pooling-in fact, their baseline result is already better than any previous semisupervised results. by enabling semi-supervised learning they achieve a 17% drop in classification error rate (from 13.60% to 11.29%), while we see a much larger relative drop of 65% (from 34.85% to 12.16%). section: svhn the street view house numbers (svhn) dataset consists of 32\u00d7 32 pixel rgb images of real-world house numbers, and the task is to classify the centermost digit. in svhn we chose to use only the. even with this choice our error rate with all labels is only 3.05% without augmentation. table 2 compares our method to the previous state-of-the-art. with the most commonly used 1000 labels we observe an improvement of 2.7 percentage points, from 8.11% to 5.43% without augmentation, and further to 4.42% with standard augmentations. we also investigated the behavior with 500 labels, where we obtained an error rate less than half of without augmentations, with a significantly lower standard deviation as well. when augmentations were enabled, temporal ensembling further reduced the error rate to 5.12%. in this test the difference between\u03c0-model and temporal ensembling was quite significant at 1.5 percentage points. in [reference] provide results without augmentation, with the caveat that they use fractional max pooling, which is a very augmentation-like technique due to the random, local stretching it introduces inside the network. it leads to a superb error rate of 2.28% in supervisedonly training, while our corresponding baseline is 3.05% (or 2.88% with translations). given that in a separate experiment our network matched the best published result for non-augmented svhn when extra data is used (1.69% from [reference]), this gap is quite surprising, and leads us to conclude that fractional max pooling leads to a powerful augmentation of the dataset, well beyond what simple translations can achieve. our temporal ensembling technique obtains better error rates for both 500 and 1000 labels (5.12% and 4.42%, respectively) compared to the 6.03% reported by sajjadi et al. for 732 labels. section: cifar-100 and tiny images the cifar-100 dataset consists of 32\u00d7 32 pixel rgb images from a hundred classes. we are not aware of previous semi-supervised results in this dataset, and chose 10000 labels for our experiments. table 3 shows error rates of 43.43% and 38.65% without and with augmentation, respectively. these correspond to 7.8 and 5.9 percentage point improvements compared to supervised learning with labeled inputs only. we ran two additional tests using unlabeled extra data from tiny images dataset [reference]: one with randomly selected 500k extra images, most not corresponding to any of the cifar-100 categories, and another with a restricted set of 237k images from the categories that correspond to those found in the cifar-100 dataset (see appendix a for details). the results are shown in table 4. the addition of randomly selected, unlabeled extra images improved the error rate by 2.7 percentage points (from 26.30% to 23.63%), indicating a desirable ability to learn from random natural images. temporal ensembling benefited much more from the extra data than the\u03c0-model. interestingly, restricting the extra data to categories that are present in cifar-100 did not improve with standard supervised training (left) the classification accuracy suffers when even a small portion of the labels give disinformation, and the situation worsens quickly as the portion of randomized labels increases to 50% or more. on the other hand, temporal ensembling (right) shows almost perfect resistance to disinformation when half of the labels are random, and retains over ninety percent classification accuracy even when 80% of the labels are random. the classification accuracy further. this indicates that in order to train a better classifier by adding extra data as unlabeled inputs, it is enough to have the extra data roughly in the same space as the actual inputs-in our case, natural images. we hypothesize that it may even be possible to use properly crafted synthetic data as unlabeled inputs to obtain improved classifiers. in order to keep the training times tolerable, we limited the number of unlabeled inputs to 50k per epoch in these tests, i.e., on every epoch we trained using all 50k labeled inputs from cifar-100 and 50k additional unlabeled inputs from tiny images. the 50k unlabeled inputs were chosen randomly on each epoch from the 500k or 237k extra inputs. in temporal ensembling, after each epoch we updated only the rows of z that corresponded to inputs used on that epoch. section: supervised learning when all labels are used for traditional supervised training, our network approximately matches the state-of-the-art error rate for a single model in cifar-10 with augmentation [reference][reference] at 6.05%, and without augmentation [reference] at 7.33%. the same is probably true for svhn as well, but there the best published results rely on extra data that we chose not to use. given this premise, it is perhaps somewhat surprising that our methods reduce the error rate also when all labels are used (tables 1 and 2). we believe that this is an indication that the consistency requirement adds a degree of resistance to ambiguous labels that are fairly common in many classification tasks, and that it encourages features to be more invariant to stochastic sampling. section: tolerance to incorrect labels in a further test we studied the hypothesis that our methods add tolerance to incorrect labels by assigning a random label to a certain percentage of the training set before starting to train. figure 2 shows the classification error graphs for standard supervised training and temporal ensembling. clearly our methods provide considerable resistance to wrong labels, and we believe this is because the unsupervised loss term encourages the mapping function implemented by the network to be flat in the vicinity of all input data points, whereas the supervised loss term enforces the mapping function to have a specific value in the vicinity of the labeled input data points. this means that even the wrongly labeled inputs play a role in shaping the mapping function-the unsupervised loss term smooths the mapping function and thus also the decision boundaries, effectively fusing the inputs into coherent clusters, whereas the excess of correct labels in each class is sufficient for locking the clusters to the right output vectors through the supervised loss term. the difference to classical regularizers is that we induce smoothness only on the manifold of likely inputs instead of over the entire input domain. for further analysis about the importance of the gradient of the mapping function, see [reference]. section: related work there is a large body of previous work on semi-supervised learning [reference]. in here we will concentrate on the ones that are most directly connected to our work.\u03b3-model is a subset of a ladder network [reference] that introduces lateral connections into an encoder-decoder type network architecture, targeted at semi-supervised learning. in\u03b3-model, all but the highest lateral connections in the ladder network are removed, and after pruning the unnecessary stages, the remaining network consists of two parallel, identical branches. one of the branches takes the original training inputs, whereas the other branch is given the same input corrupted with noise. the unsupervised loss term is computed as the squared difference between the (pre-activation) output of the clean branch and a denoised (pre-activation) output of the corrupted branch. the denoised estimate is computed from the output of the corrupted branch using a parametric nonlinearity that has 10 auxiliary trainable parameters per unit. our\u03c0-model differs from the\u03b3-model in removing the parametric nonlinearity and denoising, having two corrupted paths, and comparing the outputs of the network instead of pre-activation data of the final layer. [reference] recently introduced a new loss function for semi-supervised learning, so called transform/ stability loss, which is founded on the same principle as our work. during training, they run augmentation and network evaluation n times for each minibatch, and then compute an unsupervised loss term as the sum of all pairwise squared distances between the obtained n network outputs. as such, their technique follows the general pseudo-ensemble agreement (pea) regularization framework of [reference]. in addition, they employ a mutual exclusivity loss term [reference]) that we do not use. our\u03c0-model can be seen as a special case of the transform/ stability loss obtained by setting n= 2. the computational cost of training with transform/ stability loss increases linearly as a function of n, whereas the efficiency of our temporal ensembling technique remains constant regardless of how large effective ensemble we obtain via the averaging of previous epochs' predictions. in bootstrap aggregating, or bagging, multiple networks are trained independently based on subsets of training data [reference]. this results in an ensemble that is more stable and accurate than the individual networks. our approach can be seen as pulling the predictions from an implicit ensemble that is based on a single network, and the variability is a result of evaluating it under different dropout and augmentation conditions instead of training on different subsets of data. in work parallel to ours, [reference] store multiple snapshots of the network during training, hopefully corresponding to different local minima, and use them as an explicit ensemble. the general technique of inferring new labels from partially labeled data is often referred to as bootstrapping or self-training, and it was first proposed by [reference] in the context of linguistic analysis. [reference] analyze yarowsky's algorithm and propose a novel graph-based label propagation approach. similarly, label propagation methods [reference] infer labels for unlabeled training data by comparing the associated inputs to labeled training inputs using a suitable distance metric. our approach differs from this in two important ways. firstly, we never compare training inputs against each other, but instead only rely on the unknown labels remaining constant, and secondly, we let the network produce the likely classifications for the unlabeled inputs instead of providing them through an outside process. in addition to partially labeled data, considerable amount of effort has been put into dealing with densely but inaccurately labeled data. this can be seen as a semi-supervised learning task where part of the training process is to identify the labels that are not to be trusted. for recent work in this area, see, e.g., [reference] and [reference]. in this context of noisy labels, [reference] presented a simple bootstrapping method that trains a classifier with the target composed of a convex combination of the previous epoch output and the known but potentially noisy labels. our temporal ensembling differs from this by taking into account the evaluations over multiple epochs. generative adversarial networks (gan) have been recently used for semi-supervised learning with promising results [reference][reference][reference]. it additive gaussian noise\u03c3= 0.15 conv1a 128 filters, 3\u00d7 3, pad=' same', lrelu (\u03b1= 0.1) conv1b 128 filters, 3\u00d7 3, pad=' same', lrelu (\u03b1= 0.1) conv1c 128 filters, 3\u00d7 3, pad=' same', lrelu (\u03b1= 0.1) pool1 maxpool 2\u00d7 2 pixels drop1 dropout, p= 0.5 conv2a 256 filters, 3\u00d7 3, pad=' same', lrelu (\u03b1= 0.1) conv2b 256 filters, 3\u00d7 3, pad=' same', lrelu (\u03b1= 0.1) conv2c 256 filters, 3\u00d7 3, pad=' same', lrelu (\u03b1= 0.1) pool2 maxpool 2\u00d7 2 pixels drop2 dropout, p= 0.5 conv3a 512 filters, 3\u00d7 3, pad=' valid', lrelu (\u03b1= 0.1) conv3b 256 filters, 1\u00d7 1, lrelu (\u03b1= 0.1) conv3c 128 filters, 1\u00d7 1, lrelu (\u03b1= 0.1) pool3 global average pool (6\u00d7 6\u2192 1\u00d71 pixels) dense fully connected 128\u2192 10 output softmax could be an interesting avenue for future work to incorporate a generative component to our solution. we also envision that our methods could be applied to regression-type learning tasks. [reference]. all data layers were initialized following [reference], and we applied weight normalization and mean-only batch normalization [reference] with momentum 0.999 to all of them. we used leaky relu [reference] with\u03b1= 0.1 as the non-linearity, and chose to use max pooling instead of strided convolutions because it gave consistently better results in our experiments. all networks were trained using adam (kingma& ba, 2014) with a maximum learning rate of\u03bb max= 0.003, except for temporal ensembling in the svhn case where a maximum learning rate of\u03bb max= 0.001 worked better. adam momentum parameters were set to\u03b2 1= 0.9 and\u03b2 2= 0.999 as suggested in the paper. the maximum value for the unsupervised loss component was set to w max\u00b7 m/ n, where m is the number of labeled inputs and n is the total number of training inputs. for\u03c0-model runs, we used w max= 100 in all runs except for cifar-100 with tiny images where we set w max= 300. for temporal ensembling we used w max= 30 in most runs. for the corrupted label test in section 3.5 we used w max= 300 for 0% and 20% corruption, and w max= 3000 for corruption of 50% and higher. for basic cifar-100 runs we used w max= 100, and for cifar-100 with tiny images we used w max= 1000. the accumulation decay constant of temporal ensembling was set to\u03b1= 0.6 in all runs. in all runs we ramped up both the learning rate\u03bb and unsupervised loss component weight w during the first 80 epochs using a gaussian ramp-up curve exp [\u22125 (1\u2212 t) 2], where t advances linearly from zero to one during the ramp-up period. in addition to ramp-up, we annealed the learning rate\u03bb to zero and adam\u03b2 1 to 0.5 during the last 50 epochs, but otherwise we did not decay them during training. the ramp-down curve was similar to the ramp-up curve but time-reversed and with a scaling constant of 12.5 instead of 5. all networks were trained for 300 epochs with minibatch size of 100. section: cifar-10 following previous work in fully supervised learning, we pre-processed the images using zca and augmented the dataset using horizontal flips and random translations. the translations were drawn from [\u22122, 2] pixels, and were independently applied to both branches in the\u03c0-model. section: svhn we pre-processed the input images by biasing and scaling each input image to zero mean and unit variance. we used only the 73257 items in the official training set, i.e., did not use the provided 531131 extra items. the training setups were otherwise similar to cifar-10 except that horizontal flips were not used. section: implementation our implementation is written in python using theano [reference] and [reference], and is available at https:// github.com/ smlaine2/ tempens. model convergence as discussed in section 2.1, a slow ramp-up of the unsupervised cost is very important for getting the models to converge. furthermore, in our very preliminary tests with 250 labels in svhn we noticed that optimization tended to explode during the ramp-up period, and we eventually found that using a lower value for adam\u03b2 2 parameter (e.g., 0.99 instead of 0.999) seems to help in this regard. we do not attempt to guarantee that the occurrence of labeled inputs during training would be somehow stratified; with bad luck there might be several consecutive minibatches without any labeled inputs when the label density is very low. some previous work has identified this as a weakness, and have solved the issue by shuffling the input sequences in such a way that stratification is guaranteed, e.g. [reference] (confirmed from the authors). this kind of stratification might further improve the convergence of our methods as well. tiny images, extra data from restricted categories the restricted extra data in section 3.3 was extracted from tiny images by picking all images with labels corresponding to the 100 categories used in cifar-100. as the tiny images dataset does not contain cifar-100 categories aquarium fish and maple tree, we used images with labels fish and maple instead. the result was a total of 237 203 images that were used as unlabeled extra data. table 6 shows the composition of this extra data set. it is worth noting that the cifar-100 dataset itself is a subset of tiny images, and we did not explicitly prevent overlap between this extra set and cifar-100. this led to approximately a third of the cifar-100 training and test images being present as unlabeled inputs in the extra set. the other test with 500k extra entries picked randomly out of all 79 million images had a negligible overlap with cifar-100. section: section: acknowledgements we thank the anonymous reviewers, tero karras, pekka j\u00e4nis, tim salimans, ian goodfellow, as well as harri valpola and his colleagues at curious ai for valuable suggestions that helped to improve this article. section: section:: the tiny images [reference] labels and image counts used in the cifar-100 plus restricted extra data tests (rightmost column of table 4). note that the extra input images were supplied as unlabeled data for our networks, and the labels were used only for narrowing down the full set of 79 million images. section:",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar-10",
                        952
                    ],
                    [
                        "cifar-10 dataset",
                        12498
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "\u03c0-model",
                        2954
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        3152
                    ],
                    [
                        "accuracy",
                        12798
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "classification",
                        5644
                    ],
                    [
                        "image classification tasks",
                        11819
                    ],
                    [
                        "semi-supervised",
                        16867
                    ],
                    [
                        "classification tasks",
                        19947
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "svhn",
                        902
                    ],
                    [
                        "street view house numbers",
                        14922
                    ],
                    [
                        "svhn) dataset",
                        14950
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "\u03c0-model",
                        2954
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        3152
                    ],
                    [
                        "accuracy",
                        12798
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "classification",
                        5644
                    ],
                    [
                        "image classification tasks",
                        11819
                    ],
                    [
                        "semi-supervised",
                        16867
                    ],
                    [
                        "classification tasks",
                        19947
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "0c5c93594647d77dcbad5f3ed71139b186a23a3c-12",
    "doctext": "improving neural machine translation models with monolingual data section: abstract neural machine translation (nmt) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation, and we investigate the use of monolingual data for nmt. in contrast to previous work, which combines nmt models with separately trained language models, we note that encoder-decoder nmt architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. by pairing monolingual training data with an automatic backtranslation, we can treat it as additional parallel training data, and we obtain substantial improvements on the wmt 15 task english\u2194german (+ 2.8-3.7 bleu), and for the low-resourced iwslt 14 task turkish\u2192english (+ 2.1-3.4 bleu), obtaining new state-of-the-art results. we also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the iwslt 15 task english\u2192german. section: introduction neural machine translation (nmt) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. target-side monolingual data plays an important role in boosting fluency for phrase-based statisti-the research presented in this publication was conducted in cooperation with samsung electronics polska sp. z o.o.-samsung r& d institute poland. cal machine translation, and we investigate the use of monolingual data for nmt. language models trained on monolingual data have played a central role in statistical machine translation since the first ibm models [reference]). there are two major reasons for their importance. firstly, word-based and phrase-based translation models make strong independence assumptions, with the probability of translation units estimated independently from context, and language models, by making different independence assumptions, can model how well these translation units fit together. secondly, the amount of available monolingual data in the target language typically far exceeds the amount of parallel data, and models typically improve when trained on more data, or data more similar to the translation task. in (attentional) encoder-decoder architectures for neural machine translation [reference][reference], the decoder is essentially an rnn language model that is also conditioned on source context, so the first rationale, adding a language model to compensate for the independence assumptions of the translation model, does not apply. however, the data argument is still valid in nmt, and we expect monolingual data to be especially helpful if parallel data is sparse, or a poor fit for the translation task, for instance because of a domain mismatch. in contrast to previous work, which integrates a separately trained rnn language model into the nmt model [reference], we explore strategies to include monolingual training data in the training process without changing the neural network architecture. this makes our approach applicable to different nmt architectures. the main contributions of this paper are as follows:\u2022 we show that we can improve the machine translation quality of nmt systems by mixing monolingual target sentences into the training set.\u2022 we investigate two different methods to fill the source side of monolingual training instances: using a dummy source sentence, and using a source sentence obtained via backtranslation, which we call synthetic. we find that the latter is more effective.\u2022 we successfully adapt nmt models to a new domain by fine-tuning with either monolingual or parallel in-domain data. section: neural machine translation we follow the neural machine translation architecture by [reference], which we will briefly summarize here. however, we note that our approach is not specific to this architecture. the neural machine translation system is implemented as an encoder-decoder network with recurrent neural networks. the encoder is a bidirectional neural network with gated recurrent units [reference] that reads an input sequence x= (x 1,..., x m) and calculates a forward sequence of hidden states (\u2212\u2192 h 1,...,\u2212\u2192 h m), and a backward sequence the hidden states\u2212\u2192 h j and\u2190\u2212 h j are concatenated to obtain the annotation vector h j. the decoder is a recurrent neural network that predicts a target sequence y= (y 1,..., y n). each word y i is predicted based on a recurrent hidden state s i, the previously predicted word y i\u22121, and a context vector c i. c i is computed as a weighted sum of the annotations h j. the weight of each annotation h j is computed through an alignment model\u03b1 ij, which models the probability that y i is aligned to x j. the alignment model is a singlelayer feedforward neural network that is learned jointly with the rest of the network through backpropagation. a detailed description can be found in [reference]. training is performed on a parallel corpus with stochastic gradient descent. for translation, a beam search with small beam size is employed. section: nmt training with monolingual training data in machine translation, more monolingual data (or monolingual data more similar to the test set) serves to improve the estimate of the prior probability p (t) of the target sentence t, before taking the source sentence s into account. in contrast to [reference], who train separate language models on monolingual training data and incorporate them into the neural network through shallow or deep fusion, we propose techniques to train the main nmt model with monolingual data, exploiting the fact that encoder-decoder neural networks already condition the probability distribution of the next target word on the previous target words. we describe two strategies to do this: providing monolingual training examples with an empty (or dummy) source sentence, or providing monolingual training data with a synthetic source sentence that is obtained from automatically translating the target sentence into the source language, which we will refer to as back-translation. section: dummy source sentences the first technique we employ is to treat monolingual training examples as parallel examples with empty source side, essentially adding training examples whose context vector c i is uninformative, and for which the network has to fully rely on the previous target words for its prediction. this could be conceived as a form of dropout [reference], with the difference that the training instances that have the context vector dropped out constitute novel training data. we can also conceive of this setup as multi-task learning, with the two tasks being translation when the source is known, and language modelling when it is unknown. during training, we use both parallel and monolingual training examples in the ratio 1-to-1, and randomly shuffle them. we define an epoch as one iteration through the parallel data set, and resample from the monolingual data set for every epoch. we pair monolingual sentences with a single-word dummy source side< null> to allow processing of both parallel and monolingual training examples with the same network graph. 1 for monolingual minibatches 2, we freeze the network parameters of the encoder and the attention model. one problem with this integration of monolin-gual data is that we can not arbitrarily increase the ratio of monolingual training instances, or finetune a model with only monolingual training data, because different output layer parameters are optimal for the two tasks, and the network' unlearns' its conditioning on the source context if the ratio of monolingual training instances is too high. section: synthetic source sentences to ensure that the output layer remains sensitive to the source context, and that good parameters are not unlearned from monolingual data, we propose to pair monolingual training instances with a synthetic source sentence from which a context vector can be approximated. we obtain these through back-translation, i.e. an automatic translation of the monolingual target text into the source language. during training, we mix synthetic parallel text into the original (human-translated) parallel text and do not distinguish between the two: no network parameters are frozen. importantly, only the source side of these additional training examples is synthetic, and the target side comes from the monolingual corpus. section: evaluation we evaluate nmt training on parallel text, and with additional monolingual data, on english\u2194german and turkish\u2192english, using training and test data from wmt 15 for english\u2194german, iwslt 15 for english\u2192german, and iwslt 14 for turkish\u2192english. section: data and methods we use groundhog 3 as the implementation of the nmt system for all experiments [reference][reference]. we generally follow the settings and training procedure described by [reference]. for english\u2194german, we report case-sensitive bleu on detokenized text with mteval-v13a.pl for comparison to official wmt and iwslt results. for turkish\u2192english, we report case-sensitive bleu on tokenized text with multi-bleu.perl for comparison to results by [reference]. [reference] determine the network vocabulary based on the parallel training data, and replace out-of-vocabulary words with a special unk symbol. they remove monolingual sentences with more than 10% unk symbols. in contrast, we represent unseen words as sequences of subword units [reference], and can represent any additional training data with the existing network vocabulary that was learned on the parallel data. in all experiments, the network vocabulary remains fixed. section: english\u2194german we use all parallel training data provided by wmt 2015 [reference] 4. we use the news crawl corpora as additional training data for the experiments with monolingual data. the amount of training data is shown in table 1. baseline models are trained for a week. ensembles are sampled from the last 4 saved models of training (saved at 12h-intervals). each model is fine-tuned with fixed embeddings for 12 hours. for the experiments with synthetic parallel data, we back-translate a random sample of 3 600 000 sentences from the german monolingual data set into english. the german\u2192english system used for this is the baseline system (parallel). translation took about a week on an nvidia titan black gpu. for experiments in german\u2192english, we back-translate 4 200 000 monolingual english sentences into german, using the english\u2192german system+ synthetic. note that we always use single models for backtranslation, not ensembles. we leave it to future work to explore how sensitive nmt training with synthetic data is to the quality of the backtranslation. we tokenize and truecase the training data, and represent rare words via bpe [reference]. specifically, we follow [reference] in performing bpe on the joint vocabulary with 89 500 merge operations. dataset sentences wit 160 000 setimes 160 000 gigaword mono 177 000 000 gigaword synth 3 200 000 table 2: turkish\u2192english training data. the network vocabulary size is 90 000. we also perform experiments on the iwslt 15 test sets to investigate a cross-domain setting. [reference] the test sets consist of ted talk transcripts. as indomain training data, iwslt provides the wit 3 parallel corpus [reference], which also consists of ted talks. section: turkish\u2192english we use data provided for the iwslt 14 machine translation track [reference], namely the wit 3 parallel corpus [reference], which consists of ted talks, and the setimes corpus (tyers and alperen, 2010). [reference] after removal of sentence pairs which contain empty lines or lines with a length ratio above 9, we retain 320 000 sentence pairs of training data. for the experiments with monolingual training data, we use the english ldc gigaword corpus (fifth edition). the amount of training data is shown in table 2. with only 320 000 sentences of parallel data available for training, this is a much lower-resourced translation setting than english\u2194german. [reference] segment the turkish text with the morphology tool zemberek, followed by a disambiguation of the morphological analysis [reference], and removal of non-surface tokens produced by the analysis. we use the same preprocessing 7. for both turkish and english, we represent rare words (or morphemes in the case of turkish) as character bigram sequences [reference]. the 20 000 most frequent words (morphemes) are left unsegmented. the networks have a vocabulary size of 23 000 symbols. to obtain a synthetic parallel training set, we back-translate a random sample of 3 200 000 sentences from gigaword. we use an english\u2192turkish nmt system trained with the same settings as the turkish\u2192english baseline system. we found overfitting to be a bigger problem than with the larger english\u2194german data set, and follow [reference] in using gaussian noise (stddev 0.01) [reference], and dropout on the output layer (p=0.5) [reference]. we also use early stopping, based on bleu measured every three hours on tst2010, which we treat as development set. for turkish\u2192english, we use gradient clipping with threshold 5, following [reference], in contrast to the threshold 1 that we use for english\u2194german, following [reference]. table 3 shows english\u2192german results with wmt training and test data. we find that mixing parallel training data with monolingual data with a dummy source side in a ratio of 1-1 improves quality by 0.4-0.5 bleu for the single system, 1 bleu for the ensemble. we train the system for twice as long as the baseline to provide the training algorithm with a similar amount of parallel training instances. to ensure that the quality improvement is due to the monolingual training instances, and not just increased training time, we also continued training our baseline system for another week, but saw no improvements in bleu. section: results section: english\u2192german wmt 15 including synthetic data during training is very effective, and yields an improvement over our baseline by 2.8-3.4 bleu. our best ensemble system also outperforms a syntaxbased baseline [reference]) by 1.2-2.1 bleu. we also substantially outperform nmt results reported by [reference] and, who previously reported sota result. [reference] we note that the difference is particularly large for single systems, since our ensemble is not as diverse as that of, who used 8 independently trained ensemble components, whereas we sampled 4 ensemble components from the same training run. test sets, which are news texts. we investigate if monolingual training data is especially valuable if it can be used to adapt a model to a new genre or domain, specifically adapting a system trained on wmt data to translating ted talks. systems 1 and 2 correspond to systems in table 3, trained only on wmt data. system 2, trained on parallel and synthetic wmt data, obtains a bleu score of 25.5 on tst2015. we observe that even a small amount of fine-tuning 9, i.e. continued training of an existing model, on wit data can adapt a system trained on wmt data to the ted domain. by back-translating the monolingual wit corpus (using a german\u2192english system trained on wmt data, i.e. without in-domain knowledge), we obtain the synthetic data set wit synth. a single epoch of fine-tuning on wit synth (system 4) results in a bleu score of 26.7 on tst2015, or an improvement of 1.2 bleu. we observed no improvement from fine-tuning on wit mono, the monolingual ted corpus with dummy input (system 3). section: english\u2192german iwslt 15 these adaptation experiments with monolingual data are slightly artificial in that parallel training data is available. system 5, which is finetuned with the original wit training data, obtains a bleu of 28.4 on tst2015, which is an improve-[reference] we leave the word embeddings fixed for fine-tuning. bleu name 2014 2015 pbsmt 28.8 29.3 nmt [reference] 23.6 -+ shallow fusion 23.7 -+ deep fusion 24.0-parallel 25.9 26.7+ synthetic 29.5 30.4+ synthetic (ensemble of 4) 30.8 31.6 table 5: german\u2192english translation performance (bleu) on wmt training/ test sets (newstest2014; newstest2015). ment of 2.9 bleu. while it is unsurprising that in-domain parallel data is most valuable, we find it encouraging that nmt domain adaptation with monolingual data is also possible, and effective, since there are settings where only monolingual in-domain data is available. the best results published on this dataset are by, obtained with an ensemble of 8 independently trained models. in a comparison of single-model results, we outperform their model on tst2013 by 1 bleu. section: german\u2192english wmt 15 results for german\u2192english on the wmt 15 data sets are shown in table 5. like for the reverse translation direction, we see substantial improvements (3.6-3.7 bleu) from adding monolingual training data with synthetic source sentences, which is substantially bigger than the improvement observed with deep fusion [reference]; our ensemble outperforms the previous state of the art on newstest2015 by 2.3 bleu. table 6 shows results for turkish\u2192english. on average, we see an improvement of 0.6 bleu on the test sets from adding monolingual data with a dummy source side in a 1-1 ratio 10, although we note a high variance between different test sets. section: turkish\u2192english iwslt 14 with synthetic training data (gigaword synth), we outperform the baseline by 2.7 bleu on average, and also outperform results obtained via shallow or deep fusion by [reference] by 0.5 bleu on average. to compare to what extent synthetic data has a regularization effect, even without novel training data, we also back-translate the target side of the parallel training text to obtain the training corpus parallel synth. mixing the original parallel corpus with parallel synth (ratio 1-1) gives some improvement over the baseline (1.7 bleu on average), but the novel monolingual training data (gigaword mono) gives higher improvements, despite being out-of-domain in relation to the test sets. we speculate that novel in-domain monolingual data would lead to even higher improvements. section: back-translation quality for synthetic data one question that our previous experiments leave open is how the quality of the automatic backtranslation affects training with synthetic data. to investigate this question, we back-translate the same german monolingual corpus with three different german\u2192english systems:\u2022 with our baseline system and greedy decoding\u2022 with our baseline system and beam search (beam size 12). this is the same system used for the experiments in table 3. [reference] we also experimented with higher ratios of monolingual data, but this led to decreased bleu scores. table 7: english\u2192german translation performance (bleu) on wmt training/ test sets (newstest2014; newstest2015). systems differ in how the synthetic training data is obtained. ensembles of 4 models (unless specified otherwise).\u2022 with the german\u2192english system that was itself trained with synthetic data (beam size 12). bleu scores of the german\u2192english systems, and of the resulting english\u2192german systems that are trained on the different backtranslations, are shown in table 7. the quality of the german\u2192english back-translation differs substantially, with a difference of 6 bleu on newstest2015. regarding the english\u2192german systems trained on the different synthetic corpora, we find that the 6 bleu difference in back-translation quality leads to a 0.6-0.7 bleu difference in translation quality. this is balanced by the fact that we can increase the speed of back-translation by trading off some quality, for instance by reducing beam size, and we leave it to future research to explore how much the amount of synthetic data affects translation quality. we also show results for an ensemble of 3 models (the best single model of each training run), and 12 models (all 4 models of each training run). thanks to the increased diversity of the ensemble components, these ensembles outperform the ensembles of 4 models that were all sampled from the same training run, and we obtain another improvement of 0.8-1.0 bleu. section: contrast to phrase-based smt the back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrase-based smt [reference][reference]. while our approach is technically similar, synthetic parallel data fulfills novel name training bleu data instances tst2011 tst2012 tst2013 tst2014 baseline [reference] 18.4 18.8 19.9 18.7 deep fusion [reference] 20 table 8: phrase-based smt results (english\u2192german) on wmt test sets (average of newstest201{4, 5}), and iwslt test sets (average of tst201{3, 4, 5}), and average bleu gain from adding synthetic data for both pbsmt and nmt. roles in nmt. to explore the relative effectiveness of backtranslated data for phrase-based smt and nmt, we train two phrase-based smt systems with moses [reference], using only wmt parallel, or both wmt parallel and wmt synth_de for training the translation and reordering model. both systems contain the same language model, a 5-gram kneser-ney model trained on all available wmt data. we use the baseline features described by. results are shown in table 8. in phrasebased smt, we find that the use of back-translated training data has a moderate positive effect on the wmt test sets (+ 0.7 bleu), but not on the iwslt test sets. this is in line with the expectation that the main effect of back-translated data for phrase-based smt is domain adaptation [reference]. both the wmt test sets and the news crawl corpora which we used as monolingual data come from the same source, a web crawl of newspaper articles. 11 in contrast, news crawl is out-of-domain for the iwslt test sets. [reference] the wmt test sets are held-out from news crawl. in contrast to phrase-based smt, which can make use of monolingual data via the language model, nmt has so far not been able to use monolingual data to great effect, and without requiring architectural changes. we find that the effect of synthetic parallel data is not limited to domain adaptation, and that even out-of-domain synthetic data improves nmt quality, as in our evaluation on iwslt. the fact that the synthetic data is more effective on the wmt test sets (+ 2.9 bleu) than on the iwslt test sets (+ 1.2 bleu) supports the hypothesis that domain adaptation contributes to the effectiveness of adding synthetic data to nmt training. it is an important finding that back-translated data, which is mainly effective for domain adaptation in phrase-based smt, is more generally useful in nmt, and has positive effects that go beyond domain adaptation. in the next section, we will investigate further reasons for its effectiveness. section: analysis we previously indicated that overfitting is a concern with our baseline system, especially on small data sets of several hundred thousand training sentences, despite the regularization employed. this overfitting is illustrated in figure 1, which plots training and development set cross-entropy by training time for turkish\u2192english models. for comparability, we measure training set crossentropy for all models on the same random sample of the parallel training set. we can see that the model trained on only parallel training data quickly overfits, while all three monolingual data sets (parallel synth, gigaword mono, or gigaword synth) delay overfitting, and give better perplexity on the development set. the best development set cross-entropy is reached by gigaword synth. figure 2 shows cross-entropy for english\u2192german, comparing the system trained on only parallel data and the system that includes synthetic training data. since more training data is available for english\u2192german, there is no indication that overfitting happens during the first 40 million training instances (or 7 days of training); while both systems obtain comparable training set cross-entropies, the system with synthetic data reaches a lower cross-entropy on the development set. one explanation for this is the domain effect discussed in the previous section. a central theoretical expectation is that monolingual target-side data improves the model's flu-we compare the number of words in the system output for the newstest2015 test set which are produced via subword units, and that do not occur in the parallel training corpus. we also count how many of them are attested in the full monolingual corpus or the reference translation, which we all consider' natural'. additionally, the main authors, a native speaker of german, annotated a random subset (n= 100) of unattested words of each system according to their naturalness 13, distinguishing between natural german words (or names) such as literatur|klassen' literature classes', and nonsensical ones such as* as|best|atten (a missspelling of astbestmatten' asbestos mats'). in the results (table 9), we see that the systems trained with additional monolingual or synthetic data have a higher proportion of novel words attested in the non-parallel data, and a higher proportion that is deemed natural by our annotator. this supports our expectation that additional monolingual data improves the (word-level) fluency of the nmt system. section: related work to our knowledge, the integration of monolingual data for pure neural machine translation architectures was first investigated by [reference], who train monolingual language models independently, and then integrate them during decoding through rescoring of the beam (shallow fusion), or by adding the recurrent hidden state of the language model to the decoder state of the encoder-decoder network, with an additional controller mechanism that controls the magnitude of the lm signal (deep fusion). in deep fusion, the controller parameters and output parameters are tuned on further parallel training data, but the language model parameters are fixed during the finetuning stage. [reference] also report on experiments with reranking of nmt output with a 5-gram language model, but improvements are small (between 0.1-0.5 bleu). the production of synthetic parallel texts bears resemblance to data augmentation techniques used in computer vision, where datasets are often augmented with rotated, scaled, or otherwise distorted variants of the (limited) training set [reference]. another similar avenue of research is selftraining [reference][reference]. the main difference is that self-training typically refers to scenario where the training set is enhanced with training instances with artificially produced output labels, whereas we start with human-produced output (i.e. the translation), and artificially produce an input. we expect that this is more robust towards noise in the automatic translation. improving nmt with monolingual source data, following similar work on phrasebased smt [reference], remains possible future work. domain adaptation of neural networks via continued training has been shown to be effective for neural language models by [reference], and in work parallel to ours, for neural translation models. we are the first to show that we can effectively adapt neural translation models with monolingual data. section: conclusion in this paper, we propose two simple methods to use monolingual training data during training of nmt systems, with no changes to the network architecture. providing training examples with dummy source context was successful to some extent, but we achieve substantial gains in all tasks, and new sota results, via back-translation of monolingual target data into the source language, and treating this synthetic data as additional training data. we also show that small amounts of indomain monolingual data, back-translated into the source language, can be effectively used for domain adaptation. in our analysis, we identified domain adaptation effects, a reduction of overfitting, and improved fluency as reasons for the effectiveness of using monolingual data for training. while our experiments did make use of monolingual training data, we only used a small random sample of the available data, especially for the experiments with synthetic parallel data. it is conceivable that larger synthetic data sets, or data sets obtained via data selection, will provide bigger performance benefits. because we do not change the neural network architecture to integrate monolingual training data, our approach can be easily applied to other nmt systems. we expect that the effectiveness of our approach not only varies with the quality of the mt system used for back-translation, but also depends on the amount (and similarity to the test set) of available parallel and monolingual data, and the extent of overfitting of the baseline model. future work will explore the effectiveness of our approach in more settings. section: section: acknowledgments the research presented in this publication was conducted in cooperation with samsung electronics polska sp. z o.o.-samsung r& d institute poland. this project received funding from the european union's horizon 2020 research and innovation programme under grant agreement 645452 (qt21). section:",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "monolingual data",
                        49
                    ],
                    [
                        "monolingual",
                        670
                    ],
                    [
                        "monolingual training data",
                        748
                    ],
                    [
                        "monolingual target sentences",
                        3435
                    ],
                    [
                        "monolingual training",
                        3552
                    ],
                    [
                        "monolingual training examples",
                        5994
                    ],
                    [
                        "monolingual sentences",
                        7197
                    ],
                    [
                        "monolingual training instances",
                        7577
                    ],
                    [
                        "monolingual target text",
                        8251
                    ],
                    [
                        "nmt training",
                        8647
                    ],
                    [
                        "monolingual data set",
                        10393
                    ],
                    [
                        "monolingual in-domain data",
                        16570
                    ],
                    [
                        "monolingual corpus",
                        18572
                    ],
                    [
                        "monolingual target data",
                        20397
                    ],
                    [
                        "monolingual data sets",
                        23561
                    ],
                    [
                        "monolingual target-side data",
                        24380
                    ],
                    [
                        "monolingual source data",
                        27020
                    ]
                ]
            ],
            "Metric": [],
            "Task": []
        }
    ]
}
{
    "docid": "0c7f1d285ce069b2f7a807a4b2750695098bffe6-13",
    "doctext": "we present a semi-supervised learning framework based on graph embeddings. given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. we develop both transductive and inductive variants of our method. in the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. on a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models. revisitingsemi-section: introduction semi-supervised learning aims to leverage unlabeled data to improve performance. a large number of semi-supervised learning algorithms jointly optimize two training objective functions: the supervised loss over labeled data and the unsupervised loss over both labeled and unlabeled data. graph-based semi-supervised learning defines the loss function as a weighted sum of the supervised loss over labeled instances and a graph laplacian regularization term. the graph laplacian regularization is based on the assumption that nearby nodes in a graph are likely to have the same labels. graph laplacian regularization is effective because it constrains the labels to be consistent with the graph structure. recently developed unsupervised representation learning methods learn embeddings that predict a distributional context, e.g. a word embedding might predict nearby context words, or a node embedding might predict nearby nodes in a graph. embeddings trained with distributional context can be used to boost the performance of related tasks. for example, word embeddings trained from a language model can be applied to part-of-speech tagging, chunking and named entity recognition. in this paper we consider not word embeddings but graph embeddings. existing results show that graph embeddings are effective at classifying the nodes in a graph, such as user behavior prediction in a social network. however, the graph embeddings are usually learned separately from the supervised task, and hence do not leverage the label information in a specific task. hence graph embeddings are in some sense complementary to graph laplacian regularization that does not produce useful features itself and might not be able to fully leverage the distributional information encoded in the graph structure. the main highlight of our work is to incorporate embedding techniques into the graph-based semi-supervised learning setting. we propose a novel graph-based semi-supervised learning framework, planetoid (predicting labels and neighbors with embeddings transductively or inductively from data). the embedding of an instance is jointly trained to predict the class label of the instance and the context in the graph. we then concatenate the embeddings and the hidden layers of the original classifier and feed them to a softmax layer when making the prediction. since the embeddings are learned based on the graph structure, the above method is transductive, which means we can only predict instances that are already observed in the graph at training time. in many cases, however, it may be desirable to have an inductive approach, where predictions can be made on instances unobserved in the graph seen at training time. to address this issue, we further develop an inductive variant of our framework, where we define the embeddings as a parameterized function of input feature vectors; i.e., the embeddings can be viewed as hidden layers of a neural network. to demonstrate the effectiveness of our proposed approach, we conducted experiments on five datasets for three tasks, including text classification, distantly supervised entity extraction, and entity classification. our inductive method outperforms the second best inductive method by up to points and on average points in terms of accuracy. the best of our inductive and transductive methods outperforms the best of all the other compared methods by up to and on average. section: related work subsection: semi-supervised learning let and be the number of labeled and unlabeled instances. let and denote the feature vectors of labeled and unlabeled instances respectively. the labels are also given. based on both labeled and unlabeled instances, the problem of semi-supervised learning is defined as learning a classifier. there are two learning paradigms, transductive learning and inductive learning. transductive learning only aims to apply the classifier on the unlabeled instances observed at training time, and the classifier does not generalize to unobserved instances. for instance, transductive support vector machine (tsvm) maximizes the\" unlabeled data margin\" based on the low-density separation assumption that a good decision hyperplane lies on a sparse area of the feature space. inductive learning, on the other hand, aims to learn a parameterized classifier that is generalizable to unobserved instances. subsection: graph-based semi-supervised learning in addition to labeled and unlabeled instances, a graph, denoted as a matrix, is also given to graph-based semi-supervised learning methods. each entry indicates the similarity between instance and, which can be either labeled or unlabeled. the graph can either be derived from distances between instances, or be explicitly derived from external data, such as a knowledge graph or a citation network between documents. in this paper, we mainly focus on the setting that a graph is explicitly given and represents additional information not present in the feature vectors (e.g., the graph edges correspond to hyperlinks between documents, rather than distances between the bag-of-words representation of a document). graph-based semi-supervised learning is based on the assumption that nearby nodes tend to have the same labels. generally, the loss function of graph-based semi-supervised learning in the binary case can be written as in eq. ([reference]), the first term is the standard supervised loss function, where can be log loss, squared loss or hinge loss. the second term is the graph laplacian regularization, which incurs a large penalty when similar nodes with a large are predicted to have different labels. the graph laplacian matrix is defined as, where is a diagonal matrix with each entry defined as. is a constant weighting factor. (note that we omit the parameter regularization terms for simplicity.) various graph-based semi-supervised learning algorithms define the loss functions as variants of eq. ([reference]). label propagation forces to agree with labeled instances; is a label lookup table for unlabeled instances in the graph, and can be obtained with a closed-form solution. learning with local and global consistency defines as squared loss and as a label lookup table; it does not force to agree with labeled instances. modified adsorption (mad) is a variant of label propagation that allows prediction on labeled instances to vary and incorporates node uncertainty. manifold regularization parameterizes in the reproducing kernel hilbert space (rkhs) with being squared loss or hinge loss. since is a parameterized classifier, manifold regularization is inductive and can naturally handle unobserved instances. semi-supervised embedding extends the regularization term in eq. ([reference]) to be, where represents embeddings of instances, which can be the output labels, hidden layers or auxiliary embeddings in a neural network. by extending the regularization from to, this method imposes stronger constraints on a neural network. iterative classification algorithm (ica) uses a local classifier that takes the labels of neighbor nodes as input, and employs an iterative process between estimating the local classifier and assigning new labels. subsection: learning embeddings extensive research was done on learning graph embeddings. a probabilistic generative model was proposed to learn node embeddings that generate the edges in a graph. a clustering method was proposed to learn latent social states in a social network to predict social ties. more recently, a number of embedding learning methods are based on the skipgram model, which is a variant of the softmax model. given an instance and its context, the objective of skipgram is usually formulated as minimizing the log loss of predicting the context using the embedding of an instance as input features. formally, let be a set of pairs of instance and context, the loss function can be written as where is the set of all possible context,'s are parameters of the skipgram model, and is the embedding of instance. skipgram was first introduced to learn representations of words, known as word2vec. in word2vec, for each training pair, the instance is the current word whose embedding is under estimation; the context is each of the surrounding words of within a fixed window size in a sentence; the context space is the vocabulary of the corpus. skipgram was later extended to learn graph embeddings. deepwalk uses the embedding of a node to predict the context in the graph, where the context is generated by random walk. more specifically, for each training pair, the instance is the current node whose embedding is under estimation; the context is each of the neighbor nodes within a fixed window size in a generated random walk sequence; the context space is all the nodes in the graph. line extends the model to have multiple context spaces for modeling both first and second order proximity. although skipgram-like models for graphs have received much recent attention, many other models exist. transe learns the embeddings of entities in a knowledge graph jointly with their relations. autoencoders were used to learn graph embeddings for clustering on graphs. subsection: comparison we compare our approach in this paper with other methods in semi-supervised learning and embedding learning in table [reference]. unlike our approach, conventional graph laplacian based methods impose regularization on the labels but do not learn embeddings. semi-supervised embedding method learns embeddings in a neural network, but our approach is different from this method in that instead of imposing regularization, we use the embeddings to predict the context in the graph. graph embedding methods encode the graph structure into embeddings; however, different from our approach, these methods are purely unsupervised and do not leverage label information for a specific task. moreover, these methods are transductive and can not be directly generalized to instances unseen at training time. section: semi-supervised learning with graph embeddings following the notations in the previous section, the input to our method includes labeled instances,, unlabeled instances and a graph denoted as a matrix. each instance has an embedding denoted as. we formulate our framework based on feed-forward neural networks. given the input feature vector, the-th hidden layer of the network is denoted as, which is a nonlinear function of the previous hidden layer defined as: where and are parameters of the-th layer, and. we adopt rectified linear unit as the nonlinear function in this work. the loss function of our framework can be expressed as where is a supervised loss of predicting the labels, and is an unsupervised loss of predicting the graph context. in the following sections, we first formulate by introducing how to sample context from the graph, and then formulate to form our semi-supervised learning framework. subsection: sampling context we formulate the unsupervised loss as a variant of eq. ([reference]). given a graph, the basic idea of our approach is to sample pairs of instance and context, and then formulate the loss using the log loss as in eq. ([reference]). we first present the formulation of by introducing negative sampling, and then discuss how to sample pairs of instance and context. it is usually intractable to directly optimize eq. ([reference]) due to normalization over the whole context space. negative sampling was introduced to address this issue, which samples negative examples to approximate the normalization term. in our case, we are sampling from a distribution, where and denote instance and context respectively, means is a positive pair and means negative. given, we minimize the cross entropy loss of classifying the pair to a binary label: where is the sigmoid function defined as, and is an indicator function that outputs when the argument is true, otherwise. therefore, the unsupervised loss with negative sampling can be written as the distribution is conditioned on labels and the graph. however, since they are the input to our algorithm and kept fixed, we drop the conditioning in our notation. we now define the distribution directly using a sampling process, which is illustrated in algorithm [reference]. there are two types of context that are sampled in this algorithm. the first type of context is based on the graph, which encodes the structure (distributional) information, and the second type of context is based on the labels, which we use to inject label information into the embeddings. we use a parameter to control the ratio of positive and negative samples, and use to control the ratio of two types of context. with probability, we sample the context based on the graph. we first uniformly sample a random walk sequence. more specifically, we uniformly sample the first instance from the set. given the previous instance, the next instance is sampled with probability. with probability, we sample a positive pair from the set, where is another parameter determining the window size. with probability, we uniformly corrupt the context to sample a negative pair. with probability, we sample the context based on the class labels. positive pairs have the same labels and negative pairs have different labels. only labeled instances are sampled. our random walk based sampling method is built upon deepwalk. in contrast to their method, our method handles real-valued, incorporates negative sampling, and explicitly samples from labels with probability to inject supervised information. an example of sampling when is shown in figure [reference]. [tb] sampling context distribution\u2062p (i, c,\u03b3) input: graph, labels, parameters initialize triplet if then else uniformly sample a random walk of length uniformly sample with, if then uniformly sample from uniformly sample with uniformly sample with return subsection: transductive formulation in this section, we present a method that infers the labels of unlabeled instances without generalizing to unobserved instances. transductive learning usually performs better than inductive learning because transductive learning can leverage the unlabeled test data when training the model. we apply layers on the input feature vector to obtain, and layers on the embedding to obtain, as illustrated in figure [reference]. the two hidden layers are concatenated, and fed to a softmax layer to predict the class label of the instance. more specifically, the probability of predicting the label is written as: where denotes concatenation of two row vectors, the super script denotes the transpose of vector, and represents the model parameter. combined with eq. ([reference]), the loss function of transductive learning is defined as: where the first term is defined by eq. ([reference]), and is a constant weighting factor. the first term is the loss function of class label prediction and the second term is the loss function of context prediction. this formulation is transductive because the prediction of label depends on the embedding, which can only be learned for instances observed in the graph during training time. subsection: inductive formulation while we consider transductive learning in the above formulation, in many cases, it is desirable to learn a classifier that can generalize to unobserved instances, especially for large-scale tasks. for example, machine reading systems very frequently encounter novel entities on the web and it is not practical to train a semi-supervised learning system on the entire web. however, since learning graph embeddings is transductive in nature, it is not straightforward to do it in an inductive setting. perozzi et al. perozzi2014deepwalk addressed this issue by retraining the embeddings incrementally, which is time consuming and does not scale (and not inductive essentially). to make the method inductive, the prediction of label should only depend on the input feature vector. therefore, we define the embedding as a parameterized function of feature, as shown in figure [reference]. similar to the transductive formulation, we apply layers on the input feature vector to obtain. however, rather than using a\" free\" embedding, we apply layers on the input feature vector and define it as the embedding. then another layers are applied on the embedding, denoted as where. the embedding in this formulation can be viewed as a hidden layer that is a parameterized function of the feature. with the above formulation, the label only depends on the feature. more specifically, replacing in eq. ([reference]) with, the loss function of inductive learning is where the first term is defined by eq. ([reference]). subsection: training we adopt stochastic gradient descent (sgd) to train our model in the mini-batch mode. we first sample a batch of labeled instances and take a gradient step to optimize the loss function of class label prediction. we then sample a batch of context and take another gradient step to optimize the loss function of context prediction. we repeat the above procedures for and iterations respectively to approximate the weighting factor. algorithm [reference] illustrates the sgd-based training algorithm for the transductive formulation. similarly, we can replace with in to obtain the training algorithm for the inductive formulation. let denote all model parameters. we update both embeddings and parameters in transductive learning, and update only parameters in inductive learning. before the joint training procedure, we apply a number of training iterations that optimize the unsupervised loss alone and use the learned embeddings as initialization for joint training. [tb] model training (transductive) input:,,,, batch iterations and sizes to sample a batch of labeled instances of size take a gradient step for to sample a batch of context from of size take a gradient step for stopping section: experiments in our experiments, planetoid-t and planetoid-i denote the transductive and inductive formulation of our approach. we compare our approach with label propagation (lp), semi-supervised embedding (semiemb), manifold regularization (manireg), tsvm, and graph embeddings (graphemb). another baseline method, denoted as feat, is a linear softmax model that takes only the feature vectors as input. we also derive a variant planetoid-g that learns embeddings to jointly predict class labels and graph context without use of feature vectors. the architecture of planetoid-g is similar to figure [reference] except that the input feature and the corresponding hidden layers are removed. among the above methods, lp, graphemb and planetoid-g do not use the features, while tsvm and feat do not use the graph. we include these methods into our experimental settings to better evaluate our approach. our preliminary experiments on the text classification datasets show that the performance of our model is not very sensitive to specific choices of the network architecture. we adapt the implementation of graphemb to our skipgram implementation. we use the junto library for label propagation, and svmlight for tsvm. we also use our own implementation of manireg and semiemb by modifying the symbolic objective function in planetoid. in all of our experiments, we set the model hyper-parameters to,,, and for planetoid. we use the same, and for graphemb, and the same and for manireg and semiemb. we tune,,, the learning rate and hyper-parameters in other models based on an additional data split with a different random seed. the statistics for five of our benchmark datasets are reported in table [reference]. for each dataset, we split all instances into three parts, labeled data, unlabeled data, and test data. inductive methods are trained on the labeled and unlabeled data, and tested on the test data. transductive methods, on the other hand, are trained on the labeled, unlabeled data, and test data without labels. subsection: text classification we first considered three text classification datasets, citeseer, cora and pubmed. each dataset contains bag-of-words representation of documents and citation links between the documents. we treat the bag-of-words as feature vectors. we construct the graph based on the citation links; if document cites, then we set. the goal is to classify each document into one class. we randomly sample instances for each class as labeled data, instances as test data, and the rest are used as unlabeled data. the same data splits are used for different methods, and we compute the average accuracy for comparison. the experimental results are reported in table [reference]. among the inductive methods, planetoid-i achieves the best performance on all the three datasets with the improvement of up to on pubmed, which indicates that our embedding techniques are more effective than graph laplacian regularization. among the transductive methods, planetoid-t achieves the best performance on cora and pubmed, while tsvm performs the best on citeseer. however, tsvm does not perform well on cora and pubmed. planetoid-i slightly outperforms planetoid-t on citeseer and pubmed, while planetoid-t gets up to improvement over planetoid-i on cora. we conjecture that in planetoid-i, the feature vectors impose constraints on the learned embeddings, since they are represented by a parameterized function of the input feature vectors. if such constraints are appropriate, as is the case on citeseer and pubmed, it improves the non-convex optimization of embedding learning and leads to better performance. however, if such constraints rule out the optimal embeddings, the inductive model will suffer. planetoid-g consistently outperforms graphemb on all three datasets, which indicates that joint training with label information can improve the performance over training the supervised and unsupervised objectives separately. figure [reference] displays the-d embedding spaces on the cora dataset using t-sne. note that different classes are better separated in the embedding space of planetoid-t than that of graphemb and semiemb, which is consistent with our empirical findings. we also observe similar results for the other two datasets. subsection: distantly-supervised entity extraction we next considered the diel (distant information extraction using coordinate-term lists) dataset. the diel dataset contains pre-extracted features for each entity mention in text, and a graph that connects entity mentions to coordinate lists. the goal is to extract medical entities from text given feature vectors and the graph. we follow the exact experimental setup as in the original diel paper, including data splits of different runs, preprocessing of entity mentions and coordinate lists, and evaluation. we treat the top-entities given by a model as positive instances, and compute recall@ for evaluation (is set to following the diel paper). we report the average result of 10 runs in table [reference], where feat refers to a result obtained by svm (referred to as ds-baseline in the diel paper). the result of lp was also taken from. diel in table [reference] refers to the method proposed by the original paper, which is an improved version of label propagation that trains classifiers on feature vectors based on the output of label propagation. we did not include tsvm into the comparison since it does not scale. since we use freebase as ground truth and some entities are not present in text, the upper bound of recall as shown in table [reference] is. both planetoid-i and planetoid-t significantly outperform all other methods. each of planetoid-i and planetoid-t achieves the best performance in 5 out of 10 runs, and they give a similar recall on average, which indicates that there is no significant difference between these two methods on this dataset. planetoid-g clearly outperforms graphemb, which again shows the benefit of joint training. subsection: entity classification we sorted out an entity classification dataset from the knowledge base of never ending language learning (nell) and a hierarchical entity classification dataset that links nell entities to text in clueweb09. we extracted the entities and the relations between entities from the nell knowledge base, and then obtained text description by linking the entities to clueweb09. we use text bag-of-words representation as feature vectors of the entities. we next describe how to construct the graph based on the knowledge base. we first remove relations that are not populated in nell, including\" generalizations\",\" haswikipediaurl\", and\" atdate\". in the knowledge base, each relation is denoted as a triplet, where,, denote head entity, relation, and tail entity respectively. we treat each entity as a node in the graph, and each relation is split as two nodes and in the graph. for each, we add two edges in the graph, and. we removed all classes with less than entities. the goal is to classify the entities in the knowledge base into one of the classes given the feature vectors and the graph. let be the labeling rate. we set to,, and. instances are labeled for a class with entities, so each class has at least one entity in the labeled data. we report the results in table [reference]. we did not include tsvm since it does not scale to such a large number of classes with the one-vs-rest scheme. adding feature vectors does not improve the performance of planetoid-t, so we set the feature vectors for planetoid-t to be all empty, and therefore planetoid-t is equivalent to planetoid-g in this case. planetoid-i significantly outperforms the best of the other compared inductive methods\u2014 i.e., semiemb\u2014 by,, and respectively with three labeling rates. as the labeling rate decreases, the improvement of planetoid-i over semiemb becomes more significant. graph structure is more informative than features in this dataset, so inductive methods perform worse than transductive methods. planetoid-g outperforms graphemb by, and. section: conclusion our contribution is three-fold: a) incontrast to previous semi-supervised learning approaches that largely depend on graph laplacian regularization, we propose a novel approach by joint training of classification and graph context prediction; b) since it is difficult to generalize graph embeddings to novel instances, we design a novel inductive approach that conditions embeddings on input features; c) we empirically show substantial improvement over existing methods (up to and on average), and even more significant improvement in the inductive setting (up to and on average). our experimental results on five benchmark datasets also show that a) joint training gives improvement over unsupervised learning; b) predicting graph context is more effective than graph laplacian regularization; c) the performance of the inductive variant depends on the informativeness of feature vectors. one direction of future work would be to apply our framework to more complex networks, including recurrent networks. it would also be interesting to experiment with datasets where a graph is computed based on distances between feature vectors. section: acknowledgements this work was funded by the nsf under grants ccf-1414030 and iis-1250956, and by google. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "citeseer",
                        20870
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "planetoid",
                        2830
                    ],
                    [
                        "planetoid-t",
                        18788
                    ],
                    [
                        "planetoid-i",
                        18804
                    ],
                    [
                        "planetoid-g",
                        19186
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        4129
                    ]
                ]
            ],
            "Task": []
        },
        {
            "Material": [
                [
                    [
                        "cora",
                        20880
                    ],
                    [
                        "cora dataset",
                        22780
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "planetoid",
                        2830
                    ],
                    [
                        "planetoid-t",
                        18788
                    ],
                    [
                        "planetoid-i",
                        18804
                    ],
                    [
                        "planetoid-g",
                        19186
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        4129
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "text classification",
                        20794
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "never ending language learning",
                        24862
                    ],
                    [
                        "nell",
                        24895
                    ],
                    [
                        "nell entities",
                        24960
                    ],
                    [
                        "knowledge base",
                        25293
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "planetoid",
                        2830
                    ],
                    [
                        "planetoid-t",
                        18788
                    ],
                    [
                        "planetoid-i",
                        18804
                    ],
                    [
                        "planetoid-g",
                        19186
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        4129
                    ]
                ]
            ],
            "Task": []
        },
        {
            "Material": [
                [
                    [
                        "pubmed",
                        20889
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "planetoid",
                        2830
                    ],
                    [
                        "planetoid-t",
                        18788
                    ],
                    [
                        "planetoid-i",
                        18804
                    ],
                    [
                        "planetoid-g",
                        19186
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        4129
                    ]
                ]
            ],
            "Task": []
        }
    ]
}
{
    "docid": "0e6b990f41fc0e7fdd0662f88b5ad024375bd77b-14",
    "doctext": "learning deep parsimonious representations in this paper we aim at facilitating generalization for deep networks while supporting interpretability of the learned representations. towards this goal, we propose a clustering based regularization that encourages parsimonious representations. our k-means style objective is easy to optimize and flexible, supporting various forms of clustering, such as sample clustering, spatial clustering, as well as co-clustering. we demonstrate the effectiveness of our approach on the tasks of unsupervised learning, classification, fine grained categorization, and zero-shot learning. 1 introduction in recent years, deep neural networks have been shown to perform extremely well on a variety of tasks including classification [21], semantic segmentation [13], machine translation [27] and speech recognition [16]. this has led to their adoption across many areas such as computer vision, natural language processing and robotics [16, 21, 22, 27]. three major advances are responsible for the recent success of neural networks: the increase in available computational resources, access to large scale data sets, and several algorithmic improvements. many of these algorithmic advances are related to regularization, which is key to prevent overfitting and improve generalization of the learned classifier, as the current trend is to increase the capacity of neural nets. for example, batch normalization [18] is used to normalize intermediate representations which can be interpreted as imposing constraints. in contrast, dropout [26] removes a fraction of the learned representations at random to prevent co-adaptation. learning of de-correlated activations [6] shares a similar idea since it explicitly discourages correlation between the units. in this paper we propose a new type of regularization that encourages the network representations to form clusters. as a consequence, the learned feature space is compactly representable, facilitating generalization. furthermore, clustering supports interpretability of the learned representations. we formulate our regularization with a k-means style objective which is easy to optimize, and investigate different types of clusterings, including sample clustering, spatial clustering, and co-clustering. we demonstrate the generalization performance of our proposed method in several settings: autoencoders trained on the mnist dataset [23], classification on cifar10 and cifar100 [20], as well as fine-grained classification and zero-shot learning on the cub-200-2011 dataset [34]. we show that our approach leads to significant wins in all these scenarios. in addition, we are able to demonstrate on the cub-200-2011 dataset that the network representation captures meaningful part representations even though it is not explicitly trained to do so. 2 related work standard neural network regularization involves penalties on the weights based on the norm of the parameters [29, 30]. also popular are regularization methods applied to intermediate representations, 29th conference on neural information processing systems (nips 2016), barcelona, spain. such as dropout [26], drop-connect [32], maxout [10] and decov [6]. these approaches share the aim of preventing the activations in the network to be correlated. our work can be seen as a different form of regularization, where we encourage parsimonious representations. a variety of approaches have applied clustering to the parameters of the neural network with the aim of compressing the network. compression rates of more than an order of magnitude were demonstrated in [11] without sacrificing accuracy. in the same spirit hash functions were exploited in [5]. early approaches to compression include biased weight decay [12] and [14, 24], which prunes the network based on the hessian of the loss function. recently, various combinations of clustering with representation learning have been proposed. we categorize them broadly into two areas: (i) work that applies clustering after having learned a representation, and (ii) approaches that jointly optimize the learning and clustering objectives. [4] combines deep belief networks (dbn) with non-parametric maximum-margin clustering in a posthoc manner: a dbn is trained layer-wise to obtain an intermediate representation of the data; non-parametric maximum-margin clustering is then applied to the data representation. another line of work utilizes an embedding of the deep network, which can be based on annotated data [15], or from a learned unsupervised method such as a stacked auto-encoder [28]. in these approaches, the network is trained to approximate the embedding, and subsequently either k-means or spectral clustering is performed to partition the space. an alternative is to use non-negative matrix factorization, which represents a given data matrix as the product of components [31]. this deep non-negative matrix factorization is trained using the reconstruction loss rather than a clustering objective. nonetheless, it was shown that factors lower in the hierarchy have superior clustering performance on lowlevel concepts while factors later in the hierarchy cluster high-level concepts. the aforementioned approaches differ from our proposed technique, since we aim at jointly learning a representation that is parsimonious via a clustering regularization. also related are approaches that utilize sparse coding. wang et al. [33] unrolls the iterations forming the sparse codes and optimizes end-to-end the involved parameters using a clustering objective as loss function [33]. the proposed framework is further augmented by clustering objectives applied to intermediate representations, which act as feature regularization within the unrolled optimization. they found that features lower in the unrolled hierarchy cluster low-level concepts, while features later in the hierarchy capture high-level concepts. our method differs in that we use convolutional neural networks rather than unrolling a sparse coding optimization. in the context of unsupervised clustering [35] exploited agglomerative clustering as a regularizer; this approach was formulated as a recurrent network. in contrast we employ a k-means like clustering objective which simplifies the optimization significantly and does not require a recurrent procedure. furthermore, we investigate both unsupervised and supervised learning. 3 learning deep parsimonious representations in this section, we introduce our new clustering based regularization which not only encourages the neural network to learn more compact representations, but also enables interpretability of the neural network. we first show that by exploiting different unfoldings of the representation tensor, we obtain multiple types of clusterings, each possessing different properties. we then devise an efficient online update to jointly learn the clustering with the parameters of the neural network. 3.1 clustering of representations we first introduce some notation. we refer to [k] as the set of k positive integers, i.e., [k]= {1, 2,..., k}. we use s\\a to denote the set s with elements from the set a removed. a tensor is a multilinear map over a set of vector spaces. in tensor terminology, n-mode vectors of a d-order tensor y\u2208 ri1\u00d7i2\u00d7\u00b7\u00b7\u00b7\u00d7id are in-dimensional vectors obtained from y by varying the index in indimension, while keeping all other indices fixed. an n-mode matrix unfolding of a tensor is a matrix which has all n-mode vectors as its columns [7]. formally we use the operator t {in}\u00d7{ij|j\u2208 [d]\\n} to denote the n-mode matrix unfolding, which returns a matrix of size in\u00d7\u220f j\u2208 [d]\\n ij. similarly, we definee t {ii, ij}\u00d7{ik|k\u2208 [d]\\{i, j}} to be an (i, j)- mode matrix unfolding operator. in this case a column vector is a concatenation of one i-mode vector and one j-mode vector. we denote the m-th row vector of a matrix x as xm. in this paper we assume the representation of one layer within a neural network to be a 4-d tensor y\u2208 rn\u00d7c\u00d7h\u00d7w, where n, c, h and w are the number of samples within a mini-batch, the number of hidden units, the height and width of the representation respectively. note that c, h and w can vary between layers, and in the case of a fully connected layer, the dimensions along height and width become a singleton and the tensor degenerates to a matrix. let l be the loss function of a neural network. in addition, we refer to the clustering regularization of a single layer viar. the final objective is l+\u03bbr, where\u03bb adjusts the importance of the clustering regularization. note that we can add a regularization term for any subset of layers, but we focus on a single layer for notational simplicity. in what follows, we show three different types of clustering, each possessing different properties. in our framework any variant can be applied to any layer. (a) sample clustering: we first investigate clustering along the sample dimension. since the cluster assignments of different layers are not linked, each layer is free to cluster examples in a different way. for example, in a convnet, bottom layer representations may focus on low-level visual cues, such as color and edges, while top layer features may focus on high-level attributes which have a more semantic meaning. we refer the reader to fig. 1 (a) for an illustration. in particular, given the representation tensor y, we first unfold it into a matrix t {n}\u00d7{h, w, c} (y)\u2208 rn\u00d7hwc. we then encourage the samples to cluster as follows: rsample (y,\u00b5)= 1 2nchw n\u2211 n=1\u2225\u2225\u2225t {n}\u00d7{h, w, c} (y) n\u2212\u00b5zn\u2225\u2225\u22252, (1) where\u00b5 is a matrix of size k\u00d7 hwc encoding all cluster centers, with k the total number of clusters. zn\u2208 [k] is a discrete latent variable corresponding to the n-th sample. it indicates which cluster this sample belongs to. note that for a fully connected layer, the formulation is the same except that t {n}\u00d7{h, w, c} (y) n and\u00b5zn are c-sized vectors since h= w= 1 in this case. (b) spatial clustering: the representation of one sample can be regarded as a c-channel\" image.\" each spatial location within that\" image\" can be thought of as a\" pixel,\" and is a vector of size c (shown as a colored bar in fig. 1). for a convnet, every\" pixel\" has a corresponding receptive field covering a local region in the input image. therefore, by clustering\" pixels\" of all images during learning, we expect to model local parts shared by multiple objects or scenes. to achieve this, we adopt the unfolding operator t {n, h, w}\u00d7{c} (y) and use rspatial (y,\u00b5)= 1 2nchw nhw\u2211 i=1\u2016t {n, h, w}\u00d7{c} (y) i\u2212\u00b5zi\u20162. (2) note that although we use the analogy of a\" pixel,\" when using text data a\" pixel\" may corresponds to words. for spatial clustering the dimension of the matrix\u00b5 is k\u00d7 c. (c) channel co-clustering: this regularizer groups the channels of different samples directly, thus co-clustering samples and filters. we expect this type of regularization to model re-occurring algorithm 1: learning parsimonious representations 1: initialization: maximum training iteration r, batch size b, smooth weight\u03b1, set of clustering layers s and set of cluster centers {\u00b50k|k\u2208 [k]}, update period m 2: for iteration t= 1, 2,..., r: 3: for layer l= 1, 2,..., l: 4: compute the output representation of layer l as x. 5: if l\u2208 s: 6: assigning cluster zn= argmin k\u2016xn\u2212\u00b5t\u22121k\u20162,\u2200n\u2208 [b]. 7: compute cluster center\u00b5\u0302k= 1|nk|\u2211 n\u2208nk xn, where nk= [b]\u22c2 {n|zn= k}. 8: smooth cluster center\u00b5tk=\u03b1\u00b5\u0302k+ (1\u2212\u03b1)\u00b5 t\u22121 k 9: end 10: end 11: compute the gradients with cluster centers\u00b5tk fixed. 12: update weights. 13: update drifted cluster centers using kmeans++ every m iterations. 14: end patterns shared not only among different samples but also within each sample. relying on the unfolding operator t {n, c}\u00d7{h, w} (y), we formulate this type of clustering objective as rchannel (y,\u00b5)= 1 2nchw nc\u2211 i=1\u2016t {n, c}\u00d7{h, w} (y) i\u2212\u00b5zi\u20162. (3) note that the dimension of the matrix\u00b5 is k\u00d7hw in this case. 3.2 efficient online update we now derive an efficient online update to jointly learn the weights while clustering the representations of the neural network. in particular, we illustrate the sample clustering case while noting that the other types can be derived easily by applying the corresponding unfolding operator. for ease of notation, we denote the unfolded matrix t {n}\u00d7{h, w, c} (y) as x. the gradient of the clustering regularization layer w.r.t. its input representation x can be expressed as,\u2202r\u2202xn= 1 nchw xn\u2212\u00b5zn\u2212 1qzn\u2211 zp= zn,\u2200p\u2208 [n] (xn\u2212\u00b5zp), (4) where qzn is the number of samples which belong to the zn-th cluster. this gradient is then backpropagated through the network to obtain the gradient w.r.t. the parameters of the network. the time and space complexity of the gradient computation of one regularization layer are max (o (kchw), o (nchw)) and o (nchw) respectively. note that we can cache the centered data xn\u2212\u00b5zn in the forward pass to speed up the gradient computation. the overall learning algorithm of our framework is summarized in alg. 1. in the forward pass, we first compute the representation of the n-th sample as xn for each layer. we then infer the latent cluster label zn for each sample based on the distance to the cluster centers\u00b5t\u22121k from the last time step t\u2212 1, and assign the sample to the cluster center which has the smallest distance. once all the cluster assignments are computed, we estimate the cluster centers\u00b5\u0302k based on the new labels of the current batch. we then combine the estimate based on the current batch with the former cluster center. this is done via an online update. we found an online update together with the random restart strategy to work well in practice, as the learning of the neural network proceeds one mini-batch at a time, and as it is too expensive to recompute the cluster assignment for all data samples in every iteration. since we trust our current cluster center estimate more than older ones, we smooth the estimation by using an exponential moving average. the cluster center estimate at iteration t is obtained via\u00b5tk=\u03b1\u00b5\u0302k+ (1\u2212\u03b1)\u00b5 t\u22121 k, where\u03b1 is a smoothing weight. however, as the representation learned by the neural network may go through drastic changes, especially in the beginning of training, some of the cluster centers may quickly be less favored and the number of incoming samples assigned to it will be largely reduced. to overcome this issue, we exploit the kmeans++ [3] procedure to re-sample the cluster center from the current mini-batch. specifically, denoting the the distance between sample xn and its nearest cluster center as dn, the probability of taking xn as the new cluster center is d2n/\u2211 i d 2 i. after sampling, we replace the old cluster center with the new one and continue the learning process. in practice, at the end of every epoch, we apply the kmeans++ update to cluster centers for which the number of assigned samples is small. see alg. 1 for an outline of the steps. the overall procedure stabilizes the optimization and also increases the diversity of the cluster centers. in the backward pass, we fix the latest estimation of the cluster centers\u00b5tk and compute the gradient of loss function and the gradient of the clustering objective based on eq. (4). then we back-propagate all the gradients and update the weights. 4 experiments in this section, we conduct experiments on unsupervised, supervised and zero-shot learning on several datasets. our implementation based on tensorflow [9] is publicly available.1 for initializing the cluster centers before training, we randomly choose them from the representations obtained with the initial network. 4.1 autoencoder on mnist we first test our method on the unsupervised learning task of training an autoencoder. our architecture is identical to [17]. for ease of training we did not tie the weights between the encoder and the decoder. we use the squared` 2 reconstruction error as the loss function and sgd with momentum. the standard training-test-split is used. we compute the mean reconstruction error over all test images and repeat the experiments 4 times with different random initializations. we compare the baseline model, i.e., a plain autoencoder, with one that employs our sample-clustering regularization on all layers except the top fully connected layer. sample clustering was chosen since this autoencoder only contains fully connected layers. the number of clusters and the regularization weight\u03bb of all layers are set to 100 and 1.0e\u22122 respectively. for both models the same learning rate and momentum are used. our exact parameter choices are detailed in the appendix. as shown in table 1, our regularization facilitates generalization as it suffers less from overfitting. specifically, applying our regularization results in lower test set error despite slightly higher training error. more importantly, the standard deviation of the error is one order of magnitude smaller for both training and testing when applying our regularization. this indicates that our sample-clustering regularization stabilizes the model. 1https:// github.com/ lrjconan/ deep_parsimonious 4.2 cifar10 and cifar100 in this section, we explore the cifar10 and cifar100 datasets [20]. cifar10 consists of 60, 000 32\u00d7 32 images assigned to 10 categories, while cifar100 differentiates between 100 classes. we use the standard split on both datasets. the quick cifar10 architecture of caffe [19] is used for benchmarking both datasets. it consists of 3 convolutional layers and 1 fully connected layer followed by a softmax layer. the detailed parameters are publicly available on the caffe [19] website. we report mean accuracy averaged over 4 trials. for fully connected layers we use the sample-clustering objective. for convolutional layers, we provide the results of all three clustering objectives, which we refer to as 'sample-clustering,' 'spatial-clustering,' and 'channel-co-clustering' respectively. we set all hyper-parameters based on cross-validation. specifically, the number of cluster centers are set to 100 for all layers for both cifar10 and cifar100.\u03bb is set to 1.0e\u22123 and 1.0e\u22122 for the first two convolutional and the remaining layers respectively in cifar10; for cifar100,\u03bb is set to 10 and 1 for the first convolutional layer and the remaining layers respectively. the smoothness parameter\u03b1 is set to 0.9 and 0.95 for cifar10 and cifar100 respectively. generalization: in table 2 we compare our framework to some recent regularizers, like decov [6], dropout [26] and the baseline results obtained using caffe. we again observe that all of our methods achieve better generalization performance. visualization: to demonstrate the interpretability of our learned network, we visualize sampleclustering and spatial-clustering in fig. 2, showing the top-10 ranked images and parts per cluster. in the case of sample-clustering, for each cluster we rank all its assigned images based on the distance to the cluster center. we chose to show 2 clusters from the 4th fully connected layer. in the case of spatial-clustering, we rank all\" pixels\" belonging to one cluster based on the distance to the cluster center. note that we have one part (i.e., one receptive field region in the input image) for each\" pixel.\" we chose to show 2 clusters from the 2nd convolutional layer. the receptive field of the 2nd convolutional layer is of size 18\u00d7 18 in the original 32\u00d7 32 sized image. we observe that clusterings of the fully connected layer representations encode high-level semantic meaning. in contrast, clusterings of the convolutional layer representations encode attributes like shape. note that some parts are uninformative which may be due to the fact that images in cifar10 are very small. additional clusters and visualizations on cifar100 are shown in the appendix. quantitative evaluation of parsimonious representation: we quantitatively evaluate our learned parsimonious representation on cifar100. since only the image category is provided as ground truth, we investigate sample clustering using the 4th fully connected layer where representations capture semantic meaning. in particular, we apply k-means clustering to the learned representation extracted from the model with and without sample clustering respectively. for both cases, we set the number of clusters to be 100 and control the random seed to be the same. the most frequent class label within one cluster is assigned to all of its members. then we compute the normalized mutual information (nmi) [25] to measure the clustering accuracy. the average results over 10 runs are shown in table 3. our representations achieve significantly better clustering quality compared to the baseline which suggests that they are distributed in a more compact way in the feature space. 4.3 cub-200-2011 next we test our framework on the caltech-ucsd birds dataset [34] which contains 11, 788 images of 200 different categories. we follow the dataset split provided by [34] and the common practice of cropping the image using the ground-truth bounding box annotation of the birds [8, 36]. we use alex-net [21] pretrained on imagenet as the base model and adapt the last layer to fit classification of 200 categories. we resize the image to 227\u00d7 227 to fit the input size. we add clusterings to all layers except the softmax-layer. based on cross-validation, the number of clusters are set to 200 for all layers. for convolutional layers, we set\u03bb to 1.0e\u22125 for the first (bottom) 2 and use 1.0e\u22124 for the remaining ones. for fully connected layers, we set\u03bb to 1.0e\u22123 and\u03b1 is equal to 0.5. we apply kmeans++ to replace cluster centers with less than 10 assigned samples at the end of every epoch. generalization: we investigate the impact of our parsimonious representation on generalization performance. we compare with the decaf result reported in [8], which used the same network to extract a representation and applied logistic regression on top for fine-tuning. we also fine-tune alex-net which uses weight-decay and dropout, and report the best result we achieved in table 4. we observe that for the alex-net architecture our clustering improves the generalization compare to direct fine-tuning and the decaf result. note that alex-net pretrained on imagenet easily overfits on this dataset as all training accuracies reach 100 percent. visualization: to visualize the sample-clustering and spatial-clustering we follow the setting employed when evaluating on the cifar dataset. for the selected cluster center we show the 10 closest images in fig. 3. for sample clustering, 2 clusters from the 3rd convolutional layer and the 7th fully connected layer are chosen for visualization. for spatial clustering, 2 clusters from the 2nd and 3rd convolutional layers are chosen for visualization. more clusters are shown in the appendix. the receptive fields of pixels from the 2nd and 3rd convolutional layers are of sizes 59\u00d7 59 and 123\u00d7 123 in the resized 227\u00d7 227 image. we observe that cluster centers of sample clustering applied to layers lower in the network capture pose and shape information, while cluster centers from top layers model the fine-grained categories of birds. for spatial clustering, cluster centers from different layers capture parts of birds in different scales, like the beak, chest, etc. 4.4 zero-shot learning we also investigate a zero-shot setting on the cub dataset to see whether our parsimonious representation is applicable to unseen categories. we follow the setting in [1, 2] and use the same split where 100, 50 and 50 classes are used as training, validation and testing (unseen classes). we use a pre-trained alex-net as the baseline model and extract 4096-dimension representations from the 7th fully connected (fc) layer. we compare sample-clustering against other recent methods which also report results of using 7th fc feature of alex-net. given these features, we learn the output embedding w via the same unregularized structured svm as in [1, 2]: min w 1 n n\u2211 n=1 max y\u2208y {0,\u2206 (yn, y)+ x> nw [\u03c6 (y)\u2212\u03c6 (yn)])}, (5) where xn and yn are the feature and class label of the n-th sample and\u2206 is the 0-1 loss function..\u03c6 is the class-attribute matrix provided by the cub dataset, where each entry is a real-valued score indicating how likely a human thinks one attribute is present in a given class. we tune the hyperparameters on the validation set and report results in terms of top-1 accuracy averaged over the unseen classes. as shown in table 5 our approach significantly outperforms other approaches. 5 conclusions we have proposed a novel clustering based regularization which encourages parsimonious representations, while being easy to optimize. we have demonstrated the effectiveness of our approach on a variety of tasks including unsupervised learning, classification, fine grained categorization, and zero-shot learning. in the future we plan to apply our approach to even larger networks, e.g., residual nets, and develop a probabilistic formulation which provides a soft clustering. acknowledgments this work was partially supported by onr-n00014-14-1-0232, nvidia and the intelligence advanced research projects activity (iarpa) via department of interior/ interior business center (doi/ ibc) contract number d16pc00003. the u.s. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon. disclaimer: the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of iarpa, doi/ ibc, or the u.s. government.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cub-200-2011 dataset",
                        2541
                    ],
                    [
                        "cub dataset",
                        23356
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "sample clustering",
                        399
                    ],
                    [
                        "spatial clustering",
                        9951
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        3638
                    ],
                    [
                        "training accuracies",
                        22273
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "classification",
                        552
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "11f411eee106efac9f677149e655252463811b09-15",
    "doctext": "document: mode seeking generative adversarial networks for diverse image synthesis most conditional generation tasks expect diverse outputs given a single conditional context. however, conditional generative adversarial networks (cgans) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. recent attempts to resolve the mode collapse issue for cgans are usually task-specific and computationally expensive. in this work, we propose a simple yet effective regularization term to address the mode collapse issue for cgans. the proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. this mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. we validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-to-image translation, and text-to-image synthesis with different baseline models. both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality. section: introduction generative adversarial networks (gans) have been shown to capture complex and high-dimensional image data with numerous applications effectively. built upon gans, conditional gans (cgans) take external information as additional inputs. for image synthesis, cgans can be applied to various tasks with different conditional contexts. with class labels, cgans can be applied to categorical image generation. with text sentences, cgans can be applied to text-to-image synthesis. with images, cgans have been used in tasks including image-to-image translation, semantic manipulation and style transfer. for most conditional generation tasks, the mappings are in nature multimodal, i.e., a single input context corresponds to multiple plausible outputs. a straightforward approach to handle multimodality is to take random noise vectors along with the conditional contexts as inputs, where the contexts determine the main content and noise vectors are responsible for variations. for instance, in the dog-to-cat image-to-image translation task, the input dog images decide contents like orientations of heads and positions of facial landmarks, while the noise vectors help the generation of different species. however, cgans usually suffer from the mode collapse problem, where generators only produce samples from a single or few modes of the distribution and ignore other modes. the noise vectors are ignored or of minor impacts, since cgans pay more attention to learn from the high-dimensional and structured conditional contexts. there are two main approaches to address the mode collapse problem in gans. a number of methods focus on discriminators by introducing different divergence metrics and optimization process. the other methods use auxiliary networks such as multiple generators and additional encoders. however, mode collapse is relatively less studied in cgans. some recent efforts have been made in the image-to-image translation task to improve diversity. similar to the second category with the unconditional setting, these approaches introduce additional encoders and loss functions to encourage the one-to-one relationship between the output and the latent code. these methods either entail heavy computational overheads on training or require auxiliary networks that are often task-specific that can not be easily extended to other frameworks. in this work, we propose a mode seeking regularization method that can be applied to cgans for various tasks to alleviate the mode collapse problem. given two latent vectors and the corresponding output images, we propose to maximize the ratio of the distance between images with respect to the distance between latent vectors. in other words, this regularization term encourages generators to generate dissimilar images during training. as a result, generators can explore the target distribution, and enhance the chances of generating samples from different modes. on the other hand, we can train the discriminators with dissimilar generated samples to provide gradients from minor modes that are likely to be ignored otherwise. this mode seeking regularization method incurs marginal computational overheads and can be easily embedded in different cgan frameworks to improve the diversity of synthesized images. we validate the proposed regularization algorithm through an extensive evaluation of three conditional image synthesis tasks with different baseline models. first, for categorical image generation, we apply the proposed method on dcgan using the cifar-10 dataset. second, for image-to-image translation, we embed the proposed regularization scheme in pix2pix and drit using the facades, maps, yosemite, and cat dog datasets. third, for text-to-image synthesis, we incorporate stackgan++ with the proposed regularization term using the cub-200-2011 dataset. we evaluate the diversity of synthesized images using perceptual distance metrics. however, the diversity metric alone can not guarantee the similarity between the distribution of generated images and the distribution of real data. therefore, we adopt two recently proposed bin-based metrics, the number of statistically-different bins (ndb) metric which determines the relative proportions of samples fallen into clusters predetermined by real data, and the jensen-shannon divergence (jsd) distance which measures the similarity between bin distributions. furthermore, to verify that we do not achieve diversity at the expense of realism, we evaluate our method with the fr\u00e9chet inception distance (fid) as the metric for quality. experimental results demonstrate that the proposed regularization method can facilitate existing models from various applications achieving better diversity without loss of image quality. figure [reference] shows the effectiveness of the proposed regularization method for existing models. the main contributions of this work are: we propose a simple yet effective mode seeking regularization method to address the mode collapse problem in cgans. this regularization scheme can be readily extended into existing frameworks with marginal training overheads and modifications. we demonstrate the generalizability of the proposed regularization method on three different conditional generation tasks: categorical generation, image-to-image translation, and text-to-image synthesis. extensive experiments show that the proposed method can facilitate existing models from different tasks achieving better diversity without sacrificing visual quality of the generated images. our code and pre-trained models are available at. section: related work paragraph: conditional generative adversarial networks. generative adversarial networks have been widely used for image synthesis. with adversarial training, generators are encouraged to capture the distribution of real images. on the basis of gans, conditional gans synthesize images based on various contexts. for instances, cgans can generate high-resolution images conditioned on low-resolution images, translate images between different visual domains, generate images with desired style, and synthesize images according to sentences. although cgans have achieved success in various applications, existing approaches suffer from the mode collapse problem. since the conditional contexts provide strong structural prior information for the output images and have higher dimensions than the input noise vectors, generators tend to ignore the input noise vectors, which are responsible for the variation of generated images. as a result, the generators are prone to produce images with similar appearances. in this work, we aim to address the mode collapse problem for cgans. paragraph: reducing mode collapse. some methods focus on the discriminator with different optimization process and divergence metrics to stabilize the training process. the minibatch discrimination scheme allows the discriminator to discriminate between whole mini-batches of samples instead of between individual samples. in, durugkar et al. use multiple discriminators to address this issue. the other methods use auxiliary networks to alleviate the mode collapse issue. modegan and veegan enforce the bijection mapping between the input noise vectors and generated images with additional encoder networks. multiple generators and weight-sharing generators are developed to capture more modes of the distribution. however, these approaches either entail heavy computational overheads or require modifications of the network structure, and may not be easily applicable to cgans. in the field of cgans, some efforts have been recently made to address the mode collapse issue on the image-to-image translation task. similar to modegan and veegan, additional encoders are introduced to provide a bijection constraint between the generated images and input noise vectors. however, these approaches require other task-specific networks and objective functions. the additional components make the methods less generalizable and incur extra computational loads on training. in contrast, we propose a simple regularization term that imposes no training overheads and requires no modifications of the network structure. therefore, the proposed method can be readily applied to various conditional generation tasks. section: diverse conditional image synthesis subsection: preliminaries the training process of gans can be formulated as a mini-max problem: a discriminator learns to be a classifier by assigning higher discriminative values to the real data samples and lower ones to the generated ones. meanwhile, a generator aims to fool by synthesizing realistic examples. through adversarial training, the gradients from will guide toward generating samples with the distribution similar to the real data one. the mode collapse problem with gans is well known in the literature. several methods attribute the missing mode to the lack of penalty when this issue occurs. since all modes usually have similar discriminative values, larger modes are likely to be favored through the training process based on gradient descent. on the other hand, it is difficult to generate samples from minor modes. the mode missing problem becomes worse in cgans. generally, conditional contexts are high-dimensional and structured (e.g., images and sentences) as opposed to the noise vectors. as such, the generators are likely to focus on the contexts and ignore the noise vectors, which account for diversity. subsection: mode seeking gans in this work, we propose to alleviate the missing mode problem from the generator perspective. figure [reference] illustrates the main ideas of our approach. let a latent vector from the latent code space be mapped to the image space. when mode collapse occurs, the mapped images are collapsed into a few modes. furthermore, when two latent codes and are closer, the mapped images and are more likely to be collapsed into the same mode. to address this issue, we propose a mode seeking regularization term to directly maximize the ratio of the distance between and with respect to the distance between and, where denotes the distance metric. the regularization term offers a virtuous circle for training cgans. it encourages the generator to explore the image space and enhances the chances for generating samples of minor modes. on the other hand, the discriminator is forced to pay attention to generated samples from minor modes. figure [reference] shows a mode collapse situation where two close samples, and, are mapped onto the same mode. however, with the proposed regularization term, is mapped to, which belongs to an unexplored mode. with the adversarial mechanism, the generator will thus have better chances to generate samples of in the following training steps. as shown in figure [reference], the proposed regularization term can be easily integrated with existing cgans by appending it to the original objective function. where denotes the original objective function and the weights to control the importance of the regularization. here, can be as a simple loss function. for example, in categorical generation task, where denote class labels, real images, and noise vectors, respectively. in image-to-image translation task, where denotes input images and is the typical gan loss. can be arbitrary complex objective function from any task, as shown in figure [reference] (b). we name the proposed method as mode seeking gans (msgans). [proposed regularization] [applying proposed regularization on stackgan++] section: experiments we evaluate the proposed regularization method through extensive quantitative and qualitative evaluation. we apply msgans to the baseline models from three representative conditional image synthesis tasks: categorical generation, image-to-image translation, and text-to-image synthesis. note that we augment the original objective functions with the proposed regularization term while maintaining original network architectures and hyper-parameters. we employ norm distance as our distance metrics for both and and set the hyper-parameter in all experiments. more implementation and evaluation details, please refer to the appendixes. subsection: evaluation metrics we conduct evaluations using the following metrics. fid. to evaluate the quality of the generated images, we use fid to measure the distance between the generated distribution and the real one through features extracted by inception network. lower fid values indicate better quality of the generated images. lpips. to evaluate diversity, we employ lpips following. lipis measures the average feature distances between generated samples. higher lpips score indicates better diversity among the generated images. ndb and jsd. to measure the similarity between the distribution between real images and generated one, we adopt two bin-based metrics, ndb and jsd, proposed in. these metrics evaluate the extent of mode missing of generative models. following, the training samples are first clustered using k-means into different bins which can be viewed as modes of the real data distribution. then each generated sample is assigned to the bin of its nearest neighbor. we calculate the bin-proportions of the training samples and the synthesized samples to evaluate the difference between the generated distribution and the real data distribution. ndb score and jsd of the bin-proportion are then computed to measure the mode collapse. lower ndb score and jsd mean the generated data distribution approaches the real data distribution better by fitting more modes. please refer to for more details. subsection: conditioned on class label we first validate the proposed method on categorical generation. in categorical generation, networks take class labels as conditional contexts to synthesize images of different categories. we apply the regularization term to the baseline framework dcgan. we conduct experiments on the cifar-10 dataset which includes images of ten categories. since images in the cifar-10 dataset are of size and upsampling degrades the image quality, we do not compute lpips in this task. table [reference] and table [reference] present the results of ndb, js, and fid. msgan mitigates the mode collapse issue in most classes while maintaining image quality. subsection: conditioned on image image-to-image translation aims to learn the mapping between two visual domains. conditioned on images from the source domain, models attempt to synthesize corresponding images in the target domain. despite the multimodal nature of the image-to-image translation task, early work abandons noise vectors and performs one-to-one mapping since the latent codes are easily ignored during training as shown in. to achieve multimodality, several recent attempts introduce additional encoder networks and objective functions to impose a bijection constraint between the latent code space and the image space. to demonstrate the generalizability, we apply the proposed method to a unimodal model pix2pix using paired training data and a multimodal model drit using unpaired images. subsubsection: conditioned on paired images we take pix2pix as the baseline model. we also compare msgan to bicyclegan which generates diverse images with paired training images. for fair comparisons, architectures of the generator and the discriminator in all methods follow the ones in bicyclegan. we conduct experiments on the facades and maps datasets. msgan obtains consistent improvements on all metrics over pix2pix. moreover, msgan demonstrates comparable diversity to bicyclegan, which applies an additional encoder network. figure. [reference] and table. [reference] demonstrate the qualitative and quantitative results, respectively. subsubsection: conditioned on unpaired images we choose drit, one of the state-of-the-art frameworks to generate diverse images with unpaired training data, as the baseline framework. though drit synthesizes diverse images in most cases, mode collapse occurs in some challenging shape-variation cases (e.g., translation between cats and dogs). to demonstrate the robustness of the proposed method, we evaluate on the shape-preserving yosemite (summer winter) dataset and the cat dog dataset that requires shape variations. as the quantitative results exhibited in table. [reference], msgan performs favorably against drit in all metrics on both datasets. especially in the challenging cat dog dataset, msgan obtains substantial diversity gains. from the statistical point of view, we visualize the bin proportions of the dog-to-cat translation in figure. [reference]. the graph shows the severe mode collapse issue of drit and the substantial improvement with the proposed regularization term. qualitatively, figure. [reference] shows that msgan discovers more modes without the loss of visual quality. subsection: conditioned on text text-to-image synthesis targets at generating images conditioned on text descriptions. we integrate the proposed regularization term on stackgan++ using the cub-200-2011 dataset. to improve diversity, stackgan++ introduces a conditioning augmentation (ca) module that re-parameterizes text descriptions into text codes of the gaussian distribution. instead of applying the regularization term on the semantically meaningful text codes, we focus on exploiting the latent codes randomly sampled from the prior distribution. however, for a fair comparison, we evaluation msgan against stackgan++ in two settings: 1) perform generation without fixing text codes for text descriptions. in this case, text codes also provide variations for output images. 2) perform generation with fixed text codes. in this setting, the effects of text codes are excluded. table. [reference] presents quantitative comparisons between msgan and stackgan++. msgan improves the diversity of stackgan++ and maintains visual quality. to better illustrate the role that latent codes play for the diversity, we show qualitative comparisons with the text codes fixed. in this setting, we do not consider the diversity resulting from ca. figure. [reference] illustrates that latent codes of stackgan++ have minor effects on the variations of the image. on the contrary, latent codes of msgan contribute to various appearances and poses of birds. subsection: interpolation of latent space in msgans we perform linear interpolation between two given latent codes and generate corresponding images to have a better understanding of how well msgans exploit the latent space. figure. [reference] shows the interpolation results on the dog-to-cat translation and the text-to-image synthesis task. in the dog-to-cat translation, we can see the coat colors and patterns varies smoothly along with the latent vectors. in the text-to-image synthesis, both orientations of birds and the appearances of footholds change gradually with the variations of the latent codes. section: conclusions in this work, we present a simple but effective mode seeking regularization term on the generator to address the model collapse in cgans. by maximizing the distance between generated images with respect to that between the corresponding latent codes, the regularization term forces the generators to explore more minor modes. the proposed regularization method can be readily integrated with existing cgans framework without imposing training overheads and modifications of network structures. we demonstrate the generalizability of the proposed method on three different conditional generation tasks including categorical generation, image-to-image translation, and text-to-image synthesis. both qualitative and quantitative results show that the proposed regularization term facilitates the baseline frameworks improving the diversity without sacrificing visual quality of the generated images. bibliography: references section: implementation details table [reference] summarizes the datasets and baseline models used on various tasks. for all of the baseline methods, we incorporate the original objective functions with the proposed regularization term. note that we remain the original network architecture design and use the default setting of hyper-parameters for the training. dcgan. since the images in the cifar-10 dataset are of size, we modify the structure of the generator and discriminator in dcgan, as shown in table [reference]. we use the batch size of, learning rate of and adam optimizer with and to train both the baseline and msgan network. pix2pix. we adopt the generator and discriminator in bicyclegan to build the pix2pix model. same as bicyclegan, we use a u-net network for the generator, and inject the latent codes into every layer of the generator. the architecture of the discriminator is a two-scale patchgan network. for the training, both pix2pix and msgan framework use the same hyper-parameters as the officially released version. drit. drit involves two stages of image-to-image translations in the training process. we only apply the mode seeking regularization term to generators in the first stage, which is modified on the officially released code. stackgan++. stackgan++ is a tree-like structure with multiple generators and discriminators. we use the output images from the last generator and input latent codes to calculate the mode seeking regularization term. the implementation is based on the officially released code. section: evaluation details we employ the official implementation of fid, ndb and jsd, and lpips. for ndb and jsd, we use the k-means method on training samples to obtain the clusters. then the generated samples are assigned to the nearest cluster to compute the bin proportions. as suggested by the author of, there are at least training samples for each cluster. therefore, we cluster the number of bins in all tasks, where denotes the number of training samples for computing the clusters. we have verified that the performance is consistent within a large range of. for evaluation, we randomly generate images for a given conditional context on various tasks. we conduct five independent trials and report the mean and standard derivation based on the result of each trial. more evaluation details of one trial are presented as follows. conditioned on class label. we randomly generate images for each class label. we use all the training samples and the generated samples to compute fid. for ndb and jsd, we employ the training samples in each class to calculate clusters. conditioned on image. we randomly generate images for each input image in the test set. for lpips, we randomly select pairs of the images of each context in the test set to compute lpips and average all the values for this trial. then, we randomly choose input images and their corresponding generated images to form generated samples. we use the generated samples and all samples in training set to compute fid. for ndb and jsd, we employ all the training samples for clustering and choose bins for facades, and bins for other datasets. conditioned on text. we randomly select sentences and generate images for each sentence, which forms generated samples. then, we randomly select samples for computing fid, and clustering them into bins for ndb and jsd. for lpips, we randomly choose pairs for each sentence and average the values of all the pairs for this trial. section: ablation study on the regularization term subsection: the weighting parameter to analyze the influence of the regularization term, we conduct an ablation study by varying the weighting parameter on image-to-image translation task using the facades dataset. figure [reference] presents the qualitative and quantitative results. it can be observed that increasing improves both the quality and diversity of the generated images. nevertheless, as the weighting parameter becomes larger than a threshold value (), the training becomes unstable, which yields low quality, and even low diversity synthesized images. as a result, we empirically set the weighting parameter for all experiments. subsection: the design choice of the distance metric we have explored other design choice of the distance metric. we conduct experiments using discriminator feature distance in our regularization term in a way similar to feature matching loss, where denotes the layer of the discriminator. we apply it to pix2pix on the facades dataset. table. [reference] shows that msgan using feature distance also obtains improvement over pix2pix. however, msgan using distance has higher diversity. therefore, we employ msgan using distance for all experiments. section: computational overheads we compare msgan with pix2pix, bicyclegan in terms of training time, memory consumption, and model parameters on an nvidia titan x gpu. table. [reference] shows that our method incurs marginal overheads. however, bicyclegan requires longer time per iteration and larger memory with an additional encoder and another discriminator network. section: additional results we present more results of categorical generation, image-to-image translation, and text-to-image synthesis in figure [reference], figure [reference], figure [reference], figure [reference], figure [reference], and figure [reference], respectively.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar-10 dataset",
                        4898
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "mode seeking generative adversarial networks",
                        10
                    ],
                    [
                        "mode seeking regularization term",
                        819
                    ],
                    [
                        "mode seeking regularization method",
                        3764
                    ],
                    [
                        "mode seeking gans",
                        10862
                    ],
                    [
                        "msgans",
                        12825
                    ],
                    [
                        "msgan",
                        15583
                    ],
                    [
                        "baseline and msgan network",
                        21844
                    ],
                    [
                        "pix2pix and msgan framework",
                        22183
                    ],
                    [
                        "msgan using distance",
                        25862
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "fr\u00e9chet inception distance",
                        5881
                    ],
                    [
                        "fid",
                        5910
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "categorical image generation",
                        1751
                    ],
                    [
                        "generation",
                        18880
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "1275125b71b999b7c49bc554c6ec404a764eb299-16",
    "doctext": "document: hd-cnn: hierarchical deep convolutional neural network for large scale visual recognition in image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. such difficult categories demand more dedicated classifiers. however, existing deep convolutional neural networks (cnn) are trained as flat n-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. in this paper, we introduce hierarchical deep cnns (hd-cnns) by embedding deep cnns into a category hierarchy. an hd-cnn separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. during hd-cnn training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. in addition, conditional executions of fine category classifiers and layer parameter compression make hd-cnns scalable for large-scale visual recognition. we achieve state-of-the-art results on both cifar100 and large-scale imagenet 1000-class benchmark datasets. in our experiments, we build up three different hd-cnns and they lower the top-1 error of the standard cnns by, and, respectively..22.72 section: introduction deep cnns are well suited for large-scale learning based visual recognition tasks because of its highly scalable training algorithm, which only needs to cache a small chunk (mini-batch) of the potentially huge volume of training data during sequential scans (epochs). they have achieved increasingly better performance in recent years. as datasets become bigger and the number of object categories becomes larger, one of the complications that come along is that visual separability between different object categories is highly uneven. some categories are much harder to distinguish than others. take the categories in cifar100 as an example. it is easy to tell an apple from a bus, but harder to tell an apple from an orange. in fact, both apples and oranges belong to the same coarse category fruit and vegetables while buses belong to another coarse category vehicles 1, as defined within cifar100. nonetheless, most deep cnn models nowadays are flat n-way classifiers, which share a set of fully connected layers. this makes us wonder whether such a flat structure is adequate for distinguishing all the difficult categories. a very natural and intuitive alternative organizes classifiers in a hierarchical manner according to the divide-and-conquer strategy. although hierarchical classification has been proven effective for conventional linear classifiers, few attempts have been made to exploit category hierarchies in deep cnn models. since deep cnn models are large models themselves, organizing them hierarchically imposes the following challenges. first, instead of a handcrafted category hierarchy, how can we learn such a category hierarchy from the training data itself so that cascaded inferences in a hierarchical classifier will not degrade the overall accuracy while dedicated fine category classifiers exist for hard-to-distinguish categories? second, a hierarchical cnn classifier consists of multiple cnn models at different levels. how can we leverage the commonalities among these models and effectively train them all? third, it would also be slower and more memory-consuming to run a hierarchical cnn classifier on a novel testing image. how can we alleviate such limitations? in this paper, we propose a generic and principled hierarchical architecture, hierarchical deep convolutional neural network (hd-cnn), that decomposes an image classification task into two steps. an hd-cnn first uses a coarse category cnn classifier to separate easy classes from one another. more challenging classes are routed downstream to fine category classifiers that focus on confusing classes. we adopt a module design principle and every hd-cnn is built upon a building block cnn. the building block can be chosen to be any of the currently top ranked single cnns. thus hd-cnns can always benefit from the progress of single cnn design. an hd-cnn follows the coarse-to-fine classification paradigm and probabilistically integrates predictions from the fine category classifiers. compared with the building block cnn, the corresponding hd-cnn can achieve lower error at the cost of a manageable increase in memory footprint and classification time. in summary, this paper has the following contributions. first, we introduce a novel hierarchical architecture, called hd-cnn, for image classification. second, we develop a scheme for learning the two-level organization of coarse and fine categories, and demonstrate various components of an hd-cnn can be independently pretrained. the complete hd-cnn is further fine-tuned using a multinomial logistic loss regularized by a coarse category consistency term. third, we make the hd-cnn scalable by compressing the layer parameters and conditionally executing the fine category classifiers. we have performed evaluations on the medium-scale cifar100 dataset and the large-scale imagenet 1000-class dataset, and our method has achieved state-of-the-art performance on both of them. section: related work our work is inspired by progresses in cnn design and efforts on integrating a category hierarchy with linear classifiers. the main novelty of our method is a new scalable hd-cnn architecture that integrates a category hierarchy with deep cnns. subsection: convolutional neural networks cnn-based models hold state-of-the-art performance in various computer vision tasks, including image classifcation, object detection, and image parsing. recently, there has been considerable interest in enhancing cnn components, including pooling layers, activation units, and nonlinear layers. these enhancements either improve cnn training, or expand the network learning capacity. this work boosts cnn performance from an orthogonal angle and does not redesign a specific part within any existing cnn model. instead, we design a novel generic hierarchical architecture that uses an existing cnn model as a building block. we embed multiple building blocks into a larger hierarchical deep cnn. subsection: category hierarchy for visual recognition in visual recognition, there is a vast literature exploiting category hierarchical structures. for classification with a large number of classes using linear classifiers, a common strategy is to build a hierarchy or taxonomy of classifiers so that the number of classifiers evaluated given a testing image scales sub-linearly in the number of classes. the hierarchy can be either predefined or learnt by top-down and bottom-up approaches. in, the predefined category hierarchy of imagenet dataset is utilized to achieve the trade-offs between classification accuracy and specificity. in, a hierarchical label tree is constructed to probabilistically combine predictions from leaf nodes. such hierarchical classifier achieves significant speedup at the cost of certain accuracy loss. one of the earliest attempts to introduce a category hierarchy in cnn-based methods is reported in but their main goal is transferring knowledge between classes to improve the results for classes with insufficient training examples. in, various label relations are encoded in a hierarchy. improved accuracy is achieved only when a subset of training images are relabeled with internal nodes in the hierarchical class tree. they are not able to improve the accuracy in the original setting where all training images are labeled with leaf nodes. in, a hierarchy of cnns is introduced but they experimented with only two coarse categories mainly due to scalability constraints. hd-cnn exploits the category hierarchy in a novel way that we embed deep cnns into the hierarchy in a scalable manner and achieves superior classification results over the standard cnn. section: overview of hd-cnn subsection: notations the following notations are used below. a dataset consists of images. and denote the image data and label, respectively. there are fine categories of images in the dataset. we will learn a category hierarchy with coarse categories. subsection: hd-cnn architecture hd-cnn is designed to mimic the structure of category hierarchy where fine categories are divided into coarse categories as in fig [reference] (a). it performs end-to-end classification as illustrated in fig [reference] (b). it mainly comprises four parts, namely shared layers, a single coarse category component, multiple fine category components and a single probabilistic averaging layer. on the left side of fig [reference] (b) are the shared layers. they receive raw image pixel as input and extract low-level features. the configuration of shared layers are set to be the same as the preceding layers in the building block net. on the top of fig [reference] (b) are independent layers of coarse category component which reuses the configuration of rear layers from the building block cnn and produces a fine prediction for an image. to produce a prediction over coarse categories, we append a fine-to-coarse aggregation layer which aggregates fine predictions into coarse ones when a mapping from fine categories to coarse ones is given. the coarse category probabilities serve two purposes. first, they are used as weights for combining the predictions made by fine category components. second, when thresholded, they enable conditional executions of fine category components whose corresponding coarse probabilities are sufficiently large. in the bottom-right of fig [reference] (b) are independent layers of a set of fine category classifiers, each of which makes fine category predictions. as each fine component only excels in classifying a small set of categories, they produce a fine prediction over a partial set of categories. the probabilities of other fine categories absent in the partial set are implicitly set to zero. the layer configurations are mostly copied from the building block cnn except that in the final classification layer the number of filters is set to be the size of partial set instead of the full categories. both coarse category component and fine category components share common layers. the reason is three-fold. first, it is shown in that preceding layers in deep networks response to class-agnostic low-level features such as corners and edges, while rear layers extract more class-specific features such as dog face and bird's legs. since low-level features are useful for both coarse and fine classification tasks, we allow the preceding layers to be shared by both coarse and fine components. second, it reduces both the total floating point operations and the memory footprint of network execution. both are of practical significance to deploy hd-cnn in real applications. last but not the least, it can decrease the number of hd-cnn parameters which is critical to the success of hd-cnn training. on the right side of fig [reference] (b) is the probabilistic averaging layer which receives fine category predictions as well as coarse category prediction and produces a weighted average as the final prediction. where is the probability of coarse category for the image predicted by the coarse category component. is the fine category prediction made by the fine category component. we stress that both coarse and fine category components reuse the layer configurations from the building block cnn. this flexible modular design allows us to choose the best module cnn as the building block, depending on the task at hand. section: learning a category hierarchy our goal of building a category hierarchy is grouping confusing fine categories into the same coarse category for which a dedicated fine category classifier will be trained. we employ a top-down approach to learn the hierarchy from the training data. we randomly sample a held-out set of images with balanced class distribution from the training set. the rest of the training set is used to train a building block net. we obtain a confusion matrix by evaluating the net on the held-out set. a distance matrix is derived as and its diagonal entries are set to be zero. is further transformed by to be symmetric. the entry measures how easy it is to discriminate categories and. spectral clustering is performed on to cluster fine categories into coarse categories. the result is a two-level category hierarchy representing a many-to-one mapping from fine to coarse categories. here, the coarse categories are disjoint. overlapping coarse categories with disjoint coarse categories, the overall classification depends heavily on the coarse category classifier. if an image is routed to an incorrect fine category classifier, then the mistake can not be corrected as the probability of ground truth label is implicitly set to zero there. removing the separability constraint between coarse categories can make the hd-cnn less dependent on the coarse category classifier. therefore, we add more fine categories to the coarse categories. for a certain fine classifier, we prefer to add those fine categories that are likely to be misclassfied into the coarse category. therefore, we estimate the likelihood that an image in fine category is misclassified into a coarse category on the held-out set. is the coarse category probability which is obtained by aggregating fine category probabilities according to the mapping:. we threshold the likelihood using a parametric variable and add to the partial set all fine categories such that. note that each branching component gives a full set prediction when and a disjoint set prediction when. with overlapping coarse categories, the category hierarchy mapping is extended to be a many-to-many mapping and the coarse category predictions are updated accordingly. note the sum of exceeds and hence we perform normalization. the use of overlapping coarse categories was also shown to be useful for hierarchical linear classifiers. section: hd-cnn training as we embed fine category components into hd-cnn, the number of parameters in rear layers grows linearly in the number of coarse categories. given the same amount of training data, this increases the training complexity and the risk of over-fitting. on the other hand, the training images within the stochastic gradient descent mini-batch are probabilistically routed to different fine category components. it requires larger mini-batch to ensure parameter gradients in the fine category components are estimated by a sufficiently large number of training samples. large training mini-batch both increases the training memory footprint and slows down the training process. therefore, we decompose the hd-cnn training into multiple steps instead of training the complete hd-cnn from scratch as outlined in algorithm [reference]. subsection: pretraining hd-cnn we sequentially pretrain the coarse category component and fine category components. subsubsection: initializing the coarse category component we first pretrain a building block cnn using the training set. as both the preceding and rear layers in coarse category component resemble the layers in the building block cnn, we copy the weights of into coarse category component for initialization purpose. subsubsection: pretraining the rear layers of fine category components fine category components can be independently pretrained in parallel. each should specialize in classifying fine categories within the coarse category. therefore, the pretraining of each only uses images from the coarse category. the shared preceding layers are already initialized and kept fixed in this stage. for each, we initialize all the rear layers except the last convolutional layer by copying the learned parameters from the pretrained model. [! t] hd-cnn training algorithm [1] hd-cnn training step 1: pretrain hd-cnn step 1.1: initialize coarse category component step 1.2: pretrain fine category components step 2: fine-tune the complete hd-cnn subsection: fine-tuning hd-cnn after both coarse category component and fine category components are properly pretrained, we fine-tune the complete hd-cnn. once the category hierarchy as well as the associated mapping is learnt, each fine category component focuses on classifying a fixed subset of fine categories. during fine-tuning, the semantics of coarse categories predicted by the coarse category component should be kept consistent with those associated with fine category components. thus we add a coarse category consistency term to regularize the conventional multinomial logistic loss. coarse category consistency the learnt fine-to-coarse category mapping provides a way to specify the target coarse category distribution. specifically, is set to be the fraction of all the training images within the coarse category under the assumption the distribution over coarse categories across the training dataset is close to that within a training mini-batch. the final loss function we use for fine-tuning the hd-cnn is shown below. where is the size of training mini-batch. is a regularization constant and is set to. section: hd-cnn testing as we add fine category components into the hd-cnn, the number of parameters, memory footprint and execution time in rear layers, all scale linearly in the number of coarse categories. to ensure hd-cnn is scalable for large-scale visual recognition, we develop conditional execution and layer parameter compression techniques. conditional execution. at test time, for a given image, it is not necessary to evaluate all fine category classifiers as most of them have insignificant weights as in eqn [reference]. their contributions to the final prediction are negligible. conditional executions of the top weighted fine components can accelerate the hd-cnn classification. therefore, we threshold using a parametric variable and reset to zero when. those fine category classifiers with are not evaluated. parameter compression. in hd-cnn, the number of parameters in rear layers of fine category classifiers grows linearly in the number of coarse categories. thus we compress the layer parameters at test time to reduce memory footprint. specifically, we choose the product quantization approach to compress the parameter matrix by first partitioning it horizontally into segments of width.. then-means clustering is used to cluster the rows in. by only storing the nearest cluster indices in a 8-bit integer matrix and cluster centers in a single-precision floating number matrix, we can achieve a compression factor. the hyperparameters for parameter compression are. section: experiments subsection: overview we evaluate hd-cnn on the benchmark datasets cifar100 and imagenet. hd-cnn is implemented on the widely deployed caffe software. the network is trained by back propagation. we run all the testing experiments on a single nvidia tesla k40c card. subsection: cifar100 dataset the cifar100 dataset consists of 100 classes of natural images. there are 50 k training images and 10 k testing images. we follow to preprocess the datasets (e.g. global contrast normalization and zca whitening). randomly cropped and flipped image patches of size are used for training. we adopt a nin network with three stacked layers. we denote it as cifar100-nin which will be the hd-cnn building block. fine category components share preceding layers from conv1 to pool1 which accounts for of the total parameters and of the total floating point operations. the remaining layers are used as independent layers. for building the category hierarchy, we randomly choose 10 k images from the training set as held-out set. fine categories within the same coarse categories are visually more similar. we pretrain the rear layers of fine category components. the initial learning rate is and it is decreased by a factor of 10 every 6 k iterations. fine-tuning is performed for 20 k iterations with large mini-batches of size 256. the initial learning rate is and is reduced by a factor of 10 once after 10 k iterations. for evaluation, we use 10-view testing. we extract five patches (the 4 corner patches and the center patch) as well as their horizontal reflections and average their predictions. the cifar100-nin net obtains testing error. our hd-cnn achieves testing error of which improves the building block net by. category hierarchy. during the construction of the category hierarchy, the number of coarse categories can be adjusted by the clustering algorithm. we can also make the coarse categories either disjoint or overlapping by varying the hyperparameter. thus we investigate their impacts on the classification error. we experiment with 5, 9, 14 and 19 coarse categories and vary the value of. the best results are obtained with 9 overlapping coarse categories and as shown in fig [reference] left. a histogram of fine category occurrences in 9 overlapping coarse categories is shown in fig [reference] right. the optimal value of coarse category number and hyperparameter are dataset dependent, mainly affected by the inherent hierarchy within the categories..30.18 shared layers. the use of shared layers makes both computational complexity and memory footprint of hd-cnn sublinear in the number of fine category classifiers when compared to the building block net. our hd-cnn with 9 fine category classifiers based on cifar100-nin consumes less than three times as much memory as the building block net without parameter compression. we also want to investigate the impact of the use of shared layers on the classification error, memory footprint and the net execution time (table [reference]). we build another hd-cnn where coarse category component and all fine category components use independent preceding layers initialized from a pretrained building block net. under the single-view testing where only a central cropping is used, we observe a minor error increase from to. but using shared layers dramatically reduces the memory footprint from mb to mb and testing time from seconds to seconds. conditional executions. by varying the hyperparameter, we can effectively affect the number of fine category components that will be executed. there is a trade-off between execution time and classification error. a larger value of leads to higher accuracy at the cost of executing more components for fine categorization. by enabling conditional executions with hyperparameter, we obtain a substantial x speed up with merely a minor increase in error from to (table [reference]). the testing time of hd-cnn is about times as much as that of the building block net. parameter compression. as fine category cnns have independent layers from conv2 to cccp6, we compress them and reduce the memory footprint from mb to mb with a minor increase in error from to. comparison with a strong baseline. as our hd-cnn memory footprint is about two times as much as the building block model (table [reference]), it is necessary to compare a stronger baseline of similar complexity with hd-cnn. we adapt cifar100-nin and double the number of filters in all convolutional layers which accordingly increases the memory footprint by three times. we denote it as cifar100-nin-double and obtain error which is lower than that of the building block net but is higher than that of hd-cnn. comparison with model averaging. hd-cnn is fundamentally different from model averaging. in model averaging, all models are capable of classifying the full set of the categories and each one is trained independently. the main sources of their prediction differences are different initializations. in hd-cnn, each fine category classifier only excels at classifying a partial set of categories. to compare hd-cnn with model averaging, we independently train two cifar100-nin networks and take their averaged prediction as the final prediction. we obtain an error of, which is about higher than that of hd-cnn (table [reference]). note that hd-cnn is orthogonal to the model averaging and an ensemble of hd-cnn networks can further improve the performance. coarse category consistency. to verify the effectiveness of coarse category consistency term in our loss function ([reference]), we fine-tune a hd-cnn using the traditional multinomial logistic loss function. the testing error is, which is higher than that of a hd-cnn fine-tuned with coarse category consistency (table [reference]). comparison with state-of-the-art. our hd-cnn improves on the current two best methods and by and respectively and sets new state-of-the-art results on cifar100 (table [reference]). subsection: imagenet 1000-class dataset the ilsvrc-2012 imagenet dataset consists of about million training images, validation images. to demonstrate the generality of hd-cnn, we experiment with two different building block nets. in both cases, hd-cnns achieve significantly lower testing errors than the building block nets. subsubsection: network-in-network building block net (a) (b) (c) (d) (e) (f) (g) we choose a public 4-layer nin net as our first building block as it has greatly reduced number of parameters compared to alexnet but similar error rates. it is denoted as imagenet-nin. in hd-cnn, various components share preceding layers from conv1 to pool3 which account for of the total parameters and of the total floating point operations. we follow the training and testing protocols as in. original images are resized to. randomly cropped and horizontally reflected patches are used for training. at test time, the net makes a 10-view averaged prediction. we train imagenet-nin for 45 epochs. the top-1 and top-5 errors are and. to build the category hierarchy, we take 100 k training images as the held-out set and find 89 overlapping coarse categories. each fine category cnn is fine-tuned for 40 k iterations while the initial learning rate is decreased by a factor of every 15 k iterations. fine-tuning the complete hd-cnn is not performed as the required mini-batch size is significantly higher than that for the building block net. nevertheless, we still achieve top-1 and top-5 errors of and and improve the building block net by and, respectively (table [reference]). the class-wise top-5 error improvement over the building block net is shown in fig [reference] left. case studies we want to investigate how hd-cnn corrects the mistakes made by the building block net. in fig [reference], we collect four testing cases. in the first case, the building block net fails to predict the label of the tiny hermit crab in the top 5 guesses. in hd-cnn, two coarse categories and receive most of the coarse probability mass. the fine category component specializes in classifying crab breeds and strongly suggests the ground truth label. by combining the predictions from the top fine category classifiers, the hd-cnn predicts hermit crab as the most probable label. in the second case, the imagenet-nin confuses the ground truth hand blower with other objects of close shapes and appearances, such as plunger and barbell. for hd-cnn, the coarse category component is also not confident about which coarse category the object belongs to and thus assigns even probability mass to the top coarse categories. for the top 3 fine category classifiers, strongly predicts ground truth label while the other two and rank the ground truth label at the 2nd and 4th place respectively. overall, the hd-cnn ranks the ground truth label at the 1st place. this demonstrates hd-cnn needs to rely on multiple fine category classifiers to make correct predictions for difficult cases. overlapping coarse categories.to investigate the impact of overlapping coarse categories on the classification, we train another hd-cnn with 89 fine category classifiers using disjoint coarse categories. it achieves top-1 and top-5 errors of and respectively, which is higher than those of the hd-cnn using overlapping coarse category hierarchy by and (table [reference]). conditional executions. by varying the hyperparameter, we can control the number of fine category components that will be executed. there is a trade-off between execution time and classification error as shown in fig [reference] right. a larger value of leads to lower error at the cost of more executed fine category components. by enabling conditional executions with hyperparameter, we obtain a substantial x speed up with merely a minor increase of single-view testing top-5 error from to (table [reference]). with such speedup, the hd-cnn testing time is less than times as much as that of the building block net. parameter compression. we compress independent layers conv4 and cccp7 as they account for of the parameters in imagenet-nin. their parameter matrices are of size and and we use compression hyperparameters and. the compression factors are and. the compression decreases the memory footprint from mb to mb and merely increases the top-5 error from to under single-view testing (table [reference])..23.27 comparison with model averaging. as the hd-cnn memory footprint is about three times as much as the building block net, we independently train three imagenet-nin nets and average their predictions. we obtain top-5 error which is lower than the building block but is higher than that of hd-cnn (table [reference]). subsubsection: vgg-16-layer building block net the second building block net we use is a 16-layer cnn from. we denote it as imagenet-vgg-16-layer. the layers from conv1_1 to pool4 are shared and they account for of the total parameters and of the total floating number operations. the remaining layers are used as independent layers in coarse and fine category classifiers. we follow the training and testing protocols as in. for training, we first sample a size from the range and resize the image so that the length of short edge is. then a randomly cropped and flipped patch of size is used for training. for testing, dense evaluation is performed on three scales and the averaged prediction is used as the final prediction. please refer to for more training and testing details. on imagenet validation set, imagenet-vgg-16-layer achieves top-1 and top-5 errors and respectively. we build a category hierarchy with 84 overlapping coarse categories. we implement multi-gpu training on caffe by exploiting data parallelism and train the fine category classifiers on two nvidia tesla k40c cards. the initial learning rate is and it is decreased by a factor of 10 every 4 k iterations. hd-cnn fine-tuning is not performed. due to large memory footprint of the building block net (table [reference]), the hd-cnn with 84 fine category classifiers can not fit into the memory directly. therefore, we compress the parameters in layers fc6 and fc7 as they account for over of the parameters. parameter matrices in fc6 and fc7 are of size and. their compression hyperparameters are and. the compression factors are and respectively. the hd-cnn obtains top-1 and top-5 errors and on imagenet validation set and improves over imagenet-vgg-16-layer by and respectively. comparison with state-of-the-art. currently, the two best nets on imagenet dataset are googlenet (table [reference]) and vgg 19-layer network. using multi-scale multi-crop testing, a single googlenet net achieves top-5 error. with multi-scale dense evaluation, a single vgg 19-layer net obtains top-1 and top-5 errors and and improves top-5 error of googlenet by. our hd-cnn decreases top-1 and top-5 errors of vgg 19-layer net by and respectively. furthermore, hd-cnn slightly outperforms the results of averaging the predictions from vgg-16-layer and vgg-19-layer nets. section: conclusions and future work we demonstrated that hd-cnn is a flexible deep cnn architecture to improve over existing deep cnn models. we showed this empirically on both cifar-100 and image-net datasets using three different building block nets. as part of future work, we plan to extend hd-cnn architectures to those with more than 2 hierarchical levels and also verify our empirical results in a theoretical framework. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar100",
                        1128
                    ],
                    [
                        "cifar-100",
                        31893
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "hierarchical deep convolutional neural network",
                        18
                    ],
                    [
                        "hierarchical deep cnns",
                        538
                    ],
                    [
                        "hd-cnns",
                        563
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "top-1 error",
                        1268
                    ],
                    [
                        "error",
                        4425
                    ],
                    [
                        "testing error",
                        20361
                    ],
                    [
                        "classification error",
                        20745
                    ],
                    [
                        "testing errors",
                        24973
                    ],
                    [
                        "error rates",
                        25241
                    ],
                    [
                        "top-5 errors",
                        25713
                    ],
                    [
                        "class-wise top-5 error improvement",
                        26286
                    ],
                    [
                        "view testing top-5 error",
                        28510
                    ],
                    [
                        "top-5 error",
                        28996
                    ],
                    [
                        "top-1",
                        30225
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "image classification",
                        103
                    ],
                    [
                        "image classifcation",
                        5695
                    ],
                    [
                        "classification",
                        6449
                    ],
                    [
                        "end-to-end classification",
                        8468
                    ],
                    [
                        "coarse",
                        10631
                    ],
                    [
                        "fine classification tasks",
                        10642
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "12806c298e01083a79db77927530367d85939907-17",
    "doctext": "document: an empirical evaluation of deep learning on highway driving numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. in this paper, we presented a number of empirical evaluations of recent deep learning advances. computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. to prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. we collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. we show how existing convolutional neural networks (cnns) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. our results lend credence to the hypothesis that deep learning holds promise for autonomous driving. section: introduction since the darpa grand challenges for autonomous vehicles, there has been an explosion in applications and research for self-driving cars. among the different environments for self-driving cars, highway and urban roads are on opposite ends of the spectrum. in general, highways tend to be more predictable and orderly, with road surfaces typically well-maintained and lanes well-marked. in contrast, residential or urban driving environments feature a much higher degree of unpredictability with many generic objects, inconsistent lane-markings, and elaborate traffic flow patterns. the relative regularity and structure of highways has facilitated some of the first practical applications of autonomous driving technology. many automakers have begun pursuing highway auto-pilot solutions designed to mitigate driver stress and fatigue and to provide additional safety features; for example, certain advanced-driver assistance systems (adas) can both keep cars within their lane and perform front-view car detection. currently, the human drivers retain liability and, as such, must keep their hands on the steering wheel and prepare to control the vehicle in the event of any unexpected obstacle or catastrophic incident. financial considerations contribute to a substantial performance gap between commercially available auto-pilot systems and fully self-driving cars developed by google and others. namely, today's self-driving cars are equipped with expensive but critical sensors, such as lidar, radar and high-precision gps coupled with highly detailed maps. in today's production-grade autonomous vehicles, critical sensors include radar, sonar, and cameras. long-range vehicle detection typically requires radar, while nearby car detection can be solved with sonar. computer vision can play an important a role in lane detection as well as redundant object detection at moderate distances. radar works reasonably well for detecting vehicles, but has difficulty distinguishing between different metal objects and thus can register false positives on objects such as tin cans. also, radar provides little orientation information and has a higher variance on the lateral position of objects, making the localization difficult on sharp bends. the utility of sonar is both compromised at high speeds and, even at slow speeds, is limited to a working distance of about meters. compared to sonar and radar, cameras generate a richer set of features at a fraction of the cost. by advancing computer vision, cameras could serve as a reliable redundant sensor for autonomous driving. despite its potential, computer vision has yet to assume a significant role in today's self-driving cars. classic computer vision techniques simply have not provided the robustness required for production grade automotives; these techniques require intensive hand engineering, road modeling, and special case handling. considering the seemingly infinite number of specific driving situations, environments, and unexpected obstacles, the task of scaling classic computer vision to robust, human-level performance would prove monumental and is likely to be unrealistic. deep learning, or neural networks, represents an alternative approach to computer vision. it shows considerable promise as a solution to the shortcomings of classic computer vision. recent progress in the field has advanced the feasibility of deep learning applications to solve complex, real-world problems; industry has responded by increasing uptake of such technology. deep learning is data centric, requiring heavy computation but minimal hand-engineering. in the last few years, an increase in available storage and compute capabilities have enabled deep learning to achieve success in supervised perception tasks, such as image detection. a neural network, after training for days or even weeks on a large data set, can be capable of inference in real-time with a model size that is no larger than a few hundred mb. state-of-the-art neural networks for computer vision require very large training sets coupled with extensive networks capable of modeling such immense volumes of data. for example, the ilsrvc data-set, where neural networks achieve top results, contains million images in over categories. by using expensive existing sensors which are currently used for self-driving applications, such as lidar and mm-accurate gps, and calibrating them with cameras, we can create a video data set containing labeled lane-markings and annotated vehicles with location and relative speed. by building a labeled data set in all types of driving situations (rain, snow, night, day, etc.), we can evaluate neural networks on this data to determine if it is robust in every driving environment and situation for which we have training data. in this paper, we detail empirical evaluation on the data set we collect. in addition, we explain the neural network that we applied for detecting lanes and cars, as shown in figure [reference]. section: related work recently, computer vision has been expected to player a larger role within autonomous driving. however, due to its history of relatively low precision, it is typically used in conjunction with either other sensors or other road models. cho et al. uses multiple sensors, such as lidar, radar, and computer vision for object detection. they then fuse these sensors together in a kalman filter using motion models on the objects. held et al., uses only a deformable parts based model on images to get the detections, then uses road models to filter out false positives. carafii et al. uses a waldboost detector along with a tracker to generate pixel space detections in real time. jazayeri et al. relies on temporal information of features for detection, and then filters out false positives with a front-view motion model. in contrast to these object detectors, we do not use any road or motion-based models; instead we rely only on the robustness of a neural network to make reasonable predictions. in addition, we currently do not rely on any temporal features, and the detector operates independently on single frames from a monocular camera. to make up for the lack of other sensors, which estimate object depth, we train the neural network to predict depth based on labels extracted from radar returns. although the model only predicts a single depth value for each object, eigen et al. have shown how a neural network can predict entire depth maps from single images. the network we train likely learns some model of the road for object detection and depth predictions, but it is never explicitly engineered and instead learns from the annotations alone. before the wide spread adoption of convolutional neural networks (cnns) within computer vision, deformable parts based models were the most successful methods for detection. after the popular cnn model alexnet was proposed, state-of-the-art detection shifted towards cnns for feature extraction. girshick et al. developed r-cnn, a two part system which used selective search to propose regions and alexnet to classify them. r-cnn achieved state-of-the-art on pascal by a large margin; however, due to its nearly classification queries and inefficient re-use of convolutions, it remains impractical for real-time implementations. szegedy et al. presented a more scalable alternative to r-cnn, that relies on the cnn to propose higher quality regions compared to selective search. this reduces the number of region proposals down to as low as while keeping the map competitive with selective search. an even faster approach to image detection called overfeat was presented by sermanet et al.. by using a regular pattern of\" region proposals\", overfeat can efficiently reuse convolution computations from each layer, requiring only a single forward pass for inference. for our empirical evaluation, we use a straight-forward application of overfeat, due to its efficiencies, and combine this with labels similar to the ones proposed by szegedy et al.. we describe the model and similarities in the next section. section: real time vehicle detection convolutional neural networks (cnns) have had the largest success in image recognition in the past 3 years. from these image recognition systems, a number of detection networks were adapted, leading to further advances in image detection. while the improvements have been staggering, not much consideration had been given to the real-time detection performance required for some applications. in this paper, we present a detection system capable of operating at greater than hz using nothing but a laptop gpu. due to the requirements of highway driving, we need to ensure that the system used can detect cars more than m away and can operate at speeds greater than hz; this distance requires higher image resolutions than is typically used, and in our case is. we use the overfeat cnn detector, which is very scalable, and simulates a sliding window detector in a single forward pass in the network by efficiently reusing convolutional results on each layer. other detection systems, such as r-cnn, rely on selecting as many as candidate windows, where each is evaluated independently and does not reuse convolutional results. in our implementation, we make a few minor modifications to overfeat's labels in order to handle occlusions of cars, predictions of lanes, and accelerate performance during inference. we will first provide a brief overview of the original implementation and will then address the modifications. overfeat converts an image recognition cnn into a\" sliding window\" detector by providing a larger resolution image and transforming the fully connected layers into convolutional layers. then, after converting the fully connected layer, which would have produced a single final feature vector, to a convolutional layer, a grid of final feature vectors is produced. each of the resulting feature vectors represents a slightly different context view location within the original pixel space. to determine the stride of this window in pixel space, it is possible to simply multiply the strides on each convolutional or pool layer together. the network we used has a stride size of pixels. each final feature vector in this grid can predict the presence of an object; once an object is detected, those same features are then used to predict a single bounding box through regression. the classifier will predict\" no-object\" if it can not discern any part of an object within its entire input view. this causes large ambiguities for the classifier, which can only predict a single object, as two different objects could can easily appear in the context view of the final feature vector, which is typically larger than of the input image resolution. the network we used has a context view of pixels in size. to ensure that all objects in the image are classified at least once, many different context views are taken of the image by using skip gram kernels to reduce the stride of the context views and by using up to four different scales of the input image. the classifier is then trained to activate when an object appears anywhere within its entire context view. in the original overfeat paper, this results in different context views (or final feature vectors), where each one is likely to become active (create a bounding box). this creates two problems for our empirical evaluation. due to the l2 loss between the predicted bounding box and actual bounding proposed by sermanet et al., the ambiguity of having two valid bounding box locations to predict when two objects appear, is incorrectly handled by the network by predicting a box in the center of the two objects to minimize its expected loss. these boxes tend to cause a problem for the bounding box merging algorithm, which incorrectly decides that there must be a third object between the two ground truth objects. this could cause problems for an adas system which falsely believes there is a car where there is not, and emergency breaking is falsely applied. in addition, the merging algorithm, used only during inference, operates in where is the number of bounding boxes proposed. because the bounding box merging is not as easily parallelizable as the cnn, this merging may become the bottleneck of a real-time system in the case of an inefficient implementation or too many predicted bounding boxes. in our evaluations, we use a mask detector as described in szegedy et al. to improve some of the issues with overfeat as described above. szegedy et al. proposes a cnn that takes an image as input and outputs an object mask through regression, highlighting the object location. the idea of a mask detector is shown in fig [reference]. to distinguish multiple nearby objects, different part-detectors output object masks, from which bounding boxes are then extracted. the detector they propose must take many crops of the image, and run multiple cnns for each part on every crop. their resulting implementation takes roughly-seconds per frame per class using a 12-core machine, which would be too slow for our application. we combine these ideas by using the efficient\" sliding window\" detector of overfeat to produce an object mask and perform bounding box regression. this is shown in fig [reference]. in this implementation, we use a single image resolution of with no skip gram kernels. to help the ambiguity problem, and reduce the number of bounding boxes predicted, we alter the detector on the top layer to only activate within a pixel region at the center of its context view, as shown in the first box in fig [reference]. because it's highly unlikely that any two different object's bounding boxes appear in a pixel region, compared to the entire context view with overfeat, the bounding box regressor will no longer have to arbitrarily choose between two valid objects in its context view. in addition, because the requirement for the detector to fire is stricter, this produces many fewer bounding boxes which greatly reduces our run-time performance during inference. although these changes helped, ambiguity was still a common problem on the border of bounding boxes in the cases of occlusion. this ambiguity results in a false bounding box being predicted between the two ground truth bounding boxes. to fix this problem, the bounding boxes were first shrunk by before creating the detection mask label. this added the additional requirement that the center-pixel region of the detector window had to be within the center region of the object before activating. the bounding box regressor however, still predicts the original bounding box before shrinking. this also further reduces the number of active bounding boxes as input to our merging algorithm. we also found that switching from l2 to l1 loss on the bounding box regressions results in better performance. to merge the bounding boxes together, we used opencv's efficient implementation of grouprectangles, which clusters the bounding boxes based on a similarity metric in. the lower layers of our cnn we use for feature extraction is similar to the one proposed by krizhevsky et al.. our modifications to the network occurs on the dense layers which are converted to convolution, as described in sermanet et al.. when using our larger image sizes of this changes the previous final feature response maps of size to. as stated earlier, each of these feature vectors sees a context region of pixels, and the stride between them is pixels; however, we want each making predictions at a resolution of pixels, which would leave gaps in our input image. to fix this, we use each feature as input to softmax classifiers, which are arranged in an grid each predicting if an object is within a different pixel region. this allows for the feature vector to cover the full stride size of pixels; the end result is a grid mask detector of size where each element is pixels which covers the entire input image of size. subsection: lane detection the cnn used for vehicle detection can be easily extended for lane boundary detection by adding an additional class. whereas the regression for the vehicle class predicts a five dimensional value (four for the bounding box and one for depth), the lane regression predicts six dimensions. similar to the vehicle detector, the first four dimensions indicate the two end points of a local line segment of the lane boundary. the remaining two dimensions indicate the depth of the endpoints with respect to the camera. fig [reference] visualizes the lane boundary ground truth label overlaid on an example image. the green tiles indicate locations where the detector is trained to fire, and the line segments represented by the regression labels are explicitly drawn. the line segments have their ends connected to form continuous splines. the depth of the line segments are color-coded such that the closest segments are red and the furthest ones are blue. due to our data collection methods for lane labels, we are able to obtain ground truth in spite of objects that occlude them. this forces the neural network to learn more than a simple paint detector, and must use context to predict lanes where there are occlusions. similar to the vehicle detector, we use l1 loss to train the regressor. we use mini-batch stochastic gradient descent for optimization. the learning rate is controlled by a variant of the momentum scheduler. to obtain semantic lane information, we use dbscan to cluster the line segments into lanes. fig [reference] shows our lane predictions after dbscan clustering. different lanes are represented by different colors. since our regressor outputs depths as well, we can predict the lane shapes in 3d using inverse camera perspective mapping. section: experimental setup subsection: data collection our research vehicle is a 2014 infiniti q50. the car currently uses the following sensors: 6x point grey flea3 cameras, 1x velodyne hdl32e lidar, and 1x novatel span-se receiver. we also have access to the q50 built-in continental mid-range radar system. the sensors are connected to a linux pc with a core i7-4770k processor. once the raw videos are collected, we annotate the 3d locations for vehicles and lanes as well as the relative speed of all the vehicles. to get vehicle annotations, we follow the conventional approach of using amazon mechanical turk to get accurate bounding box locations within pixel space. then, we match bounding boxes and radar returns to obtain the distance and relative speed of the vehicles. unlike vehicles that can be annotated with bounding boxes, highway lane borders often need to be annotated as curves of various shapes. this makes frame-level labelling not only tedious and inefficient, but also prone to human errors. fortunately, lane markings can be considered as\u00e2\u0080\u009cstatic\u00e2\u0080\u009d objects that do not change their geolocations very often. we follow the process descried in to create lidar maps of the environment using the velodyne and gnss systems. using these maps, labeling is straight forward. first, we filter the 3d point clouds based on lidar return intensity and position to obtain the left and right boundaries of the ego-lane. then, we replicate the left and right ego-lane boundaries to obtain initial guesses for all the lane boundaries. a human annotator inspects the generated lane boundaries and makes appropriate corrections using our 3d labelling tool. for completeness, we describe each of these steps in details. subsubsection: ego-lane boundary generation since we do not change lanes during data collection drives, the gps trajectory of our research vehicle already gives a decent estimate of the shape of the road. we can then easily locate the ego-lane boundaries using a few heuristic filters. noting that lane boundaries on highways are usually marked with retro-reflective materials, we first filter out low-reflectivity surfaces such as asphalt in our 3d point cloud maps and only consider points with high enough laser return intensities. we then filter out other reflective surfaces such as cars and traffic signs by only considering points whose heights are close enough the ground plane. lastly, assuming our car drives close to the center of the lane, we filter out ground paint other than the ego-lane boundaries, such as other lane boundaries, car-pool or directional signs, by only considering markings whose absolute lateral distances from the car are smaller than 2.2 meters and greater than 1.4 meters. we can also distinguish the left boundary from the right one using the sign of the lateral distance. after obtaining the points in the left and right boundaries, we fit a piecewise linear curve similar to the gps trajectory to each boundary. subsubsection: semi-automatic generation of multiple lane boundaries we observe that the width of lanes during a single data collection run stays constant most of the time, with occasional exceptions such as merges and splits. therefore, if we predefine the number of lanes to the left and right of the car for a single run, we can make a good initial guess of all the lane boundaries by shifting the auto-generated ego-lane boundaries laterally by multiples of the lane width. we will then rely on human annotators to fix the exception cases. subsection: data set at the time of this writing our annotated data-set consists of days of driving in the san francisco bay area during the months of april-june for a few hours each day. the vehicle annotated data is sampled at and contains nearly thousand frames with thousand bounding boxes. the lane annotated data is sampled at and contains over thousand frames. during training, translation and 7 different perspective distortions are applied to the raw data sets. fig [reference] shows an example image after perspective distortions are applied. note that we apply the same perspective distortion to the ground truth labels so that they match correctly with the distorted image. subsection: results the detection network used is capable of running at hz using a desktop pc equipped with a gtx 780 ti. when using a mobile gpu, such as the tegra k1, we were capable of running the network at hz, and would expect the system to run at hz using the nvidia px1 chipset. our lane detection test set consists of 22 video clips collected using both left and right cameras during 11 different data collection runs, which correspond to about 50 minutes of driving. we evaluate detection results for four lane boundaries, namely, the left and right boundaries of the ego lane, plus the outer boundaries of the two adjacent lanes. for each of these lane boundaries, we further break down the evaluation by longitudinal distances, which range from 15 to 80 meters ahead of the car, spaced by 5 meters. thus, there are at maximum positions at which we evaluate the detection results. we pair up the prediction and ground truth points at each of these locations using greedy nearest neighbor matching. true positives, false positives and false negatives are accumulated at every evaluation location in a standard way: a true positive is counted when the matched prediction and ground truth differ by less than 0.5 meter. if the matched prediction and ground truth differ by more than 0.5 meter, both false positive and false negative counts are incremented. fig [reference] shows a visualization of this evaluation method on one image. the blue dots are true positives. the red dots are false positives, and the yellow ones are false negatives. fig [reference] shows the aggregated precision, recall and f1 score on all test videos. for the ego-lane boundaries, we obtain f1 score up to 50 meters. recall starts to drop fast beyond 65 meters, mainly because the resolution of the image can not capture the width of the lane markings at that distance. for the adjacent lanes, recall is low for the nearest point because it is outside the field of view of the camera. [b] 1.6 in [b] 1.6 in [b] 1.6 in [b] 1.6 in the vehicle detection test set consists of 13 video clips collected from a single day, which corresponds to 1 hour and 30 mins of driving. the accuracy of the vehicle bounding box predictions were measured using intersection over union (iou) against the ground truth boxes from amazon mechanical turk (amt). a bounding box prediction matched with ground truth if iou. the performance of our car detection as a function of depth can be seen in fig [reference]. nearby false positives can cause the largest problems for adas systems which could cause the system to needlessly apply the brakes. in our system, we found overpasses and shading effects to cause the largest problems. two examples of these situations are shown in fig [reference]. [b] 1.7 in [b] 1.7 in as a baseline to our car detector, we compared the detection results to the continental mid-range radar within our data collection vehicle. while matching radar returns to ground truth bounding boxes, we found that although radar had nearly precision, false positives were being introduced through errors in radar/ camera calibration. therefore, to ensure a fair comparison we matched every radar return to a ground truth bounding box even if iou, giving our radar returns precision. this comparison is shown in fig [reference], the f1 score for radar is simply the recall. in addition to the bounding box locations, we measured the accuracy of the predicted depth by using radar returns as ground truth. the standard error in the depth predictions as a function of depth can be seen in fig [reference]. for a qualitative review of the detection system, we have uploaded a hour video of the vehicle detector ran on our test set. this may be found at youtu.be/ gj0czbkhohc. a short video of our lane detector may also be found online at youtu.be/__ f5pqqp6am. in these videos, we evaluate the detector on every frame independently and display the raw detections, without the use of any kalman filters or road models. the red locations in the video correspond to the mask detectors that are activated. this network was only trained on the rear view of cars traveling in the same direction, which is why cars across the highway barrier are commonly missed. we have open sourced the code for the vehicle and lane detector online at github.com/ brodyh/ caffe. our repository was forked from the original caffe code base from the bvlc group. section: conclusion by using camera, lidar, radar, and gps we built a highway data set consisting of thousand image frames with vehicle bounding boxes and over thousand image frames with lane annotations. we then trained on this data using a cnn architecture capable of detecting all lanes and cars in a single forward pass. using a single gtx 780 ti our system runs at hz, which is more than adequate for real-time use. our results show existing cnn algorithms are capable of good performance in highway lane and vehicle detection. future work will focus on acquiring frame level annotations that will allow us to develop new neural networks capable of using temporal information across frames. section: acknowledgment this research was funded in part by nissan who generously donated the car used for data collection. we thank our colleagues yuta yoshihata from nissan who provided technical support and expertise on vehicles that assisted the research. in addition, the authors would like to thank the author of overfeat, pierre sermanet, for their helpful suggestions on image detection. bibliography: references",
    "templates": [
        {
            "Material": [],
            "Method": [],
            "Metric": [
                [
                    [
                        "f1 score",
                        24646
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "lane detection",
                        747
                    ],
                    [
                        "lane",
                        844
                    ],
                    [
                        "detecting lanes",
                        5984
                    ],
                    [
                        "detection",
                        6805
                    ],
                    [
                        "real-time detection",
                        9498
                    ],
                    [
                        "highway lane",
                        27948
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "148eb2b649ce0f66214fceba5c061b417ab88d79-18",
    "doctext": "document: inferencing based on unsupervised learning of disentangled representations combining generative adversarial networks (gans) with encoders that learn to encode data points has shown promising results in learning data representations in an unsupervised way. we propose a framework that combines an encoder and a generator to learn disentangled representations which encode meaningful information about the data distribution without the need for any labels. while current approaches focus mostly on the generative aspects of gans, our framework can be used to perform inference on both real and generated data points. experiments on several data sets show that the encoder learns interpretable, disentangled representations which encode descriptive properties and can be used to sample images that exhibit specific characteristics. 0 networks, (esann) 2018 section: introduction learning meaningful representations of data is an important step for models to understand the world. recently, the generative adversarial network (gan) has been proposed as a method that can learn characteristics of data distributions without the need for labels. gans traditionally consist of a generator, which generates data from randomly sampled vectors, and a discriminator, which tries to distinguish generated data from real data. during training, the generator learns to generate realistic data samples, while the discriminator becomes better at distinguishing between the generated and the real data. as a result, both the generator and the discriminator learn characteristics about the underlying data distribution without the need for any labels. one desirable characteristic of learned representations is disentanglement, which means that different parts of the representation encode different factors of the data-generating distribution. this makes representations more interpretable, easier to modify, and is a useful property for many tasks such as classification, clustering, or image captioning. to achieve this, chen et al. introduced a gan variant in which the generator's input is split into two parts and. here, encodes unstructured noise while encodes meaningful, data-generating factors. through enforcing high mutual information between and and the generated images the generator is trained using the inputs as meaningful encodings for certain image characteristics. for example, a ten-dimensional categorical code for could represent the ten different digit classes in the mnist data set. since no labels are provided the generator has to learn by itself which image characteristics can be represented through. one drawback of this model is that the only way to perform inference, i.e. map real data samples into a (disentangled) representation, is to use the discriminator. however, there is no guarantee that the discriminator learns good representations of the data in general, as it is trained to discriminate between real and generated data and may therefore focus only on features that are helpful for discriminating these two, but are not necessarily descriptive of the data distribution in general. zhang et al. tried to enforce disentangled representations in order to improve the controllability of the generator. the latent representation is split up into two parts encoding meaningful information and unknown factors of variation. two additional inference networks are introduced to enforce the disentanglement between the two parts of the latent representation. while this setup yields a better controllability over the generative process it depends on labeled samples for its training objective and can not discover unknown data-generating factors, but only encodes known factors of variation (obtained through labels) in its disentangled representation. donahue et al. and dumoulin et al. introduced an extension which includes an encoder that learns the encodings of real data samples. the discriminator gets as input both the data sample (either real or generated) and the according representation (either or) and has to classify them as either coming from the generator or the encoder. the generator and the encoder try to fool the discriminator into misclassifying the samples. as a result, the encoder learns to approximate the inverse of the generator and can be used to map real data samples into representations for other applications. however, in these approaches the representations follow a simple prior, e.g. a gaussian or uniform distribution, and do not exhibit any disentangled properties. our model, the bidirectional-infogan, integrates some of these approaches by extending traditional gans with an encoder that learns disentangled representations in an unsupervised setting. after training, the encoder can map data points to meaningful, disentangled representations which can potentially be used for different tasks such as classification, clustering, or image captioning. compared to the infogan we introduce an encoder to mitigate the problems of using a discriminator for both the adversarial loss and the inference task. unlike the structured gan our training procedure is completely unsupervised, can detect unknown data-generating factors, and only introduces one additional inference network (the encoder). in contrast to the bidirectional gan we replace the simple prior on the latent representation with a distribution that is amenable to disentangled representations and introduce an additional loss for the encoder and the generator to achieve disentangled representations. on the mnist, celeba, and svhn data sets we show that the encoder does learn interpretable representations which encode meaningful properties of the data distribution. using these we can sample images that exhibit certain characteristics, e.g. digit identity and specific stroke widths for the mnist data set, or different hair colors and clothing accessories in the celeba data set. every picture/.style= line width=0.2 mm [scale=0.39] [] at (2.5, 9.5) features; fill= white, rounded corners=0.2 cm] (0.6, 1.5) rectangle (4.6, 2.8) node [pos=.5]; [fill= white, rounded corners=0.2 cm] (0.6,- 0.2) rectangle (4.6, 1.1) node [pos=.5]; [fill= gray, opacity=0.3, rounded corners=0.2 cm] (0.6, 6.65) rectangle (4.6, 7.95) node [pos=.5, opacity=1]; [-\u00bf] (4.6, 7.3)- (7, 7.3); [-\u00bf] (9, 7.3)- (10.8, 7.3); (18.5, 5.75) ellipse (2.7 and 1) node []; [] at (13, 9.5) data; fill= gray, opacity=0.3, rounded corners=0.2 cm] (10.9, 1.5) rectangle (14.9, 2.8) node [pos=.5, opacity=1]; [-\u00bf] (10.9, 2.15)- (10.5, 2.15)- (10.5, 1.7)- (9, 1.7); [fill= white, rounded corners=0.2 cm] (10.9,- 0.2) rectangle (14.9, 1.1) node [pos=.5]; [-\u00bf] (10.9, 0.45)- (10.5, 0.45)- (10.5, 0.9)- (9, 0.9); [fill= white, rounded corners=0.2 cm] (10.9, 6.65) rectangle (14.9, 7.95) node [pos=.5]; [-\u00bf] (2.5, 6.6)- (2.5, 5.8)- (15.8, 5.8); [] (13, 6.6)- (13, 5.8); [] (13, 2.8)- (13, 3.4); [-\u00bf] (2.5, 2.8)- (2.5, 3.4)- (15.8, 3.4) node [pos=.4, above, inner sep=0.5pt] if image node [pos=.4, below, inner sep=0.5pt] is real; [] (13,- 1)- (13,- 0.2); [-\u00bf] (2.5,- 0.2)- (2.5,- 1)- (15.8,- 1) node [pos=.4, above, inner sep=0.5pt] if image node [pos=.4, below, inner sep=0.5pt] is generated; (18.5, 3.35) ellipse (2.7 and 1) node []; (19.8,- 1) ellipse (4 and 1) node []; [fill= yellow!30] (27.5,- 1) ellipse (1.9 and 0.9) node [] node [above, inner sep=11pt] mutual information; [-\u00bf] (23.8,- 1)- (25.6,- 1); [fill= red!10] (7, 8.3)- (8, 8.3)- (9, 7.3)- (8, 6.3)- (7, 6.3)- (7, 8.3); [] at (7.75, 7.3); [fill= red!10] (8, 2.3)- (9, 2.3)- (9, 0.3)- (8, 0.3)- (7, 1.3)- (8, 2.3); [] at (8.25, 1.3); [-\u00bf] (7, 1.3)- (6, 1.3)- (6, 2.1)- (4.7, 2.1); [-\u00bf] (6, 1.3)- (6, 0.5)- (4.7, 0.5); [fill= red!10] (23.5, 5.6)- (22.5, 5.6)- (22.5, 3.6)- (23.5, 3.6)- (24.5, 4.6)- (23.5, 5.6); [] at (23.25, 4.6); [-\u00bf] (21.2, 5.75)- (21.5, 5.75)- (21.5, 5.2)- (22.5, 5.2); [-\u00bf] (21.2, 3.45)- (21.5, 3.45)- (21.5, 4.2)- (22.5, 4.2); [fill= yellow!30] (28, 4.6) ellipse (2.5 and 0.9) node [] node [above, inner sep=11pt] adversarial cost; [-\u00bf] (24.5, 4.6)- (25.5, 4.6); section: methodology our model, shown in fig. [reference], consists of a generator, a discriminator, and an encoder, which are implemented as neural networks. the input vector that is given to the generator is made up of two parts. here, is sampled from a uniform distribution,, and is used to represent unstructured noise in the images. on the other hand, is the part of the representation that encodes meaningful information in a disentangled manner and is made up of both categorical values and continuous values. takes as input and transforms it into an image, i.e.. is a convolutional network that gets as input either real or fake images and encodes them into a latent representation. gets as input an image and the corresponding representation concatenated along the channel axis. it then tries to classify the pair as coming either from the generator or the encoder, i.e., while both and try to fool the discriminator into misclassifying its input. as a result the original gan minimax game is extended and becomes: where is the adversarial cost as depicted in fig. [reference]. [b] 0.8 [b] 0.8 in order to force the generator to use the information provided in we maximize the mutual information between and. maximizing the mutual information directly is hard, as it requires the posterior and we therefore follow the approach by chen et al. and define an auxiliary distribution to approximate. we then maximize the lower bound, where is the mutual information depicted in fig. [reference]. for simplicity reasons we fix the distribution over and, therefore, the entropy term is treated as a constant. in our case is the encoder network which gets images generated by as input and is trained to approximate the unknown posterior. for categorical we use the softmax nonlinearity to represent while we treat the posterior of continuous as a factored gaussian. given this structure, the minimax game for the bidirectional-infogan (binfogan) is then where determines the strength of the impact of the mutual information criterion and is set to in all our experiments. section: experiments we perform experiments on the mnist, the celeba, and the svhn data set. while the final performance of the model is likely influenced by choosing the\" optimal\" characteristics for this is usually not possible, since we do not know all data-generating factors beforehand. when choosing the characteristics and dimensionality of the disentangled vector we therefore mostly stick with the values previously chosen by chen et al.. for further information on the network architectures and more examples of the learned characteristics on the different data sets see our git:. [b] 0.8 [b] 0.8 on the mnist data set we model the latent code with one categorical variable and two continuous variables. during the optimization process and without the use of any labels the encoder learns to use to encode different digit classes, while and encode stroke width and digit rotation. fig. [reference] shows images randomly sampled from the test set according to the ten different categorical values. we can see that the encoder has learned to reliably assign a different categorical value for different digits. indeed, by manually matching the different categories in to a digit type, we achieve a test set accuracy of 96.61% (, averaged over 10 independent runs) without ever using labels during the training, compared to chen et al. (unsupervised) with an accuracy of 95%, and zhang et al. (semi-supervised, 20 labels) with an accuracy of 96%. fig. [reference] shows images sampled from the test set for different values of and. we see that we can use the encodings from to now sample for digits with certain characteristics such as stroke width and rotation, even though this information was not explicitly provided during training. on the celeba data set the latent code is modeled with four categorical codes and four continuous variables. again, the encoder learns to associate certain image characteristics with specific codes in. this includes characteristics such as the presence of glasses, hair color, and background color and is visualized in fig. [reference]. on the svhn data set we use the same network architecture and latent code representations as for the celeba data set. again, the encoder learns interpretable, disentangled representations encoding characteristics such as image background, contrast and digit type. see fig. [reference] for examples sampled from the svhn test set. these results indicate that the bidirectional-infogan is indeed capable of mapping data points into disentangled representations that encode meaningful characteristics in a completely unsupervised manner. section: conclusion we showed that an encoder coupled with a generator in a generative adversarial network can learn disentangled representations of the data without the need for any explicit labels. using the encoder network we maximize the mutual information between certain parts of the generator's input and the images that are generated from it. through this the generator learns to associate certain image characteristics with specific parts of its input. additionally, the adversarial cost from the discriminator forces both the generator to generate realistic looking images and the encoder to approximate the inverse of the generator, leading to disentangled representations that can be used for inference. the learned characteristics are often meaningful and humanly interpretable, and can potentially help with other tasks such as classification and clustering. additionally, our method can be used as a pre-training step on unlabeled data sets, where it can lead to better representations for the final task. however, currently we have no influence over which characteristics are learned in the unsupervised setting which means that the model can also learn characteristics or features that are meaningless or not interpretable by humans. in the future, this can be mitigated by combining our approach with semi-supervised approaches, in which we can supply a limited amount of labels for the characteristics we are interested in to exert more control over which data-generating factors are learned while still being able to discover\" new\" generating factors which do not have to be known or specified beforehand. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "mnist data set",
                        2484
                    ],
                    [
                        "mnist",
                        5538
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "bidirectional-infogan",
                        4546
                    ],
                    [
                        "binfogan",
                        9977
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        11326
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "unsupervised setting",
                        4698
                    ],
                    [
                        "semi-supervised",
                        11513
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "14cb3de564377cc4ba2b9bfe8ebd8c0a7c2787ce-19",
    "doctext": "document: taking a deeper look at pedestrians in this paper we study the use of convolutional neural networks (convnets) for the task of pedestrian detection. despite their recent diverse successes, convnets historically underperform compared to other pedestrian detectors. we deliberately omit explicitly modelling the problem into the network (e.g. parts or occlusion modelling) and show that we can reach competitive performance without bells and whistles. in a wide range of experiments we analyse small and big convnets, their architectural choices, parameters, and the influence of different training data, including pre-training on surrogate tasks. we present the best convnet detectors on the caltech and kitti dataset. on caltech our convnets reach top performance both for the caltech1x and caltech10x training setup. using additional data at training time our strongest convnet model is competitive even to detectors that use additional data (optical flow) at test time.=-1 section: introduction in recent years the field of computer vision has seen an explosion of success stories involving convolutional neural networks (convnets). such architectures currently provide top results for general object classification, general object detection, feature matching, stereo matching, scene recognition, pose estimation, action recognition and many other tasks. pedestrian detection is a canonical case of object detection with relevant applications in car safety, surveillance, and robotics. a diverse set of ideas has been explored for this problem and established benchmark datasets are available. we would like to know if the success of convnets is transferable to the pedestrian detection task. previous work on neural networks for pedestrian detection has relied on special-purpose designs, e.g. hand-crafted features, part and occlusion modelling. although these proposed methods perform ably, current top methods are all based on decision trees learned via adaboost. in this work we revisit the question, and show that both small and large vanilla convnets can reach top performance on the challenging caltech pedestrians dataset. we provide extensive experiments regarding the details of training, network parameters, and different proposal methods. subsection: related work despite the popularity of the task of pedestrian detection, only few works have applied deep neural networks to this task: we are aware of only six. the first paper using convnets for pedestrian detection focuses on how to handle the limited training data (they use the inria dataset, which provides 614 positives and 1218 negative images for training). first, each layer is initialized using a form of convolutional sparse coding, and the entire network is subsequently fine-tuned for the detection task. they propose an architecture that uses features from the last and second last layer for detection. this method is named convnet. a different line of work extends a deformable parts model (dpm) with a stack of restricted boltzmann machines (rbms) trained to reason about parts and occlusion (dbn-isol). this model was extended to account for person-to-person relations (dbn-mut) and finally to jointly optimize all these aspects: jointdeep jointly optimizes features, parts deformations, occlusions, and person-to-person relations. the multisdp network feeds each layer with contextual features computed at different scales around the candidate pedestrian detection. finally sdn, the current best performing convnet for pedestrian detection, uses additional ''switchable layers'' (rbm variants) to automatically learn both low-level features and high-level parts (e.g. ''head'', ''legs'', etc.). note that none of the existing papers rely on a ''straightforward'' convolutional network similar to the original lenet (layers of convolutions, non-linearities, pooling, inner products, and a softmax on top). we will revisit this decision in this paper. paragraph: object detection other than pedestrian detection, related convnets have been used for detection of imagenet and pascal voc categories. the most successful general object detectors are based on variants of the r-cnn framework. given an input image, a reduced set of detection proposals is created, and these are then evaluated via a convnet. this is essentially a two-stage cascade sliding window method. see for a review of recent proposal methods. paragraph: detection proposals the most popular proposal method for generic objects is selectivesearch. the recent review also points out edgeboxes as a fast and effective method. for pedestrian detection dbn-isol and dbn-mut use dpm for proposals. jointdeep, multisdp, and sdn use a hog+ css+ linear svm detector (similar to) for proposals. only convnet applies a convnet in a sliding fashion. paragraph: decision forests most methods proposed for pedestrian detection do not use convnets for detection. leaving aside methods that use optical flow, the current top performing methods (on caltech and kitti datasets) are squareschnftrs, informedhaar, spatialpooling, ldcf, and regionlets. all of them are boosted decision forests, and can be considered variants of the integral channels features architecture. regionlets and spatialpooling use an large set of features, including hog, lbp and css, while squareschnftrs, informedhaar, and ldcf build over hog+ luv. on the caltech benchmark, the best convnet (sdn) is outperformed by all aforementioned methods. paragraph: input to convnets it is important to highlight that convnet learns to predict from yuv input pixels, whereas all other methods use additional hand-crafted features. dbn-isol and dbn-mut use hog features as input. multisdp uses hog+ css features as input. jointdeep and sdn uses yuv+ gradients as input (and hog+ css for the detection proposals). we will show in our experiments that good performance can be reached using rgb alone, but we also show that more sophisticated inputs systematically improve detection quality. our data indicates that the antagonism ''hand-crafted features versus convnets'' is an illusion. subsection: contributions in this paper we propose to revisit pedestrian detection with convolutional neural networks by carefully exploring the design space (number of layers, filter sizes, etc.), and the critical implementation choices (training data preprocessing, effect of detections proposal, etc.). we show that both small (parameters) and large (parameters) networks can reach good performance when trained from scratch (even when using the same data as previous methods). we also show the benefits of using extended and external data, which leads to the strongest single-frame detector on caltech. we report the best known performance for a convnet on the challenging caltech dataset (improving by more than percent points), and the first convnet results on the kitti dataset. section: training data it is well known that for convnets the volume of training data is quite important to reach good performance. below are the datasets we consider along the paper. paragraph: caltech the caltech dataset and its associated benchmark is one of the most popular pedestrian detection datasets. it consists of videos captured from a car traversing u.s. streets under good weather conditions. the standard training set in the ''reasonable'' setting consists of frames with annotated pedestrians, and the test set covers frames with pedestrians. paragraph: caltech validation set in our experiments we also use caltech training data for validation. for those experiments we use one of the suggested validation splits: the first five training videos are used for validation training and the sixth training video for validation testing. paragraph: caltech10x because the caltech dataset videos are fully annotated, the amount of training data can be increased by resampling the videos. inspired by, we increase the training data tenfold by sampling one out of three frames (instead of one out of thirty frames in the standard setup). this yields annotated pedestrians for training, extracted from frames. paragraph: kitti the kitti dataset consists of videos captured from a car traversing german streets, also under good weather conditions. although similar in appearance to caltech, it has been shown to have different statistics (see). its training set contains pedestrians (taller than pixels) over frames, and its test set frames. paragraph: imagenet, places in section [reference] we will consider using large convnets that can exploit pre-training for surrogate tasks. we consider two such tasks (and their associated datasets), the imagenet 2012 classification of a thousand object categories and the classification of scene categories. the datasets provide and annotated images for training, respectively. section: from decision forests to neural networks before diving into the experiments, it is worth noting that the proposal method we are using can be converted into a convnet so that the overall system can be seen as a cascade of two neural networks. squareschnftrs is a decision forest, where each tree node pools and thresholds information from one out of several feature channels. as mentioned in section [reference] it is common practice to learn pedestrian detection convnets on handcrafted features, thus the feature channels need not be part of the conversion. in this case, a decision node can be realised using (i) a fully connected layer with constant non-zero weights corresponding to the original pooling region and zero weights elsewhere, (ii) a bias term that applies the threshold, (iii) and a sigmoid non-linearity that yields a decision. a two-layer network is sufficient to model a level-2 decision tree given the three simulated node outputs. finally, the weighted sum over the tree decisions can be modelled with yet another fully-connected layer. the mapping from squareschnftrs to a deep neural network is exact: evaluating the same inputs it will return the exact same outputs. what is special about the resulting network is that it has not been trained by back-propagation, but by adaboost. this network already performs better than the best known convnet on caltech, sdn. unfortunately, experiments to soften the non-linearities and use back-propagation to fine-tune the model parameters did not show significant improvements. section: vanilla convolutional networks in our experience many convnet architectures and training hyper-parameters do not enable effective learning for diverse and challenging tasks. it is thus considered best practice to start exploration from architectures and parameters that are known to work well and progressively adapt it to the task at hand. this is the strategy of the following sections. in this section we first consider cifarnet, a small network designed to solve the cifar-10 classification problem (objects categories, colour images of). in section [reference] we consider alexnet, a network that has times more parameters than cifarnet and designed to solve the ilsvrc2012 classification problem (objects categories, colour images of vga resolution). both of these networks were introduced in and are re-implemented in the open source caffe project. although pedestrian detection is quite a different task than cifar-10, we decide to start our exploration from the cifarnet, which provides fair performance on cifar-10. its architecture is depicted in figure [reference], unless otherwise specified we use raw rgb input. we first discuss how to use the cifarnet network (section [reference]). this naive approach already improves over the best known convnets (section [reference]). sections [reference] and [reference] explore the design space around cifarnet and further push the detection quality. all models in this section are trained using caltech data only (see section [reference]). subsection: how to use cifarnet? given an initial network specification, there are still several design choices that affect the final detection quality. we discuss some of them in the following paragraphs. paragraph: detection proposals unless otherwise specified we use the squareschnftrs detector to generate proposals because, at the time of writing, it is the best performing pedestrian detector (on caltech) with source code available. in figure [reference] we compare squareschnftrs against edgeboxes, a state of the art class-agnostic proposal method. using class-specific proposals allows to reduce the number of proposals by three orders of magnitude. paragraph: thresholds for positive and negative samples given both training proposals and ground truth (gt) annotations, we now consider which training label to assign to each proposal. a proposal is considered to be a positive example if it exceeds a certain intersection-over-union (iou) threshold for at least one gt annotation. it is considered negative if it does not exceed a second iou threshold for any gt annotation, and is ignored otherwise. we find that using gt annotations as positives is beneficial (i.e. not applying significant jitter). paragraph: model window size a typical choice for pedestrian detectors is a model window of in which the pedestrian occupies an area of. it is unclear that this is the ideal input size for convnets. despite cifarnet being designed to operate over, table [reference] shows that a model size of indeed works best. we experimented with other variants (stretching versus cropping, larger context border) with no clear improvement. paragraph: training batch in a detection setup, training samples are typically highly imbalanced towards the background class. although in our validation setup the imbalance is limited (see table [reference]), we found it beneficial throughout our experiments to enforce a strict ratio of positive to negative examples per batch of the stochastic gradient descend optimisation. the final performance is not sensitive to this parameter as long as some ratio (vs. none) is maintained. we use a ratio of. in the supplementary material we detail all other training parameters. subsection: how far can we get with the cifarnet? given the parameter selection on the validation set from previous sections, how does cifarnet compare to previously reported convnet results on the caltech test set? in table [reference] and figure [reference], we see that our naive network right away improves over the best known convnet (versus sdn). to decouple the contribution of our strong squareschnftrs proposals to the cifarnet performance, we also train a cifarnet using the proposal from jointdeep. when using the same detection proposals at training and test time, the vanilla cifarnet already improves over both custom-designed jointdeep and sdn. our cifarnet results are surprisingly close to the best known pedestrian detector trained on caltech1x (versus spatialpooling). subsection: exploring different architectures encouraged by our initial results, we proceed to explore different parameters for the cifarnet architecture. subsubsection: number and size of convolutional filters using the caltech validation set we perform a swipe of convolutional filter sizes (,, or pixels) and number of filters at each layer (,, or filters). we include the full table in the supplementary material. we observe that using large filter sizes hurts quality, while the varying the number of filters shows less impact. although some fluctuation in miss-rate is observed, overall there is no clear trend indicating that a configuration is clearly better than another. thus, for sake of simplicity, we keep using cifarnet (--filters of pixel) in the subsequent experiments. subsubsection: number and type of layers in table [reference] we evaluate the effect of changing the number and type of layers, while keeping other cifarnet parameters fix. besides convolutional layers (conv) and fully-connected layers (fc), we also consider locally-connected layers (lc), and concatenating features across layers (concat23) (used in convnet). none of the considered architecture changes improves over the original three convolutional layers of cifarnet. subsection: input channels as discussed in section [reference], the majority of previous convnets for pedestrian detection use gradient and colour features as input, instead of raw rgb. in table [reference] we evaluate the effect of different input features over cifarnet. it seems that hog+ l channel provide a small advantage over rgb. for purposes of direct comparison with the large networks, in the next sections we keep using raw rgb as input for our cifarnet experiments. we report the cifarnet test set results in section [reference]. section: large convolutional network one appealing characteristic of convnets is their ability to scale in size of training data volume. in this section we explore larger networks trained with more data. we base our experiments on the r-cnn [] approach, which is currently one of the best performer on the pascal voc detection task. we are thus curious to evaluate its performance for pedestrian detection. subsection: surrogate tasks for improved detections the r-cnn approach (''regions with cnn features'') wraps the large network previously trained for the imagenet classification task, which we refer to as alexnet (see figure [reference]). we also use ''alexnet'' as shorthand for ''r-cnn with alexnet'' with the distinction made clear by the context. during r-cnn training alexnet is fine-tuned for the (pedestrian) detection task, and in a second step, the softmax output is replaced by a linear svm. unless otherwise specified, we use the default parameters of the open source, caffe based, r-cnn implementation. like in the previous sections, we use squareschnftrs for detection proposals. paragraph: pre-training if we only train the top layer svm, without fine-tuning the lower layers of alexnet, we obtain on the caltech test set. this is already surprisingly close to the best known convnet for the task (sdn). when fine-tuning all layers on caltech, the test set performance increases dramatically, reaching. this confirms the effectiveness of the general r-cnn recipe for detection (train alexnet on imagenet, fine-tune for the task of interest). in table [reference] we investigate the influence of the pre-training task by considering alexnets that have been trained for scene recognition (''places'', see section [reference]) and on both places and imagenet (''hybrid''). ''places'' provides results close to imagenet, suggesting that the exact pre-training task is not critical and that there is nothing special about imagenet. paragraph: caltech10x due to the large number of parameters of alexnet, we consider providing additional training data using caltech10x for fine-tuning the network (see section [reference]). despite the strong correlation across training samples, we do observe further improvement (see table [reference]). interestingly, the bulk of the improvement is due to more pedestrians (positives10x, uses positives from caltech10x and negatives from caltech1x). our top result,, makes our alexnet setup the best reported single-frame detector on caltech (i.e. no optical flow). subsection: caltech-only training to compare with cifarnet, and to verify whether pre-training is necessary at all, we train alexnet ''from scratch'' using solely the caltech training data. we collect results in table [reference]. training alexnet solely on caltech, yields, which improves over the proposals (squareschnftrs) and the previous best known convnet on caltech (sdn). using caltech10x further improves the performance, down to. although these numbers are inferior than the ones obtained with imagenet pre-training (, see table [reference]), we can get surprisingly competitive results using only pedestrian data despite the free parameters of the alexnet model. alexnet with caltech10x is second best known single-frame pedestrian detector on caltech (best known is ldcf, which also uses caltech10x). subsection: additional experiments paragraph: how many layers? so far all experiments use the default parameters of r-cnn. previous works have reported that, depending on the task, using features from lower alexnet layers can provide better results. table [reference] reports caltech validation results when training the svm output layer on top of layers four to seven (see figure [reference]). we report results when using the default parameters and parameters that have been optimised by grid search (detailed grid search included in supplementary material). we observe a negligible difference between default and optimized parameter (at most percent points). results for default parameters exhibit a slight trend of better performance for higher levels. these validation set results indicate that, for pedestrian detection, the r-cnn default parameters are a good choice overall. paragraph: effect of proposal method when comparing the performance of alexnet fine-tuned on caltech1x to the proposal method, we see an improvement of (percent points) in miss-rate. in table [reference] we study the impact of using weaker or stronger proposals. both acf and squareschnftrs provide source code, allowing us to generate training proposals. katamari and spatialpooling+ [] are current top performers on the caltech dataset, both using optical flow, i.e. additional information at test time. there is a gap between the detectors acf, squareschnftrs, and katamari/ spatialpooling, allowing us to cover different operating points. the results of table [reference] indicate that, despite the gap, there is no noticeable difference between alexnet models trained with acf or squareschnftrs. it is seems that as long as the proposals are not random (see top row of table [reference]), the obtained quality is rather stable. the results also indicate that the quality improvement from alexnet saturates around. using stronger proposals does not lead to further improvement. this means that the discriminative power of our trained alexnet is on par with the best known models on the caltech dataset, but does not overtake them. paragraph: kitti test set in figure [reference] we show performance of the alexnet in context of the kitti pedestrian detection benchmark. the network is pre-trained on imagenet and fine-tuned using kitti training data. squareschnftrs reaches (average precision), which the alexnet can improve to. these are the first published results for convnets on the kitti pedestrian detection dataset. subsection: error analysis results from the previous section are encouraging, but not as good as could be expected from looking at improvements on pascal voc. so what bounds performance? the proposal method? the localization quality of the convnet? looking at the highest scoring false positives paints a picture of localization errors of the proposal method, the r-cnn, and even the ground truth. to quantify this effect we rerun the caltech evaluation but remove all false positives that touch an annotation. this experiment provides an upper bound on performance when solving localisation issues in detectors and doing perfect non-maximum suppression. we see a surprisingly consistent improvement for all methods of about mr. this means that the intuition we gathered from looking at false positives is wrong and actually almost all of the mistakes that worsen the mr are actually background windows that are mistaken for pedestrians. what is striking about this result is that this is not just the case for our r-cnn experiments on detection proposals but also for methods that are trained as a sliding window detector. section: small or big convnet? since we have analysed the cifarnet and alexnet separately, we compare their performance in this section side by side. table [reference] shows performance on the caltech test set for models that have been trained only on caltech1x and caltech10x. with less training data the cifarnet reaches mr, performing 2 percent points better than the alexnet. on caltech10x, we find the cifarnet performance improved to, while the alexnet improves to mr. the trend confirms the intuition that models with lower capacity saturate earlier when increasing the amount of training data than models with higher capacity. we can also conclude that the alexnet would profit from better regularisation when training on caltech1x. paragraph: timing the runtime during detection is about 3ms per proposal window. this is too slow for sliding window detection, but given a fast proposal method that has high recall with less than windows per image, scoring takes about 300ms per image. in our experience squareschnftrs runs in 2s per image, so proposing detections takes most of the detection time. section: takeaways previous work suggests that convnets for pedestrian detection underperform, despite having involved architectures (see for a survey of pedestrian detection). in this paper we showed that neither has to be the case. we present a wide range of experiments with two off-the-shelf models that reach competitive performance: the small cifarnet and the big alexnet. we present two networks that are trained on caltech only, which outperform all previously published convnets on caltech. the cifarnet shows better performance than related work, even when using the same training data as the respective methods (section [reference]). despite its size, the alexnet also improves over all convnets even when it is trained on caltech only (section [reference]). we push the state of the art for pedestrian detectors that have been trained on caltech1x and caltech10x. the cifarnet is the best single-frame pedestrian detector that has been trained on caltech1x (section [reference]), while alexnet is the best single-frame pedestrian detector trained on caltech10x (section [reference]). in figure [reference], we include include all published methods on caltech into the comparison, which also adds methods that use additional information at test time. the alexnet that has been pre-trained on imagenet reaches competitive results to the best published methods, but without using additional information at test time (section [reference]). we report first results for convnets on the kitti pedestrian detection benchmark. the alexnet improves over the proposal method alone, delivering encouraging results to further push kitti performance with convnets. section: conclusion we have presented extensive and systematic experimental evidence on the effectiveness of convnets for pedestrian detection. compared to previous convnets applied to pedestrian detection our approach avoids custom designs. when using the exact same proposals and training data as previous approaches our ''vanilla'' networks outperform previous results. we have shown that with pre-training on surrogate tasks, convnets can reach top performance on this task. interestingly we have shown that even without pre-training competitive results can be achieved, and this result is quite insensitive to the model size (from to parameters). our experiments also detail which parameters are most critical to achieve top performance. we report the best known results for convnets on both the challenging caltech and kitti datasets. our experience with convnets indicate that they show good promise on pedestrian detection, and that reported best practices do transfer to said task. that being said, on this more mature field we do not observe the large improvement seen on datasets such as pascal voc and imagenet. paragraph: acknowledgements we thank shanshan zhang for the help provided setting up some of the experiments. bibliography: references appendix: cifarnet training, the devil is in the details training neural networks is sensitive to a large number of parameters, including the learning rate, how the network weights are initialised, the type of regularisation applied to the weights, and the training batch size. it is difficult to isolate the effects of the individual parameters, and the best parameters will largely depend on the specific setup. we report here the parameters we used. we train cifarnet via stochastic gradient descent (sgd) with a learning rate of, a momentum of, and a batch size of. after epochs, we reduce the learning rate by a factor of and train for an additional epochs. reducing the learning rate even further did not improve the classification accuracy. the other learning rate policies we explored yielded inferior performance (e.g. gradually reducing the learning rate each training iteration). careful tuning of the learning rate while adjusting the batch size was critical. other than the softmax classification loss, the training loss includes a regularisation of the network weights. in the objective function, this regularization term has a weight of for all layers but the last one (softmax weights), which receives weight. this parameter is referred in caffe as ''weight decay''. the network weights are initialised by drawing values from a gaussian distribution with standard deviation, with the exception of the first layer, for which we set. appendix: grid search around cifarnet table [reference] shows the detection quality of different variants of cifarnet obtained by changing the number and size of the convolutional filters of each layer. see related section 4.3.1 of the main paper. since different training rounds have different random initial weights, we train four networks for each parameter set and average the results. we report both mean and standard deviation of the miss rate on our validation set. we observe that using either too small or too large filter sizes throughout the network hurts quality. the network width also seems to matter, a network too narrow or too wide can negatively impact classification accuracy. all and all the ''middle-section'' of the table shows only small fluctuations in miss-rate (specially when considering the variance). in addition to filter size and layer width, we also experimented with different types of pooling layers (max-pooling versus mean-pooling), see figure 2 of main paper. other than on the first layer, replacing mean-pooling with max-pooling hurts performance. the results of table indicate that there is no set of parameters close to cifarnet with a clear advantage over the default cifarnet parameters. when going too far from cifarnet parameters, classification accuracy plunges. appendix: grid search for alexnet table [reference] presents the swipe of parameters used to construct the ''best parameters'' entries in table 8 of the main paper. we vary the criterion to select negative samples and the svm regularization parameters. defaults are parameters are, and. overall we notice that neither parameter is very sensitive (percent points fluctuations). when c is far from optimal large degradation is observed (per cent points). as seen in table 8 of the main paper the gap between default and tuned parameters is rather small (percent points). [layer fc7] [layer fc6] [layer pool5] [layer conv4] appendix: datasets statistics in figure [reference] we plot the height distribution for pedestrians in caltech and kitti training sets. although the datasets are visually similar, the height distributions are somewhat dissimilar (for reference imagenet and pascal distributions are more look alike among each other). it was shown in that models trained in each dataset, do not transfer well across each other (compared to models trained on the smaller inria dataset). [caltech reasonable training set] [kitti training set] appendix: proposals statistics in figures [reference] and [reference] we show statistics of different detectors on the caltech test set, including the ones we use as proposals in our experiments. these figures complement table 9 of the main paper. our initial experiments indicated that it is important to keep a low number of average proposals per image in order to reduce the false positives rate (post re-scoring). this is in contrast to common practice when using class-agnostic proposal methods, where using more windows is considered better because they provide higher recall. we filter proposals via a threshold on the detection score. as can be seen in figure [reference] a recall higher than can be achieved with only proposals per image on average (for intersection-over-union threshold above, the evaluation criterion). the average number of proposals per image is quite low because most frames of the caltech test set do not contain any pedestrian. in figure [reference] we show the number of false positives at different overlap levels with the ground truth annotations. the bump around iou, most visible for spatialpooling and ldcf, is an artefact of the non-maximum suppression method used by each method. both these method obtain high quality detection, thus they must assign (very) low-scores to these false positives windows. to further improve quality the re-scoring method must do the same. when using a method for proposals one desires to have high recall with high overlap with the ground truth (figure [reference]), as well has having false positives with low overlap with the ground truth (figure [reference]). false positive proposals overlapping true pedestrians will have pieces of persons, which might confuse the re-scoring classifier. classifying fully centred persons versus random background is assumed to be easier task. in table 9 of the main paper we see that alexnet reaches top detection quality by improving over ldcf, squareschnftrs, and katamari.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "caltech",
                        701
                    ],
                    [
                        "caltech1x",
                        787
                    ],
                    [
                        "caltech10x",
                        801
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "alexnet",
                        10946
                    ],
                    [
                        "alexnets",
                        18320
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "miss-rate",
                        15428
                    ],
                    [
                        "mr",
                        23232
                    ],
                    [
                        "miss rate",
                        29548
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "pedestrian detection",
                        137
                    ],
                    [
                        "detection task",
                        2779
                    ],
                    [
                        "detection",
                        2883
                    ],
                    [
                        "pedestrian detectors",
                        13128
                    ],
                    [
                        "detections",
                        17115
                    ],
                    [
                        "pedestrian) detection task",
                        17477
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "1510cf4b8abea80b9f352325ca4c132887de21a0-20",
    "doctext": "many machine learning algorithms require the input to be represented as a fixed-length feature vector. when it comes to texts, one of the most common fixed-length features is bag-of-words. despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. for example,\" powerful,\"\" strong\" and\" paris\" are equally distant. in this paper, we propose paragraph vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. our algorithm represents each document by a dense vector which is trained to predict words in the document. its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. empirical results show that paragraph vectors outperform bag-of-words models as well as other techniques for text representations. finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. section: introduction text classification and clustering play an important role in many applications, e.g, document retrieval, web search, spam filtering. at the heart of these applications is machine learning algorithms such as logistic regression or k-means. these algorithms typically require the text input to be represented as a fixed-length vector. perhaps the most common fixed-length vector representation for texts is the bag-of-words or bag-of-n-grams due to its simplicity, efficiency and often surprising accuracy. however, the bag-of-words (bow) has many disadvantages. the word order is lost, and thus different sentences can have exactly the same representation, as long as the same words are used. even though bag-of-n-grams considers the word order in short context, it suffers from data sparsity and high dimensionality. bag-of-words and bag-of-n-grams have very little sense about the semantics of the words or more formally the distances between the words. this means that words\" powerful,\"\" strong\" and\" paris\" are equally distant despite the fact that semantically,\" powerful\" should be closer to\" strong\" than\" paris.\" in this paper, we propose paragraph vector, an unsupervised framework that learns continuous distributed vector representations for pieces of texts. the texts can be of variable-length, ranging from sentences to documents. the name paragraph vector is to emphasize the fact that the method can be applied to variable-length pieces of texts, anything from a phrase or sentence to a large document. in our model, the vector representation is trained to be useful for predicting words in a paragraph. more precisely, we concatenate the paragraph vector with several word vectors from a paragraph and predict the following word in the given context. both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropagation. while paragraph vectors are unique among paragraphs, the word vectors are shared. at prediction time, the paragraph vectors are inferred by fixing the word vectors and training the new paragraph vector until convergence. our technique is inspired by the recent work in learning vector representations of words using neural networks. in their formulation, each word is represented by a vector which is concatenated or averaged with other word vectors in a context, and the resulting vector is used to predict other words in the context. for example, the neural network language model proposed in uses the concatenation of several previous word vectors to form the input of a neural network, and tries to predict the next word. the outcome is that after the model is trained, the word vectors are mapped into a vector space such that semantically similar words have similar vector representations (e.g.,\" strong\" is close to\" powerful\"). following these successful techniques, researchers have tried to extend the models to go beyond word level to achieve phrase-level or sentence-level representations. for instance, a simple approach is using a weighted average of all the words in the document. a more sophisticated approach is combining the word vectors in an order given by a parse tree of a sentence, using matrix-vector operations. both approaches have weaknesses. the first approach, weighted averaging of word vectors, loses the word order in the same way as the standard bag-of-words models do. the second approach, using a parse tree to combine word vectors, has been shown to work for only sentences because it relies on parsing. paragraph vector is capable of constructing representations of input sequences of variable length. unlike some of the previous approaches, it is general and applicable to texts of any length: sentences, paragraphs, and documents. it does not require task-specific tuning of the word weighting function nor does it rely on the parse trees. further in the paper, we will present experiments on several benchmark datasets that demonstrate the advantages of paragraph vector. for example, on sentiment analysis task, we achieve new state-of-the-art results, better than complex methods, yielding a relative improvement of more than 16% in terms of error rate. on a text classification task, our method convincingly beats bag-of-words models, giving a relative improvement of about 30%. section: algorithms we start by discussing previous methods for learning word vectors. these methods are the inspiration for our paragraph vector methods. subsection: learning vector representation of words this section introduces the concept of distributed vector representation of words. a well known framework for learning the word vectors is shown in figure [reference]. the task is to predict a word given the other words in a context. in this framework, every word is mapped to a unique vector, represented by a column in a matrix. the column is indexed by position of the word in the vocabulary. the concatenation or sum of the vectors is then used as features for prediction of the next word in a sentence. more formally, given a sequence of training words, the objective of the word vector model is to maximize the average log probability the prediction task is typically done via a multiclass classifier, such as softmax. there, we have each of is un-normalized log-probability for each output word, computed as where are the softmax parameters. is constructed by a concatenation or average of word vectors extracted from. in practice, hierarchical softmax is preferred to softmax for fast training. in our work, the structure of the hierarical softmax is a binary huffman tree, where short codes are assigned to frequent words. this is a good speedup trick because common words are accessed quickly. this use of binary huffman code for the hierarchy is the same with. the neural network based word vectors are usually trained using stochastic gradient descent where the gradient is obtained via backpropagation. this type of models is commonly known as neural language models. a particular implementation of neural network based algorithm for training the word vectors is available at code.google.com/ p/ word2vec/. after the training converges, words with similar meaning are mapped to a similar position in the vector space. for example,\" powerful\" and\" strong\" are close to each other, whereas\" powerful\" and\" paris\" are more distant. the difference between word vectors also carry meaning. for example, the word vectors can be used to answer analogy questions using simple vector algebra:\" king\" -\" man\"+\" woman\"=\" queen\". it is also possible to learn a linear matrix to translate words and phrases between languages. these properties make word vectors attractive for many natural language processing tasks such as language modeling, natural language understanding, statistical machine translation, image understanding and relational extraction. subsection: paragraph vector: a distributed memory model our approach for learning paragraph vectors is inspired by the methods for learning the word vectors. the inspiration is that the word vectors are asked to contribute to a prediction task about the next word in the sentence. so despite the fact that the word vectors are initialized randomly, they can eventually capture semantics as an indirect result of the prediction task. we will use this idea in our paragraph vectors in a similar manner. the paragraph vectors are also asked to contribute to the prediction task of the next word given many contexts sampled from the paragraph. in our paragraph vector framework (see figure [reference]), every paragraph is mapped to a unique vector, represented by a column in matrix and every word is also mapped to a unique vector, represented by a column in matrix. the paragraph vector and word vectors are averaged or concatenated to predict the next word in a context. in the experiments, we use concatenation as the method to combine the vectors. more formally, the only change in this model compared to the word vector framework is in equation [reference], where is constructed from and. the paragraph token can be thought of as another word. it acts as a memory that remembers what is missing from the current context- or the topic of the paragraph. for this reason, we often call this model the distributed memory model of paragraph vectors (pv-dm). the contexts are fixed-length and sampled from a sliding window over the paragraph. the paragraph vector is shared across all contexts generated from the same paragraph but not across paragraphs. the word vector matrix, however, is shared across paragraphs. i.e., the vector for\" powerful\" is the same for all paragraphs. the paragraph vectors and word vectors are trained using stochastic gradient descent and the gradient is obtained via backpropagation. at every step of stochastic gradient descent, one can sample a fixed-length context from a random paragraph, compute the error gradient from the network in figure [reference] and use the gradient to update the parameters in our model. at prediction time, one needs to perform an inference step to compute the paragraph vector for a new paragraph. this is also obtained by gradient descent. in this step, the parameters for the rest of the model, the word vectors and the softmax weights, are fixed. suppose that there are paragraphs in the corpus, words in the vocabulary, and we want to learn paragraph vectors such that each paragraph is mapped to dimensions and each word is mapped to dimensions, then the model has the total of parameters (excluding the softmax parameters). even though the number of parameters can be large when is large, the updates during training are typically sparse and thus efficient. after being trained, the paragraph vectors can be used as features for the paragraph (e.g., in lieu of or in addition to bag-of-words). we can feed these features directly to conventional machine learning techniques such as logistic regression, support vector machines or k-means. in summary, the algorithm itself has two key stages: 1) training to get word vectors, softmax weights and paragraph vectors on already seen paragraphs; and 2)\" the inference stage\" to get paragraph vectors for new paragraphs (never seen before) by adding more columns in and gradient descending on while holding fixed. we use to make a prediction about some particular labels using a standard classifier, e.g., logistic regression. paragraph: advantages of paragraph vectors: an important advantage of paragraph vectors is that they are learned from unlabeled data and thus can work well for tasks that do not have enough labeled data. paragraph vectors also address some of the key weaknesses of bag-of-words models. first, they inherit an important property of the word vectors: the semantics of the words. in this space,\" powerful\" is closer to\" strong\" than to\" paris.\" the second advantage of the paragraph vectors is that they take into consideration the word order, at least in a small context, in the same way that an n-gram model with a large n would do. this is important, because the n-gram model preserves a lot of information of the paragraph, including the word order. that said, our model is perhaps better than a bag-of-n-grams model because a bag of n-grams model would create a very high-dimensional representation that tends to generalize poorly. subsection: paragraph vector without word ordering: distributed bag of words the above method considers the concatenation of the paragraph vector with the word vectors to predict the next word in a text window. another way is to ignore the context words in the input, but force the model to predict words randomly sampled from the paragraph in the output. in reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window, then sample a random word from the text window and form a classification task given the paragraph vector. this technique is shown in figure [reference]. we name this version the distributed bag of words version of paragraph vector (pv-dbow), as opposed to distributed memory version of paragraph vector (pv-dm) in previous section. in addition to being conceptually simple, this model requires to store less data. we only need to store the softmax weights as opposed to both softmax weights and word vectors in the previous model. this model is also similar to the skip-gram model in word vectors. in our experiments, each paragraph vector is a combination of two vectors: one learned by the standard paragraph vector with distributed memory (pv-dm) and one learned by the paragraph vector with distributed bag of words (pv-dbow). pv-dm alone usually works well for most tasks (with state-of-art performances), but its combination with pv-dbow is usually more consistent across many tasks that we try and therefore strongly recommended. section: experiments we perform experiments to better understand the behavior of the paragraph vectors. to achieve this, we benchmark paragraph vector on two text understanding problems that require fixed-length vector representations of paragraphs: sentiment analysis and information retrieval. for sentiment analysis, we use two datasets: stanford sentiment treebank dataset and imdb dataset. documents in these datasets differ significantly in lengths: every example in socher et al.'s dataset is a single sentence while every example in maas et al.'s dataset consists of several sentences. we also test our method on an information retrieval task, where the goal is to decide if a document should be retrieved given a query. subsection: sentiment analysis with the stanford sentiment treebank dataset paragraph: dataset: this dataset was first proposed by and subsequently extended by as a benchmark for sentiment analysis. it has 11855 sentences taken from the movie review site rotten tomatoes. the dataset consists of three sets: 8544 sentences for training, 2210 sentences for test and 1101 sentences for validation (or development). every sentence in the dataset has a label which goes from very negative to very positive in the scale from 0.0 to 1.0. the labels are generated by human annotators using amazon mechanical turk. the dataset comes with detailed labels for sentences, and subphrases in the same scale. to achieve this, socher et al. used the stanford parser to parse each sentence to subphrases. the subphrases were then labeled by human annotators in the same way as the sentences were labeled. in total, there are 239, 232 labeled phrases in the dataset. the dataset can be downloaded at: http:// nlp. stanford.edu/ sentiment/ paragraph: tasks and baselines: in, the authors propose two ways of benchmarking. first, one could consider a 5-way fine-grained classification task where the labels are {very negative, negative, neutral, positive, very positive} or a 2-way coarse-grained classification task where the labels are {negative, positive}. the other axis of variation is in terms of whether we should label the entire sentence or all phrases in the sentence. in this work we only consider labeling the full sentences. socher et al. apply several methods to this dataset and find that their recursive neural tensor network works much better than bag-of-words model. it can be argued that this is because movie reviews are often short and compositionality plays an important role in deciding whether the review is positive or negative, as well as similarity between words does given the rather tiny size of the training set. paragraph: experimental protocols: we follow the experimental protocols as described in. to make use of the available labeled data, in our model, each subphrase is treated as an independent sentence and we learn the representations for all the subphrases in the training set. after learning the vector representations for training sentences and their subphrases, we feed them to a logistic regression to learn a predictor of the movie rating. at test time, we freeze the vector representation for each word, and learn the representations for the sentences using gradient descent. once the vector representations for the test sentences are learned, we feed them through the logistic regression to predict the movie rating. in our experiments, we cross validate the window size using the validation set, and the optimal window size is 8. the vector presented to the classifier is a concatenation of two vectors, one from pv-dbow and one from pv-dm. in pv-dbow, the learned vector representations have 400 dimensions. in pv-dm, the learned vector representations have 400 dimensions for both words and paragraphs. to predict the 8-th word, we concatenate the paragraph vectors and 7 word vectors. special characters such as,.!? are treated as a normal word. if the paragraph has less than 9 words, we pre-pad with a special null word symbol. paragraph: results: we report the error rates of different methods in table [reference]. the first highlight for this table is that bag-of-words or bag-of-n-grams models (nb, svm, binb) perform poorly. simply averaging the word vectors (in a bag-of-words fashion) does not improve the results. this is because bag-of-words models do not consider how each sentence is composed (e.g., word ordering) and therefore fail to recognize many sophisticated linguistic phenomena, for instance sarcasm. the results also show that more advanced methods (such as recursive neural network), which require parsing and take into account the compositionality, perform much better. our method performs better than all these baselines, e.g., recursive networks, despite the fact that it does not require parsing. on the coarse-grained classification task, our method has an absolute improvement of 2.4% in terms of error rates. this translates to 16% relative improvement. subsection: beyond one sentence: sentiment analysis with imdb dataset some of the previous techniques only work on sentences, but not paragraphs/ documents with several sentences. for instance, recursive neural tensor network is based on the parsing over each sentence and it is unclear how to combine the representations over many sentences. such techniques therefore are restricted to work on sentences but not paragraphs or documents. our method does not require parsing, thus it can produce a representation for a long document consisting of many sentences. this advantage makes our method more general than some of the other approaches. the following experiment on imdb dataset demonstrates this advantage. paragraph: dataset: the imdb dataset was first proposed by maas et al. as a benchmark for sentiment analysis. the dataset consists of 100, 000 movie reviews taken from imdb. one key aspect of this dataset is that each movie review has several sentences. the 100, 000 movie reviews are divided into three datasets: 25, 000 labeled training instances, 25, 000 labeled test instances and 50, 000 unlabeled training instances. there are two types of labels: positive and negative. these labels are balanced in both the training and the test set. the dataset can be downloaded at http:// ai. stanford.edu/ amaas/ data/ sentiment/ index.html paragraph: experimental protocols: we learn the word vectors and paragraph vectors using 75, 000 training documents (25, 000 labeled and 50, 000 unlabeled instances). the paragraph vectors for the 25, 000 labeled instances are then fed through a neural network with one hidden layer with 50 units and a logistic classifier to learn to predict the sentiment. at test time, given a test sentence, we again freeze the rest of the network and learn the paragraph vectors for the test reviews by gradient descent. once the vectors are learned, we feed them through the neural network to predict the sentiment of the reviews. the hyperparameters of our paragraph vector model are selected in the same manner as in the previous task. in particular, we cross validate the window size, and the optimal window size is 10 words. the vector presented to the classifier is a concatenation of two vectors, one from pv-dbow and one from pv-dm. in pv-dbow, the learned vector representations have 400 dimensions. in pv-dm, the learned vector representations have 400 dimensions for both words and documents. to predict the 10-th word, we concatenate the paragraph vectors and word vectors. special characters such as,.!? are treated as a normal word. if the document has less than 9 words, we pre-pad with a special null word symbol. paragraph: results: the results of paragraph vector and other baselines are reported in table [reference]. as can be seen from the table, for long documents, bag-of-words models perform quite well and it is difficult to improve upon them using word vectors. the most significant improvement happened in 2012 in the work of where they combine a restricted boltzmann machines model with bag-of-words. the combination of two models yields an improvement approximately 1.5% in terms of error rates. another significant improvement comes from the work of. among many variations they tried, nbsvm on bigram features works the best and yields a considerable improvement of 2% in terms of the error rate. the method described in this paper is the only approach that goes significantly beyond the barrier of 10% error rate. it achieves 7.42% which is another 1.3% absolute improvement (or 15% relative improvement) over the best previous result of. subsection: information retrieval with paragraph vectors we turn our attention to an information retrieval task which requires fixed-length representations of paragraphs. here, we have a dataset of paragraphs in the first 10 results returned by a search engine given each of 1, 000, 000 most popular queries. each of these paragraphs is also known as a\" snippet\" which summarizes the content of a web page and how a web page matches the query. from such collection, we derive a new dataset to test vector representations of paragraphs. for each query, we create a triplet of paragraphs: the two paragraphs are results of the same query, whereas the third paragraph is a randomly sampled paragraph from the rest of the collection (returned as the result of a different query). our goal is to identify which of the three paragraphs are results of the same query. to achieve this, we will use paragraph vectors and compute the distances the paragraphs. a better representation is one that achieves a small distance for pairs of paragraphs of the same query and a larg distance for pairs of paragraphs of different queries. here is a sample of three paragraphs, where the first paragraph should be closer to the second paragraph than the third paragraph: paragraph 1: calls from (000) 000-0000. 3913 calls reported from this number. according to 4 reports the identity of this caller is american airlines. paragraph 2: do you want to find out who called you from+ 1 000-000-0000,+ 1 0000000000 or (000) 000-0000? see reports and share information you have about this caller paragraph 3: allina health clinic patients for your convenience, you can pay your allina health clinic bill online. pay your clinic bill now, question and answers\u2026 the triplets are split into three sets: 80% for training, 10% for validation, and 10% for testing. any method that requires learning will be trained on the training set, while its hyperparameters will be selected on the validation set. we benchmark four methods to compute features for paragraphs: bag-of-words, bag-of-bigrams, averaging word vectors and paragraph vector. to improve bag-of-bigrams, we also learn a weighting matrix such that the distance between the first two paragraphs is minimized whereas the distance between the first and the third paragraph is maximized (the weighting factor between the two losses is a hyperparameter). we record the number of times when each method produces smaller distance for the first two paragraphs than the first and the third paragraph. an error is made if a method does not produce that desirable distance metric on a triplet of paragraphs. the results of paragraph vector and other baselines are reported in table [reference]. in this task, we find that tf-idf weighting performs better than raw counts, and therefore we only report the results of methods with tf-idf weighting. the results show that paragraph vector works well and gives a 32% relative improvement in terms of error rate. the fact that the paragraph vector method significantly outperforms bag of words and bigrams suggests that our proposed method is useful for capturing the semantics of the input text. subsection: some further observations we perform further experiments to understand various aspects of the models. here's some observations pv-dm is consistently better than pv-dbow. pv-dm alone can achieve results close to many results in this paper (see table [reference]). for example, in imdb, pv-dm only achieves 7.63%. the combination of pv-dm and pv-dbow often work consistently better (7.42% in imdb) and therefore recommended. using concatenation in pv-dm is often better than sum. in imdb, pv-dm with sum can only achieve 8.06%. perhaps, this is because the model loses the ordering information. it's better to cross validate the window size. a good guess of window size in many applications is between 5 and 12. in imdb, varying the window sizes between 5 and 12 causes the error rate to fluctuate 0.7%. paragraph vector can be expensive, but it can be done in parallel at test time. on average, our implementation takes 30 minutes to compute the paragraph vectors of the imdb test set, using a 16 core machine (25, 000 documents, each document on average has 230 words). section: related work distributed representations for words were first proposed in and have become a successful paradigm, especially for statistical language modeling. word vectors have been used in nlp applications such as word representation, named entity recognition, word sense disambiguation, parsing, tagging and machine translation. representing phrases is a recent trend and received much attention. in this direction, autoencoder-style models have also been used to model paragraphs. distributed representations of phrases and sentences are also the focus of socher et al.. their methods typically require parsing and is shown to work for sentence-level representations. and it is not obvious how to extend their methods beyond single sentences. their methods are also supervised and thus require more labeled data to work well. paragraph vector, in contrast, is mostly unsupervised and thus can work well with less labeled data. our approach of computing the paragraph vectors via gradient descent bears resemblance to a successful paradigm in computer vision known as fisher kernels. the basic construction of fisher kernels is the gradient vector over an unsupervised generative model. section: discussion we described paragraph vector, an unsupervised learning algorithm that learns vector representations for variable-length pieces of texts such as sentences and documents. the vector representations are learned to predict the surrounding words in contexts sampled from the paragraph. our experiments on several text classification tasks such as stanford treebank and imdb sentiment analysis datasets show that the method is competitive with state-of-the-art methods. the good performance demonstrates the merits of paragraph vector in capturing the semantics of paragraphs. in fact, paragraph vectors have the potential to overcome many weaknesses of bag-of-words models. although the focus of this work is to represent texts, our method can be applied to learn representations for sequential data. in non-text domains where parsing is not available, we expect paragraph vector to be a strong alternative to bag-of-words and bag-of-n-grams models. bibliography: references",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "paragraph vector",
                        438
                    ],
                    [
                        "distributed memory model of paragraph vectors",
                        9347
                    ],
                    [
                        "pv-dm",
                        9395
                    ],
                    [
                        "distributed bag of words version of paragraph vector",
                        13064
                    ],
                    [
                        "pv-dbow",
                        13119
                    ],
                    [
                        "paragraph vector model",
                        20865
                    ],
                    [
                        "vector representations of paragraphs",
                        22974
                    ],
                    [
                        "paragraph vector method",
                        25470
                    ],
                    [
                        "gradient vector",
                        27861
                    ]
                ]
            ],
            "Metric": [],
            "Task": []
        },
        {
            "Material": [],
            "Method": [
                [
                    [
                        "paragraph vectors",
                        853
                    ]
                ]
            ],
            "Metric": [],
            "Task": []
        }
    ]
}
{
    "docid": "1594d954abc650bce2db445c52a76e49655efb0c-21",
    "doctext": "document: recurrent neural network grammars we introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. we explain efficient inference procedures that allow application to both parsing and language modeling. experiments show that they provide better parsing in english than any single previously published supervised generative model and better language modeling than state-of-the-art sequential rnns in english and chinese. due to an implentation bug in the rnng's recursive composition function, the results reported in dyer et al. (2016) did not correspond to the model as it was presented. this corrigendum describes the buggy implementation and reports results with a corrected implementation. after correction, on the ptb\u00a7 23 and ctb 5.1 test sets, respectively, the generative model achieves language modeling perplexities of 105.2 and 148.5, and phrase-structure parsing f1 of 93.3 and 86.9, a new state of the art in phrase-structure parsing for both languages.* subsubsection: this is modified version of a paper originally published at naacl 2016 that contains a corrigendum at the end, with improved results after fixing an implementation bug in the rnng composition function. section: introduction sequential recurrent neural networks (rnns) are remarkably effective models of natural language. in the last few years, language model results that substantially improve over long-established state-of-the-art baselines have been obtained using rnns as well as in various conditional language modeling tasks such as machine translation, image caption generation, and dialogue generation. despite these impressive results, sequential models are a priori inappropriate models of natural language, since relationships among words are largely organized in terms of latent nested structures rather than sequential surface order. in this paper, we introduce recurrent neural network grammars (rnngs;\u00a7 [reference]), a new generative probabilistic model of sentences that explicitly models nested, hierarchical relationships among words and phrases. rnngs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using rnns that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. the foundation of this work is a top-down variant of transition-based parsing (\u00a7 [reference]). we give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. while several transition-based neural models of syntactic generation exist, these have relied on structure building operations based on parsing actions in shift-reduce and left-corner parsers which operate in a largely bottom-up fashion. while this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation. rnngs maintain the algorithmic convenience of transition-based parsing but incorporate top-down (i.e., root-to-terminal) syntactic information (\u00a7 [reference]). the top-down transition set that rnngs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. similar to previously published discriminative bottom-up transition-based parsers, greedy prediction with our model yields a linear-time deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. the discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve a second practical challenge with rnngs: approximating the marginal likelihood and map tree of a sentence under the generative model. we present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model (\u00a7 [reference]). experiments show that rnngs are effective for both language modeling and parsing (\u00a7 [reference]). our generative model obtains (i) the best-known parsing results using a single supervised generative model and (ii) better perplexities in language modeling than state-of-the-art sequential lstm language models. surprisingly\u2014 although in line with previous parsing results showing the effectiveness of generative models\u2014parsing with the generative model obtains significantly better results than parsing with the discriminative model. section: rnn grammars formally, an rnng is a triple consisting of a finite set of nonterminal symbols (), a finite set of terminal symbols () such that, and a collection of neural network parameters. it does not explicitly define rules since these are implicitly characterized by. the algorithm that the grammar uses to generate trees and strings in the language is characterized in terms of a transition-based algorithm, which is outlined in the next section. in the section after that, the semantics of the parameters that are used to turn this into a stochastic algorithm that generates pairs of trees and strings are discussed. section: top-down parsing and generation rnngs are based on a top-down generation algorithm that relies on a stack data structure of partially completed syntactic constituents. to emphasize the similarity of our algorithm to more familiar bottom-up shift-reduce recognition algorithms, we first present the parsing (rather than generation) version of our algorithm (\u00a7 [reference]) and then present modifications to turn it into a generator (\u00a7 [reference]). subsection: parser transitions the parsing algorithm transforms a sequence of words into a parse tree using two data structures (a stack and an input buffer). as with the bottom-up algorithm of, our algorithm begins with the stack () empty and the complete sequence of words in the input buffer (). the buffer contains unprocessed terminal symbols, and the stack contains terminal symbols,\" open\" nonterminal symbols, and completed constituents. at each timestep, one of the following three classes of operations (fig. [reference]) is selected by a classifier, based on the current contents on the stack and buffer: introduces an\" open nonterminal\" x onto the top of the stack. open nonterminals are written as a nonterminal symbol preceded by an open parenthesis, e.g.,\" (vp\", and they represent a nonterminal whose child nodes have not yet been fully constructed. open nonterminals are\" closed\" to form complete constituents by subsequent reduce operations. shift removes the terminal symbol from the front of the input buffer, and pushes it onto the top of the stack. reduce repeatedly pops completed subtrees or terminal symbols from the stack until an open nonterminal is encountered, and then this open nt is popped and used as the label of a new constituent that has the popped subtrees as its children. this new completed constituent is pushed onto the stack as a single composite item. a single reduce operation can thus create constituents with an unbounded number of children. the parsing algorithm terminates when there is a single completed constituent on the stack and the buffer is empty. fig. [reference] shows an example parse using our transition set. note that in this paper we do not model preterminal symbols (i.e., part-of-speech tags) and our examples therefore do not include them. our transition set is closely related to the operations used in earley's algorithm which likewise introduces nonterminals symbols with its predict operation and later complete s them after consuming terminal symbols one at a time using scan. it is likewise closely related to the\" linearized\" parse trees proposed by and to the top-down, left-to-right decompositions of trees used in previous generative parsing and language modeling work. a further connection is to parsing which uses an unbounded lookahead (compactly represented by a dfa) to distinguish between parse alternatives in a top-down parser; however, our parser uses an rnn encoding of the lookahead rather than a dfa. input: the hungry cat meows. paragraph: constraints on parser transitions. to guarantee that only well-formed phrase-structure trees are produced by the parser, we impose the following constraints on the transitions that can be applied at each step which are a function of the parser state where is the number of open nonterminals on the stack: the operation can only be applied if is not empty and. the shift operation can only be applied if is not empty and. the reduce operation can only be applied if the top of the stack is not an open nonterminal symbol. the reduce operation can only be applied if or if the buffer is empty. to designate the set of valid parser transitions, we write. subsection: generator transitions the parsing algorithm that maps from sequences of words to parse trees can be adapted with minor changes to produce an algorithm that stochastically generates trees and terminal symbols. two changes are required: (i) there is no input buffer of unprocessed words, rather there is an output buffer (), and (ii) instead of a shift operation there are operations which generate terminal symbol and add it to the top of the stack and the output buffer. at each timestep an action is stochastically selected according to a conditional distribution that depends on the current contents of and. the algorithm terminates when a single completed constituent remains on the stack. fig. [reference] shows an example generation sequence. paragraph: constraints on generator transitions. the generation algorithm also requires slightly modified constraints. these are: the operation can only be applied if. the reduce operation can only be applied if the top of the stack is not an open nonterminal symbol and. to designate the set of valid generator transitions, we write. this transition set generates trees using nearly the same structure building actions and stack configurations as the\" top-down pda\" construction proposed by, albeit without the restriction that the trees be in chomsky normal form. subsection: transition sequences from trees any parse tree can be converted to a sequence of transitions via a depth-first, left-to-right traversal of a parse tree. since there is a unique depth-first, left-ro-right traversal of a tree, there is exactly one transition sequence of each tree. for a tree and a sequence of symbols, we write to indicate the corresponding sequence of generation transitions, and to indicate the parser transitions. subsection: runtime analysis a detailed analysis of the algorithmic properties of our top-down parser is beyond the scope of this paper; however, we briefly state several facts. assuming the availability of constant time push and pop operations, the runtime is linear in the number of the nodes in the parse tree that is generated by the parser/ generator (intuitively, this is true since although an individual reduce operation may require applying a number of pops that is linear in the number of input symbols, the total number of pop operations across an entire parse/ generation run will also be linear). since there is no way to bound the number of output nodes in a parse tree as a function of the number of input words, stating the runtime complexity of the parsing algorithm as a function of the input size requires further assumptions. assuming our fixed constraint on maximum depth, it is linear. subsection: comparison to other models our generation algorithm algorithm differs from previous stack-based parsing/ generation algorithms in two ways. first, it constructs rooted tree structures top down (rather than bottom up), and second, the transition operators are capable of directly generating arbitrary tree structures rather than, e.g., assuming binarized trees, as is the case in much prior work that has used transition-based algorithms to produce phrase-structure trees. section: generative model rnngs use the generator transition set just presented to define a joint distribution on syntax trees () and words (). this distribution is defined as a sequence model over generator transitions that is parameterized using a continuous space embedding of the algorithm state at each time step (); i.e., and where action-specific embeddings and bias vector are parameters in. the representation of the algorithm state at time,, is computed by combining the representation of the generator's three data structures: the output buffer (), represented by an embedding, the stack (), represented by an embedding, and the history of actions () taken by the generator, represented by an embedding, where and are parameters. refer to figure [reference] for an illustration of the architecture. the output buffer, stack, and history are sequences that grow unboundedly, and to obtain representations of them we use recurrent neural networks to\" encode\" their contents. since the output buffer and history of actions are only appended to and only contain symbols from a finite alphabet, it is straightforward to apply a standard rnn encoding architecture. the stack () is more complicated for two reasons. first, the elements of the stack are more complicated objects than symbols from a discrete alphabet: open nonterminals, terminals, and full trees, are all present on the stack. second, it is manipulated using both push and pop operations. to efficiently obtain representations of under push and pop operations, we use stack lstms. to represent complex parse trees, we define a new syntactic composition function that recursively defines representations of trees. subsection: syntactic composition function when a reduce operation is executed, the parser pops a sequence of completed subtrees and/ or tokens (together with their vector embeddings) from the stack and makes them children of the most recent open nonterminal on the stack,\" completing\" the constituent. to compute an embedding of this new subtree, we use a composition function based on bidirectional lstms, which is illustrated in fig. [reference]. the first vector read by the lstm in both the forward and reverse directions is an embedding of the label on the constituent being constructed (in the figure, np). this is followed by the embeddings of the child subtrees (or tokens) in forward or reverse order. intuitively, this order serves to\" notify\" each lstm what sort of head it should be looking for as it processes the child node embeddings. the final state of the forward and reverse lstms are concatenated, passed through an affine transformation and a nonlinearity to become the subtree embedding. because each of the child node embeddings (,, in fig. [reference]) is computed similarly (if it corresponds to an internal node), this composition function is a kind of recursive neural network. subsection: word generation to reduce the size of, word generation is broken into two parts. first, the decision to generate is made (by predicting gen as an action), and then choosing the word, conditional on the current parser state. to further reduce the computational complexity of modeling the generation of a word, we use a class-factored softmax. by using classes for a vocabulary of size, this prediction step runs in time rather than the of the full-vocabulary softmax. to obtain clusters, we use the greedy agglomerative clustering algorithm of. subsection: training the parameters in the model are learned to maximize the likelihood of a corpus of trees. subsection: discriminative parsing model a discriminative parsing model can be obtained by replacing the embedding of at each time step with an embedding of the input buffer. to train this model, the conditional likelihood of each sequence of actions given the input string is maximized. section: inference via importance sampling our generative model defines a joint distribution on trees () and sequences of words (). to evaluate this as a language model, it is necessary to compute the marginal probability. and, to evaluate the model as a parser, we need to be able to find the map parse tree, i.e., the tree that maximizes. however, because of the unbounded dependencies across the sequence of parsing actions in our model, exactly solving either of these inference problems is intractable. to obtain estimates of these, we use a variant of importance sampling. our importance sampling algorithm uses a conditional proposal distribution with the following properties: (i); (ii) samples can be obtained efficiently; and (iii) the conditional probabilities of these samples are known. while many such distributions are available, the discriminatively trained variant of our parser (\u00a7 [reference]) fulfills these requirements: sequences of actions can be sampled using a simple ancestral sampling approach, and, since parse trees and action sequences exist in a one-to-one relationship, the product of the action probabilities is the conditional probability of the parse tree under. we therefore use our discriminative parser as our proposal distribution. importance sampling uses importance weights, which we define as, to compute this estimate. under this definition, we can derive the estimator as follows: we now replace this expectation with its monte carlo estimate as follows, using samples from: to obtain an estimate of the map tree, we choose the sampled tree with the highest probability under the joint model. section: experiments we present results of our two models both on parsing (discriminative and generative) and as a language model (generative only) in english and chinese. paragraph: data. for english,\u00a7 2-21 of the penn treebank are used as training corpus for both, with\u00a7 24 held out as validation, and\u00a7 23 used for evaluation. singleton words in the training corpus with unknown word classes using the the berkeley parser's mapping rules. orthographic case distinctions are preserved, and numbers (beyond singletons) are not normalized. for chinese, we use the penn chinese treebank version 5.1 (ctb). for the chinese experiments, we use a single unknown word class. corpus statistics are given in table [reference]. paragraph: model and training parameters. for the discriminative model, we used hidden dimensions of 128 and 2-layer lstms (larger numbers of dimensions reduced validation set performance). for the generative model, we used 256 dimensions and 2-layer lstms. for both models, we tuned the dropout rate to maximize validation set likelihood, obtaining optimal rates of 0.2 (discriminative) and 0.3 (generative). for the sequential lstm baseline for the language model, we also found an optimal dropout rate of 0.3. for training we used stochastic gradient descent with a learning rate of 0.1. all parameters were initialized according to recommendations given by. paragraph: english parsing results. table [reference] (last two rows) gives the performance of our parser on section 23, as well as the performance of several representative models. for the discriminative model, we used a greedy decoding rule as opposed to beam search in some shift-reduce baselines. for the generative model, we obtained 100 independent samples from a flattened distribution of the discriminative parser (by exponentiating each probability by and renormalizing) and reranked them according to the generative model. paragraph: chinese parsing results. chinese parsing results were obtained with the same methodology as in english and show the same pattern (table [reference]). paragraph: language model results. we report held-out per-word perplexities of three language models, both sequential and syntactic. log probabilities are normalized by the number of words (excluding the stop symbol), inverted, and exponentiated to yield the perplexity. results are summarized in table [reference]. section: discussion it is clear from our experiments that the proposed generative model is quite effective both as a parser and as a language model. this is the result of (i) relaxing conventional independence assumptions (e.g., context-freeness) and (ii) inferring continuous representations of symbols alongside non-linear models of their syntactic relationships. the most significant question that remains is why the discriminative model\u2014 which has more information available to it than the generative model\u2014 performs worse than the generative model. this pattern has been observed before in neural parsing by, who hypothesized that larger, unstructured conditioning contexts are harder to learn from, and provide opportunities to overfit. our discriminative model conditions on the entire history, stack, and buffer, while our generative model only accesses the history and stack. the fully discriminative model of was able to obtain results similar to those of our generative model (albeit using much larger training sets obtained through semisupervision) but similar results to those of our discriminative parser using the same data. in light of their results, we believe henderson's hypothesis is correct, and that generative models should be considered as a more statistically efficient method for learning neural networks from small data. section: related work our language model combines work from two modeling traditions: (i) recurrent neural network language models and (ii) syntactic language modeling. recurrent neural network language models use rnns to compute representations of an unbounded history of words in a left-to-right language model. syntactic language models jointly generate a syntactic structure and a sequence of words. there is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space. the neural-network- based model of is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize generative parsing based on a left-corner model. dependency-only language models have also been explored. modeling generation top-down as a rooted branching process that recursively rewrites nonterminals has been explored by and. of particular note is the work of, which uses random forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take. the neural networks we use to model sentences are structured according to the syntax of the sentence being generated. syntactically structured neural architectures have been explored in a number of applications, including discriminative parsing, sentiment analysis, and sentence representation. however, these models have been, without exception, discriminative; this is the first work to use syntactically structured neural models to generate language. earlier work has demonstrated that sequential rnns have the capacity to recognize context-free (and beyond) languages. in contrast, our work may be understood as a way of incorporating a context-free inductive bias into the model structure. section: outlook rnngs can be combined with a particle filter inference scheme (rather than the importance sampling method based on a discriminative parser,\u00a7 [reference]) to produce a left-to-right marginalization algorithm that runs in expected linear time. thus, they could be used in applications that require language models. a second possibility is to replace the sequential generation architectures found in many neural network transduction problems that produce sentences conditioned on some input. previous work in machine translation has showed that conditional syntactic models can function quite well without the computationally expensive marginalization process at decoding time. a third consideration regarding how rnngs, human sentence processing takes place in a left-to-right, incremental order. while an rnng is not a processing model (it is a grammar), the fact that it is left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of. finally, although we considered only the supervised learning scenario, rnngs are joint models that could be trained without trees, for example, using expectation maximization. section: conclusion we introduced recurrent neural network grammars, a probabilistic model of phrase-structure trees that can be trained generatively and used as a language model or a parser, and a corresponding discriminative model that can be used as a parser. apart from out-of-vocabulary preprocessing, the approach requires no feature design or transformations to treebank data. the generative model outperforms every previously published parser built on a single supervised generative model in english, and a bit behind the best-reported generative model in chinese. as language models, rnngs outperform the best single-sentence language models. section: acknowledgments we thank brendan o'connor, swabha swayamdipta, and brian roark for feedback on drafts of this paper, and jan buys, phil blunsom, and yue zhang for help with data preparation. this work was sponsored in part by the defense advanced research projects agency (darpa) information innovation office (i2o) under the low resource languages for emergent incidents (lorelei) program issued by darpa/ i2o under contract no. hr0011-15-c-0114; it was also supported in part by contract no. w911nf-15-1-0543 with the darpa and the army research office (aro). approved for public release, distribution unlimited. the views expressed are those of the authors and do not reflect the official policy or position of the department of defense or the u.s. government. miguel ballesteros was supported by the european commission under the contract numbers fp7-ict-610411 (project multisensor) and h2020-ria-645012 (project kristina). bibliography: references chris dyer\u2062\u2660\u2663 adhiguna kuncoro\u2660 miguel ballesteros\u2662 noah a. smith\u2661 school of computer science, carnegie mellon university, pittsburgh, pa, usa nlp group, pompeu fabra university, barcelona, spain google deepmind, london, uk computer science& engineering, university of washington, seattle, wa, usa {cdyer,, nasmith@cs.washington.edu section: rnng composition function and implementation error the composition function reduces a completed constituent into a single vector representation using a bidirectional lstm (figure [reference]) over embeddings of the constituent's children as well as an embedding of the resulting nonterminal symbol type. the implementation error (figure [reference]) composed the constituent (np the hungry cat) by reading the sequence\" np the hungry np\", that is, it discarded the rightmost child of every constituent and replaced it with a second copy of the constituent's nonterminal symbol. this error occurs for every constituent and means crucial information is not properly propagated upwards in the tree. section: results after correction the implementation error affected both the generative and discriminative rnngs. we summarize corrected english phrase-structure ptb\u00a7 23 parsing result in table [reference], chinese (ctb 5.1\u00a7 271-300) in table [reference] (achieving the the best reported result on both datasets), and english and chinese language modeling perplexities in table [reference]. the considerable improvement in parsing accuracy indicates that properly composing the constituent and propagating information upwards is crucial. despite slightly higher language modeling perplexity on ptb\u00a7 23, the fixed rnng still outperforms a highly optimized sequential lstm baseline.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "ptb",
                        778
                    ],
                    [
                        "penn treebank",
                        17916
                    ],
                    [
                        "treebank data",
                        24883
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "recurrent neural network grammars",
                        10
                    ],
                    [
                        "rnngs",
                        1952
                    ],
                    [
                        "rnn grammars",
                        4799
                    ],
                    [
                        "rnng",
                        4825
                    ],
                    [
                        "grammar",
                        5094
                    ],
                    [
                        "explicit grammars",
                        24283
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "f1",
                        933
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "parsing",
                        231
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "15c10b24ef645d83ff4059affd86945c33e00328-22",
    "doctext": "document: snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces this paper presents the machine learning architecture of the snips voice platform, a software solution to perform spoken language understanding on microprocessors typical of iot devices. the embedded inference is fast and accurate while enforcing privacy by design, as no personal user data is ever collected. focusing on automatic speech recognition and natural language understanding, we detail our approach to training high-performance machine learning models that are small enough to run in real-time on small devices. additionally, we describe a data generation procedure that provides sufficient, high-quality training data without compromising user privacy. section: introduction over the last years, thanks in part to steady improvements brought by deep learning approaches to speech recognition, voice interfaces have greatly evolved from spotting limited and predetermined keywords to understanding arbitrary formulations of a given intention. they also became much more reliable, with state-of-the-art speech recognition engines reaching human level in english. this achievement unlocked many practical applications of voice assistants which are now used in many fields from customer support, to autonomous cars, or smart homes. in particular, smart speaker adoption by the public is on the rise, with a recent study showing that nearly 20% of u.s. adults reported having a smart speaker at home. these recent developments however raise questions about user privacy- especially since unique speaker identification is an active field of research using voice as a sensitive biometric feature. the cnil (french data protection authority) advises owners of connected speakers to switch off the microphone when possible and to warn guests of the presence of such a device in their home. the general data protection regulation which harmonizes data privacy laws across the european union indeed requires companies to ask for explicit consent before collecting user data. some of the most popular commercial solutions for voice assistants include microsoft's cortana, google's dialogflow, ibm's watson, or amazon alexa. in this paper, we introduce a competing solution, the snips voice platform which, unlike the previous ones, is completely cloud independent and runs offline on typical iot microprocessors, thus guaranteeing privacy by design, with no user data ever collected nor stored. the natural language understanding component of the platform is already open source, while the other components will be opensourced in the future. the aim of this paper is to contribute to the collective effort towards ever more private and efficient cloud-independent voice interfaces. to this end, we devote this introduction to a brief description of the snips ecosystem and of some of the design principles behind the snips voice platform. subsection: the snips ecosystem the snips ecosystem comprises a web console to build voice assistants and train the corresponding spoken language understanding (slu) engine, made of an automatic speech recognition (asr) engine and a natural language understanding (nlu) engine. the console can be used as a self-service development environment by businesses or individuals, or through professional services. the snips voice platform is free for non-commercial use. since its launch in summer 2017, over 23, 000 snips voice assistants have been created by over 13, 000 developers. the languages currently supported by the snips platform are english, french and german, with additional nlu support for spanish and korean. more languages are added regularly. an assistant is composed of a set of skills- e.g. smartlights, smartthermostat, or smartoven skills for a smarthome assistant- that may be either selected from preexisting ones in a skill store or created from scratch on the web console. a given skill may contain several intents, or user intention- e.g. switchlighton and switchlightoff for a smartlights skill. finally, a given intent is bound to a list of entities that must be extracted from the user's query- e.g. room for the switchlighton intent. we call slot the particular value of an entity in a query- e.g. kitchen for the entity room. when a user speaks to the assistant, the slu engine trained on the different skills will handle the request by successively converting speech into text, classifying the user's intent, and extracting the relevant slots. once the user's request has been processed and based on the information that has been extracted from the query and fed to the device, a dialog management component is responsible for providing a feedback to the user, or performing an action. it may take multiple forms, such as an audio response via speech synthesis or a direct action on a connected device- e.g. actually turning on the lights for a smartlights skill. figure [reference] illustrates the typical interaction flow. subsection: a private-by-design embedded platform the privacy by design principle sets privacy as the default standard in the design and engineering of a system. in the context of voice assistants, that can be deployed anywhere including users' homes, this principle calls for a strong interpretation to protect users against any future misuse of their private data. in the following, we call private-by-design a system that does not transfer user data to any remote location, such as cloud servers. within the snips ecosystem, the slu components are trained on servers, but the inference happens directly on the device once the assistant has been deployed-no data from the user is ever collected nor stored. this design choice adds engineering complexity as most iot devices run on specific hardware with limited memory and computing power. cross-platform support is also a requirement in the iot industry, since iot devices are powered by many different hardware boards, with sustained innovation in that field. for these reasons, the snips voice platform has been built with portability and footprint in mind. its embedded inference runs on common iot hardware as light as the raspberry pi 3 (cpu with 1.4 ghz and 1 gb of ram), a popular choice among developers and therefore our reference hardware setting throughout this paper. other linux boards are also supported, such as imx.7d, i. mx8 m, dragonboard 410c, and jetson tx2. the snips sdk for android works with devices with android 5 and arm cpu, while the ios sdk targets ios 11 and newer. for efficiency and portability reasons, the algorithms have been re-implemented whenever needed in rust- a modern programming language offering high performance, low memory overhead, and cross-compilation. slu engines are usually broken down into two parts: automatic speech recognition (asr) and natural language understanding (nlu). the asr engine translates a spoken utterance into text through an acoustic model, mapping raw audio to a phonetic representation, and a language model (lm), mapping this phonetic representation to text. the nlu then extracts intent and slots from the decoded query. as discussed in section [reference], lm and nlu have to be mutually consistent in order to optimize the accuracy of the slu engine. it is therefore useful to introduce a language modeling component composed of the lm and nlu. figure [reference] describes the building blocks of the slu pipeline. as stated above, asr engines relying on large deep learning models have improved drastically over the past few years. yet, they still have a major drawback today. for example, the model achieving human parity in is a combination of several neural networks, each containing several hundreds of millions of parameters, and large-vocabulary language models made of several millions of n-grams. the size of these models, along with the computational resources necessary to run them in real-time, make them unfit for deployment on small devices, so that solutions implementing them are bound to rely on the cloud for speech recognition. enforcing privacy by design therefore implies developing new tools to build reliable slu engines that are constrained in size and computational requirements, which we detail in this paper. in section [reference], we describe strategies to obtain small (10 mb) and robust acoustic models trained on general speech corpora of a few hundred to a few thousand hours. section [reference] is devoted to a description of the language modeling approach of the snips slu engine. notably, we show how to ensure consistency between the language model of the asr engine and the nlu engine while specializing them to a particular use case. the resulting slu engine is lightweight and fast to execute, making it fit for deployment on small devices and the nlu component is open source. in section [reference], we illustrate the high generalization accuracy of the slu engine in the context of real-word voice assistants. finally, we discuss in section [reference] a data generation procedure to automatically create training sets replacing user data. section: acoustic model the acoustic model is the first step of the slu pipeline, and is therefore crucial to its functioning. if the decoding contains errors, it might compromise the subsequent steps and trigger a different action than that intended by the user. the acoustic model is responsible for converting raw audio data to what can approximately be interpreted as phone probabilities, i.e. context-dependent clustered hidden markov model (hmm) state probabilities. these probabilities are then fed to a language model, which decodes a sequence of words corresponding to the user utterance. the acoustic and language models are thus closely related in the automatic speech recognition (asr) engine, but are often designed and trained separately. the construction of the language model used in the slu engine is detailed in section [reference]. in this section, we present the acoustic model. first, we give details about how the training data is collected, processed, cleaned, and augmented. then, we present the acoustic model itself (a hybrid of neural networks and hidden markov models, or nn/ hmm) and how it is trained. finally, we present the performance of the acoustic model in a large-vocabulary setup, in terms of word error rate (wer), speed, and memory usage. subsection: data paragraph: training data. to train the acoustic model, we need several hundreds to thousands of hours of audio data with corresponding transcripts. the data is collected from public or commercial sources. a realignment of transcripts to the audio is performed to match transcripts to timestamps. this additionally helps in removing transcription errors that might be present in the data. the result is a set of audio extracts and matching transcripts, with lengths suitable for acoustic training (up to a few dozen seconds). this data is split in a training, testing, and development sets. paragraph: data augmentation. one of the main issues regarding the training of the acoustic model is the lack of data corresponding to real usage scenari. most of the available training data is clear close-field speech, but voice assistants will often be used in noisy conditions (music, television, environment noise), from a distance of several meters in far-field conditions, and in rooms or cars with reverberation. from a machine learning perspective, data corresponding to real usage of the system--or in-domain data- is extremely valuable. since spoken utterances from the user are not collected by our platform for privacy reasons, noisy and reverberant conditions are simulated by augmenting the data. thousands of virtual rooms of different sizes are thus generated with random microphone and speaker locations, and the rerecording of the original data in those conditions is simulated using a method close to that presented in. subsection: model training acoustic models are hybrid nn/ hmm models. more specifically, they are a custom version of the s5 training recipe of the kaldi toolkit. 40 mfcc features are extracted from the audio signal with windows of size 25ms every 10ms. models with a variable number of layers and neurons can be trained, which will impact their accuracy and computational cost. we can thus train different model architectures depending on the target hardware and the desired application accuracy. in the following evaluation (section [reference]), we present performance results of a model targeted for the raspberry pi 3. first, a speaker-adaptive gaussian mixture model hidden markov model (gmm-hmm) is trained on the speech corpus to obtain a context-dependent bootstrapping model with which we align the full dataset and extract lattices to prepare the neural network training. we train a deep neural network, consisting of time-delay layers similar to those presented in, and long short-term memory layers similar to those of. the architecture, close to that of, is summarized in figure [reference] and table [reference]. the raspberry pi 3 model uses 7 layers, and is trained with the lattice-free maximum mutual information (mmi) criterion, using natural gradient descent, with a learning rate of 0.0005 and the backstitching trick. we follow the approach described in to create fast acoustic models, namely an hmm topology with one state for each of the 1, 700 context-dependent senones, operating at a third of the original frame rate. subsection: acoustic model evaluation in this section, we present an evaluation of our acoustic model for english. our goal is the design of an end-to-end slu pipeline which runs in real-time on small embedded devices, but has state-of-the-art accuracy. this requires tradeoffs between speed and the generality of slu task. more precisely, we use domain-adapted language models described in section [reference], to compensate for the decrease of accuracy of smaller acoustic models. however in order to assess the quality of the acoustic model in a more general setting, the evaluation of this section is carried out in a large vocabulary setup, on the librispeech evaluation dataset, chosen because it is freely available and widely used in state-of-the-art comparisons. the language model for the large-vocabulary evaluation is also freely available online. it is a pruned trigram lm with a vocabulary of 200k words, trained on the content of public domain books. two sets of experiments are reported. in the first set, the models are trained only on the librispeech training set (or on a subset of it). it allows us to validate our training approach and keep track of how the models we develop compare to the state of the art when trained on public data. then, the performance of the model in terms of speed and memory usage is studied, which allows us to select a good tradeoff for the targeted raspberry pi 3 setting. subsubsection: model architecture trained and evaluated on librispeech to evaluate the impact of the dataset and model sizes on the model accuracy, neural networks of different sizes are trained on different subsets of the librispeech dataset, with and without data augmentation. the results obtained with nnet-512 are reported in table [reference]. the num. hours column corresponds to the number of training hours (460h in the train-clean split of the librispeech dataset and 500h in the train-other split). the data augmentation was only applied to the clean data. for example 460x2 means 460h of clean data+ 460h of augmented data. we observe that adding data does not have much impact on librispeech's clean test sets (dev-clean and test-clean). the wer however decreases when adding data on the datasets marked as other (dev-other, test-other). in general (not shown in those tests), adding more data and using data augmentation increases significantly the performance on noisy and reverberant conditions. in the next experiment, the neural networks are trained with the same architecture but different layer sizes on the 460x6+ 500 hours dataset. results are reported in table [reference]. this shows that larger models are capable of fitting the data and generalizing better, as expected. this allows us to choose the best tradeoff between precision and computational cost depending on each target hardware and assistant needs. subsubsection: online recognition performance while it is possible to get closer to the state of the art using larger neural network architectures, their associated memory and computational costs would prohibit their deployment on small devices. in section [reference], we show how carefully adapting the lm allows to reach high end-to-end accuracies using the acoustic models described here. we now report experiments on the processing speed of these models on our target raspberry pi 3 hardware setting. we trained models with various sizes enjoying a faster-than-real-time processing factor, to account for additional processing time (necessitated e.g. by the lm decoding or the nlu engine), and chose a model with a good compromise of accuracy to real-time factor and model size (on disk and in ram). as a reference, in terms of model size (as reported in table [reference]) nnet-256 is nearly six times smaller than nnet-768, with 2.6 m parameters vs 15.4 m, representing 10 mb vs 59 mb on disk. the gain is similar in ram. in terms of speed, the nnet-256 is 6 to 10 times faster than the nnet-768. these tradeoffs and comparison with other trained models led us to select the nnet-256. it has a reasonable speed and memory footprint, and the loss in accuracy is compensated by the adapted lm and robust nlu. this network architecture and size will be the one used in the subsequent experiments. the different architecture variations presented in this section were chosen for the sake of comparison and demonstration. this experimental comparison, along with optional layer factorization (similar to) or weight quantization are carried out for each target hardware setting, but this analysis is out of the scope of this paper. section: language modeling we now turn to the description of the language modeling component of the snips platform, which is responsible for the extraction of the intent and slots from the output of the acoustic model. this component is made up of two closely-interacting parts. the first is the language model (lm), that turns the predictions of the acoustic model into likely sentences, taking into account the probability of co-occurrence of words. the second is the natural language understanding (nlu) model, that extracts intent and slots from the prediction of the automatic speech recognition (asr) engine. in typical commercial large vocabulary speech recognition systems, the lm component is usually the largest in size, and can take up to terabytes of storage. indeed, to account for the high variability of general spoken language, large vocabulary language models need to be trained on very large text corpora. the size of these models also has an impact on decoding performance: the search space of the asr is expanded, making speech recognition harder and more computationally demanding. additionally, the performance of an asr engine on a given domain will strongly depend on the perplexity of its lm on queries from this domain, making the choice of the training text corpus critical. this question is sometimes addressed through massive use of users' private data. one option to overcome these challenges is to specialize the language model of the assistant to a certain domain, e.g. by restricting its vocabulary as well as the variety of the queries it should model. while this approach appears to restrict the range of queries that can be made to an assistant, we argue that it does not impair the usability of the resulting assistant. in fact, while the performance of an asr engine alone can be measured using e.g. the word error rate as in the previous section, we assess the performance of the slu system through its end-to-end, speech-to-meaning accuracy, i.e. its ability to correctly predict the intent and slots of a spoken utterance. as a consequence, it is sufficient for the lm to correctly model the sentences that are in the domain that the nlu supports. the size of the model is thus greatly reduced, and the decoding speed increases. the resulting asr is particularly robust within the use case, with an accuracy unreachable under our hardware constraints for an all-purpose, general asr model. in the following, we detail the implementation of this design principle, allowing the snips slu component to run efficiently on small devices with high accuracy, and illustrate its performance on two real-world assistants. subsection: data in application of the principles outlined above, we use the same data to train both lm and nlu. the next section is devoted to a description of this dataset. the generation of this dataset is discussed in section [reference]. subsubsection: training dataset the dataset used to train both the lm and nlu contains written queries exemplifying intents that depend on entities. entities are bound to an intent and used to describe all the possible values for a given attribute. for example, in the case of a smartlights assistant handling connected lights, these entities are room, brightness and color. they are required by the assistant logic to execute the right action. another example dealing with weather-related queries is described in section [reference]. an intent often has several entities that it can share with other intents. for instance, the room entity is used by several intents (switchlighton and switchlightoff), since the user might want to specify the room for both switching on and switching off the lights. entities can be of two types, either custom or built-in. custom entities are user-defined entities that can be exhaustively specified by a list of values (e.g. room: kitchen, bedroom, etc. and color: blue, red, etc.). built-in entities are common entities that can not be easily listed exhaustively by a user, and are therefore provided by the platform (numbers, ordinals, amounts with unit, date and times, durations, etc.). in our smartlights example, the entity brightness can be any number between 0 and 100, so that the built-in entity type snips/ number can be used. a query is the written expression of an intent. for instance, the query ''set the kitchen lights intensity to 65'' is associated with the intent setlightbrightness. slot labeling is done by specifying chunks of the query that should be bound to a given entity. using the same examples, the slots associated with the room and brightness entities in the query can be specified as follows: ''set the (kitchen)[room] lights intensity to (65)[brightness]''. the number of queries per intent ranges from a few ones to several thousands depending on the variability needed to cover most common wordings. subsubsection: normalization one key challenge related to end-to-end slu is data consistency between training and inference. the dataset described above is collected via the console where no specific writing system, nor cleaning rules regarding non-alphanumeric characters are enforced. before training the lm, this dataset therefore needs to be verbalized: entity values and user queries are tokenized, normalized to a canonical form, and verbalized to match entries from a lexicon. for instance, numbers and dates are spelled out, so that their pronunciation can be generated from their written form. importantly, we apply the same preprocessing before training the nlu. this step ensures consistency when it comes to inference. more precisely, it guarantees that the words output by the asr match those seen by the nlu during training. the normalization pipeline is used to handle languages specificities, through the use of a class-based tokenizer that allows support for case-by-case verbalization for each token class. for instance, numeric values are transliterated to words, punctuation tokens skipped, while quantities with units such as amounts of money require a more advanced verbalization (in english, ''$ 25'' should be verbalized as ''twenty five dollars''). the tokenizer is implemented as a character-level finite state transducer, and is designed to be easily extensible to accommodate new token types as more languages are supported. subsection: language model the mapping from the output of the acoustic model to likely word sequences is done via a viterbi search in a weighted finite state transducer (wfst), called asr decoding graph in the following. formally, the decoding graph may be written as the composition of four wfsts, where denotes transducer composition (see section [reference]), represents hidden markov models (hmms) modeling context-dependent phones, represents the context-dependency, is the lexicon and is the lm, typically a bigram or a trigram model represented as a wfst. determinization and minimization operations are also applied at each step in order to compute equivalent optimized transducers with less states, allowing the composition and the inference to run faster. more detailed definitions of the previous classical transducers are beyond the scope of this paper, and we refer the interested reader to and references therein. in the following, we focus on the construction of the g transducer, encoding the lm, from the domain-specific dataset presented above. subsubsection: language model adaptation as explained earlier, the asr engine is required to understand arbitrary formulations of a finite set of intents described in the dataset. in particular, it should be able to generalize to unseen queries within the same domain, and allow entity values to be interchangeable. the generalization properties of the asr engine are preserved by using a statistical n-gram lm allowing to mix parts of the training queries to create new ones, and by using class-based language modeling where the value of each entity may be replaced by any other. we now detail the resulting lm construction strategy. the first step in building the lm is the creation of patterns abstracting the type of queries the user may make to the assistant. starting from the dataset described above, we replace all occurrences of each entity by a symbol for the entity. for example, the query ''play some music by (the rolling stones)[artist]'' is abstracted to ''play some music by artist''. an n-gram model is then trained on the resulting set of patterns, which is then converted to a wfst called. next, for each entity where and is the number of entities, an acceptor is defined to encode the values the entity can take. the construction of depends on the type of the entity. for custom entities, whose values are listed exhaustively in the dataset, can be defined either as a union of acceptors of the different values of the entity, or as an n-gram model trained specifically on the values of the entity. for built-in entities such as numbers or dates and times, is a wfst representation of a generative grammar describing the construction of any instance of the entity. the lm transducer is then defined as where replace denotes wfst replacement. for instance, in the example above, the arcs of carrying the ''artist'' symbol are expanded into the wfst representing the ''artist'' entity. this process is represented on a simple lm on figure [reference]. the resulting allows the asr to generalize to unseen queries and to swap entity values. continuing with the simple example introduced above, the query ''play me some music by the beatles'' has the same weight as ''play me some music by the rolling stones'' in the lm, while the sentence ''play music by the rolling stones'' also has a finite weight thanks to the n-gram back-off mechanism. the lexicon transducer encodes the pronunciations of all the words in both and. the pronunciations are obtained from large base dictionaries, with a fall-back to a statistical grapheme-to-phoneme (g2p) system to generate the missing pronunciations. subsubsection: dynamic language model the standard way to compute the decoding graph is to perform compositions from right to left with the following formula where each composition is followed by a determinization and a minimization. the order in which the compositions are done is important, as the composition is known to be intractable when is not deterministic, as is the case when g is a wfst representing an n-gram model. we will refer to the result of equation [reference] as a static model in the following. in the context of embedded inference, a major drawback of this standard method is the necessity to compute and load the static hclg decoding graph in memory in order to perform speech recognition. the size of this decoding graph can claim a large chunk of the gb of ram available on a raspberry pi 3, or even be too big for smaller devices. additionally, since lms are trained synchronously in the snips web console after the user has created their dataset (see section [reference]), it is important for the decoding graph to be generated as fast as possible. for these reasons, a dynamic language model, composing the various transducers upon request instead of ahead of time, is employed. this is achieved by replacing the compositions of equation ([reference]) by delayed (or lazy) ones. consequently, the states and transitions of the complete hclg decoding graph are not computed at the time of creation, but rather at runtime during the inference, notably speeding up the building of the decoding graph. additionally, employing lazy composition allows to break the decoding graph into two pieces (hcl on one hand, and g on the other). the sum of the sizes of these pieces is typically several times smaller than the equivalent, statically-composed hclg. in order to preserve the decoding speed of the asr engine using a dynamic language model, a better composition algorithm using lazy look-ahead operations must be used. indeed, a naive lazy composition typically creates many non co-accessible states in the resulting wfst, wasting both time and memory. in the case of a static decoding graph, these states are removed through a final optimization step of the hclg that can not be applied in the dynamic case because the full hclg is never built. this issue can be addressed through the use of composition filters. in particular, the use of look-ahead filters followed by label-reachability filters with weights and labels pushing allows to discard inaccessible and costly decoding hypotheses early in the decoding. the lexicon can therefore be composed with the language model while simultaneously optimizing the resulting transducer. finally, the replace operation of equation ([reference]) is also delayed. this allows to further break the decoding graph into smaller distinct pieces: the transducer mapping the output of the acoustic model to words, the query language model, and the entities languages models. formally, at runtime, the dynamic decoding graph is created using the following formula where the hcl transducer is computed beforehand using regular transducer compositions (i.e.) and denotes the delayed transducer composition with composition filters. these improvements yield real time decoding on a raspberry pi 3 with a small overhead compared to a static model, and preserve decoding accuracy while reducing drastically the size of the model on disk. additionally, this greatly reduces the training time of the lm. finally, breaking down the lm into smaller, separate parts makes it possible to efficiently update it. it particular, performing on-device injection of new values in the lm becomes straightforward, enabling users to locally customize their slu engine without going through the snips web console. this feature is described in the following. subsubsection: on-device personalization using contextual information in asr is a promising approach to improving the recognition results by biasing the language model towards a user-specific vocabulary. a straightforward way of customizing the lm previously described is to update the list of values each entity can take. for instance, if we consider an assistant dedicated to making phone calls (''call (jane doe)[contact]''), the user's list of contacts could be added to the values of the entity ''contact'' in an embedded way, without this sensitive data ever leaving the device. this operation is called entity injection in the following. in order to perform entity injection, two modifications of the decoding graph are necessary. first, the new words and their pronunciations are added to the transducer. second, the new values are added to the corresponding entity wfst. the pronunciations of the words already supported by the asr are cached to avoid recomputing them on-device. pronunciations for words absent from the transducer are computed via an embedded g2p. the updated transducer can then be fully recompiled and optimized. the procedure for adding a new value to varies depending on whether a union of word acceptors or an n-gram model is used. in the former case, an acceptor of the new value is created and its union with is computed. in the latter case, we update the n-gram counts with the new values and recompute using an embedded n-gram engine. the time required for the complete entity injection procedure just described ranges from a few seconds for small assistants, to a few dozen seconds for larger assistants supporting a vocabulary comprising tens of thousands of words. breaking down the decoding graph into smaller, computationally manageable pieces, therefore allows to modify the model directly on device in order to provide a personalized user experience and increase the overall accuracy of the slu component. subsubsection: confidence scoring an important challenge of specialized slu systems trained on small amounts of domain-specific text data is the ability to detect out-of-vocabulary (oov) words. indeed, while a sufficient amount of specific training data may guarantee sampling the important words which allow to discriminate between different intents, it will in general prove unable to correctly sample filler words from general spoken language. as a consequence, a specialized asr such as the one described in the previous sections will tend to approximate unknown words using phonetically related ones from its vocabulary, potentially harming the subsequent nlu. one way of addressing this issue is to extract a word-level confidence score from the asr, assigning a probability for the word to be correctly decoded. confidence scoring is a notoriously hard problem in speech recognition. our approach is based on the so-called ''confusion network'' representation of the hypotheses of the asr (see figure [reference]). a confusion network is a graph encoding, for each speech segment in an utterance, the competing decoding hypotheses along with their posterior probability, thus providing a richer output than the-best decoding hypothesis. in particular, confusion networks in conjunction with nlu systems typically improve end-to-end performance in speech-to-meaning tasks. in the following, we restrict our use of confusion networks to a greedy decoder that outputs, for each speech segment, the most probable decoded word along with its probability. in this context, our strategy for identifying oovs is to set a threshold on this word-level probability. below this threshold, the word is declared misunderstood. in practice, a post-processing step is used to remove these words from the decoded sentence, replacing them with a special oov symbol. this allows the slu pipeline to proceed with the words the asr has understood with sufficient probability, leaving out the filler words which are unimportant to extract the intent and the slots from the query, thus preserving the generalization properties of the slu in the presence of unknown filler words (see section [reference] for a quantitative evaluation). finally, we may define a sentence-level confidence by simply taking the geometric mean of the word-level confidence scores. subsection: natural language understanding the natural language understanding component of the snips voice platform extracts structured data from queries written in natural language. snips nlu- a python library- can be used for training and inference, with a rust implementation focusing solely on inference. both have been recently open-sourced. three tasks are successively performed. intent classification consists in extracting the intent expressed in the query (e.g. settemperature or switchlighton). once the intent is known, slot filling aims to extract the slots, i.e. the values of the entities present in the query. finally, entity resolution focuses on built-in entities, such as date and times, durations, temperatures, for which snips provides an extra resolution step. it basically transforms entity values such as\" tomorrow evening\" into formatted values such as\" 2018-04-19 19:00:00+ 00:00\". snippet [reference] illustrates a typical output of the nlu component. subsubsection: models the snips nlu pipeline (figure [reference]) contains a main component, the nlu engine, which itself is composed of several components. a first component is the intent parser, which performs both intent classification and slot filling. it does not resolve entity values. the nlu engine calls two intent parsers successively: a deterministic intent parser a probabilistic intent parser the second one is called only when nothing is extracted by the first one. paragraph: deterministic intent parser. the goal of the deterministic intent parser is to provide robustness and a predictable experience for the user as it is guaranteed to achieve a f1-score on the training examples. its implementation relies on regular expressions. the queries contained in the training data are used to build patterns covering all combinations of entity values. let us consider, for instance, the training sample: set the [kitchen](room) lights to [blue](color) let us assume that the set of possible values for the room entity are kitchen, hall, bedroom and those for the color entity are blue, yellow, red. a representation of the generated pattern for this sample is: set the (? p< room> kitchen|hall|bedroom) lights to (? p< color> blue|yellow|red) paragraph: probabilistic intent parser. the probabilistic intent parser aims at extending parsing beyond training examples and recognizing variations which do not appear in the training data. it provides the generalization power that the deterministic parser lacks. this parser runs in two cascaded steps: intent classification and slot filling. the intent classification is implemented with a logistic regression trained on the queries from every intent. the slot-filling step consists in several linear-chain conditional random fields (crfs), each of them being trained for a specific intent. once the intent is extracted by the intent classifier, the corresponding slot filler is used to extract slots from the query. the choice of crfs for the slot-filling step results from careful considerations and experiments. they are indeed a standard approach for this task, and are known to have low generalization error. recently, more computationally demanding approaches based on deep learning models have been proposed. our experiments however showed that these approaches do not yield any significant gain in accuracy in the typical training size regime of custom voice assistants (a few hundred queries). the lightest option was therefore favored. on top of the classical features used in slot-filling tasks such as n-grams, case, shape, etc., additional features are crafted. in this kind of task, it appears that leveraging external knowledge is crucial. hence, we apply a built-in entity extractor (see next paragraph about entity resolution) to build features that indicate whether or not a token in the sentence is part of a built-in entity. the value of the feature is the corresponding entity, if one is found, augmented with a bilou coding scheme, indicating the position of the token in the matching entity value. we find empirically that the presence of such features improves the overall accuracy, thanks to the robustness of the built-in entities extractor. the problem of data sparsity is addressed by integrating features based on word clusters. more specifically, we use brown clusters released by the authors of, as well as word clusters built from word2vec embeddings using k-means clustering. we find that the use of these features helps in reducing generalization error, by bringing the effective size of the vocabulary from typically 50 k words down to a few hundred word clusters. finally, gazetteer features are built, based on entity values provided in the training data. one gazetteer is created per entity type, and used to match tokens via a bilou coding scheme. table [reference] displays some examples of features used in snips nlu. overfitting is avoided by dropping a fraction of the features during training. more precisely, each feature is assigned a dropout probability. for each training example, we compute the features and then erase feature with probability. without this mechanism, we typically observed that the crf learns to tag every value matching the entity gazetteer while discarding all those absent from it. paragraph: entity resolution. the last step of the nlu pipeline consists in resolving slot values (e.g. from raw strings to iso formatted values for date and time entities). entity values that can be resolved (e.g. dates, temperatures, numbers) correspond to the built-in entities introduced in section [reference], and are supported natively without requiring training examples. the resolution is done with rustling, an in-house re-implementation of facebook's duckling library in rust, which we also open sourced, with modifications to make its runtime more stable with regards to the length of the sentences parsed. subsubsection: evaluation snips nlu is evaluated and compared to various nlu services on two datasets: a previously published comparison, and an in-house open dataset. the latter has been made freely accessible on github to promote transparency and reproducibility. paragraph: evaluation on braun et al., 2017. in january 2018, we evaluated snips nlu on a previously published comparison between various nlu services: a few of the main cloud-based solutions (microsoft's luis, ibm watson, api.ai now google's dialogflow), and the open-source platform rasa nlu. for the raw results and methodology, see. the main metric used in this benchmark is the average f1-score of intent classification and slot filling. the data consists in three corpora. two of the corpora were extracted from stackexchange, one from a telegram chatbot. the exact same splits as in the original paper were used for the ubuntu and web applications corpora. at the date we ran the evaluation, the train and test splits were not explicit for the chatbot dataset (although they were added later on). in that case, we ran a 5-fold cross-validation. the results are presented in table [reference]. figure [reference] presents the average results on the three corpora, corresponding to the overall section of table [reference]. for rasa, we considered all three possible backends (spacy, sklearn+ mitie, mitie), see the abovementioned github repository for more details. however, only spacy was run on all 3 datasets, for train time reasons. for fairness, the latest version of rasa nlu is also displayed. results show that snips nlu ranks second highest overall. paragraph: evaluation on an in-house open dataset. in june 2017, snips nlu was evaluated on an in-house dataset of over 16 k crowdsourced queries (freely available) distributed among 7 user intents of various complexity: searchcreativework (e.g. find me the i, robot television show), getweather (e.g. is it windy in boston, ma right now?), bookrestaurant (e.g. i want to book a highly rated restaurant in paris tomorrow night), playmusic (e.g. play the last track from beyonc\u00e9 off spotify), addtoplaylist (e.g. add diamonds to my roadtrip playlist) ratebook (e.g. give 6 stars to of mice and men) searchscreeningevent (e.g. check the showtimes for wonder woman in paris) the full ontology is available on table [reference] in appendix. in this experiment, the comparison is done separately on each intent to focus on slot filling (rather than intent classification). the main metric used in this benchmark is the average f1-score of slot filling on all slots. three training sets of 70 and 2000 queries have been drawn from the total pool of queries to gain in statistical relevance. validation sets consist in 100 queries per intent. five different cloud-based providers are compared to snips nlu (microsoft's luis, api.ai now google's dialogflow, facebook's wit.ai, and amazon alexa). for more details about the specific methodology for each provider and access to the full dataset, see. each solution is trained and evaluated on the exact same datasets. table [reference] shows the precision, recall and f1-score averaged on all slots and on all intents. results specific to each intent are available in tables [reference]& [reference] in appendix. snips nlu is as accurate or better than competing cloud-based solutions in slot filling, regardless of the training set size. subsubsection: embedded performance using rust for the nlu inference pipeline allows to keep the memory footprint and the inference runtime very low. memory usage has been optimized, with model sizes ranging from a few hundred kilobytes of ram for common cases to a few megabytes for the most complex assistants. they are therefore fit for deployment on a raspberry pi or a mobile app, and more powerful servers can handle hundreds of parallel instances. using the embedded snips voice platform significantly reduces the inference runtime compared to a roundtrip to a cloud service, as displayed on table [reference]. section: end-to-end evaluation in this section, we evaluate the performance of the snips spoken language understanding (slu) pipeline in an end-to-end, speech-to-meaning setting. to this end, we consider two real-world assistants of different sizes, namely smartlights and weather. the smartlights assistant specializes in interacting with light devices supporting different colors and levels of brightness, and positioned in various rooms. the weather assistant is targeted at weather queries in general, and supports various types of formulations and places. tables [reference] and [reference] sum up the constitution and size of the datasets corresponding to these two assistants, while tables [reference] and [reference] describe their entities. note in particular the use of the built-in ''snips/ number'' (respectively ''snips/ datetime'') entity to define the brightness (resp. datetime) slot, which allows the assistant to generalize to values absent from the dataset. we are interested in computing end-to-end metrics quantifying the ability of the assistants to extract intent and slots from spoken utterances. we create a test set by crowdsourcing a spoken corpus corresponding to the queries of each dataset. for each sentence of the speech corpus, we apply the asr engine followed by the nlu engine, and compare the predicted output to the ground true intent and slots in the dataset. in the following, we present our results in terms of the classical precision, recall, and f1 scores. subsection: language model generalization error to be able to understand arbitrary formulations of an intent, the slu engine must be able to generalize to unseen queries in the same domain. to test the generalization ability of the snips slu components, we use 5-fold cross-validation, and successively train the lm and nlu on four fifth of the dataset, testing on the last, unseen, fifth of the data. the training procedure is identical to the one detailed in section [reference]. we note that all the values of the entities are always included in the training set. tables [reference] and [reference] sum up the results of this experiment, highlighting in particular the modest effect of the introduction of the asr engine compared to the accuracy of the nlu evaluated directly on the ground true query. because unseen test queries may contain out of vocabulary words absent from the training splits, the ability of the slu to generalize in this setting relies heavily on the identification of unknown words through the strategy detailed in section [reference]. as noted earlier and confirmed by these results, this confidence scoring strategy also allows to favor precision over recall by rejecting uncertain words that may be misinterpreted by the nlu. figure [reference] illustrates the correlation between the sentence-level confidence score defined in section [reference] and the word error rate. while noisy, the confidence allows to detect misunderstood queries, and can be mixed with the intent classification probability output by the nlu to reject dubious queries, thus promoting precision over recall. subsection: embedded performance the embedded slu components corresponding to the assistants described in the previous section are trained in under thirty seconds through the snips web console (see section [reference]). the resulting language models have a size of the order of the megabyte for the smartlights assistant (mb in total, with the acoustic model), and mb for the weather assistant (mb in total). the slu components run faster than real time on a single core on a raspberry pi 3, as well as on the smaller nxp imx7d. section: training models without user data the private-by-design approach described in the previous sections requires to train high-performance machine learning models without access to users queries. this problem is especially critical for the specialized language modeling components- language model and natural language understanding engine- as both need to be trained on an assistant-specific dataset. a solution is to develop a data generation pipeline. once the scope of an assistant has been defined, a mix of crowdsourcing and semi-supervised machine learning is used to generate thousands of high-quality training examples, mimicking user data collection without compromising on privacy. the aim of this section is to describe the data generation pipeline, and to demonstrate its impact on the performance of the nlu. subsection: data generation pipeline a first simple approach to data generation is grammar-based generation, which consists in breaking down a written query into consecutive semantic blocks and requires enumerating every possible pattern in the chosen language. while this method guarantees an exact slot and intent supervision, queries generated in this way are highly correlated: their diversity is limited to the expressive power of the used grammar and the imagination of the person having created it. moreover, the pattern definition and enumeration can be very time consuming and requires an extensive knowledge of the given language. this approach is therefore unfit for generating queries in natural language. on the other hand, crowdsourcing- widely used in natural language processing research- ensures diversity in formulation by sampling queries from a large number of demographically diverse contributors. however, the accuracy of intent and slot supervision decreases as soon as humans are in the loop. any mislabeling of a query's intent or slots has a strong impact on the end-to-end performance of the slu. to guarantee a fast and accurate generation of training data for the language modeling components, we complement crowdsourcing with machine-learning-based disambiguation techniques. we new detail the implementation of the data generation pipeline. subsubsection: crowdsourcing crowdsourcing tasks were originally submitted to amazon mechanical turk, a widely used platform in non-expert annotations for natural language tasks. while a sufficient number of english-speaking contributors can be reached easily, other languages such as french, german or japanese suffer from a comparatively smaller available crowd. local crowdsourcing platforms therefore had to be integrated. a text query generation task consists in generating an example of user query matching a provided set of intent and slots- e.g. the following set: intent: the user wants to switch the lights on; slot: (bedroom)[room] could result in the generated query ''i want lights in the bedroom right now!''. fixing entity values reduces the task to a sentence generation and removes the need for a slot labeling step, limiting the sources of error. diversity is enforced by both submitting this task to the widest possible crowd while limiting the number of available tasks per contributor and by selecting large sets of slot values. each generated query goes through a validation process taking the form of a second crowdsourcing task, where at least two out of three new contributors must confirm its formulation, spelling, and intent. majority voting is indeed a simple and straightforward approach for quality assessment in crowdsourcing. a custom dashboard hosted on our servers has been developed to optimize the contributor's workflow, with clear descriptions of the task. the dashboard also prevents a contributor from submitting a query that does not contain the imposed entity values, with a fuzzy matching rule allowing for inflections in all supported languages (conjugation, plural, gender, compounds, etc.). subsubsection: disambiguation while the previous validation step allows to filter out most spelling and formulation mistakes, it does not always guarantee the correctness of the intent or the absence of spurious entities. indeed, in a first type of errors, the intent of the query may not match the provided one- e.g. switch off the lights when the intent switchlighton was required. in a second type of errors, spurious entities may be added by the contributor, so that they are not labeled as such- e.g. ''i want lights in the guest [bedroom](room) at 60 right now!'' when only [bedroom](room) was mentioned in the task specifications. an unlabeled entity has a particularly strong impact on the crf features in the nlu component, and limits the ability of the lm to generalize. these errors in the training queries must be fixed to achieve a high accuracy of the slu. to do so, we perform a 3-fold cross validation of the nlu engine on this dataset. this yields predicted intents and slots for each sentence in the dataset. by repeating this procedure several times, we obtain several predictions for each sentence. we then apply majority voting on these predictions to detect missing slots and wrong intents. slots may therefore be extended- e.g. (bedroom)[room] (guest bedroom)[room] in the previous example- or added- (60)[intensity]- and ill-formed queries (with regard to spelling or intent) are filtered-out. subsection: evaluation we illustrate the impact of data generation on the slu performance on the specific case of the slot-filling task in the nlu component. the same in-house open dataset of over 16 k crowdsourced query presented in section [reference] is used. unsurprisingly, slot-filling performance drastically increases with the number of training samples. the f1-scores averaged over all slots are computed, depending on the number of training queries per intent. an nlu engine has been trained on each individual intent. training queries are freely available on github. the data has been generated with our data generation pipeline. figure [reference] shows the influence of the number of training samples on the performance of the slot-filling task of the nlu component. compared to 10 training queries, the gain in performance with 500 queries is of 32% absolute on average, ranging from 22% for the ratebook intent (from 0.76 to 0.98) to 44% for the getweather intent (from 0.44 to 0.88). this gain indeed strongly depends on the intent's complexity, which is mainly defined by its entities (number of entities, built-in or custom, number of entity values, etc.). while a few tens of training queries might suffice for some simple use cases (such as ratebook), more complicated intents with larger sets of entity values (playmusic for instance) require larger training datasets. while it is easy to manually generate up to 50 queries, being able to come up with hundreds or thousands of diverse formulations of the same intent is nearly impossible. for private-by-design assistants that do not gather user queries, the ability to generate enough queries is key to training efficient machine learning models. moreover, being able to generate training data allows us to validate the performance of our models before deploying them. section: conclusion in this paper, we have described the design of the snips voice platform, a spoken language understanding solution that can be embedded in small devices and runs entirely offline. in compliance with the privacy-by-design principle, assistants created through the snips voice platform never send user queries to the cloud and offer state-of-the-art performance. focusing on the automatic speech recognition and natural language understanding engines, we have described the challenges of embedding high-performance machine learning models on small iot devices. on the acoustic modeling side, we have shown how small-sized neural networks can be trained that enjoy near state-of-the-art accuracy while running in real-time on small devices. on the language modeling side, we have described how to train the language model of the asr and the nlu in a consistent way, efficiently specializing them to a particular use case. we have also demonstrated the accuracy of the resulting slu engine on real-world assistants. finally, we have shown how sufficient, high-quality training data can be obtained without compromising user privacy through a combination of crowdsourcing and machine learning. we hope that the present paper can contribute to a larger effort towards ever more private and ubiquitous artificial intelligence. future research directions will include private analytics, allowing to receive privacy-preserving feedback from assistant usage, and federated learning, as a complement to data generation. section: acknowledgments all of the work presented here has been done closely together with the engineering teams at snips. we are grateful to the crowd of contributors who regularly work with us on the data generation pipeline. we are indebted to the community of users of the snips voice platform for valuable feedback and contributions. section: appendix: nlu benchmark on an in-house dataset bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "speech corpus",
                        12673
                    ],
                    [
                        "librispeech evaluation dataset",
                        14151
                    ],
                    [
                        "librispeech training set",
                        14555
                    ],
                    [
                        "librispeech",
                        14980
                    ],
                    [
                        "librispeech dataset",
                        15144
                    ],
                    [
                        "clean data",
                        15477
                    ],
                    [
                        "librispeech's clean test sets",
                        15614
                    ],
                    [
                        "dev-clean",
                        15646
                    ],
                    [
                        "test-clean",
                        15659
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "snips voice platform",
                        10
                    ],
                    [
                        "design principles",
                        2908
                    ],
                    [
                        "snips",
                        3470
                    ],
                    [
                        "slu",
                        35949
                    ],
                    [
                        "snips nlu",
                        36357
                    ],
                    [
                        "snippet",
                        37082
                    ],
                    [
                        "snips spoken language understanding",
                        46193
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "word error rate",
                        10358
                    ],
                    [
                        "wer",
                        10376
                    ],
                    [
                        "generalization error",
                        39303
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "speech recognition",
                        905
                    ],
                    [
                        "recognition",
                        32000
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "librispeech training set",
                        14555
                    ],
                    [
                        "librispeech dataset",
                        15144
                    ],
                    [
                        "augmented data",
                        15541
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "snips voice platform",
                        10
                    ],
                    [
                        "design principles",
                        2908
                    ],
                    [
                        "snips",
                        3470
                    ],
                    [
                        "slu",
                        35949
                    ],
                    [
                        "snips nlu",
                        36357
                    ],
                    [
                        "snippet",
                        37082
                    ],
                    [
                        "snips spoken language understanding",
                        46193
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "word error rate",
                        10358
                    ],
                    [
                        "wer",
                        10376
                    ],
                    [
                        "generalization error",
                        39303
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "speech recognition",
                        905
                    ],
                    [
                        "recognition",
                        32000
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "16028acc465c434a43599f0f900c91f38d690e02-23",
    "doctext": "cornernet: detecting objects as paired keypoints section: abstract we propose cornernet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. by detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. in addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. experiments show that cornernet achieves a 42.2% ap on ms coco, outperforming all existing one-stage detectors. section: introduction object detectors based on convolutional neural networks (convnets) [reference][reference][reference] have achieved state-of-the-art results on various challenging benchmarks [reference][reference][reference]. a common component of state-of-the-art approaches is anchor boxes [reference][reference], which are boxes of various sizes and aspect ratios that serve as detection candidates. anchor boxes are extensively used in one-stage detectors [reference][reference][reference], which can achieve results highly competitive with two-stage detectors [reference][reference][reference][reference] while being more efficient. one-stage detectors place anchor boxes densely over an image and generate final box predictions by scoring anchor boxes and refining their coordinates through regression. but the use of anchor boxes has two drawbacks. first, we typically need a very large set of anchor boxes, e.g. more than 40k in dssd and more than 100k in retinanet. this is because the detector is trained to classify whether each anchor box sufficiently overlaps with a ground truth box, and a large number of anchor boxes is needed to ensure sufficient overlap with most ground truth boxes. as a result, only a tiny fraction of anchor boxes will overlap with ground truth; this creates a huge imbalance between positive and negative anchor boxes and slows down training. second, the use of anchor boxes introduces many hyperparameters and design choices. these include how many boxes, what sizes, and what aspect ratios. such choices have largely been made via ad-hoc heuristics, and can become even more complicated when combined with multiscale architectures where a single network makes separate predictions at multiple resolutions, with each scale using different features and its own set of anchor boxes [reference][reference][reference]. in this paper we introduce cornernet, a new onestage approach to object detection that does away with anchor boxes. we detect an object as a pair of keypointsthe top-left corner and bottom-right corner of the bounding box. we use a single convolutional network to predict a heatmap for the top-left corners of all instances of the same object category, a heatmap for all bottomright corners, and an embedding vector for each detected corner. the embeddings serve to group a pair of corners that belong to the same object-the network is trained to predict similar embeddings for them. our ap-section: convnet embeddings heatmaps top-left corners bottom-right corners fig. 1 we detect an object as a pair of bounding box corners grouped together. a convolutional network outputs a heatmap for all top-left corners, a heatmap for all bottom-right corners, and an embedding vector for each detected corner. the network is trained to predict similar embeddings for corners that belong to the same object. proach greatly simplifies the output of the network and eliminates the need for designing anchor boxes. our approach is inspired by the associative embedding method proposed by, who detect and group keypoints in the context of multiperson human-pose estimation. fig. 1 illustrates the overall pipeline of our approach. another novel component of cornernet is corner pooling, a new type of pooling layer that helps a convolutional network better localize corners of bounding boxes. a corner of a bounding box is often outside the object-consider the case of a circle as well as the examples in fig. 2. in such cases a corner can not be localized based on local evidence. instead, to determine whether there is a top-left corner at a pixel location, we need to look horizontally towards the right for the topmost boundary of the object, and look vertically towards the bottom for the leftmost boundary. this motivates our corner pooling layer: it takes in two feature maps; at each pixel location it max-pools all feature vectors to the right from the first feature map, maxpools all feature vectors directly below from the second feature map, and then adds the two pooled results together. an example is shown in fig. 3. we hypothesize two reasons why detecting corners would work better than bounding box centers or proposals. first, the center of a box can be harder to localize because it depends on all 4 sides of the object, whereas locating a corner depends on 2 sides and is thus easier, and even more so with corner pooling, which encodes some explicit prior knowledge about the definition of corners. second, corners provide a more efficient way of densely discretizing the space of boxes: we just need o (wh) corners to represent o (w 2 h 2) possible anchor boxes. we demonstrate the effectiveness of cornernet on ms coco [reference]. cornernet achieves a 42.2% ap, outperforming all existing one-stage detectors. in addition, through ablation studies we show that corner pooling is critical to the superior performance of cornernet. code is available at https:// github.com/ princeton-vl/ cornernet. section: related works section: two-stage object detectors two-stage approach was first introduced and popularized by r-cnn [reference]. two-stage detectors generate a sparse set of regions of interest (rois) and classify each of them by a network. r-cnn generates rois using a low level vision algorithm [reference][reference]. each region is then extracted from the image and processed by a convnet independently, which creates lots of redundant computations. later, spp [reference] and fast-rcnn [reference] improve r-cnn by designing a special pooling layer that pools each region from feature maps instead. however, both still rely on separate proposal algorithms and can not be trained end-to-end. faster-rcnn [reference] does away low level proposal algorithms by introducing a region proposal network (rpn), which generates proposals from a set of pre-determined candidate boxes, usually known as anchor boxes. this not only makes the detectors more efficient but also allows the detectors to be trained end-toend. r-fcn [reference] further improves the efficiency of faster-rcnn by replacing the fully connected sub-detection network with a fully convolutional subdetection network. other works focus on incorporating sub-category information [reference], generating object proposals at multiple scales with more contextual information [reference][reference][reference][reference], selecting better features [reference], improving speed, cascade procedure [reference] and better training procedure. section: one-stage object detectors on the other hand, yolo and ssd [reference] have popularized the one-stage approach, which removes the roi pooling step and detects objects in a single network. one-stage detectors are usually more computationally efficient than twostage detectors while maintaining competitive performance on different challenging benchmarks. ssd places anchor boxes densely over feature maps from multiple scales, directly classifies and refines each anchor box. yolo predicts bounding box coordinates directly from an image, and is later improved in yolo9000 by switching to anchor boxes. dssd and ron [reference] adopt networks similar to the hourglass network [reference], enabling them to combine low-level and high-level features via skip connections to predict bounding boxes more accurately. however, these one-stage detectors are still outperformed by the two-stage detectors until the introduction of retinanet. in, the authors suggest that the dense anchor boxes create a huge imbalance between positive and negative anchor boxes during training. this imbalance causes the training to be inefficient and hence the performance to be suboptimal. they propose a new loss, focal loss, to dynamically adjust the weights of each anchor box and show that their onestage detector can outperform the two-stage detectors. refinedet proposes to filter the an-chor boxes to reduce the number of negative boxes, and to coarsely adjust the anchor boxes. denet [reference]) is a two-stage detector which generates rois without using anchor boxes. it first determines how likely each location belongs to either the top-left, top-right, bottomleft or bottom-right corner of a bounding box. it then generates rois by enumerating all possible corner combinations, and follows the standard two-stage approach to classify each roi. our approach is very different from denet. first, denet does not identify if two corners are from the same objects and relies on a sub-detection network to reject poor rois. in contrast, our approach is a one-stage approach which detects and groups the corners using a single convnet. second, denet selects features at manually determined locations relative to a region for classification, while our approach does not require any feature selection step. third, we introduce corner pooling, a novel type of layer to enhance corner detection. point linking network (pln) is an one-stage detector without anchor boxes. it first predicts the locations of the four corners and the center of a bounding box. then, at each corner location, it predicts how likely each pixel location in the image is the center. similarly, at the center location, it predicts how likely each pixel location belongs to either the top-left, top-right, bottom-left or bottom-right corner. it combines the predictions from each corner and center pair to generate a bounding box. finally, it merges the four bounding boxes to give a bounding box. cornernet is very different from pln. first, cornernet groups the corners by predicting embedding vectors, while pln groups the corner and center by predicting pixel locations. second, cornernet uses corner pooling to better localize the corners. our approach is inspired by on associative embedding in the context of multi-person pose estimation. newell et al. propose an approach that detects and groups human joints in a single network. in their approach each detected human joint has an embedding vector. the joints are grouped based on the distances between their embeddings. to the best of our knowledge, we are the first to formulate the task of object detection as a task of detecting and grouping corners with embeddings. another novelty of ours is the corner pooling layers that help better localize the corners. we also significantly modify the hourglass architecture and add our novel variant of focal loss to help better train the network. section: cornernet section: overview in cornernet, we detect an object as a pair of keypointsthe top-left corner and bottom-right corner of the bounding box. a convolutional network predicts two sets of heatmaps to represent the locations of corners of different object categories, one set for the top-left corners and the other for the bottom-right corners. the network also predicts an embedding vector for each detected corner such that the distance between the embeddings of two corners from the same object is small. to produce tighter bounding boxes, the network also predicts offsets to slightly adjust the locations of the corners. with the predicted heatmaps, embeddings and offsets, we apply a simple post-processing algorithm to obtain the final bounding boxes. fig. 4 provides an overview of cornernet. we use the hourglass network [reference] as the backbone network of cornernet. the hourglass network is followed by two prediction modules. one module is for the top-left corners, while the other one is for the bottomright corners. each module has its own corner pooling module to pool features from the hourglass network before predicting the heatmaps, embeddings and offsets. unlike many other object detectors, we do not use features from different scales to detect objects of different sizes. we only apply both modules to the output of the hourglass network. section: detecting corners we predict two sets of heatmaps, one for top-left corners and one for bottom-right corners. each set of heatmaps has c channels, where c is the number of categories, and is of size h\u00d7 w. there is no background channel. each channel is a binary mask indicating the locations of the corners for a class. for each corner, there is one ground-truth positive location, and all other locations are negative. during training, instead of equally penalizing negative locations, we reduce the penalty given to negative locations within a radius of the positive location. this is because a pair of false corner detections, if they are close to their respective ground truth locations, can still produce a box that sufficiently overlaps the ground-truth box (fig. 5). we determine the radius by the size of an object by ensuring that a pair of points within the radius would generate a bounding box with at least t iou with the ground-truth annotation (we set t to 0.3 in all experiments). given the radius, the amount of penalty reduction is given by an unnormalized 2d gaussian, fig. 4 overview of cornernet. the backbone network is followed by two prediction modules, one for the top-left corners and the other for the bottom-right corners. using the predictions from both modules, we locate and group the corners. e\u2212 x 2+ y 2 2\u03c3 2, whose center is at the positive location and whose\u03c3 is 1/ 3 of the radius. let p cij be the score at location (i, j) for class c in the predicted heatmaps, and let y cij be the\" groundtruth\" heatmap augmented with the unnormalized gaussians. we design a variant of focal loss: where n is the number of objects in an image, and\u03b1 and\u03b2 are the hyper-parameters which control the contribution of each point (we set\u03b1 to 2 and\u03b2 to 4 in all experiments). with the gaussian bumps encoded in y cij, the (1\u2212 y cij) term reduces the penalty around the ground truth locations. many networks [reference]) involve downsampling layers to gather global information and to reduce memory usage. when they are applied to an image fully convolutionally, the size of the output is usually smaller than the image. hence, a location (x, y) in the image is mapped to the location x n, y n in the heatmaps, where n is the downsampling factor. when we remap the locations from the heatmaps to the input image, some precision may be lost, which can greatly affect the iou of small bounding boxes with their ground truths. to address this issue we predict location offsets to slightly adjust the corner locations before remapping them to the input resolution. where o k is the offset, x k and y k are the x and y coordinate for corner k. in particular, we predict one set of offsets shared by the top-left corners of all categories, and another set shared by the bottom-right corners. for training, we apply the smooth l1 loss [reference] at ground-truth corner locations: section: grouping corners multiple objects may appear in an image, and thus multiple top-left and bottom-right corners may be detected. we need to determine if a pair of the top-left corner and bottom-right corner is from the same bounding box. our approach is inspired by the associative embedding method proposed by for the task of multi-person pose estimation. newell et al. detect all human joints and generate an embedding for each detected joint. they group the joints based on the distances between the embeddings. the idea of associative embedding is also applicable to our task. the network predicts an embedding vector for each detected corner such that if a top-left corner and a bottom-right corner belong to the same bounding box, the distance between their embeddings should be small. we can then group the corners based on the distances between the embeddings of the top-left and bottom-right corners. the actual values of the embeddings are unimportant. only the distances between the embeddings are used to group the corners. we follow and use embeddings of 1 dimension. let e t k be the embedding for the top-left corner of object k and e b k for the bottom-right corner. as in, we use the\" pull\" loss to train the network to group the corners and the\" push\" loss to separate the corners: where e k is the average of e t k and e b k and we set\u2206 to be 1 in all our experiments. similar to the offset loss, we only apply the losses at the ground-truth corner location. section: corner pooling as shown in fig. 2, there is often no local visual evidence for the presence of corners. to determine if a pixel is a top-left corner, we need to look horizontally towards the right for the topmost boundary of an object and vertically towards the bottom for the leftmost boundary. we thus propose corner pooling to better localize the corners by encoding explicit prior knowledge. suppose we want to determine if a pixel at location (i, j) is a top-left corner. let f t and f l be the feature maps that are the inputs to the top-left corner pooling layer, and let f tij and f lij be the vectors at location (i, j) in f t and f l respectively. with h\u00d7 w feature maps, the corner pooling layer first max-pools all feature vectors between (i, j) and (i, h) in f t to a feature vector t ij, and max-pools all feature vectors between (i, j) and (w, j) in f l to a feature vector l ij. finally, it adds t ij and l ij together. this computation can be expressed by the following equations: where we apply an elementwise max operation. both t ij and l ij can be computed efficiently by dynamic programming as shown fig. 8. we define bottom-right corner pooling layer in a similar way. it max-pools all feature vectors between (0, j) and (i, j), and all feature vectors between (i, 0) and (i, j) before adding the pooled results. the corner pooling layers are used in the prediction modules to predict heatmaps, embeddings and offsets. the architecture of the prediction module is shown in fig. 7. the first part of the module is a modified version of the residual block. in this modified residual block, we replace the first 3\u00d7 3 convolution module with a corner pooling module, which first processes the features from the backbone network by two 3\u00d7 3 convolution modules 1 with 128 channels and then applies a corner pooling layer. following the design of a residual block, we then feed the pooled features into a 3\u00d7 3 conv-bn layer with 256 channels and add back the projection shortcut. the modified residual block is followed by a 3\u00d73 convolution module with 256 channels, and 3 conv-relu-conv layers to produce the heatmaps, embeddings and offsets. section: hourglass network cornernet uses the hourglass network [reference] as its backbone network. the hourglass network was first introduced for the human pose estimation task. it is a fully convolutional neural network that consists of one or more hourglass modules. an hourglass module first downsamples the input features by a series of convolution and max pooling layers. it then upsamples the features back to the original resolution by a series of upsampling and convolution layers. since details are lost in the max pooling layers, skip layers are added to bring back the details to the upsampled features. the hourglass module captures both global and local features in a single unified structure. when multiple hourglass modules are stacked in the network, the hourglass modules can reprocess the features to capture higher-level of information. these properties make the hourglass network an ideal choice for object detection as well. in fact, many current detectors [reference][reference][reference][reference]) already adopted networks similar to the hourglass network. our hourglass network consists of two hourglasses, and we make some modifications to the architecture of the hourglass module. instead of using max pool-9 10 fig. 6 the top-left corner pooling layer can be implemented very efficiently. we scan from right to left for the horizontal max-pooling and from bottom to top for the vertical max-pooling. we then add two max-pooled feature maps. fig. 7 the prediction module starts with a modified residual block, in which we replace the first convolution module with our corner pooling module. the modified residual block is then followed by a convolution module. we have multiple branches for predicting the heatmaps, embeddings and offsets. ing, we simply use stride 2 to reduce feature resolution. we reduce feature resolutions 5 times and increase the number of feature channels along the way [reference][reference][reference][reference][reference]. when we upsample the features, we apply 2 residual modules followed by a nearest neighbor upsampling. every skip connection also consists of 2 residual modules. there are 4 residual modules with 512 channels in the middle of an hourglass module. before the hourglass modules, we reduce the image resolution by 4 times using a 7\u00d7 7 convolution module with stride 2 and 128 channels followed by a residual block with stride 2 and 256 channels. following [reference], we also add intermediate supervision in training. however, we do not add back the intermediate predictions to the network as we find that this hurts the performance of the network. we apply a 1\u00d7 1 conv-bn module to both the input and output of the first hourglass module. we then merge them by element-wise addition followed by a relu and a residual block with 256 channels, which is then used as the input to the second hourglass module. the depth of the hourglass network is 104. unlike many other stateof-the-art detectors, we only use the features from the last layer of the whole network to make predictions. section: experiments section: training details we implement cornernet in pytorch [reference]. the network is randomly initialized under the default setting of pytorch with no pretraining on any external dataset. as we apply focal loss, we follow to set the biases in the convolution layers that predict the corner heatmaps. during training, we set the input resolution of the network to 511\u00d7 511, which leads to an output resolution of 128\u00d7 128. to reduce overfitting, we adopt standard data augmentation techniques including random horizontal flipping, random scaling, random cropping and random color jittering, which includes adjusting the brightness, saturation and contrast of an image. finally, we apply pca [reference] to the input image. we use adam [reference] to optimize the full training loss: where\u03b1,\u03b2 and\u03b3 are the weights for the pull, push and offset loss respectively. we set both\u03b1 and\u03b2 to 0.1 and\u03b3 to 1. we find that 1 or larger values of\u03b1 and\u03b2 lead to poor performance. we use a batch size of 49 and train the network on 10 titan x (pascal) gpus (4 images on the master gpu, 5 images per gpu for the rest of the gpus). to conserve gpu resources, in our ablation experiments, we train the networks for 250k iterations with a learning rate of 2.5\u00d7 10\u22124. when we compare our results with other detectors, we train the networks for an extra 250k iterations and reduce the learning rate to 2.5\u00d7 10\u22125 for the last 50k iterations. section: testing details during testing, we use a simple post-processing algorithm to generate bounding boxes from the heatmaps, embeddings and offsets. we first apply non-maximal suppression (nms) by using a 3\u00d73 max pooling layer on the corner heatmaps. then we pick the top 100 top-left and top 100 bottom-right corners from the heatmaps. the corner locations are adjusted by the corresponding offsets. we calculate the l1 distances between the embeddings of the top-left and bottom-right corners. pairs that have distances greater than 0.5 or contain corners from different categories are rejected. the average scores of the top-left and bottom-right corners are used as the detection scores. instead of resizing an image to a fixed size, we maintain the original resolution of the image and pad it with zeros before feeding it to cornernet. both the original and flipped images are used for testing. we combine the detections from the original and flipped images, and apply soft-nms [reference] to suppress redundant detections. only the top 100 detections are reported. the average inference time is 244ms per image on a titan x (pascal) gpu. section: ms coco we evaluate cornernet on the very challenging ms coco dataset [reference]. ms coco contains 80k images for training, 40k for validation and 20k for testing. all images in the training set and 35k images in the validation set are used for training. the remaining 5k images in validation set are used for hyper-parameter searching and ablation study. all results on the test set are submitted to an external server for evaluation. to provide fair comparisons with other detectors, we report our main results on the test-dev set. ms coco uses average precisions (aps) at different ious and aps for different object sizes as the main evaluation metrics. section: ablation study section: corner pooling corner pooling is a key component of cornernet. to understand its contribution to performance, we train another network without corner pooling but with the same number of parameters. tab. 1 shows that adding corner pooling gives significant improvement: 2.0% on ap, 2.1% on ap 50 and 2.1% on ap 75. we also see that corner pooling is especially helpful for medium and large objects, improving their aps by 2.4% and 3.6% respectively. this is expected because the topmost, bottommost, leftmost, rightmost boundaries of medium and large objects are likely to be further away from the corner locations. fig. 8 shows four qualitative examples with and without corner pooling. section: stability of corner pooling over larger area corner pooling pools over different sizes of area in different quadrants of an image. for example, the top-left corner pooling pools over larger areas both horizontally and vertically in the upper-left quadrant of an image, compared to the lower-right quadrant. therefore, the location of a corner may affect the stability of the corner pooling. we evaluate the performance of our network on detecting both the top-left and bottom-right corners in different quadrants of an image. detecting corners can be seen as a binary classification task i.e. the groundtruth location of a corner is positive, and any location outside of a small radius of the corner is negative. we measure the performance using maps over all categories on the ms coco validation set. tab. 3 shows that without corner pooling, the topleft corner maps of upper-left and lower-right quadrant are 66.1% and 60.8% respectively. top-left corner pooling improves the maps by 3.1% (to 69.2%) and 2.7% (to 63.5%) respectively. similarly, bottomright corner pooling improves the bottom-right corner maps of upper-left quadrant by 2.8% (from 53.4% to 56.2%), and lower-right quadrant by 2.6% (from 65.0% to 67.6%). corner pooling gives similar improvement to corners at different quadrants, show that corner pooling is effective and stable over both small and large areas. section: reducing penalty to negative locations we reduce the penalty given to negative locations around a positive location, within a radius determined by the size of the object (sec. 3.2). to understand how this helps train cornernet, we train one network with no penalty reduction and another network with a fixed radius of 2.5. we compare them with cornernet on the validation set. tab. 2 shows that a fixed radius improves ap over the baseline by 2.7%, ap m by 1.5% and ap l by 5.3%. object-dependent radius further improves the ap by 2.8%, ap m by 2.0% and ap l by 5.8%. in addition, we see that the penalty reduction especially benefits medium and large objects. section: hourglass network cornernet uses the hourglass network [reference] as its backbone network. since the hourglass network is not commonly used in other state-of-the-art detectors, we perform an experiment to study the contribution of the hourglass network in cornernet. we train a cornernet in which we replace the hourglass network with fpn (w/ resnet-101), which is more commonly used in state-of-the-art object detectors. we only use the final output of fpn for predictions. meanwhile, we train an anchor box based detector which uses the hourglass network as its backbone. each hourglass module predicts anchor boxes at multiple resolutions by using features at multiple scales during upsampling stage. we follow the anchor box design in retinanet and add intermediate supervisions during training. in both experiments, we initialize the networks from scratch and follow the same training procedure as we train cornernet (sec. 4.1). tab. 4 shows that cornernet with hourglass network outperforms cornernet with fpn by 8.2% ap, and the anchor box based detector with hourglass network by 5.5% ap. the results suggest that the choice of the backbone network is important and the hourglass network is crucial to the performance of cornernet. section: quality of the bounding boxes a good detector should predict high quality bounding boxes that cover objects tightly. to understand the quality of the bounding boxes predicted by cornernet, we evaluate the performance of cornernet at multiple iou thresholds, and compare the results with other state-of-the-art detectors, including retinanet, cascade r-cnn [reference] and iou-net [reference]. tab. 5 shows that cornernet achieves a much higher ap at 0.9 iou than other detectors, outperforming cascade r-cnn+ iou-net by 3.9%, cascade r-cnn by 7.6% and retinanet 2 by 7.3%. this suggests that cor-2 we use the best model publicly available on https:// github.com/ facebookresearch/ detectron/ blob/ master/ model_zoo.md nernet is able to generate bounding boxes of higher quality compared to other state-of-the-art detectors. section: error analysis cornernet simultaneously outputs heatmaps, offsets, and embeddings, all of which affect detection performance. an object will be missed if either corner is missed; precise offsets are needed to generate tight bounding boxes; incorrect embeddings will result in many false bounding boxes. to understand how each part contributes to the final error, we perform an error analysis by replacing the predicted heatmaps and offsets with the ground-truth values and evaluting performance on the validation set. tab. 6 shows that using the ground-truth corner heatmaps alone improves the ap from 38.4% to 73.1%. section: ap s, ap m and ap l also increase by 42.3%, 40.7% and 30.0% respectively. if we replace the predicted offsets with the ground-truth offsets, the ap further increases by 13.0% to 86.1%. this suggests that although there is still ample room for improvement in both detecting and grouping corners, the main bottleneck is detecting corners. fig. 9 shows some qualitative examples where the corner locations or embeddings are incorrect. section: comparisons with state-of-the-art detectors we compare cornernet with other state-of-the-art detectors on ms coco test-dev (tab. 7). with multiscale evaluation, cornernet achieves an ap of 42.2%, the state of the art among existing one-stage methods and competitive with two-stage methods. section: conclusion we have presented cornernet, a new approach to object detection that detects bounding boxes as pairs of corners. we evaluate cornernet on ms coco and demonstrate competitive results. section: section: acknowledgements this work is partially supported by a grant from toyota research institute and a darpa grant fa8750-18-2-0019. this article solely reflects the opinions and conclusions of its authors. section:",
    "templates": [
        {
            "Material": [
                [
                    [
                        "ms coco",
                        624
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "cornernet",
                        0
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "ap",
                        618
                    ],
                    [
                        "average precisions",
                        25209
                    ],
                    [
                        "aps",
                        25230
                    ],
                    [
                        "ap m",
                        27886
                    ],
                    [
                        "ap l",
                        27991
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "detecting objects",
                        11
                    ],
                    [
                        "object detection",
                        107
                    ],
                    [
                        "detection",
                        30294
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "170cc3b72dd2793920f8b0bbd0926d9244c49562-24",
    "doctext": "document: deep exploration via bootstrapped dqn efficient exploration remains a major challenge for reinforcement learning (rl). common dithering strategies for exploration, such as-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. however, most algorithms for statistically efficient rl are not computationally tractable in complex environments. randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. as a first step towards addressing such contexts we develop bootstrapped dqn. we demonstrate that bootstrapped dqn can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. in the arcade learning environment bootstrapped dqn substantially improves learning speed and cumulative performance across most games. section: introduction we study the reinforcement learning (rl) problem where an agent interacts with an unknown environment. the agent takes a sequence of actions in order to maximize cumulative rewards. unlike standard planning problems, an rl agent does not begin with perfect knowledge of the environment, but learns through experience. this leads to a fundamental trade-off of exploration versus exploitation; the agent may improve its future rewards by exploring poorly understood states and actions, but this may require sacrificing immediate rewards. to learn efficiently an agent should explore only when there are valuable learning opportunities. further, since any action may have long term consequences, the agent should reason about the informational value of possible observation sequences. without this sort of temporally extended (deep) exploration, learning times can worsen by an exponential factor. the theoretical rl literature offers a variety of provably-efficient approaches to deep exploration. however, most of these are designed for markov decision processes (mdps) with small finite state spaces, while others require solving computationally intractable planning tasks. these algorithms are not practical in complex environments where an agent must generalize to operate effectively. for this reason, large-scale applications of rl have relied upon statistically inefficient strategies for exploration or even no exploration at all. we review related literature in more detail in section [reference]. common dithering strategies, such as-greedy, approximate the value of an action by a single number. most of the time they pick the action with the highest estimate, but sometimes they choose another action at random. in this paper, we consider an alternative approach to efficient exploration inspired by thompson sampling. these algorithms have some notion of uncertainty and instead maintain a distribution over possible values. they explore by randomly select a policy according to the probability it is the optimal policy. recent work has shown that randomized value functions can implement something similar to thompson sampling without the need for an intractable exact posterior update. however, this work is restricted to linearly-parameterized value functions. we present a natural extension of this approach that enables use of complex non-linear generalization methods such as deep neural networks. we show that the bootstrap with random initialization can produce reasonable uncertainty estimates for neural networks at low computational cost. bootstrapped dqn leverages these uncertainty estimates for efficient (and deep) exploration. we demonstrate that these benefits can extend to large scale problems that are not designed to highlight deep exploration. bootstrapped dqn substantially reduces learning times and improves performance across most games. this algorithm is computationally efficient and parallelizable; on a single machine our implementation runs roughly% slower than dqn. section: uncertainty for neural networks deep neural networks (dnn) represent the state of the art in many supervised and reinforcement learning domains. we want an exploration strategy that is statistically computationally efficient together with a dnn representation of the value function. to explore efficiently, the first step to quantify uncertainty in value estimates so that the agent can judge potential benefits of exploratory actions. the neural network literature presents a sizable body of work on uncertainty quantification founded on parametric bayesian inference. we actually found the simple non-parametric bootstrap with random initialization more effective in our experiments, but the main ideas of this paper would apply with any other approach to uncertainty in dnns. the bootstrap princple is to approximate a population distribution by a sample distribution. in its most common form, the bootstrap takes as input a data set and an estimator. to generate a sample from the bootstrapped distribution, a data set of cardinality equal to that of is sampled uniformly with replacement from. the bootstrap sample estimate is then taken to be. the bootstrap is widely hailed as a great advance of 20th century applied statistics and even comes with theoretical guarantees. in figure [reference] we present an efficient and scalable method for generating bootstrap samples from a large and deep neural network. the network consists of a shared architecture with bootstrapped ''heads'' branching off independently. each head is trained only on its bootstrapped sub-sample of the data and represents a single bootstrap sample. the shared network learns a joint feature representation across all the data, which can provide significant computational advantages at the cost of lower diversity between heads. this type of bootstrap can be trained efficiently in a single forward/ backward pass; it can be thought of as a data-dependent dropout, where the dropout mask for each head is fixed for each data point. [b] 0.31 [b] 0.31 [b] 0.31 figure [reference] presents an example of uncertainty estimates from bootstrapped neural networks on a regression task with noisy data. we trained a fully-connected 2-layer neural networks with 50 rectified linear units (relu) in each layer on 50 bootstrapped samples from the data. as is standard, we initialize these networks with random parameter values, this induces an important initial diversity in the models. we were unable to generate effective uncertainty estimates for this problem using the dropout approach in prior literature. further details are provided in appendix [reference]. section: bootstrapped dqn for a policy we define the value of an action in state, where is a discount factor that balances immediate versus future rewards. this expectation indicates that the initial state is, the initial action is, and thereafter actions are selected by the policy. the optimal value is. to scale to large problems, we learn a parameterized estimate of the q-value function rather than a tabular encoding. we use a neural network to estimate this value. the q-learning update from state, action, reward and new state is given by where is the scalar learning rate and is the target value. are target network parameters fixed. several important modifications to the q-learning update improve stability for dqn. first the algorithm learns from sampled transitions from an experience buffer, rather than learning fully online. second the algorithm uses a target network with parameters that are copied from the learning network only every time steps and then kept fixed in between updates. double dqn modifies the target and helps further: bootstrapped dqn modifies dqn to approximate a distribution over q-values via the bootstrap. at the start of each episode, bootstrapped dqn samples a single q-value function from its approximate posterior. the agent then follows the policy which is optimal for that sample for the duration of the episode. this is a natural adaptation of the thompson sampling heuristic to rl that allows for temporally extended (or deep) exploration. we implement this algorithm efficiently by building up bootstrapped estimates of the q-value function in parallel as in figure [reference]. importantly, each one of these value function function heads is trained against its own target network. this means that each provide a temporally extended (and consistent) estimate of the value uncertainty via td estimates. in order to keep track of which data belongs to which bootstrap head we store flags indicating which heads are privy to which data. we approximate a bootstrap sample by selecting uniformly at random and following for the duration of that episode. we present a detailed algorithm for our implementation of bootstrapped dqn in appendix [reference]. section: related work the observation that temporally extended exploration is necessary for efficient reinforcement learning is not new. for any prior distribution over mdps, the optimal exploration strategy is available through dynamic programming in the bayesian belief state space. however, the exact solution is intractable even for very simple systems. many successful rl applications focus on generalization and planning but address exploration only via inefficient exploration or even none at all. however, such exploration strategies can be highly inefficient. many exploration strategies are guided by the principle of ''optimism in the face of uncertainty'' (ofu). these algorithms add an exploration bonus to values of state-action pairs that may lead to useful learning and select actions to maximize these adjusted values. this approach was first proposed for finite-armed bandits, but the principle has been extended successfully across bandits with generalization and tabular rl. except for particular deterministic contexts, ofu methods that lead to efficient rl in complex domains have been computationally intractable. the work of aims to add an effective bonus through a variation of dqn. the resulting algorithm relies on a large number of hand-tuned parameters and is only suitable for application to deterministic problems. we compare our results on atari to theirs in appendix [reference] and find that bootstrapped dqn offers a significant improvement over previous methods. perhaps the oldest heuristic for balancing exploration with exploitation is given by thompson sampling. this bandit algorithm takes a single sample from the posterior at every time step and chooses the action which is optimal for that time step. to apply the thompson sampling principle to rl, an agent should sample a value function from its posterior. naive applications of thompson sampling to rl which resample every timestep can be extremely inefficient. the agent must also commit to this sample for several time steps in order to achieve deep exploration. the algorithm psrl does exactly this, with state of the art guarantees. however, this algorithm still requires solving a single known mdp, which will usually be intractable for large systems. our new algorithm, bootstrapped dqn, approximates this approach to exploration via randomized value functions sampled from an approximate posterior. recently, authors have proposed the rlsvi algorithm which accomplishes this for linearly parameterized value functions. surprisingly, rlsvi recovers state of the art guarantees in the setting with tabular basis functions, but its performance is crucially dependent upon a suitable linear representation of the value function. we extend these ideas to produce an algorithm that can simultaneously perform generalization and exploration with a flexible nonlinear value function representation. our method is simple, general and compatible with almost all advances in deep rl at low computational cost and with few tuning parameters. section: deep exploration uncertainty estimates allow an agent to direct its exploration at potentially informative states and actions. in bandits, this choice of directed exploration rather than dithering generally categorizes efficient algorithms. the story in rl is not as simple, directed exploration is not enough to guarantee efficiency; the exploration must also be deep. deep exploration means exploration which is directed over multiple time steps; it can also be called ''planning to learn'' or ''far-sighted'' exploration. unlike bandit problems, which balance actions which are immediately rewarding or immediately informative, rl settings require planning over several time steps. for exploitation, this means that an efficient agent must consider the future rewards over several time steps and not simply the myopic rewards. in exactly the same way, efficient exploration may require taking actions which are neither immediately rewarding, nor immediately informative. to illustrate this distinction, consider a simple deterministic chain with three step horizon starting from state. this mdp is known to the agent a priori, with deterministic actions ''left'' and ''right''. all states have zero reward, except for the leftmost state which has known reward and the rightmost state which is unknown. in order to reach either a rewarding state or an informative state within three steps from the agent must plan a consistent strategy over several time steps. figure [reference] depicts the planning and look ahead trees for several algorithmic approaches in this example mdp. the action ''left'' is gray, the action ''right'' is black. rewarding states are depicted as red, informative states as blue. dashed lines indicate that the agent can plan ahead for either rewards or information. unlike bandit algorithms, an rl agent can plan to exploit future rewards. only an rl agent with deep exploration can plan to learn. [b] 0.99 [b] 0.23 [b] 0.23 [b] 0.23 [b] 0.23 subsection: testing for deep exploration we now present a series of didactic computational experiments designed to highlight the need for deep exploration. these environments can be described by chains of length in figure [reference]. each episode of interaction lasts steps after which point the agent resets to the initial state. these are toy problems intended to be expository rather than entirely realistic. balancing a well known and mildly successful strategy versus an unknown, but potentially more rewarding, approach can emerge in many practical applications. these environments may be described by a finite tabular mdp. however, we consider algorithms which interact with the mdp only through raw pixel features. we consider two feature mappings and in. we present results for, which worked better for all dqn variants due to better generalization, but the difference was relatively small-see appendix [reference]. thompson dqn is the same as bootstrapped dqn, but resamples every timestep. ensemble dqn uses the same architecture as bootstrapped dqn, but with an ensemble policy. we say that the algorithm has successfully learned the optimal policy when it has successfully completed one hundred episodes with optimal reward of. for each chain length, we ran each learning algorithm for 2000 episodes across three seeds. we plot the median time to learn in figure [reference], together with a conservative lower bound of on the expected time to learn for any shallow exploration strategy. only bootstrapped dqn demonstrates a graceful scaling to long chains which require deep exploration. subsection: how does bootstrapped dqn drive deep exploration? bootstrapped dqn explores in a manner similar to the provably-efficient algorithm psrl but it uses a bootstrapped neural network to approximate a posterior sample for the value. unlike psrl, bootstrapped dqn directly samples a value function and so does not require further planning steps. this algorithm is similar to rlsvi, which is also provably-efficient, but with a neural network instead of linear value function and bootstrap instead of gaussian sampling. the analysis for the linear setting suggests that this nonlinear approach will work well so long as the distribution remains stochastically optimistic, or at least as spread out as the ''correct'' posterior. bootstrapped dqn relies upon random initialization of the network weights as a prior to induce diversity. surprisingly, we found this initial diversity was enough to maintain diverse generalization to new and unseen states for large and deep neural networks. this is effective for our experimental setting, but will not work in all situations. in general it may be necessary to maintain some more rigorous notion of ''prior'', potentially through the use of artificial prior data to maintain diversity. one potential explanation for the efficacy of simple random initialization is that unlike supervised learning or bandits, where all networks fit the same data, each of our heads has a unique target network. this, together with stochastic minibatch and flexible nonlinear representations, means that even small differences at initialization may become bigger as they refit to unique td errors. bootstrapped dqn does not require that any single network is initialized to the correct policy of ''right'' at every step, which would be exponentially unlikely for large chains. for the algorithm to be successful in this example we only require that the networks generalize in a diverse way to the actions they have never chosen in the states they have not visited very often. imagine that, in the example above, the network has made it as far as state, but never observed the action right. as long as one head imagines then td bootstrapping can propagate this signal back to through the target network to drive deep exploration. the expected time for these estimates at to propagate to at least one head grows gracefully in, even for relatively small, as our experiments show. we expand upon this intuition with a video designed to highlight how bootstrapped dqn demonstrates deep exploration. we present further evaluation on a difficult stochastic mdp in appendix [reference]. section: arcade learning environment we now evaluate our algorithm across 49 atari games on the arcade learning environment. importantly, and unlike the experiments in section [reference], these domains are not specifically designed to showcase our algorithm. in fact, many atari games are structured so that small rewards always indicate part of an optimal policy. this may be crucial for the strong performance observed by dithering strategies. we find that exploration via bootstrapped dqn produces significant gains versus-greedy in this setting. bootstrapped dqn reaches peak performance roughly similar to dqn. however, our improved exploration mean we reach human performance on average 30% faster across all games. this translates to significantly improved cumulative rewards through learning. we follow the setup of for our network architecture and benchmark our performance against their algorithm. our network structure is identical to the convolutional structure of dqn except we split 10 separate bootstrap heads after the convolutional layer as per figure [reference]. recently, several authors have provided architectural and algorithmic improvements to ddqn. we do not compare our results to these since their advances are orthogonal to our concern and could easily be incorporated to our bootstrapped dqn design. full details of our experimental set up are available in appendix [reference]. subsection: implementing bootstrapped dqn at scale we now examine how to generate online bootstrap samples for dqn in a computationally efficient manner. we focus on three key questions: how many heads do we need, how should we pass gradients to the shared network and how should we bootstrap data online? we make significant compromises in order to maintain computational cost comparable to dqn. figure [reference] presents the cumulative reward of bootstrapped dqn on the game breakout, for different number of heads. more heads leads to faster learning, but even a small number of heads captures most of the benefits of bootstrapped dqn. we choose. [b] 0.4 [b] 0.4 the shared network architecture allows us to train this combined network via backpropagation. feeding network heads to the shared convolutional network effectively increases the learning rate for this portion of the network. in some games, this leads to premature and sub-optimal convergence. we found the best final scores by normalizing the gradients by, but this also leads to slower early learning. see appendix [reference] for more details. 4mu plus 2mu minus 4mu=0mu 3 mu=0mu 5mu plus 5mu=0mu to implement an online bootstrap we use an independent bernoulli mask for each head in each episode. these flags are stored in the memory replay buffer and identify which heads are trained on which data. however, when trained using a shared minibatch the algorithm will also require an effective more iterations; this is undesirable computationally. surprisingly, we found the algorithm performed similarly irrespective of and all outperformed dqn, as shown in figure [reference]. this is strange and we discuss this phenomenon in appendix [reference]. however, in light of this empirical observation for atari, we chose to save on minibatch passes. as a result bootstrapped dqn runs at similar computational speed to vanilla dqn on identical hardware. subsection: efficient exploration in atari we find that bootstrapped dqn drives efficient exploration in several atari games. for the same amount of game experience, bootstrapped dqn generally outperforms dqn with-greedy exploration. figure [reference] demonstrates this effect for a diverse selection of games. on games where dqn performs well, bootstrapped dqn typically performs better. bootstrapped dqn does not reach human performance on amidar (dqn does) but does on beam rider and battle zone (dqn does not). to summarize this improvement in learning time we consider the number of frames required to reach human performance. if bootstrapped dqn reaches human performance in frames of dqn we say it has improved by. figure [reference] shows that bootstrapped dqn typically reaches human performance significantly faster. on most games where dqn does not reach human performance, bootstrapped dqn does not solve the problem by itself. on some challenging atari games where deep exploration is conjectured to be important our results are not entirely successful, but still promising. in frostbite, bootstrapped dqn reaches the second level much faster than dqn but network instabilities cause the performance to crash. in montezuma's revenge, bootstrapped dqn reaches the first key after 20 m frames (dqn never observes a reward even after 200 m frames) but does not properly learn from this experience. our results suggest that improved exploration may help to solve these remaining games, but also highlight the importance of other problems like network instability, reward clipping and temporally extended rewards. subsection: overall performance bootstrapped dqn is able to learn much faster than dqn. figure [reference] shows that bootstrapped dqn also improves upon the final score across most games. however, the real benefits to efficient exploration mean that bootstrapped dqn outperforms dqn by orders of magnitude in terms of the cumulative rewards through learning (figure [reference]. in both figures we normalize performance relative to a fully random policy. the most similar work to ours presents several other approaches to improved exploration in atari they optimize for auc-20, a normalized version of the cumulative returns after 20 m frames. according to their metric, averaged across the 14 games they consider, we improve upon both base dqn (0.29) and their best method (0.37) to obtain 0.62 via bootstrapped dqn. we present these results together with results tables across all 49 games in appendix [reference]. subsection: visualizing bootstrapped dqn we now present some more insight to how bootstrapped dqn drives deep exploration in atari. in each game, although each head learns a high scoring policy, the policies they find are quite distinct. in the video we show the evolution of these policies simultaneously for several games. although each head performs well, they each follow a unique policy. by contrast,-greedy strategies are almost indistinguishable for small values of and totally ineffectual for larger values. we believe that this deep exploration is key to improved learning, since diverse experiences allow for better generalization. disregarding exploration, bootstrapped dqn may be beneficial as a purely exploitative policy. we can combine all the heads into a single ensemble policy, for example by choosing the action with the most votes across heads. this approach might have several benefits. first, we find that the ensemble policy can often outperform any individual policy. second, the distribution of votes across heads to give a measure of the uncertainty in the optimal policy. unlike vanilla dqn, bootstrapped dqn can know what it does n't know. in an application where executing a poorly-understood action is dangerous this could be crucial. in the video we visualize this ensemble policy across several games. we find that the uncertainty in this policy is surprisingly interpretable: all heads agree at clearly crucial decision points, but remain diverse at other less important steps. section: closing remarks in this paper we present bootstrapped dqn as an algorithm for efficient reinforcement learning in complex environments. we demonstrate that the bootstrap can produce useful uncertainty estimates for deep neural networks. bootstrapped dqn is computationally tractable and also naturally scalable to massive parallel systems. we believe that, beyond our specific implementation, randomized value functions represent a promising alternative to dithering for exploration. bootstrapped dqn practically combines efficient generalization with exploration for complex nonlinear value functions. bibliography: references appendices appendix: uncertainty for neural networks in this appendix we discuss some of the experimental setup to qualitatively evaluate uncertainty methods for deep neural networks. to do this, we generated twenty noisy regression pairs with: where are drawn uniformly from and. we set and. none of these numerical choices were important except to represent a highly nonlinear function with lots of noise and several clear regions where we should be uncertain. we present the regression data together with an indication of the generating distribution in figure [reference]. interestingly, we did not find that using dropout produced satisfying confidence intervals for this task. we present one example of this dropout posterior estimate in figure [reference]. [b] 0.45 [b] 0.45 these results are unsatisfactory for several reasons. first, the network extrapolates the mean posterior far outside the range of any actual data for. we believe this is because dropout only perturbs locally from a single neural network fit, unlike bootstrap. second, the posterior samples from the dropout approximation are very spiky and do not look like any sensible posterior sample. third, the network collapses to almost zero uncertainty in regions with data. we spent some time altering our dropout scheme to fix this effect, which might be undesirable for stochastic domains and we believed might be an artefact of our implementation. however, after further thought we believe this to be an effect which you would expect for dropout posterior approximations. in figure [reference] we present a didactic example taken from the author's website. on the right hand side of the plot we generate noisy data with wildly different values. training a neural network using mse criterion means that the network will surely converge to the mean of the noisy data. any dropout samples remain highly concentrated around this mean. by contrast, bootstrapped neural networks may include different subsets of this noisy data and so may produce a more intuitive uncertainty estimates for our settings. note this is n't necessarily a failure of dropout to approximate a gaussian process posterior, but this artefact could be shared by any homoskedastic posterior. the authors of propose a heteroskedastic variant which can help, but does not address the fundamental issue that for large networks trained to convergence all dropout samples may converge to every single datapoint\u2026 even the outliers. in this paper we focus on the bootstrap approach to uncertainty for neural networks. we like its simplicity, connections to established statistical methodology and empirical good performance. however, the key insights of this paper is the use of deep exploration via randomized value functions. this is compatible with any approximate posterior estimator for deep neural networks. we believe that this area of uncertainty estimates for neural networks remains an important area of research in its own right. bootstrapped uncertainty estimates for the q-value functions have another crucial advantage over dropout which does not appear in the supervised problem. unlike random dropout masks trained against random target networks, our implementation of bootstrap dqn trains against its own temporally consistent target network. this means that our bootstrap estimates (in the sense of), are able to ''bootstrap'' (in the td sense of) on their own estimates of the long run value. this is important to quantify the long run uncertainty over q and drive deep exploration. appendix: bootstrapped dqn implementation algorithm [reference] gives a full description of bootstrapped dqn. it captures two modes of operation where either neural networks are used to estimate the-value functions, or where one neural network with heads is used to estimate-value functions. in both cases, as this is largely a parameterisation issue, we denote the value function networks as, where is output of the th network or the th head. a core idea to the full bootstrapped dqn algorithm is the bootstrap mask. the mask decides, for each value function, whether or not it should train upon the experience generated at step. in its simplest form is a binary vector of length, masking out or including each value function for training on that time step of experience (i.e., should it receive gradients from the corresponding tuple). the masking distribution is responsible for generating each. for example, when yields whose components are independently drawn from a bernoulli distribution with parameter then this corresponds to the double-or-nothing bootstrap. on the other hand, if yields a mask with all ones, then the algorithm reduces to an ensemble method. poisson masks provides the most natural parallel with the standard non-parameteric boostrap since as. exponential masks closely resemble the standard bayesian nonparametric posterior of a dirichlet process. bootstrapped dqn [1] input: value function networks with outputs. masking distribution. let be a replay buffer storing experience for training. each episode obtain initial state from environment pick a value function to act using step until end of episode pick an action according to receive state and reward from environment, having taking action sample bootstrap mask add to replay buffer periodically, the replay buffer is played back to update the parameters of the value function network. the gradients of the th value function for the th tuple in the replay buffer, is: where is given by ([reference]). note that the mask modulates the gradient, giving rise to the bootstrap behaviour. appendix: experiments for deep exploration subsection: bootstrap methodology a naive implementation of bootstrapped dqn builds up complete networks with distinct memory buffers. this method is parallelizable up to many machines, however we wanted to produce an algorithm that was efficient even on a single machine. to do this, we implemented the bootstrap heads in a single larger network, like figure [reference] but without any shared network. we implement bootstrap by masking each episode of data according to. in figure [reference] we demonstrate that bootstrapped dqn can implement deep exploration even with relatively small values of. however, the results are more robust and scalable with larger. we run our experiments on the example from figure [reference]. surprisingly, this method is even effective with and complete data sharing between heads. this degenerate full sharing of information turns out to be remarkably efficient for training large and deep neural networks. we discuss this phenomenon more in appendix [reference]. generating good estimates for uncertainty is not enough for efficient exploration. in figure [reference] we see that other methods trained with the same network architecture are totally ineffective at implementing deep exploration. the-greedy policy follows just one-value estimate. we allow this policy to be evaluated without dithering. the ensemble policy is trained exactly as per bootstrapped dqn except at each stage the algorithm follows the policy which is majority vote of the bootstrap heads. thompson sampling is the same as bootstrapped dqn except a new head is sampled every timestep, rather than every episode. we can see that only bootstrapped dqn demonstrates efficient and deep exploration in this domain. subsection: a difficult stochastic mdp figure [reference] shows that bootstrapped dqn can implement effective (and deep) exploration where similar deep rl architectures fail. however, since the underlying system is a small and finite mdp there may be several other simpler strategies which would also solve this problem. we will now consider a difficult variant of this chain system with significant stochastic noise in transitions as depicted in figure [reference]. action ''left'' deterministically moves the agent left, but action ''right'' is only successful 50% of the time and otherwise also moves left. the agent interacts with the mdp in episodes of length and begins each episode at. once again the optimal policy is to head right. bootstrapped dqn is unique amongst scalable approaches to efficient exploration with deep rl in stochastic domains. for benchmark performance we implement three algorithms which, unlike bootstrapped dqn, will receive the true tabular representation for the mdp. these algorithms are based on three state of the art approaches to exploration via dithering (- greedy), optimism and posterior sampling. we discuss the choice of these benchmarks in appendix [reference]. [b] 0.45 [b] 0.45 in figure [reference] we present the empirical regret of each algorithm averaged over 10 seeds over the first two thousand episodes. the empirical regret is the cumulative difference between the expected rewards of the optimal policy and the realized rewards of each algorithm. we find that bootstrapped dqn achieves similar performance to state of the art efficient exploration schemes such as psrl even without prior knowledge of the tabular mdp structure and in noisy environments. most telling is how much better bootstrapped dqn does than the state of the art optimistic algorithm ucrl2. although figure [reference] seems to suggest ucrl2 incurs linear regret, actually it follows its bounds where is the number of states and is the number of actions. for the example in figure [reference] we attempted to display our performance compared to several benchmark tabula rasa approaches to exploration. there are many other algorithms we could have considered, but for a short paper we chose to focus against the most common approach (- greedy) the pre-eminent optimistic approach (ucrl2) and posterior sampling (psrl). other common heuristic approaches, such as optimistic initialization for q-learning can be tuned to work well on this domain, however the precise parameters are sensitive to the underlying mdp. to make a general-purpose version of this heuristic essentially leads to optimistic algorithms. since ucrl2 is originally designed for infinite-horizon mdps, we use the natural adaptation of this algorithm, which has state of the art guarantees in finite horizon mdps as well. figure [reference] displays the empirical regret of these algorithms together with bootstrapped dqn on the example from figure [reference]. it is somewhat disconcerting that ucrl2 appears to incur linear regret, but it is proven to satisfy near-optimal regret bounds. actually, as we show in figure [reference], the algorithm produces regret which scales very similarly to its established bounds. similarly, even for this tiny problem size, the recent analysis that proves a near optimal sample complexity in fixed horizon problems only guarantees that we will have fewer than suboptimal episodes. while these bounds may be acceptable in worst case scaling, they are not of much practical use. subsection: one-hot features in figure [reference] we include the mean performance of bootstrapped dqn with one-hot feature encodings. we found that, using these features, bootstrapped dqn learned the optimal policy for most seeds, but was somewhat less robust than the thermometer encoding. two out of ten seeds failed to learn the optimal policy within 2000 episodes, this is presented in figure [reference]. appendix: experiments for atari subsection: experimental setup we use the same 49 atari games as for our experiments. each step of the agent corresponds to four steps of the emulator, where the same action is repeated, the reward values of the agents are clipped between-1 and 1 for stability. we evaluate our agents and report performance based upon the raw scores. the convolutional part of the network used is identical to the one used in. the input to the network is 4x84x84 tensor with a rescaled, grayscale version of the last four observations. the first convolutional (conv) layer has 32 filters of size 8 with a stride of 4. the second conv layer has 64 filters of size 4 with stride 2. the last conv layer has 64 filters of size 3. we split the network beyond the final layer into distinct heads, each one is fully connected and identical to the single head of dqn. this consists of a fully connected layer to 512 units followed by another fully connected layer to the q-values for each action. the fully connected layers all use rectified linear units (relu) as a non-linearity. we normalize gradients that flow from each head. we trained the networks with rmsprop with a momentum of 0.95 and a learning rate of 0.00025 as in. the discount was set to, the number of steps between target updates was set to steps. we trained the agents for a total of 50 m steps per game, which corresponds to 200 m frames. the agents were every 1 m frames, for evaluation in bootstrapped dqn we use an ensemble voting policy. the experience replay contains the 1 m most recent transitions. we update the network every steps by randomly sampling a minibatch of transitions from the replay buffer to use the exact same minibatch schedule as dqn. for training we used an-greedy policy with being annealed linearly from to over the first 1 m timesteps. subsection: gradient normalization in bootstrap heads most literature in deep rl for atari focuses on learning the best single evaluation policy, with particular attention to whether this above or below human performance. this is unusual for the rl literature, which typically focuses upon cumulative or final performance. bootstrapped dqn makes significant improvements to the cumulative rewards of dqn on atari, as we display in figure [reference], while the peak performance is much more we found that using bootstrapped dqn without gradient normalization on each head typically learned even faster than our implementation with rescaling, but it was somewhat prone to premature and suboptimal convergence. we present an example of this phenomenon in figure [reference]. we found that, in order to better the benchmark ''best'' policies reported by dqn, it was very helpful for us to use the gradient normalization. however, it is not entirely clear whether this represents an improvement for all settings. in figures [reference] and [reference] we present the cumulative rewards of the same algorithms on beam rider. [b] 0.45 [b] 0.45 where an rl system is deployed to learn with real interactions, cumulative rewards present a better measure for performance. in these settings the benefits of gradient normalization are less clear. however, even with normalization bootstrapped dqn significantly outperforms dqn in terms of cumulative rewards. this is reflected most clearly in figure [reference] and table [reference]. subsection: sharing data in bootstrap heads in this setting all network heads share all the data, so they are not actually a traditional bootstrap at all. this is different from the regression task in section [reference], where bootstrapped data was essential to obtain meaningful uncertainty estimates. we have several theories for why the networks maintain significant diversity even without data bootstrapping in this setting. we build upon the intuition of section [reference]. first, they all train on different target networks. this means that even when facing the same datapoint this can still lead to drastically different q-value updates. second, atari is a deterministic environment, any transition observation is the unique correct datapoint for this setting. third, the networks are deep and initialized from different random values so they will likely find quite diverse generalization even when they agree on given data. finally, since all variants of dqn take many many frames to update their policy, it is likely that even using they would still populate their replay memory with identical datapoints. this means using to save on minibatch passes seems like a reasonable compromise and it does n't seem to negatively affect performance too much in this setting. more research is needed to examine exactly where/ when this data sharing is important. subsection: results tables in table [reference] the average score achieved by the agents during the most successful evaluation period, compared to human performance and a uniformly random policy. dqn is our implementation of dqn with the hyperparameters specified above, using the double q-learning update.. we find that peak final performance is similar under bootstrapped dqn to previous benchmarks. to compare the benefits of exploration via bootstrapped dqn we benchmark our performance against the most similar prior work on incentivizing exploration in atari. to do this, we compute the auc-100 measure specified in this work. we present these results in table [reference] compare to their best performing strategy as well as their implementation of dqn. importantly, bootstrapped dqn outperforms this prior work significantly. we now compare our method against the results in. in this paper they introduce a new measure of performance called auc-100, which is something similar to normalized cumulative rewards up to 20 million frames. table [reference] displays the results for our reference dqn and bootstrapped dqn as boot-dqn. we reproduce their reference results for dqn as dqn* and their best performing algorithm, dynamic ae. we also present bootstrapped dqn without head rescaling as boot-dqn+. we see that, on average, both bootstrapped dqn implementations outperform dynamic ae, the best algorithm from previous work. the only game in which dynamic ae produces best results is bowling, but this difference in bowling is dominated by the implementation of dqn* vs dqn. bootstrapped dqn still gives over 100% improvement over its relevant dqn baseline. overall it is clear that boot-dqn+ (bootstrapped dqn without rescaling) performs best in terms of auc-100 metric. averaged across the 14 games it is over 50% better than the next best competitor, which is bootstrapped dqn with gradient normalization. however, in terms of peak performance over 200 m frames boot-dqn generally reached higher scores. boot-dqn+ sometimes plateaud early as in figure [reference]. this highlights an important distinction between evaluation based on best learned policy versus cumulative rewards, as we discuss in appendix [reference]. bootstrapped dqn displays the biggest improvements over dqn when doing well during learning is important.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "atari games",
                        18159
                    ],
                    [
                        "atari",
                        21263
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "bootstrapped dqn",
                        31
                    ],
                    [
                        "bootstrapped distribution",
                        5007
                    ],
                    [
                        "bootstrapped neural network",
                        15635
                    ],
                    [
                        "bootstrapped uncertainty estimates",
                        29077
                    ],
                    [
                        "data bootstrapping",
                        41156
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "score",
                        23198
                    ],
                    [
                        "raw scores",
                        37750
                    ]
                ]
            ],
            "Task": []
        }
    ]
}
{
    "docid": "1762baa638866a13dcc6d146fd5a49b36cbd9c30-25",
    "doctext": "document: semi-supervised classification with graph convolutional networks we present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. we motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. in a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin. section: introduction we consider the problem of classifying nodes (such as documents) in a graph (such as a citation network), where labels are only available for a small subset of nodes. this problem can be framed as graph-based semi-supervised learning, where label information is smoothed over the graph via some form of explicit graph-based regularization zhu2003semi, zhou2004learning, belkin2006manifold, weston2012deep, e.g. by using a graph laplacian regularization term in the loss function: here, denotes the supervised loss w.r.t. the labeled part of the graph, can be a neural network-like differentiable function, is a weighing factor and is a matrix of node feature vectors. denotes the unnormalized graph laplacian of an undirected graph with nodes, edges, an adjacency matrix (binary or weighted) and a degree matrix. the formulation of eq. [reference] relies on the assumption that connected nodes in the graph are likely to share the same label. this assumption, however, might restrict modeling capacity, as graph edges need not necessarily encode node similarity, but could contain additional information. in this work, we encode the graph structure directly using a neural network model and train on a supervised target for all nodes with labels, thereby avoiding explicit graph-based regularization in the loss function. conditioning on the adjacency matrix of the graph will allow the model to distribute gradient information from the supervised loss and will enable it to learn representations of nodes both with and without labels. our contributions are two-fold. firstly, we introduce a simple and well-behaved layer-wise propagation rule for neural network models which operate directly on graphs and show how it can be motivated from a first-order approximation of spectral graph convolutions hammond2011wavelets. secondly, we demonstrate how this form of a graph-based neural network model can be used for fast and scalable semi-supervised classification of nodes in a graph. experiments on a number of datasets demonstrate that our model compares favorably both in classification accuracy and efficiency (measured in wall-clock time) against state-of-the-art methods for semi-supervised learning. section: fast approximate convolutions on graphs in this section, we provide theoretical motivation for a specific graph-based neural network model that we will use in the rest of this paper. we consider a multi-layer graph convolutional network (gcn) with the following layer-wise propagation rule: here, is the adjacency matrix of the undirected graph with added self-connections. is the identity matrix, and is a layer-specific trainable weight matrix. denotes an activation function, such as the. is the matrix of activations in the layer;. in the following, we show that the form of this propagation rule can be motivated via a first-order approximation of localized spectral filters on graphs hammond2011wavelets,. subsection: spectral graph convolutions we consider spectral convolutions on graphs defined as the multiplication of a signal (a scalar for every node) with a filter parameterized by in the fourier domain, i.e.: where is the matrix of eigenvectors of the normalized graph laplacian, with a diagonal matrix of its eigenvalues and being the graph fourier transform of. we can understand as a function of the eigenvalues of, i.e.. evaluating eq. [reference] is computationally expensive, as multiplication with the eigenvector matrix is. furthermore, computing the eigendecomposition of in the first place might be prohibitively expensive for large graphs. to circumvent this problem, it was suggested in that can be well-approximated by a truncated expansion in terms of chebyshev polynomials up to order: with a rescaled. denotes the largest eigenvalue of. is now a vector of chebyshev coefficients. the chebyshev polynomials are recursively defined as, with and. the reader is referred to for an in-depth discussion of this approximation. going back to our definition of a convolution of a signal with a filter, we now have: with; as can easily be verified by noticing that. note that this expression is now-localized since it is a-order polynomial in the laplacian, i.e. it depends only on nodes that are at maximum steps away from the central node (- order neighborhood). the complexity of evaluating eq. [reference] is, i.e. linear in the number of edges. use this-localized convolution to define a convolutional neural network on graphs. subsection: layer-wise linear model a neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of eq. [reference], each layer followed by a point-wise non-linearity. now, imagine we limited the layer-wise convolution operation to (see eq. [reference]), i.e. a function that is linear w.r.t. and therefore a linear function on the graph laplacian spectrum. in this way, we can still recover a rich class of convolutional filter functions by stacking multiple such layers, but we are not limited to the explicit parameterization given by, e.g., the chebyshev polynomials. we intuitively expect that such a model can alleviate the problem of overfitting on local neighborhood structures for graphs with very wide node degree distributions, such as social networks, citation networks, knowledge graphs and many other real-world graph datasets. additionally, for a fixed computational budget, this layer-wise linear formulation allows us to build deeper models, a practice that is known to improve modeling capacity on a number of domains he2015deep. in this linear formulation of a gcn we further approximate, as we can expect that neural network parameters will adapt to this change in scale during training. under these approximations eq. [reference] simplifies to: with two free parameters and. the filter parameters can be shared over the whole graph. successive application of filters of this form then effectively convolve the-order neighborhood of a node, where is the number of successive filtering operations or convolutional layers in the neural network model. in practice, it can be beneficial to constrain the number of parameters further to address overfitting and to minimize the number of operations (such as matrix multiplications) per layer. this leaves us with the following expression: with a single parameter. note that now has eigenvalues in the range. repeated application of this operator can therefore lead to numerical instabilities and exploding/ vanishing gradients when used in a deep neural network model. to alleviate this problem, we introduce the following renormalization trick:, with and. we can generalize this definition to a signal with input channels (i.e. a-dimensional feature vector for every node) and filters or feature maps as follows: where is now a matrix of filter parameters and is the convolved signal matrix. this filtering operation has complexity, as can be efficiently implemented as a product of a sparse matrix with a dense matrix. section: semi-supervised node classification having introduced a simple, yet flexible model for efficient information propagation on graphs, we can return to the problem of semi-supervised node classification. as outlined in the introduction, we can relax certain assumptions typically made in graph-based semi-supervised learning by conditioning our model both on the data and on the adjacency matrix of the underlying graph structure. we expect this setting to be especially powerful in scenarios where the adjacency matrix contains information not present in the data, such as citation links between documents in a citation network or relations in a knowledge graph. the overall model, a multi-layer gcn for semi-supervised learning, is schematically depicted in figure [reference]. [b] 0.67 [b] 0.33 subsection: example in the following, we consider a two-layer gcn for semi-supervised node classification on a graph with a symmetric adjacency matrix (binary or weighted). we first calculate in a pre-processing step. our forward model then takes the simple form: here, is an input-to-hidden weight matrix for a hidden layer with feature maps. is a hidden-to-output weight matrix. the softmax activation function, defined as with, is applied row-wise. for semi-supervised multi-class classification, we then evaluate the cross-entropy error over all labeled examples: where is the set of node indices that have labels. the neural network weights and are trained using gradient descent. in this work, we perform batch gradient descent using the full dataset for every training iteration, which is a viable option as long as datasets fit in memory. using a sparse representation for, memory requirement is, i.e. linear in the number of edges. stochasticity in the training process is introduced via dropout srivastava2014dropout. we leave memory-efficient extensions with mini-batch stochastic gradient descent for future work. subsection: implementation in practice, we make use of tensorflow tensorflow2015-whitepaper for an efficient gpu-based implementation of eq. [reference] using sparse-dense matrix multiplications. the computational complexity of evaluating eq. [reference] is then, i.e. linear in the number of graph edges. section: related work our model draws inspiration both from the field of graph-based semi-supervised learning and from recent work on neural networks that operate on graphs. in what follows, we provide a brief overview on related work in both fields. subsection: graph-based semi-supervised learning a large number of approaches for semi-supervised learning using graph representations have been proposed in recent years, most of which fall into two broad categories: methods that use some form of explicit graph laplacian regularization and graph embedding-based approaches. prominent examples for graph laplacian regularization include label propagation zhu2003semi, manifold regularization belkin2006manifold and deep semi-supervised embedding weston2012deep. recently, attention has shifted to models that learn graph embeddings with methods inspired by the skip-gram model mikolov2013distributed. deepwalk perozzi2014deepwalk learns embeddings via the prediction of the local neighborhood of nodes, sampled from random walks on the graph. line tang2015line and node2vec grovernode2vec extend deepwalk with more sophisticated random walk or breadth-first search schemes. for all these methods, however, a multi-step pipeline including random walk generation and semi-supervised training is required where each step has to be optimized separately. planetoid yang2016revisiting alleviates this by injecting label information in the process of learning embeddings. subsection: neural networks on graphs neural networks that operate on graphs have previously been introduced in as a form of recurrent neural network. their framework requires the repeated application of contraction maps as propagation functions until node representations reach a stable fixed point. this restriction was later alleviated in by introducing modern practices for recurrent neural network training to the original graph neural network framework. introduced a convolution-like propagation rule on graphs and methods for graph-level classification. their approach requires to learn node degree-specific weight matrices which does not scale to large graphs with wide node degree distributions. our model instead uses a single weight matrix per layer and deals with varying node degrees through an appropriate normalization of the adjacency matrix (see section [reference]). a related approach to node classification with a graph-based neural network was recently introduced in. they report complexity, limiting the range of possible applications. in a different yet related model, convert graphs locally into sequences that are fed into a conventional 1d convolutional neural network, which requires the definition of a node ordering in a pre-processing step. our method is based on spectral graph convolutional neural networks, introduced in and later extended by with fast localized convolutions. in contrast to these works, we consider here the task of transductive node classification within networks of significantly larger scale. we show that in this setting, a number of simplifications (see section [reference]) can be introduced to the original frameworks of and that improve scalability and classification performance in large-scale networks. section: experiments we test our model in a number of experiments: semi-supervised document classification in citation networks, semi-supervised entity classification in a bipartite graph extracted from a knowledge graph, an evaluation of various graph propagation models and a run-time analysis on random graphs. subsection: datasets we closely follow the experimental setup in. dataset statistics are summarized in table [reference]. in the citation network datasets\u2014 citeseer, cora and pubmed sen2008collective\u2014 nodes are documents and edges are citation links. label rate denotes the number of labeled nodes that are used for training divided by the total number of nodes in each dataset. nell carlson2010toward, yang2016revisiting is a bipartite graph dataset extracted from a knowledge graph with 55, 864 relation nodes and 9, 891 entity nodes. paragraph: citation networks we consider three citation network datasets: citeseer, cora and pubmed sen2008collective. the datasets contain sparse bag-of-words feature vectors for each document and a list of citation links between documents. we treat the citation links as (undirected) edges and construct a binary, symmetric adjacency matrix. each document has a class label. for training, we only use 20 labels per class, but all feature vectors. paragraph: nell nell is a dataset extracted from the knowledge graph introduced in carlson2010toward. a knowledge graph is a set of entities connected with directed, labeled edges (relations). we follow the pre-processing scheme as described in. we assign separate relation nodes and for each entity pair as and. entity nodes are described by sparse feature vectors. we extend the number of features in nell by assigning a unique one-hot representation for every relation node, effectively resulting in a 61, 278-dim sparse feature vector per node. the semi-supervised task here considers the extreme case of only a single labeled example per class in the training set. we construct a binary, symmetric adjacency matrix from this graph by setting entries, if one or more edges are present between nodes and. paragraph: random graphs we simulate random graph datasets of various sizes for experiments where we measure training time per epoch. for a dataset with nodes we create a random graph assigning edges uniformly at random. we take the identity matrix as input feature matrix, thereby implicitly taking a featureless approach where the model is only informed about the identity of each node, specified by a unique one-hot vector. we add dummy labels for every node. subsection: experimental set-up unless otherwise noted, we train a two-layer gcn as described in section [reference] and evaluate prediction accuracy on a test set of 1, 000 labeled examples. we provide additional experiments using deeper models with up to 10 layers in appendix [reference]. we choose the same dataset splits as in with an additional validation set of 500 labeled examples for hyperparameter optimization (dropout rate for all layers, l2 regularization factor for the first gcn layer and number of hidden units). we do not use the validation set labels for training. for the citation network datasets, we optimize hyperparameters on cora only and use the same set of parameters for citeseer and pubmed. we train all models for a maximum of 200 epochs (training iterations) using adam kingma2014adam with a learning rate of and early stopping with a window size of, i.e. we stop training if the validation loss does not decrease for 10 consecutive epochs. we initialize weights using the initialization described in and accordingly (row -) normalize input feature vectors. on the random graph datasets, we use a hidden layer size of 32 units and omit regularization (i.e. neither dropout nor l2 regularization). subsection: baselines we compare against the same baseline methods as in, i.e. label propagation (lp) zhu2003semi, semi-supervised embedding (semiemb) weston2012deep, manifold regularization (manireg) belkin2006manifold and skip-gram based graph embeddings (deepwalk) perozzi2014deepwalk. we omit tsvm joachims1999transductive, as it does not scale to the large number of classes in one of our datasets. we further compare against the iterative classification algorithm (ica) proposed in in conjunction with two logistic regression classifiers, one for local node features alone and one for relational classification using local features and an aggregation operator as described in. we first train the local classifier using all labeled training set nodes and use it to bootstrap class labels of unlabeled nodes for relational classifier training. we run iterative classification (relational classifier) with a random node ordering for 10 iterations on all unlabeled nodes (bootstrapped using the local classifier). l2 regularization parameter and aggregation operator (count vs. prop, see) are chosen based on validation set performance for each dataset separately. lastly, we compare against planetoid yang2016revisiting, where we always choose their best-performing model variant (transductive vs. inductive) as a baseline. section: results subsection: semi-supervised node classification results are summarized in table [reference]. reported numbers denote classification accuracy in percent. for ica, we report the mean accuracy of 100 runs with random node orderings. results for all other baseline methods are taken from the planetoid paper yang2016revisiting. planetoid* denotes the best model for the respective dataset out of the variants presented in their paper. we further report wall-clock training time in seconds until convergence (in brackets) for our method (incl. evaluation of validation error) and for planetoid. for the latter, we used an implementation provided by the authors and trained on the same hardware (with gpu) as our gcn model. we trained and tested our model on the same dataset splits as in and report mean accuracy of 100 runs with random weight initializations. we used the following sets of hyperparameters for citeseer, cora and pubmed: 0.5 (dropout rate), (l2 regularization) and (number of hidden units); and for nell: 0.1 (dropout rate), (l2 regularization) and (number of hidden units). in addition, we report performance of our model on 10 randomly drawn dataset splits of the same size as in, denoted by gcn (rand. splits). here, we report mean and standard error of prediction accuracy on the test set split in percent. subsection: evaluation of propagation model we compare different variants of our proposed per-layer propagation model on the citation network datasets. we follow the experimental set-up described in the previous section. results are summarized in table [reference]. the propagation model of our original gcn model is denoted by renormalization trick (in bold). in all other cases, the propagation model of both neural network layers is replaced with the model specified under propagation model. reported numbers denote mean classification accuracy for 100 repeated runs with random weight matrix initializations. in case of multiple variables per layer, we impose l2 regularization on all weight matrices of the first layer. subsection: training time per epoch here, we report results for the mean training time per epoch (forward pass, cross-entropy calculation, backward pass) for 100 epochs on simulated random graphs, measured in seconds wall-clock time. see section [reference] for a detailed description of the random graph dataset used in these experiments. we compare results on a gpu and on a cpu-only implementation in tensorflow tensorflow2015-whitepaper. figure [reference] summarizes the results. section: discussion subsection: semi-supervised model in the experiments demonstrated here, our method for semi-supervised node classification outperforms recent related methods by a significant margin. methods based on graph-laplacian regularization zhu2003semi, belkin2006manifold, weston2012deep are most likely limited due to their assumption that edges encode mere similarity of nodes. skip-gram based methods on the other hand are limited by the fact that they are based on a multi-step pipeline which is difficult to optimize. our proposed model can overcome both limitations, while still comparing favorably in terms of efficiency (measured in wall-clock time) to related methods. propagation of feature information from neighboring nodes in every layer improves classification performance in comparison to methods like ica lu2003link, where only label information is aggregated. we have further demonstrated that the proposed renormalized propagation model (eq. [reference]) offers both improved efficiency (fewer parameters and operations, such as multiplication or addition) and better predictive performance on a number of datasets compared to a na\u00efve-order model (eq. [reference]) or higher-order graph convolutional models using chebyshev polynomials (eq. [reference]). subsection: limitations and future work here, we describe several limitations of our current model and outline how these might be overcome in future work. paragraph: memory requirement in the current setup with full-batch gradient descent, memory requirement grows linearly in the size of the dataset. we have shown that for large graphs that do not fit in gpu memory, training on cpu can still be a viable option. mini-batch stochastic gradient descent can alleviate this issue. the procedure of generating mini-batches, however, should take into account the number of layers in the gcn model, as the-order neighborhood for a gcn with layers has to be stored in memory for an exact procedure. for very large and densely connected graph datasets, further approximations might be necessary. paragraph: directed edges and edge features our framework currently does not naturally support edge features and is limited to undirected graphs (weighted or unweighted). results on nell however show that it is possible to handle both directed edges and edge features by representing the original directed graph as an undirected bipartite graph with additional nodes that represent edges in the original graph (see section [reference] for details). paragraph: limiting assumptions through the approximations introduced in section [reference], we implicitly assume locality (dependence on the-order neighborhood for a gcn with layers) and equal importance of self-connections vs. edges to neighboring nodes. for some datasets, however, it might be beneficial to introduce a trade-off parameter in the definition of: this parameter now plays a similar role as the trade-off parameter between supervised and unsupervised loss in the typical semi-supervised setting (see eq. [reference]). here, however, it can be learned via gradient descent. section: conclusion we have introduced a novel approach for semi-supervised classification on graph-structured data. our gcn model uses an efficient layer-wise propagation rule that is based on a first-order approximation of spectral convolutions on graphs. experiments on a number of network datasets suggest that the proposed gcn model is capable of encoding both graph structure and node features in a way useful for semi-supervised classification. in this setting, our model outperforms several recently proposed methods by a significant margin, while being computationally efficient. subsubsection: acknowledgments we would like to thank christos louizos, taco cohen, joan bruna, zhilin yang, dave herman, pramod sinha and abdul-saboor sheikh for helpful discussions. this research was funded by sap. bibliography: references appendix: relation to weisfeiler-lehman algorithm a neural network model for graph-structured data should ideally be able to learn representations of nodes in a graph, taking both the graph structure and feature description of nodes into account. a well-studied framework for the unique assignment of node labels given a graph and (optionally) discrete initial node labels is provided by the 1-dim weisfeiler-lehman (wl-1) algorithm weisfeiler1968reduction: [h] initial node coloring final node coloring t 0 stable node coloring is reached wl-1 algorithm here, denotes the coloring (label assignment) of node (at iteration) and is its set of neighboring node indices (irrespective of whether the graph includes self-connections for every node or not). is a hash function. for an in-depth mathematical discussion of the wl-1 algorithm see, e.g.,. we can replace the hash function in algorithm [reference] with a neural network layer-like differentiable function with trainable parameters as follows: where is an appropriately chosen normalization constant for the edge. further, we can take now to be a vector of activations of node in the neural network layer. is a layer-specific weight matrix and denotes a differentiable, non-linear activation function. by choosing, where denotes the degree of node, we recover the propagation rule of our graph convolutional network (gcn) model in vector form (see eq. [reference]). this\u2014 loosely speaking\u2014 allows us to interpret our gcn model as a differentiable and parameterized generalization of the 1-dim weisfeiler-lehman algorithm on graphs. subsection: node embeddings with random weights from the analogy with the weisfeiler-lehman algorithm, we can understand that even an untrained gcn model with random weights can serve as a powerful feature extractor for nodes in a graph. as an example, consider the following 3-layer gcn model: with weight matrices initialized at random using the initialization described in., and are defined as in section [reference]. we apply this model on zachary's karate club network zachary1977information. this graph contains 34 nodes, connected by 154 (undirected and unweighted) edges. every node is labeled by one of four classes, obtained via modularity-based clustering brandes2008modularity. see figure [reference] for an illustration. [b] 0.5 [b] 0.5 we take a featureless approach by setting, where is the by identity matrix. is the number of nodes in the graph. note that nodes are randomly ordered (i.e. ordering contains no information). furthermore, we choose a hidden layer dimensionality of and a two-dimensional output (so that the output can immediately be visualized in a 2-dim plot). figure [reference] shows a representative example of node embeddings (outputs) obtained from an untrained gcn model applied to the karate club network. these results are comparable to embeddings obtained from deepwalk perozzi2014deepwalk, which uses a more expensive unsupervised training procedure. subsection: semi-supervised node embeddings on this simple example of a gcn applied to the karate club network it is interesting to observe how embeddings react during training on a semi-supervised classification task. such a visualization (see figure [reference]) provides insights into how the gcn model can make use of the graph structure (and of features extracted from the graph structure at later layers) to learn embeddings that are useful for a classification task. we consider the following semi-supervised learning setup: we add a softmax layer on top of our model (eq. [reference]) and train using only a single labeled example per class (i.e. a total number of 4 labeled nodes). we train for 300 training iterations using adam kingma2014adam with a learning rate of on a cross-entropy loss. figure [reference] shows the evolution of node embeddings over a number of training iterations. the model succeeds in linearly separating the communities based on minimal supervision and the graph structure alone. a video of the full training process can be found on our website. [b] 0.5 [b] 0.5 [b] 0.5 [b] 0.5 [b] 0.5 [b] 0.5 appendix: experiments on model depth in these experiments, we investigate the influence of model depth (number of layers) on classification performance. we report results on a 5-fold cross-validation experiment on the cora, citeseer and pubmed datasets sen2008collective using all labels. in addition to the standard gcn model (eq. [reference]), we report results on a model variant where we use residual connections he2015deep between hidden layers to facilitate training of deeper models by enabling the model to carry over information from the previous layer's input: on each cross-validation split, we train for 400 epochs (without early stopping) using the adam optimizer kingma2014adam with a learning rate of. other hyperparameters are chosen as follows: 0.5 (dropout rate, first and last layer), (l2 regularization, first layer), 16 (number of units for each hidden layer) and 0.01 (learning rate). results are summarized in figure [reference]. [b] 0.33 [b] 0.33 [b] 0.33 for the datasets considered here, best results are obtained with a 2-or 3-layer model. we observe that for models deeper than 7 layers, training without the use of residual connections can become difficult, as the effective context size for each node increases by the size of its-order neighborhood (for a model with layers) with each additional layer. furthermore, overfitting can become an issue as the number of parameters increases with model depth.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "citeseer",
                        13699
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "graph convolutional network",
                        3166
                    ],
                    [
                        "gcn",
                        3196
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        2816
                    ],
                    [
                        "prediction accuracy",
                        15930
                    ],
                    [
                        "accuracy",
                        18553
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semi-supervised node classification",
                        7772
                    ],
                    [
                        "semi-supervised multi-class classification",
                        9023
                    ],
                    [
                        "node classification",
                        12372
                    ],
                    [
                        "transductive node classification",
                        12932
                    ],
                    [
                        "classification",
                        13177
                    ],
                    [
                        "semi-supervised entity classification",
                        13358
                    ],
                    [
                        "relational classification",
                        17619
                    ],
                    [
                        "semi-supervised classification",
                        24079
                    ],
                    [
                        "semi-supervised classification task",
                        28013
                    ],
                    [
                        "classification task",
                        28284
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "cora",
                        13709
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "graph convolutional networks",
                        46
                    ],
                    [
                        "spectral graph convolutional neural networks",
                        12759
                    ],
                    [
                        "higher-order graph convolutional models",
                        22102
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        2816
                    ],
                    [
                        "prediction accuracy",
                        15930
                    ],
                    [
                        "accuracy",
                        18553
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semi-supervised document classification",
                        13296
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "cora",
                        13709
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "graph convolutional network",
                        3166
                    ],
                    [
                        "gcn",
                        3196
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        2816
                    ],
                    [
                        "prediction accuracy",
                        15930
                    ],
                    [
                        "accuracy",
                        18553
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semi-supervised node classification",
                        7772
                    ],
                    [
                        "semi-supervised multi-class classification",
                        9023
                    ],
                    [
                        "node classification",
                        12372
                    ],
                    [
                        "transductive node classification",
                        12932
                    ],
                    [
                        "classification",
                        13177
                    ],
                    [
                        "semi-supervised entity classification",
                        13358
                    ],
                    [
                        "relational classification",
                        17619
                    ],
                    [
                        "semi-supervised classification",
                        24079
                    ],
                    [
                        "semi-supervised classification task",
                        28013
                    ],
                    [
                        "classification task",
                        28284
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "nell",
                        14540
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "graph convolutional network",
                        3166
                    ],
                    [
                        "gcn",
                        3196
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        2816
                    ],
                    [
                        "prediction accuracy",
                        15930
                    ],
                    [
                        "accuracy",
                        18553
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semi-supervised node classification",
                        7772
                    ],
                    [
                        "semi-supervised multi-class classification",
                        9023
                    ],
                    [
                        "node classification",
                        12372
                    ],
                    [
                        "transductive node classification",
                        12932
                    ],
                    [
                        "classification",
                        13177
                    ],
                    [
                        "semi-supervised entity classification",
                        13358
                    ],
                    [
                        "relational classification",
                        17619
                    ],
                    [
                        "semi-supervised classification",
                        24079
                    ],
                    [
                        "semi-supervised classification task",
                        28013
                    ],
                    [
                        "classification task",
                        28284
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "pubmed",
                        13718
                    ],
                    [
                        "pubmed datasets",
                        29199
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "graph convolutional network",
                        3166
                    ],
                    [
                        "gcn",
                        3196
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        2816
                    ],
                    [
                        "prediction accuracy",
                        15930
                    ],
                    [
                        "accuracy",
                        18553
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semi-supervised node classification",
                        7772
                    ],
                    [
                        "semi-supervised multi-class classification",
                        9023
                    ],
                    [
                        "node classification",
                        12372
                    ],
                    [
                        "transductive node classification",
                        12932
                    ],
                    [
                        "classification",
                        13177
                    ],
                    [
                        "semi-supervised entity classification",
                        13358
                    ],
                    [
                        "relational classification",
                        17619
                    ],
                    [
                        "semi-supervised classification",
                        24079
                    ],
                    [
                        "semi-supervised classification task",
                        28013
                    ],
                    [
                        "classification task",
                        28284
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "1827de6fa9c9c1b3d647a9d707042e89cf94abf0-26",
    "doctext": "document: batch normalization: accelerating deep network training by reducing internal covariate shift training deep neural networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. this slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. we refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. batch normalization allows us to use much higher learning rates and be less careful about initialization. it also acts as a regularizer, in some cases eliminating the need for dropout. applied to a state-of-the-art image classification model, batch normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. using an ensemble of batch-normalized networks, we improve upon the best published result on imagenet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters. section: introduction deep learning has dramatically advanced the state of the art in vision, speech, and many other areas. stochastic gradient descent (sgd) has proved to be an effective way of training deep networks, and sgd variants such as momentum and adagrad have been used to achieve state of the art performance. sgd optimizes the parameters of the network, so as to minimize the loss where is the training data set. with sgd, the training proceeds in steps, and at each step we consider a mini-batch of size. the mini-batch is used to approximate the gradient of the loss function with respect to the parameters, by computing using mini-batches of examples, as opposed to one example at a time, is helpful in several ways. first, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. second, computation over a batch can be much more efficient than computations for individual examples, due to the parallelism afforded by the modern computing platforms. while stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. the training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers- so that small changes to the network parameters amplify as the network becomes deeper. the change in the distributions of layers' inputs presents a problem because the layers need to continuously adapt to the new distribution. when the input distribution to a learning system changes, it is said to experience covariate shift. this is typically handled via domain adaptation. however, the notion of covariate shift can be extended beyond the learning system as a whole, to apply to its parts, such as a sub-network or a layer. consider a network computing where and are arbitrary transformations, and the parameters are to be learned so as to minimize the loss. learning can be viewed as if the inputs are fed into the sub-network for example, a gradient descent step (for batch size and learning rate) is exactly equivalent to that for a stand-alone network with input. therefore, the input distribution properties that make training more efficient- such as having the same distribution between the training and test data- apply to training the sub-network as well. as such it is advantageous for the distribution of to remain fixed over time. then, does not have to readjust to compensate for the change in the distribution of. fixed distribution of inputs to a sub-network would have positive consequences for the layers outside the sub-network, as well. consider a layer with a sigmoid activation function where is the layer input, the weight matrix and bias vector are the layer parameters to be learned, and. as increases, tends to zero. this means that for all dimensions of except those with small absolute values, the gradient flowing down to will vanish and the model will train slowly. however, since is affected by and the parameters of all the layers below, changes to those parameters during training will likely move many dimensions of into the saturated regime of the nonlinearity and slow down the convergence. this effect is amplified as the network depth increases. in practice, the saturation problem and the resulting vanishing gradients are usually addressed by using rectified linear units, careful initialization, and small learning rates. if, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate. we refer to the change in the distributions of internal nodes of a deep network, in the course of training, as internal covariate shift. eliminating it offers a promise of faster training. we propose a new mechanism, which we call batch normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. it accomplishes this via a normalization step that fixes the means and variances of layer inputs. batch normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. this allows us to use much higher learning rates without the risk of divergence. furthermore, batch normalization regularizes the model and reduces the need for dropout. finally, batch normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes. in sec. [reference], we apply batch normalization to the best-performing imagenet classification network, and show that we can match its performance using only 7% of the training steps, and can further exceed its accuracy by a substantial margin. using an ensemble of such networks trained with batch normalization, we achieve the top-5 error rate that improves upon the best known results on imagenet classification. section: towards reducing internal covariate shift we define internal covariate shift as the change in the distribution of network activations due to the change in network parameters during training. to improve the training, we seek to reduce the internal covariate shift. by fixing the distribution of the layer inputs as the training progresses, we expect to improve the training speed. it has been long known that the network training converges faster if its inputs are whitened- i.e., linearly transformed to have zero means and unit variances, and decorrelated. as each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer. by whitening the inputs to each layer, we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift. we could consider whitening activations at every training step or at some interval, either by modifying the network directly or by changing the parameters of the optimization algorithm to depend on the network activation values. however, if these modifications are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect of the gradient step. for example, consider a layer with the input that adds the learned bias, and normalizes the result by subtracting the mean of the activation computed over the training data: where, is the set of values of over the training set, and. if a gradient descent step ignores the dependence of on, then it will update, where. then. thus, the combination of the update to and subsequent change in normalization led to no change in the output of the layer nor, consequently, the loss. as the training continues, will grow indefinitely while the loss remains fixed. this problem can get worse if the normalization not only centers but also scales the activations. we have observed this empirically in initial experiments, where the model blows up when the normalization parameters are computed outside the gradient descent step. the issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place. to address this issue, we would like to ensure that, for any parameter values, the network always produces activations with the desired distribution. doing so would allow the gradient of the loss with respect to the model parameters to account for the normalization, and for its dependence on the model parameters. let again be a layer input, treated as a vector, and be the set of these inputs over the training data set. the normalization can then be written as a transformation which depends not only on the given training example but on all examples- each of which depends on if is generated by another layer. for backpropagation, we would need to compute the jacobians ignoring the latter term would lead to the explosion described above. within this framework, whitening the layer inputs is expensive, as it requires computing the covariance matrix and its inverse square root, to produce the whitened activations, as well as the derivatives of these transforms for backpropagation. this motivates us to seek an alternative that performs input normalization in a way that is differentiable and does not require the analysis of the entire training set after every parameter update. some of the previous approaches (e.g.) use statistics computed over a single training example, or, in the case of image networks, over different feature maps at a given location. however, this changes the representation ability of a network by discarding the absolute scale of activations. we want to a preserve the information in the network, by normalizing the activations in a training example relative to the statistics of the entire training data. section: normalization via mini-batch statistics since the full whitening of each layer's inputs is costly and not everywhere differentiable, we make two necessary simplifications. the first is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently, by making it have the mean of zero and the variance of 1. for a layer with-dimensional input, we will normalize each dimension where the expectation and variance are computed over the training data set. as shown in, such normalization speeds up convergence, even when the features are not decorrelated. note that simply normalizing each input of a layer may change what the layer can represent. for instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. to address this, we make sure that the transformation inserted in the network can represent the identity transform. to accomplish this, we introduce, for each activation, a pair of parameters, which scale and shift the normalized value: these parameters are learned along with the original model parameters, and restore the representation power of the network. indeed, by setting and, we could recover the original activations, if that were the optimal thing to do. in the batch setting where each training step is based on the entire training set, we would use the whole set to normalize activations. however, this is impractical when using stochastic optimization. therefore, we make the second simplification: since we use mini-batches in stochastic gradient training, each mini-batch produces estimates of the mean and variance of each activation. this way, the statistics used for normalization can fully participate in the gradient backpropagation. note that the use of mini-batches is enabled by computation of per-dimension variances rather than joint covariances; in the joint case, regularization would be required since the mini-batch size is likely to be smaller than the number of activations being whitened, resulting in singular covariance matrices. consider a mini-batch of size. since the normalization is applied to each activation independently, let us focus on a particular activation and omit for clarity. we have values of this activation in the mini-batch, let the normalized values be, and their linear transformations be. we refer to the transform as the batch normalizing transform. we present the bn transform in algorithm [reference]. in the algorithm, is a constant added to the mini-batch variance for numerical stability. batch normalizing transform, applied to activation x over a mini-batch. the bn transform can be added to a network to manipulate any activation. in the notation, we indicate that the parameters and are to be learned, but it should be noted that the bn transform does not independently process the activation in each training example. rather, depends both on the training example and the other examples in the mini-batch. the scaled and shifted values are passed to other network layers. the normalized activations are internal to our transformation, but their presence is crucial. the distributions of values of any has the expected value of and the variance of, as long as the elements of each mini-batch are sampled from the same distribution, and if we neglect. this can be seen by observing that and, and taking expectations. each normalized activation can be viewed as an input to a sub-network composed of the linear transform, followed by the other processing done by the original network. these sub-network inputs all have fixed means and variances, and although the joint distribution of these normalized can change over the course of training, we expect that the introduction of normalized inputs accelerates the training of the sub-network and, consequently, the network as a whole. during training we need to backpropagate the gradient of loss through this transformation, as well as compute the gradients with respect to the parameters of the bn transform. we use chain rule, as follows (before simplification): thus, bn transform is a differentiable transformation that introduces normalized activations into the network. this ensures that as the model is training, layers can continue learning on input distributions that exhibit less internal covariate shift, thus accelerating the training. furthermore, the learned affine transform applied to these normalized activations allows the bn transform to represent the identity transformation and preserves the network capacity. subsection: training and inference with batch-normalized networks to batch-normalize a network, we specify a subset of activations and insert the bn transform for each of them, according to alg. [reference]. any layer that previously received as the input, now receives. a model employing batch normalization can be trained using batch gradient descent, or stochastic gradient descent with a mini-batch size, or with any of its variants such as adagrad. the normalization of activations that depends on the mini-batch allows efficient training, but is neither necessary nor desirable during inference; we want the output to depend only on the input, deterministically. for this, once the network has been trained, we use the normalization using the population, rather than mini-batch, statistics. neglecting, these normalized activations have the same mean 0 and variance 1 as during training. we use the unbiased variance estimate, where the expectation is over training mini-batches of size and are their sample variances. using moving averages instead, we can track the accuracy of a model as it trains. since the means and variances are fixed during inference, the normalization is simply a linear transform applied to each activation. it may further be composed with the scaling by and shift by, to yield a single linear transform that replaces. algorithm [reference] summarizes the procedure for training batch-normalized networks. training a batch-normalized network [1] batch-normalized network for inference, training bn network add transformation to (alg. [reference]) modify each layer in with input to take instead train to optimize the parameters for clarity,, etc. process multiple training mini-batches, each of size, and average over them: in, replace the transform with subsection: batch-normalized convolutional networks batch normalization can be applied to any set of activations in the network. here, we focus on transforms that consist of an affine transformation followed by an element-wise nonlinearity: where and are learned parameters of the model, and is the nonlinearity such as sigmoid or relu. this formulation covers both fully-connected and convolutional layers. we add the bn transform immediately before the nonlinearity, by normalizing. we could have also normalized the layer inputs, but since is likely the output of another nonlinearity, the shape of its distribution is likely to change during training, and constraining its first and second moments would not eliminate the covariate shift. in contrast, is more likely to have a symmetric, non-sparse distribution, that is\" more gaussian\"; normalizing it is likely to produce activations with a stable distribution. note that, since we normalize, the bias can be ignored since its effect will be canceled by the subsequent mean subtraction (the role of the bias is subsumed by in alg. [reference]). thus, is replaced with where the bn transform is applied independently to each dimension of, with a separate pair of learned parameters, per dimension. for convolutional layers, we additionally want the normalization to obey the convolutional property- so that different elements of the same feature map, at different locations, are normalized in the same way. to achieve this, we jointly normalize all the activations in a mini-batch, over all locations. in alg. [reference], we let be the set of all values in a feature map across both the elements of a mini-batch and spatial locations- so for a mini-batch of size and feature maps of size, we use the effective mini-batch of size. we learn a pair of parameters and per feature map, rather than per activation. alg. [reference] is modified similarly, so that during inference the bn transform applies the same linear transformation to each activation in a given feature map. subsection: batch normalization enables higher learning rates in traditional deep networks, too-high learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. batch normalization helps address these issues. by normalizing activations throughout the network, it prevents small changes to the parameters from amplifying into larger and suboptimal changes in activations in gradients; for instance, it prevents the training from getting stuck in the saturated regimes of nonlinearities. batch normalization also makes training more resilient to the parameter scale. normally, large learning rates may increase the scale of layer parameters, which then amplify the gradient during backpropagation and lead to the model explosion. however, with batch normalization, backpropagation through a layer is unaffected by the scale of its parameters. indeed, for a scalar, and we can show that the scale does not affect the layer jacobian nor, consequently, the gradient propagation. moreover, larger weights lead to smaller gradients, and batch normalization will stabilize the parameter growth. we further conjecture that batch normalization may lead the layer jacobians to have singular values close to 1, which is known to be beneficial for training. consider two consecutive layers with normalized inputs, and the transformation between these normalized vectors:. if we assume that and are gaussian and uncorrelated, and that is a linear transformation for the given model parameters, then both and have unit covariances, and. thus,, and so all singular values of are equal to 1, which preserves the gradient magnitudes during backpropagation. in reality, the transformation is not linear, and the normalized values are not guaranteed to be gaussian nor independent, but we nevertheless expect batch normalization to help make gradient propagation better behaved. the precise effect of batch normalization on gradient propagation remains an area of further study. subsection: batch normalization regularizes the model when training with batch normalization, a training example is seen in conjunction with other examples in the mini-batch, and the training network no longer producing deterministic values for a given training example. in our experiments, we found this effect to be advantageous to the generalization of the network. whereas dropout is typically used to reduce overfitting, in a batch-normalized network we found that it can be either removed or reduced in strength. section: experiments subsection: activations over time to verify the effects of internal covariate shift on training, and the ability of batch normalization to combat it, we considered the problem of predicting the digit class on the mnist dataset. we used a very simple network, with a 28x28 binary image as input, and 3 fully-connected hidden layers with 100 activations each. each hidden layer computes with sigmoid nonlinearity, and the weights initialized to small random gaussian values. the last hidden layer is followed by a fully-connected layer with 10 activations (one per class) and cross-entropy loss. we trained the network for 50000 steps, with 60 examples per mini-batch. we added batch normalization to each hidden layer of the network, as in sec. [reference]. we were interested in the comparison between the baseline and batch-normalized networks, rather than achieving the state of the art performance on mnist (which the described architecture does not). figure [reference] (a) shows the fraction of correct predictions by the two networks on held-out test data, as training progresses. the batch-normalized network enjoys the higher test accuracy. to investigate why, we studied inputs to the sigmoid, in the original network n and batch-normalized network (alg. [reference]) over the course of training. in fig. [reference] (b, c) we show, for one typical activation from the last hidden layer of each network, how its distribution evolves. the distributions in the original network change significantly over time, both in their mean and the variance, which complicates the training of the subsequent layers. in contrast, the distributions in the batch-normalized network are much more stable as training progresses, which aids the training. subsection: imagenet classification we applied batch normalization to a new variant of the inception network, trained on the imagenet classification task. the network has a large number of convolutional and pooling layers, with a softmax layer to predict the image class, out of 1000 possibilities. convolutional layers use relu as the nonlinearity. the main difference to the network described in is that the convolutional layers are replaced by two consecutive layers of convolutions with up to filters. the network contains parameters, and, other than the top softmax layer, has no fully-connected layers. more details are given in the appendix. we refer to this model as inception in the rest of the text. the model was trained using a version of stochastic gradient descent with momentum, using the mini-batch size of 32. the training was performed using a large-scale, distributed architecture (similar to). all networks are evaluated as training progresses by computing the validation accuracy, i.e. the probability of predicting the correct label out of 1000 possibilities, on a held-out set, using a single crop per image. in our experiments, we evaluated several modifications of inception with batch normalization. in all cases, batch normalization was applied to the input of each nonlinearity, in a convolutional way, as described in section [reference], while keeping the rest of the architecture constant. subsubsection: accelerating bn networks simply adding batch normalization to a network does not take full advantage of our method. to do so, we further changed the network and its training parameters, as follows: increase learning rate. in a batch-normalized model, we have been able to achieve a training speedup from higher learning rates, with no ill side effects (sec. [reference]). remove dropout. as described in sec. [reference], batch normalization fulfills some of the same goals as dropout. removing dropout from modified bn-inception speeds up training, without increasing overfitting. reduce the l2 weight regularization. while in inception an loss on the model parameters controls overfitting, in modified bn-inception the weight of this loss is reduced by a factor of 5. we find that this improves the accuracy on the held-out validation data. accelerate the learning rate decay. in training inception, learning rate was decayed exponentially. because our network trains faster than inception, we lower the learning rate 6 times faster. remove local response normalization while inception and other networks benefit from it, we found that with batch normalization it is not necessary. shuffle training examples more thoroughly. we enabled within-shard shuffling of the training data, which prevents the same examples from always appearing in a mini-batch together. this led to about 1% improvements in the validation accuracy, which is consistent with the view of batch normalization as a regularizer (sec. [reference]): the randomization inherent in our method should be most beneficial when it affects an example differently each time it is seen. reduce the photometric distortions. because batch-normalized networks train faster and observe each training example fewer times, we let the trainer focus on more\" real\" images by distorting them less. subsubsection: single-network classification we evaluated the following networks, all trained on the lsvrc2012 training data, and tested on the validation data: inception: the network described at the beginning of section [reference], trained with the initial learning rate of 0.0015. bn-baseline: same as inception with batch normalization before each nonlinearity. bn-x5: inception with batch normalization and the modifications in sec. [reference]. the initial learning rate was increased by a factor of 5, to 0.0075. the same learning rate increase with original inception caused the model parameters to reach machine infinity. bn-x30: like bn-x5, but with the initial learning rate 0.045 (30 times that of inception). bn-x5-sigmoid: like bn-x5, but with sigmoid nonlinearity instead of relu. we also attempted to train the original inception with sigmoid, but the model remained at the accuracy equivalent to chance. in figure [reference], we show the validation accuracy of the networks, as a function of the number of training steps. inception reached the accuracy of 72.2% after training steps. the figure [reference] shows, for each network, the number of training steps required to reach the same 72.2% accuracy, as well as the maximum validation accuracy reached by the network and the number of steps to reach it. by only using batch normalization (bn-baseline), we match the accuracy of inception in less than half the number of training steps. by applying the modifications in sec. [reference], we significantly increase the training speed of the network. bn-x5 needs 14 times fewer steps than inception to reach the 72.2% accuracy. interestingly, increasing the learning rate further (bn-x30) causes the model to train somewhat slower initially, but allows it to reach a higher final accuracy. it reaches 74.8% after steps, i.e. 5 times fewer steps than required by inception to reach 72.2%. we also verified that the reduction in internal covariate shift allows deep networks with batch normalization to be trained when sigmoid is used as the nonlinearity, despite the well-known difficulty of training such networks. indeed, bn-x5-sigmoid achieves the accuracy of 69.8%. without batch normalization, inception with sigmoid never achieves better than accuracy. subsubsection: ensemble classification the current reported best results on the imagenet large scale visual recognition competition are reached by the deep image ensemble of traditional models and the ensemble model of. the latter reports the top-5 error of 4.94%, as evaluated by the ilsvrc server. here we report a top-5 validation error of 4.9%, and test error of 4.82% (according to the ilsvrc server). this improves upon the previous best result, and exceeds the estimated accuracy of human raters according to. for our ensemble, we used 6 networks. each was based on bn-x30, modified via some of the following: increased initial weights in the convolutional layers; using dropout (with the dropout probability of 5% or 10%, vs. 40% for the original inception); and using non-convolutional, per-activation batch normalization with last hidden layers of the model. each network achieved its maximum accuracy after about training steps. the ensemble prediction was based on the arithmetic average of class probabilities predicted by the constituent networks. the details of ensemble and multicrop inference are similar to. we demonstrate in fig. [reference] that batch normalization allows us to set new state-of-the-art by a healthy margin on the imagenet classification challenge benchmarks. section: conclusion we have presented a novel mechanism for dramatically accelerating the training of deep networks. it is based on the premise that covariate shift, which is known to complicate the training of machine learning systems, also applies to sub-networks and layers, and removing it from internal activations of the network may aid in training. our proposed method draws its power from normalizing activations, and from incorporating this normalization in the network architecture itself. this ensures that the normalization is appropriately handled by any optimization method that is being used to train the network. to enable stochastic optimization methods commonly used in deep network training, we perform the normalization for each mini-batch, and backpropagate the gradients through the normalization parameters. batch normalization adds only two extra parameters per activation, and in doing so preserves the representation ability of the network. we presented an algorithm for constructing, training, and performing inference with batch-normalized networks. the resulting networks can be trained with saturating nonlinearities, are more tolerant to increased training rates, and often do not require dropout for regularization. merely adding batch normalization to a state-of-the-art image classification model yields a substantial speedup in training. by further increasing the learning rates, removing dropout, and applying other modifications afforded by batch normalization, we reach the previous state of the art with only a small fraction of training steps- and then beat the state of the art in single-network image classification. furthermore, by combining multiple models trained with batch normalization, we perform better than the best known system on imagenet, by a significant margin. interestingly, our method bears similarity to the standardization layer of, though the two methods stem from very different goals, and perform different tasks. the goal of batch normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution. on the contrary, apply the standardization layer to the output of the nonlinearity, which results in sparser activations. in our large-scale image classification experiments, we have not observed the nonlinearity inputs to be sparse, neither with nor without batch normalization. other notable differentiating characteristics of batch normalization include the learned scale and shift that allow the bn transform to represent identity (the standardization layer did not require this since it was followed by the learned linear transform that, conceptually, absorbs the necessary scale and shift), handling of convolutional layers, deterministic inference that does not depend on the mini-batch, and batch-normalizing each convolutional layer in the network. in this work, we have not explored the full range of possibilities that batch normalization potentially enables. our future work includes applications of our method to recurrent neural networks, where the internal covariate shift and the vanishing or exploding gradients may be especially severe, and which would allow us to more thoroughly test the hypothesis that normalization improves gradient propagation (sec. [reference]). we plan to investigate whether batch normalization can help with domain adaptation, in its traditional sense- i.e. whether the normalization performed by the network would allow it to more easily generalize to new data distributions, perhaps with just a recomputation of the population means and variances (alg. [reference]). finally, we believe that further theoretical analysis of the algorithm would allow still more improvements and applications. bibliography: references section: appendix subsection: variant of the inception model used figure [reference] documents the changes that were performed compared to the architecture with respect to the googlenet archictecture. for the interpretation of this table, please consult. the notable architecture changes compared to the googlenet model include: the 5 5 convolutional layers are replaced by two consecutive 3 3 convolutional layers. this increases the maximum depth of the network by 9 weight layers. also it increases the number of parameters by 25% and the computational cost is increased by about 30%. the number 28 28 inception modules is increased from 2 to 3. inside the modules, sometimes average, sometimes maximum-pooling is employed. this is indicated in the entries corresponding to the pooling layers of the table. there are no across the board pooling layers between any two inception modules, but stride-2 convolution/ pooling layers are employed before the filter concatenation in the modules 3c, 4e. our model employed separable convolution with depth multiplier on the first convolutional layer. this reduces the computational cost while increasing the memory consumption at training time.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "imagenet classification challenge benchmarks",
                        30006
                    ],
                    [
                        "imagenet",
                        31835
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "inception network",
                        23283
                    ],
                    [
                        "inception",
                        23867
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "top-5 validation error",
                        1232
                    ],
                    [
                        "top-5 error rate",
                        6440
                    ],
                    [
                        "top-5 error",
                        28998
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "imagenet classification",
                        1193
                    ],
                    [
                        "single-network image classification",
                        31674
                    ],
                    [
                        "large-scale image classification",
                        32437
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "188123b5afe0d3b59e9e44f33729649536fe6ea6-27",
    "doctext": "document: unifying count-based exploration and intrinsic motivation we consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. specifically, we focus on the problem of exploration in non-tabular reinforcement learning. drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. this technique enables us to generalize count-based exploration algorithms to the non-tabular case. we apply our ideas to atari 2600 games, providing sensible pseudo-counts from raw pixels. we transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult montezuma's revenge. section: introduction exploration algorithms for markov decision processes (mdps) are typically concerned with reducing the agent's uncertainty over the environment's reward and transition functions. in a tabular setting, this uncertainty can be quantified using confidence intervals derived from chernoff bounds, or inferred from a posterior over the environment parameters. in fact, both confidence intervals and posterior shrink as the inverse square root of the state-action visit count, making this quantity fundamental to most theoretical results on exploration. count-based exploration methods directly use visit counts to guide an agent's behaviour towards reducing uncertainty. for example, model-based interval estimation with exploration bonuses [mbie-eb;][] strehl08analysis solves the augmented bellman equation involving the empirical reward, the empirical transition function, and an exploration bonus proportional to. this bonus accounts for uncertainties in both transition and reward functions and enables a finite-time bound on the agent's suboptimality. in spite of their pleasant theoretical guarantees, count-based methods have not played a role in the contemporary successes of reinforcement learning [e.g.][] mnih15human. instead, most practical methods still rely on simple rules such as-greedy. the issue is that visit counts are not directly useful in large domains, where states are rarely visited more than once. answering a different scientific question, intrinsic motivation aims to provide qualitative guidance for exploration schmidhuber91possibility, oudeyer07intrinsic, barto13intrinsic. this guidance can be summarized as\" explore what surprises you\". a typical approach guides the agent based on change in prediction error, or learning progress. if is the error made by the agent at time over some event a, and the same error after observing a new piece of information, then learning progress is intrinsic motivation methods are attractive as they remain applicable in the absence of the markov property or the lack of a tabular representation, both of which are required by count-based algorithms. yet the theoretical foundations of intrinsic motivation remain largely absent from the literature, which may explain its slow rate of adoption as a standard approach to exploration. in this paper we provide formal evidence that intrinsic motivation and count-based exploration are but two sides of the same coin. specifically, we consider a frequently used measure of learning progress, information gain cover91elements. defined as the kullback-leibler divergence of a prior distribution from its posterior, information gain can be related to the confidence intervals used in count-based exploration. our contribution is to propose a new quantity, the pseudo-count, which connects information-gain-as-learning-progress and count-based exploration. we derive our pseudo-count from a density model over the state space. this is in departure from more traditional approaches to intrinsic motivation that consider learning progress with respect to a transition model. we expose the relationship between pseudo-counts, a variant of schmidhuber91possibility's compression progress we call prediction gain, and information gain. combined to kolter09near's negative result on the frequentist suboptimality of bayesian bonuses, our result highlights the theoretical advantages of pseudo-counts compared to many existing intrinsic motivation methods. the pseudo-counts we introduce here are best thought of as\" function approximation for exploration\". we bring them to bear on atari 2600 games from the arcade learning environment bellemare13arcade, focusing on games where myopic exploration fails. we extract our pseudo-counts from a simple density model and use them within a variant of mbie-eb. we apply them to an experience replay setting and to an actor-critic setting, and find improved performance in both cases. our approach produces dramatic progress on the reputedly most difficult atari 2600 game, montezuma's revenge: within a fraction of the training time, our agent explores a significant portion of the first level and obtains significantly higher scores than previously published agents. section: notation we consider a countable state space. we denote a sequence of length from by, the set of finite sequences from by, write to mean the concatenation of and a state, and denote the empty sequence by. a model over is a mapping from to probability distributions over. that is, for each the model provides a probability distribution note that we do not require to be strictly positive for all and. when it is, however, we may understand to be the usual conditional probability of given. we will take particular interest in the empirical distribution derived from the sequence. if is the number of occurrences of a state in the sequence, then we call the the empirical count function, or simply empirical count. the above notation extends to state-action spaces, and we write to explicitly refer to the number of occurrences of a state-action pair when the argument requires it. when is generated by an ergodic markov chain, for example if we follow a fixed policy in a finite-state mdp, then the limit point of is the chain's stationary distribution. in our setting, a density model is any model that assumes states are independently (but not necessarily identically) distributed; a density model is thus a particular kind of generative model. we emphasize that a density model differs from a forward model, which takes into account the temporal relationship between successive states. note that is itself a density model. section: from densities to counts in the introduction we argued that the visit count (and consequently,) is not directly useful in practical settings, since states are rarely revisited. specifically, is almost always zero and can not help answer the question\" how novel is this state?\" nor is the problem solved by a bayesian approach: even variable-alphabet models [e.g.][] hutter13sparse must assign a small, diminishing probability to yet-unseen states. to estimate the uncertainty of an agent's knowledge, we must instead look for a quantity which generalizes across states. guided by ideas from the intrinsic motivation literature, we now derive such a quantity. we call it a pseudo-count as it extends the familiar notion from bayesian estimation. subsection: pseudo-counts and the recoding probability we are given a density model over. this density model may be approximate, biased, or even inconsistent. we begin by introducing the recoding probability of a state: this is the probability assigned to by our density model after observing a new occurrence of. the term\" recoding\" is inspired from the statistical compression literature, where coding costs are inversely related to probabilities cover91elements. when admits a conditional probability distribution, we now postulate two unknowns: a pseudo-count function, and a pseudo-count total. we relate these two unknowns through two constraints: in words: we require that, after observing one instance of, the density model's increase in prediction of that same should correspond to a unit increase in pseudo-count. the pseudo-count itself is derived from solving the linear system ([reference]): note that the equations ([reference]) yield (with) when, and are inconsistent when. these cases may arise from poorly behaved density models, but are easily accounted for. from here onwards we will assume a consistent system of equations. theorem: (learning-positive density model). a density model\u03c1 is learning-positive if for all\u2208x:1nxn and all\u2208xx,\u2265\u2062\u03c1\u2032n (x)\u2062\u03c1n (x). by inspecting ([reference]), we see that if and only if is learning-positive; if and only if; and if and only if. in many cases of interest, the pseudo-count matches our intuition. if then. similarly, if is a dirichlet estimator then recovers the usual notion of pseudo-count. more importantly, if the model generalizes across states then so do pseudo-counts. subsection: estimating the frequency of a salient event in freeway as an illustrative example, we employ our method to estimate the number of occurrences of an infrequent event in the atari 2600 video game freeway (figure [reference], screenshot). we use the arcade learning environment bellemare13arcade. we will demonstrate the following: pseudo-counts are roughly zero for novel events, they exhibit credible magnitudes, they respect the ordering of state frequency, they grow linearly (on average) with real counts, they are robust in the presence of nonstationary data. these properties suggest that pseudo-counts provide an appropriate generalized notion of visit counts in non-tabular settings. in freeway, the agent must navigate a chicken across a busy road. as our example, we consider estimating the number of times the chicken has reached the very top of the screen. as is the case for many atari 2600 games, this naturally salient event is associated with an increase in score, which ale translates into a positive reward. we may reasonably imagine that knowing how certain we are about this part of the environment is useful. after crossing, the chicken is teleported back to the bottom of the screen. to highlight the robustness of our pseudo-count, we consider a nonstationary policy which waits for 250, 000 frames, then applies the up action for 250, 000 frames, then waits, then goes up again. the salient event only occurs during up periods. it also occurs with the cars in different positions, thus requiring generalization. as a point of reference, we record the pseudo-counts for both the salient event and visits to the chicken's start position. we use a simplified, pixel-level version of the cts model for atari 2600 frames proposed by bellemare14skip, ignoring temporal dependencies. while the cts model is rather impoverished in comparison to state-of-the-art density models for images [e.g.][] vandenoord16pixel, its count-based nature results in extremely fast learning, making it an appealing candidate for exploration. further details on the model may be found in the appendix. examining the pseudo-counts depicted in figure [reference] confirms that they exhibit the desirable properties listed above. in particular, the pseudo-count is almost zero on the first occurrence of the salient event; it increases slightly during the 3rd period, since the salient and reference events share some common structure; throughout, it remains smaller than the reference pseudo-count. the linearity on average and robustness to nonstationarity are immediate from the graph. note, however, that the pseudo-counts are a fraction of the real visit counts (inasmuch as we can define\" real\"): by the end of the trial, the start position has been visited about 140, 000 times, and the topmost part of the screen, 1285 times. furthermore, the ratio of recorded pseudo-counts differs from the ratio of real counts. both effects are quantifiable, as we shall show in section [reference]. section: the connection to intrinsic motivation having argued that pseudo-counts appropriately generalize visit counts, we will now show that they are closely related to information gain, which is commonly used to quantify novelty or curiosity and consequently as an intrinsic reward. information gain is defined in relation to a mixture model over a class of density models. this model predicts according to a weighted combination from: with the posterior weight of. this posterior is defined recursively, starting from a prior distribution over: information gain is then the kullback-leibler divergence from prior to posterior that results from observing: computing the information gain of a complex density model is often impractical, if not downright intractable. however, a quantity which we call the prediction gain provides us with a good approximation of the information gain. we define the prediction gain of a density model (and in particular,) as the difference between the recoding log-probability and log-probability of: prediction gain is nonnegative if and only if is learning-positive. it is related to the pseudo-count: with equality when. as the following theorem shows, prediction gain allows us to relate pseudo-count and information gain. theorem:. consider a sequence\u2208x:1nxn. let\u03be be a mixture model over a class of learning-positive models m. let^nn be the pseudo-count derived from\u03be (equation). for this model, theorem 1 suggests that using an exploration bonus proportional to, similar to the mbie-eb bonus, leads to a behaviour at least as exploratory as one derived from an information gain bonus. since pseudo-counts correspond to empirical counts in the tabular setting, this approach also preserves known theoretical guarantees. in fact, we are confident pseudo-counts may be used to prove similar results in non-tabular settings. on the other hand, it may be difficult to provide theoretical guarantees about existing bonus-based intrinsic motivation approaches. kolter09near showed that no algorithm based on a bonus upper bounded by for any can guarantee pac-mdp optimality. again considering the tabular setting and combining their result to theorem [reference], we conclude that bonuses proportional to immediate information (or prediction) gain are insufficient for theoretically near-optimal exploration: to paraphrase kolter09near, these methods produce explore too little in comparison to pseudo-count bonuses. by inspecting ([reference]) we come to a similar negative conclusion for bonuses proportional to the l1 or l2 distance between and. unlike many intrinsic motivation algorithms, pseudo-counts also do not rely on learning a forward (transition and/ or reward) model. this point is especially important because a number of powerful density models for images exist vandenoord16pixel, and because optimality guarantees can not in general exist for intrinsic motivation algorithms based on forward models. section: asymptotic analysis in this section we analyze the limiting behaviour of the ratio. we use this analysis to assert the consistency of pseudo-counts derived from tabular density models, i.e. models which maintain per-state visit counts. in the appendix we use the same result to bound the approximation error of pseudo-counts derived from directed graphical models, of which our cts model is a special case. consider a fixed, infinite sequence from. we define the limit of a sequence of functions with respect to the length of the subsequence. we additionally assume that the empirical distribution converges pointwise to a distribution, and write for the recoding probability of under. we begin with two assumptions on our density model. theorem:. the limits exist for all x; furthermore,>\u2062\u02d9r (x) 0. assumption (a) states that should eventually assign a probability to proportional to the limiting empirical distribution. in particular there must be a state for which, unless. assumption (b), on the other hand, imposes a restriction on the learning rate of relative to' s. as both and exist, assumption [reference] also implies that and have a common limit. theorem:. under assumption, the limit of the ratio of pseudo-counts\u2062^nn (x) to empirical counts\u2062nn (x) exists for all x. this limit is the model's relative rate of change, whose convergence to we require, plays an essential role in the ratio of pseudo-to empirical counts. to see this, consider a sequence generated i.i.d. from a distribution over a finite state space, and a density model defined from a sequence of nonincreasing step-sizes: with initial condition. for, this density model is the empirical distribution. for, we may appeal to well-known results from stochastic approximation [e.g.][] bertsekas96neurodynamic and find that almost surely since, we may think of assumption 1 (b) as also requiring to converge at a rate of for a comparison with the empirical count to be meaningful. note, however, that a density model that does not satisfy assumption 1 (b) may still yield useful (but incommensurable) pseudo-counts. theorem:. let>\u2062\u03d5 (x) 0 with<\u2211\u2208xx\u2062\u03d5 (x)\u221e and consider the count-based estimator if^nn is the pseudo-count corresponding to\u03c1n then\u2192\u2062/\u2062^nn (x) nn (x) 1 for all x with>\u2062\u03bc (x) 0. section: empirical evaluation in this section we demonstrate the use of pseudo-counts to guide exploration. we return to the arcade learning environment, now using the cts model to generate an exploration bonus. subsection: exploration in hard atari 2600 games from 60 games available through the arcade learning environment we selected five\" hard\" games, in the sense that an-greedy policy is inefficient at exploring them. we used a bonus of the form where was selected from a coarse parameter sweep. we also compared our method to the optimistic initialization trick proposed by. we trained our agents' q-functions with double dqn vanhasselt16deep, with one important modification: we mixed the double q-learning target with the monte carlo return. this modification led to improved results both with and without exploration bonuses (details in the appendix). figure [reference] depicts the result of our experiment, averaged across 5 trials. although optimistic initialization helps in freeway, it otherwise yields performance similar to dqn. by contrast, the count-based exploration bonus enables us to make quick progress on a number of games, most dramatically in montezuma's revenge and venture. montezuma's revenge is perhaps the hardest atari 2600 game available through the ale. the game is infamous for its hostile, unforgiving environment: the agent must navigate a number of different rooms, each filled with traps. due to its sparse reward function, most published agents achieve an average score close to zero and completely fail to explore most of the 24 rooms that constitute the first level (figure [reference], top). by contrast, within 50 million frames our agent learns a policy which consistently navigates through 15 rooms (figure [reference], bottom). our agent also achieves a score higher than anything previously reported, with one run consistently achieving 6600 points by 100 million frames (half the training samples used by mnih15human). we believe the success of our method in this game is a strong indicator of the usefulness of pseudo-counts for exploration. subsection: exploration for actor-critic methods we next used our exploration bonuses in conjunction with the a3c (asynchronous advantage actor-critic) algorithm of mnih16asynchronous. one appeal of actor-critic methods is their explicit separation of policy and q-function parameters, which leads to a richer behaviour space. this very separation, however, often leads to deficient exploration: to produce any sensible results, the a3c policy must be regularized with an entropy cost. we trained a3c on 60 atari 2600 games, with and without the exploration bonus ([reference]). we refer to our augmented algorithm as a3c+. full details and additional results may be found in the appendix. we found that a3c fails to learn in 15 games, in the sense that the agent does not achieve a score 50% better than random. in comparison, there are only 10 games for which a3c+ fails to improve on the random agent; of these, 8 are games where dqn fails in the same sense. we normalized the two algorithms' scores so that 0 and 1 are respectively the minimum and maximum of the random agent's and a3c's end-of-training score on a particular game. figure [reference] depicts the in-training median score for a3c and a3c+, along with 1st and 3rd quartile intervals. not only does a3c+ achieve slightly superior median performance, but it also significantly outperforms a3c on at least a quarter of the games. this is particularly important given the large proportion of atari 2600 games for which an-greedy policy is sufficient for exploration. section: related work information-theoretic quantities have been repeatedly used to describe intrinsically motivated behaviour. closely related to prediction gain is schmidhuber91possibility's notion of compression progress, which equates novelty with an agent's improvement in its ability to compress its past. more recently, lopes12exploration showed the relationship between time-averaged prediction gain and visit counts in a tabular setting; their result is a special case of theorem [reference]. orseau13universal demonstrated that maximizing the sum of future information gains does lead to optimal behaviour, even though maximizing immediate information gain does not (section [reference]). finally, there may be a connection between sequential normalized maximum likelihood estimators and our pseudo-count derivation [see e.g.][] ollivier15laplace. intrinsic motivation has also been studied in reinforcement learning proper, in particular in the context of discovering skills singh04intrinsically, barto13intrinsic. recently, stadie15incentivizing used a squared prediction error bonus for exploring in atari 2600 games. closest to our work is houthooft16curiosity's variational approach to intrinsic motivation, which is equivalent to a second order taylor approximation to prediction gain. mohamed15variational also considered a variational approach to the different problem of maximizing an agent's ability to influence its environment. aside for orseau13universal's above-cited work, it is only recently that theoretical guarantees for exploration have emerged for non-tabular, stateful settings. we note pazis16efficient's pac-mdp result for metric spaces and leike16thompson's asymptotic analysis of thompson sampling in general environments. section: future directions the last few years have seen tremendous advances in learning representations for reinforcement learning. surprisingly, these advances have yet to carry over to the problem of exploration. in this paper, we reconciled counts, the fundamental unit of uncertainty, with prediction-based heuristics and intrinsic motivation. combining our work with more ideas from deep learning and better density models seems a plausible avenue for quick progress in practical, efficient exploration. we now conclude by outlining a few research directions we believe are promising. induced metric. we did not address the question of where the generalization comes from. clearly, the choice of density model induces a particular metric over the state space. a better understanding of this metric should allow us to tailor the density model to the problem of exploration. compatible value function. there may be a mismatch in the learning rates of the density model and the value function: dqn learns much more slowly than our cts model. as such, it should be beneficial to design value functions compatible with density models (or vice-versa). the continuous case. although we focused here on countable state spaces, we can as easily define a pseudo-count in terms of probability density functions. at present it is unclear whether this provides us with the right notion of counts for continuous spaces. subsubsection: acknowledgments the authors would like to thank laurent orseau, alex graves, joel veness, charles blundell, shakir mohamed, ivo danihelka, ian osband, matt hoffman, greg wayne, will dabney, and a\u00e4ron van den oord for their excellent feedback early and late in the writing, and pierre-yves oudeyer and yann ollivier for pointing out additional connections to the literature. bibliography: references appendix: the connection to intrinsic motivation the following provides an identity connecting information gain and prediction gain. theorem:. consider a mixture model\u03be over m with prediction gain pgn and information gain ign, a fixed\u2208xx, and let:=\u2062w\u2032n (x)\u2062wn (\u03c1, x) be the posterior of\u03be over m after observing x. let:=\u2062w\u2032\u2032n (x)\u2062w\u2032n (\u03c1, x) be the same posterior after observing x a second time, and let\u2062pg\u03c1n (x) denote the prediction gain of\u2208\u03c1m. then in particular, if m is a class of non-adaptive models in the sense that=\u2062\u03c1n (x)\u2062\u03c1 (x) for all x:1n, then a model which is non-adaptive is also learning-positive in the sense of definition [reference]. many common mixture models, for example dirichlet-multinomial estimators, are mixtures over non-adaptive models. proof: proof. we rewrite the posterior update rule ([reference]) to show that for any and any, write. now the second statement follows immediately. theorem:. the functions:=\u2062f (x)- ex1x and:=\u2062g (x)- ex1x2 are nonnegative on\u2208x [0,\u221e). proof: proof. the statement regarding follows directly from the taylor expansion for. now, the first derivative of is. it is clearly positive for. for, since, the second result follows.\u220e proof: proof (theorem [reference]). the inequality follows directly from lemma [reference], the nonnegativity of the kullback-leibler divergence, and the fact that all models in are learning-positive. for the inequality, we write where (a) follows by definition of prediction gain, (b) from, and (c) from lemma [reference]. using the second part of lemma [reference] in (c) yields the inequality.\u220e appendix: asymptotic analysis we begin with a simple lemma which will prove useful throughout. theorem:. the rate of change of the empirical distribution, -\u2062\u03bc\u2032n (x)\u2062\u03bcn (x), is such that proof: proof. we expand the definition of and:\u220e using this lemma, we derive an asymptotic relationship between and. proof: proof (theorem [reference]). we expand the definition of and: with the last line following from lemma [reference]. under assumption [reference], all terms of the right-hand side converge as. taking the limit on both sides, where (a) is justified by the existence of the relevant limits and, and (b) follows from writing as, where all limits involved exist.\u220e subsection: directed graphical models we say that is a factored state space if it is the cartesian product of subspaces, i.e.. this factored structure allows us to construct approximate density models over, for example by modelling the joint density as a product of marginals. we write the factor of a state as, and write the sequence of the factor across as. we will show that directed graphical models wainwright08graphical satisfy assumption [reference]. a directed graphical model describes a probability distribution over a factored state space. to the factor is associated a parent set. let denote the value of the factors in the parent set. the factor model is, with the understanding that is allowed to make a different prediction for each value of. the state is assigned the joint probability common choices for include the conditional empirical distribution and the dirichlet estimator. theorem:. suppose that each factor model\u03c1in converges to the conditional probability distribution\u03bc (xi|x\u2062\u03c0 (i)) and that for each xi with\u03bc (xi|x\u2062\u03c0 (i)), then for all x with>\u2062\u03bc (x) 0, the density model\u03c1gm satisfies assumption with the cts density model used in our experiments is in fact a particular kind of induced graphical model. the result above thus describes how the pseudo-counts computed in section [reference] are asymptotically related to the empirical counts. proof: proof. by hypothesis,. combining this with, similarly, where in (a) we used the identity derived in the proof of theorem [reference]. now let and. the difference of products above is and by the hypothesis on the rate of change of and the identity, we have since the limits of and are both, we deduce that now, if then also for each factor. hence.\u220e subsection: tabular density models (corollary [reference]) we shall prove the following, which includes corollary [reference] as a special case. theorem:. consider:\u03d5\u2192\u00d7xx*r+. suppose that for all (: xn\u2208nn) and every\u2208xx, and. let\u2062\u03c1n (x) be the count-based estimator if^nn is the pseudo-count corresponding to\u03c1n then\u2192\u2062/\u2062^nn (x) nn (x) 1 for all x with>\u2062\u03bc (x) 0. condition 2 is satisfied if with monotonically increasing in (but not too quickly!) and converging to some distribution for all sequences. this is the case for most tabular density models. proof: proof. we will show that the condition on the rate of change required by proposition [reference] is satisfied under the stated conditions. let,, and. by hypothesis, note that we do not require. now using lemma [reference] we deduce that since and similarly for, then pointwise implies that also. for any, where a) follows from and b) is justified by and the hypothesis that. therefore. hence since, we further deduce from theorem [reference] that the condition, which was also needed in proposition [reference], is necessary for the ratio to converge to 1: for example, if grows as but grows as (with finite) then will grow as the larger. appendix: experimental methods subsection: cts density model our state space is the set of all preprocessed atari 2600 frames. each raw frame is composed of 7-bit ntsc pixels bellemare13arcade. we preprocess these frames by first converting them to grayscale (luminance), then downsampling to by averaging over pixel values (figure [reference]). aside from this preprocessing, our model is very similar to the model used by bellemare14skip and veness15compress. the cts density model treats as a factored state, where each pixel corresponds to a factor. the parents of this factor are its upper-left neighbours, i.e. pixels,, and (in this order). the probability of is then the product of the probability assigned to its factors. each factor is modelled using a location-dependent cts model, which predicts the pixel's colour value conditional on some, all, or possibly none, of the pixel's parents (figure [reference]). subsection: a taxonomy of exploration we provide in table [reference] a rough taxonomy of the atari 2600 games available through the ale in terms of the difficulty of exploration. we first divided the games into two groups: those for which local exploration (e.g.-greedy) is sufficient to achieve a high scoring policy (easy), and those for which it is not (hard). for example, space invaders versus pitfall!. we further divided the easy group based on whether an-greedy scheme finds a score exploit, that is maximizes the score without achieving the game's stated objective. for example, kung-fu master versus boxing. while this distinction is not directly used here, score exploits lead to behaviours which are optimal from an ale perspective but uninteresting to humans. we divide the games in the hard category into dense reward games (ms. pac-man) and sparse reward games (montezuma's revenge). subsection: exploration in montezuma's revenge montezuma's revenge is divided into three levels, each composed of 24 rooms arranged in a pyramidal shape (figure [reference]). as discussed above, each room poses a number of challenges: to escape the very first room, the agent must climb ladders, dodge a creature, pick up a key, then backtrack to open one of two doors. the number of rooms reached by an agent is therefore a good measure of its ability. by accessing the game ram, we recorded the location of the agent at each step during the course of training. we computed the visit count to each room, averaged over epochs each lasting one million frames. from this information we constructed a map of the agent's\" known world\", that is, all rooms visited at least once. the agent's current room number ranges from 0 to 23 (figure [reference]) and is stored at ram location 0x83. figure [reference] shows the set of rooms explored by our dqn agents at different points during training. figure [reference] paints a clear picture: after 50 million frames, the agent using exploration bonuses has seen a total of 15 rooms, while the no-bonus agent has seen two. at that point in time, our agent achieves an average score of 2461; by 100 million frames, this figure stands at 3439, higher than anything previously reported. we believe the success of our method in this game is a strong indicator of the usefulness of pseudo-counts for exploration. we remark that without mixing in the monte-carlo return, our bonus-based agent still explores significantly more than the no-bonus agent. however, the deep network seems unable to maintain a sufficiently good approximation to the value function, and performance quickly deteriorates. comparable results using the a3c method provide another example of the practical importance of eligibility traces and return-based methods in reinforcement learning. subsection: improving exploration for actor-critic methods our implementation of a3c was along the lines mentioned in and uses 16 threads. each thread corresponds to an actor learner and maintains a copy of the density model. all the threads are synchronized with the master thread at regular intervals of 250, 000 steps. we followed the same training procedure as that reported in the a3c paper with the following additional steps: we update our density model with the states generated by following the policy. during the policy gradient step, we compute the intrinsic rewards by querying the density model and add it to the extrinsic rewards before clipping them in the range as was done in the a3c paper. this resulted in minimal overhead in computation costs and the memory footprint was manageable (32 gb) for most of the atari games. our training times were almost the same as the ones reported in the a3c paper. we picked after performing a short parameter sweep over the training games. the choice of training games is the same as mentioned in the a3c paper. the games on which dqn achieves a score of 150% or less of the random score are: asteroids, double dunk, gravitar, ice hockey, montezuma's revenge, pitfall!, skiing, surround, tennis, time pilot. the games on which a3c achieves a score of 150% or less of the random score are: battle zone, bowling, enduro, freeway, gravitar, kangaroo, pitfall!, robotank, skiing, solaris, surround, tennis, time pilot, venture. the games on which a3c+ achieves a score of 150% or less of the random score are: double dunk, gravitar, ice hockey, pitfall!, skiing, solaris, surround, tennis, time pilot, venture. our experiments involved the stochastic version of the arcade learning environment (ale) without a terminal signal for life loss, which is now the default ale setting. briefly, the stochasticity is achieved by accepting the agent\u00e2\u0080\u0099 action at each frame with probability and using the agent\u00e2\u0080\u0099s previous action during rejection. we used the ale's default value of as has been previously used in. for comparison, table [reference] also reports the deterministic+ life loss setting also used in the literature. anecdotally, we found that using the life loss signal, while helpful in achieving high scores in some games, is detrimental in montezuma's revenge. recall that the life loss signal was used by mnih15human to treat each of the agent' lives as a separate episode. for comparison, after 200 million frames a3c+ achieves the following average scores: 1) stochastic+ life loss: 142.50; 2) deterministic+ life loss: 273.70 3) stochastic without life loss: 1127.05 4) deterministic without life loss: 273.70. the maximum score achieved by 3) is 3600, in comparison to the maximum of 500 achieved by 1) and 3). this large discrepancy is not unsurprising when one considers that losing a life in montezuma's revenge, and in fact in most games, is very different from restarting a new episode. subsection: comparing exploration bonuses in this section we compare the effect of using different exploration bonuses derived from our density model. we consider the following variants: no exploration bonus,, as per mbie-eb strehl08analysis;, as per beb kolter09near; and, related to compression progress schmidhuber08driven. the exact form of these bonuses is analogous to ([reference]). we compare these variants after 10, 50, 100, and 200 million frames of training, again in the a3c setup. to compare scores across 60 games, we use inter-algorithm score distributions bellemare13arcade. inter-algorithm scores are normalized so that 0 corresponds to the worst score on a game, and 1, to the best. if is a game and the inter-algorithm score on for algorithm, then the score distribution function is the score distribution effectively depicts a kind of cumulative distribution, with a higher overall curve implying better scores across the gamut of atari 2600 games. a higher curve at implies top performance on more games; a higher curve at indicates the algorithm does not perform poorly on many games. the scale parameter was optimized to for each variant separately. figure [reference] shows that, while prediction gain initially achieves strong performance, by 50 million frames all three algorithms perform equally well. by 200 million frames, the exploration bonus outperforms both prediction gain and no bonus. the prediction gain achieves a decent, but not top-performing score on all games. this matches our earlier argument that using prediction gain results in too little exploration. we hypothesize that the poor performance of the bonus stems from too abrupt a decay from a large to small intrinsic reward, although more experiments are needed. as a whole, these results show how using pg offers an advantage over the baseline a3c algorithm, which is furthered by using our count-based exploration bonus.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "freeway",
                        8969
                    ],
                    [
                        "atari 2600 video game freeway",
                        9094
                    ],
                    [
                        "atari 2600 frames",
                        10642
                    ],
                    [
                        "preprocessed atari 2600 frames",
                        29605
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "score",
                        9894
                    ],
                    [
                        "average scores",
                        35730
                    ],
                    [
                        "inter-algorithm scores",
                        36775
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "atari 2600 games",
                        602
                    ],
                    [
                        "hard atari 2600 games",
                        17372
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "atari 2600 game",
                        18380
                    ],
                    [
                        "atari games",
                        34055
                    ],
                    [
                        "gravitar",
                        34400
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "score",
                        9894
                    ],
                    [
                        "average scores",
                        35730
                    ],
                    [
                        "inter-algorithm scores",
                        36775
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "atari 2600 games",
                        602
                    ],
                    [
                        "hard atari 2600 games",
                        17372
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "montezuma's revenge",
                        836
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "dqn",
                        18175
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "score",
                        9894
                    ],
                    [
                        "average scores",
                        35730
                    ],
                    [
                        "inter-algorithm scores",
                        36775
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "atari 2600 games",
                        602
                    ],
                    [
                        "hard atari 2600 games",
                        17372
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "venture",
                        18328
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "score",
                        9894
                    ],
                    [
                        "average scores",
                        35730
                    ],
                    [
                        "inter-algorithm scores",
                        36775
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "atari 2600 games",
                        602
                    ],
                    [
                        "hard atari 2600 games",
                        17372
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "189e6bb7523733c4e524214b9e6ae92d4ed50dac-28",
    "doctext": "document: transfer learning for sequence tagging with hierarchical recurrent networks recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. one appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering. however, it is unclear if such systems can be used for tasks without large amounts of training data. in this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (e.g., pos tagging on penn treebank) is used to improve performance on a target task with fewer available annotations (e.g., pos tagging for microblogs). we examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained. these improvements lead to improvements over the current state-of-the-art on several well-studied tasks. section: introduction sequence tagging is an important problem in natural language processing, which has wide applications including part-of-speech (pos) tagging, text chunking, and named entity recognition (ner). given a sequence of words, sequence tagging aims to predict a linguistic tag for each word such as the pos tag. an important challenge for sequence tagging is how to transfer knowledge from one task to another, which is often referred to as transfer learning pan2010survey. transfer learning can be used in several settings, notably for low-resource languages zirikly2cross, wang2013cross and low-resource domains such as biomedical corpora kim2003genia and twitter corpora ritter2011named). in these cases, transfer learning can improve performance by taking advantage of more plentiful labels from related tasks. even on datasets with relatively abundant labels, multi-task transfer can sometimes achieve improvement over state-of-the-art results collobert2011natural. recently, a number of approaches based on deep neural networks have addressed the problem of sequence tagging in an end-to-end manner collobert2011natural, lample2016neural, ling2015finding, ma2016end. these neural networks consist of multiple layers of neurons organized in a hierarchy and can transform the input tokens to the output labels without explicit hand-engineered feature extraction. the aforementioned neural networks require minimal assumptions about the task at hand and thus demonstrate significant generality\u2014 one single model can be applied to multiple applications in multiple languages without changing the architecture. a natural question is whether the representation learned from one task can be useful for another task. in other words, is there a way we can exploit the generality of neural networks to improve task performance by sharing model parameters and feature representations with another task? to address the above question, we study the transfer learning setting, which aims to improve the performance on a target task by joint training with a source task. we present a transfer learning approach based on a deep hierarchical recurrent neural network, which shares the hidden feature representation and part of the model parameters between the source task and the target task. our approach combines the objectives of the two tasks and uses gradient-based methods for efficient training. we study cross-domain, cross-application, and cross-lingual transfer, and present a parameter-sharing architecture for each case. experimental results show that our approach can significantly improve the performance of the target task when the the target task has few labels and is more related to the source task. furthermore, we show that transfer learning can improve performance over state-of-the-art results even if the amount of labels is relatively abundant. we have novel contributions in two folds. first, our work is, to the best of our knowledge, the first that focuses on studying the transferability of different layers of representations for hierarchical rnns. second, different from previous transfer learning methods that usually focus on one specific transfer setting, our framework exploits different levels of representation sharing and provides a unified framework to handle cross-application, cross-lingual, and cross-domain transfer. section: related work there are two common paradigms for transfer learning for natural language processing (nlp) tasks, resource-based transfer and model-based transfer. resource-based transfer utilizes additional linguistic annotations as weak supervision for transfer learning, such as cross-lingual dictionaries zirikly2cross, corpora wang2013cross, and word alignments yarowsky2001inducing. resource-based methods demonstrate considerable success in cross-lingual transfer, but are quite sensitive to the scale and quality of the additional resources. resource-based transfer is mostly limited to cross-lingual transfer in previous works, and there is not extensive research on extending resource-based methods to cross-domain and cross-application settings. model-based transfer, on the other hand, does not require additional resources. model-based transfer exploits the similarity and relatedness between the source task and the target task by adaptively modifying the model architectures, training algorithms, or feature representation. for example, proposed a transfer learning framework that shares structural parameters across multiple tasks, and improve the performance on various tasks including ner; presented a task-independent convolutional neural network and employed joint training to transfer knowledge from ner and pos tagging to chunking; studied transfer learning between named entity recognition and word segmentation in chinese based on recurrent neural networks. cross-domain transfer, or domain adaptation, is also a well-studied branch of model-based transfer in nlp. techniques in cross-domain transfer include the design of robust feature representations schnabel2014flors, co-training chen2011co, hierarchical bayesian prior finkel2009hierarchical, and canonical component analysis kim2015new. while our approach falls into the paradigm of model-based transfer, in contrast to the above methods, our method focuses on exploiting the generality of deep recurrent neural networks and is applicable to transfer between domains, applications, and languages. our work builds on previous work on sequence tagging based on deep neural networks. develop end-to-end neural networks for sequence tagging without hand-engineered features. later architectures based on different combinations of convolutional networks and recurrent networks have achieved state-of-the-art results on many tasks collobert2011natural, huang2015bidirectional, chiu2015named, lample2016neural, ma2016end. these models demonstrate significant generality since they can be applied to multiple applications in multiple languages with a unified network architecture and without task-specific feature extraction. section: approach in this section, we introduce our transfer learning approach. we first introduce an abstract framework for neural sequence tagging, summarizing previous work, and then discuss three different transfer learning architectures. subsection: base model though many different variants of neural networks have been proposed for the problem of sequence tagging, we find that most of the models can be described with the hierarchical framework illustrated in figure [reference]. a character-level layer takes a sequence of characters (represented as embeddings) as input, and outputs a representation that encodes the morphological information at the character level. a word-level layer subsequently combines the character-level feature representation and a word embedding, and further incorporates the contextual information to output a new feature representation. after two levels of feature extraction (encoding), the feature representation output by the word-level layer is fed to a conditional random field (crf) layer that outputs the label sequence. both of the word-level layer and the character-level layer can be implemented as convolutional neural networks (cnns) or recurrent neural networks (rnns) collobert2011natural, chiu2015named, lample2016neural, ma2016end. we discuss the details of the model we use in this work in section [reference]. subsection: transfer learning architectures we develop three architectures for transfer learning, t-a, t-b, and t-c, are illustrated in figures [reference], [reference], and [reference] respectively. the three architectures are all extensions of the base model discussed in the previous section with different parameter sharing schemes. we now discuss the use cases for the different architectures. subsubsection: cross-domain transfer since different domains are\" sub-languages\" that have domain-specific regularities, sequence taggers trained on one domain might not have optimal performance on another domain. the goal of cross-domain transfer is to learn a sequence tagger that transfers knowledge from a source domain to a target domain. we assume that few labels are available in the target domain. there are two cases of cross-domain transfer. the two domains can have label sets that can be mapped to each other, or disparate label sets. for example, pos tags in the genia biomedical corpus can be mapped to penn treebank tags barrett2014token, while some pos tags in twitter (e.g.,\" url\") can not be mapped to penn treebank tags ritter2011named. if the two domains have mappable label sets, we share all the model parameters and feature representation in the neural networks, including the word and character embedding, the word-level layer, the character-level layer, and the crf layer. we perform a label mapping step on top of the crf layer. this becomes the model t-a as shown in figure [reference]. if the two domains have disparate label sets, we untie the parameter sharing in the crf layer\u2014 i.e., each task learns a separate crf layer. this parameter sharing scheme reduces to model t-b as shown in figure [reference]. subsubsection: cross-application transfer sequence tagging has a couple of applications including pos tagging, chunking, and named entity recognition. similar to the motivation in collobert2011natural, it is usually desirable to exploit the underlying similarities and regularities of different applications, and improve the performance of one application via joint training with another. moreover, transfer between multiple applications can be helpful when the labels are limited. in the cross-application setting, we assume that multiple applications are in the same language. since different applications share the same alphabet, the case is similar to cross-domain transfer with disparate label sets. we adopt the architecture of model t-b for cross-application transfer learning where only the crf layers are disjoint for different applications. subsubsection: cross-lingual transfer though cross-lingual transfer is usually accomplished with additional multi-lingual resources, these methods are sensitive to the size and quality of the additional resources yarowsky2001inducing, wang2013cross. in this work, instead, we explore a complementary method that exploits the cross-lingual regularities purely on the model level. our approach focuses on transfer learning between languages with similar alphabets, such as english and spanish, since it is very difficult for transfer learning between languages with disparate alphabets (e.g., english and chinese) to work without additional resources zirikly2cross. model-level transfer learning is achieved through exploiting the morphologies shared by the two languages. for example,\" canada\" in english and\" canad\u00e1\" in spanish refer to the same named entity, and the morphological similarities can be leveraged for ner and also pos tagging with nouns. thus we share the character embeddings and the character-level layer between different languages for transfer learning, which is illustrated as the model t-c in figure [reference]. subsection: training in the above sections, we introduced three neural architectures with different parameter sharing schemes, designed for different transfer learning settings. now we describe how we train the neural networks jointly for two tasks. suppose we are transferring from a source task to a target task, with the training instances being and. let and denote the set of model parameters for the source and target tasks respectively. the model parameters are divided into two sets, task specific parameters and shared parameters, i.e., where shared parameters are jointly optimized by the two tasks, while task specific parameters and are trained for each task separately. the training procedure is as follows. at each iteration, we sample a task (i.e., either or) from based on a binomial distribution (the binomial probability is set as a hyperparameter). given the sampled task, we sample a batch of training instances from the given task, and then perform a gradient update according to the loss function of the given task. we update both the shared parameters and the task specific parameters. we repeat the above iterations until stopping. we adopt adagrad duchi2011adaptive to dynamically compute the learning rates for each iteration. since the source and target tasks might have different convergence rates, we do early stopping on the target task performance. subsection: model implementation in this section, we describe our implementation of the base model. both the character-level and word-level neural networks are implemented as rnns. more specifically, we employ gated recurrent units (grus) cho2014properties. let be a sequence of inputs that can be embeddings or hidden states of other layers. let be the gru hidden state at time step. formally, a gru unit at time step can be expressed as where's are model parameters of each unit, is a candidate hidden state that is used to compute, is an element-wise sigmoid logistic function defined as, and denotes element-wise multiplication of two vectors. intuitively, the update gate controls how much the unit updates its hidden state, and the reset gate determines how much information from the previous hidden state needs to be reset. the input to the character-level grus is character embeddings, while the input to the word-level grus is the concatenation of character-level gru hidden states and word embeddings. both grus are bi-directional and have two layers. given an input sequence of words, the word-level grus and the character-level grus together learn a feature representation for the-th word in the sequence, which forms a sequence. let denote the tag sequence. given the feature representation and the tag sequence for each training instance, the crf layer defines the objective function to maximize based on a max-margin principle gimpel2010softmax as: where is a function that assigns a score for each pair of and, and denotes the space of tag sequences for. the cost function is added based on the max-margin principle gimpel2010softmax that high-cost tags should be penalized more heavily. our base model is similar to, but in contrast to their model, we employ grus for the character-level and word-level networks instead of long short-term memory (lstm) units, and define the objective function based on the max-margin principle. we note that our transfer learning framework does not make assumptions about specific model implementation, and could be applied to other neural architectures collobert2011natural, chiu2015named, lample2016neural, ma2016end as well. section: experiments subsection: datasets we use the following benchmark datasets in our experiments: penn treebank (ptb) pos tagging, conll 2000 chunking, conll 2003 english ner, conll 2002 dutch ner, conll 2002 spanish ner, the genia biomedical corpus kim2003genia, and a twitter corpus ritter2011named. the statistics of the datasets are described in table [reference]. we construct the pos tagging dataset with the instructions described in. note that as a standard practice, the pos tags are extracted from the parsed trees. for the conll 2003 english ner dataset, we follow previous works collobert2011natural to append one-hot gazetteer features to the input of the crf layer for fair comparison. since there is no standard training/ dev/ test data split for the genia and twitter corpora, we randomly sample 10% for test, 10% for development, and 80% for training. we follow previous work barrett2014token to map genia pos tags to ptb pos tags. subsection: transfer learning performance we evaluate our transfer learning approach on the above datasets. we fix the hyperparameters for all the results reported in this section: we set the character embedding dimension at, the word embedding dimension at for english and for spanish, the dimension of hidden states of the character-level grus at, the dimension of hidden states of the word-level grus at, and the initial learning rate at. except for the twitter datasets, these datasets are fairly large. to simulate a low-resource setting, we also use random subsets of the data. we vary the labeling rate of the target task at,, and. given a labeling rate, we randomly sample a ratio of the sentences from the training set and discard the rest of the training data\u2014 e.g., a labeling rate of results in around 900 training tokens on ptb pos tagging (cf. table [reference]). the results on transfer learning are plotted in figure [reference], where we compare the results with and without transfer learning under various labeling rates. the numbers in the y-axes are accuracies for pos tagging, and chunk-level f1 scores for chunking and ner. the numbers are shown in table [reference]. we can see that our transfer learning approach consistently improved over the non-transfer results. we also observe that the improvement by transfer learning is more substantial when the labeling rate is lower. for cross-domain transfer, we obtained substantial improvement on the genia and twitter corpora by transferring the knowledge from ptb pos tagging and conll 2003 ner. for example, as shown in figure [reference], we can obtain an tagging accuracy of with zero labels and with only labels when transferring from ptb to genia. as shown in figures [reference] and [reference], our transfer learning approach can improve the performance on twitter pos tagging and ner for all labeling rates, and the improvements with labels are more than for both datasets. cross-application transfer also leads to substantial improvement under low-resource conditions. for example, as shown in figures [reference] and [reference], the improvements with labels are and on conll 2000 chunking and conll 2003 ner respectively when transferring from ptb pos tagging. figures [reference] and [reference] show that cross-lingual transfer can improve the performance when few labels are available. figure [reference] further shows that the improvements by different architectures are in the following order: t-a t-b t-c. this phenomenon can be explained by the fact that t-a shares the most model parameters while t-c shares the least. transfer settings like cross-lingual transfer can only use t-c because the underlying similarities between the source task and the target task are less prominent (i.e., less transferable), and in those cases the improvement by transfer learning is less substantial. another interesting comparison is among figures [reference], [reference], and [reference]. figure [reference] is cross-domain transfer, figure [reference] is transfer across domains and applications at the same time, and figure [reference] combines all the three transfer settings (i.e., from spanish ner in the general domain to english pos tagging in the biomedical domain). the results show that the improvement by transfer learning diminishes when the transfer becomes\" indirect\" (i.e., the source task and the target task are more loosely related). we also study using different transfer learning models for the same task. we study the effects of using t-a, t-b, and t-c when transferring from ptb to genia, and the results are included in the lower part of table [reference]. we observe that the performance gain decreases when less parameters are shared (i.e., t-a t-b t-c). subsection: comparison with state-of-the-art results in the above section, we examine the effects of different transfer learning architectures. now we compare our approach with state-of-the-art systems on these datasets. we use publicly available pretrained word embeddings as initialization. on the english datasets, following previous works that are based on neural networks collobert2011natural, huang2015bidirectional, chiu2015named, ma2016end, we experiment with both the 50-dimensional senna embeddings collobert2011natural and the 100-dimensional glove embeddings pennington2014glove and use the development set to choose the embeddings for different tasks and settings. for spanish and dutch, we use the 64-dimensional polyglot embeddings al2013polyglot. we set the hidden state dimensions to be 300 for the word-level gru. the initial learning rate for adagrad is fixed at 0.01. we use the development set to tune the other hyperparameters of our model. our results are reported in table [reference]. since there are no standard data splits on the genia and twitter corpora, we do not include these datasets into our comparison. the results for conll 2000 chunking, conll 2003 ner, and ptb pos tagging are obtained by transfer learning between the three tasks, i.e., transferring from two tasks to the other. the results for spanish and dutch ner are obtained with transfer learning between the ner datasets in three languages (english, spanish, and dutch). from table [reference], we can draw two conclusions. first, our transfer learning approach achieves new state-of-the-art results on all the considered benchmark datasets except ptb pos tagging, which indicates that transfer learning can still improve the performance even on datasets with relatively abundant labels. second, our base model (w/ o transfer) performs competitively compared to the state-of-the-art systems, which means that the improvements shown in section [reference] are obtained over a strong baseline. section: conclusion in this paper we develop a transfer learning approach for sequence tagging, which exploits the generality demonstrated by deep neural networks in previous work. we design three neural network architectures for the settings of cross-domain, cross-application, and cross-lingual transfer. our transfer learning approach achieves significant improvement on various datasets under low-resource conditions, as well as new state-of-the-art results on some of the benchmarks. with thorough experiments, we observe that the following factors are crucial for the performance of our transfer learning approach: a) label abundance for the target task, b) relatedness between the source and target tasks, and c) the number of parameters that can be shared. in the future, it will be interesting to combine model-based transfer (as in this work) with resource-based transfer for cross-lingual transfer learning. subsubsection: acknowledgments this work was funded by nvidia, the office of naval research grant n000141512791, the adelaide grant fa8750-16c-0130-001, the nsf grant iis1250956, and google research. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "conll 2003 english ner",
                        15942
                    ],
                    [
                        "english datasets",
                        20797
                    ],
                    [
                        "english",
                        21935
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "chunk-level f1 scores",
                        17841
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "named entity recognition",
                        1256
                    ],
                    [
                        "ner",
                        1283
                    ],
                    [
                        "conll 2003 ner",
                        18291
                    ],
                    [
                        "spanish ner",
                        19908
                    ],
                    [
                        "dutch ner",
                        21843
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "penn treebank",
                        646
                    ],
                    [
                        "ptb",
                        15904
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "accuracies",
                        17809
                    ],
                    [
                        "tagging accuracy",
                        18369
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "pos tagging",
                        631
                    ],
                    [
                        "part-of-speech",
                        1207
                    ],
                    [
                        "pos",
                        1224
                    ],
                    [
                        "pos tags",
                        9466
                    ],
                    [
                        "penn treebank tags",
                        9523
                    ],
                    [
                        "ptb pos tagging",
                        17576
                    ],
                    [
                        "twitter pos tagging",
                        18575
                    ],
                    [
                        "english pos tagging",
                        19945
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "19d583bf8c5533d1261ccdc068fdc3ef53b9ffb9-29",
    "doctext": "document: facenet: a unified embedding for face recognition and clustering despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. in this paper we present a system, called facenet, that directly learns a mapping from face images to a compact euclidean space where distances directly correspond to a measure of face similarity. once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with facenet embeddings as feature vectors. our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. to train, we use triplets of roughly aligned matching/ non-matching face patches generated using a novel online triplet mining method. the benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. on the widely used labeled faces in the wild (lfw) dataset, our system achieves a new record accuracy of 99.63%. on youtube faces db it achieves 95.12%. our system cuts the error rate in comparison to the best published result by 30% on both datasets. we also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other. section: introduction in this paper we present a unified system for face verification (is this the same person), recognition (who is this person) and clustering (find common people among these faces). our method is based on learning a euclidean embedding per image using a deep convolutional network. the network is trained such that the squared l2 distances in the embedding space directly correspond to face similarity: faces of the same person have small distances and faces of distinct people have large distances. once this embedding has been produced, then the aforementioned tasks become straight-forward: face verification simply involves thresholding the distance between the two embeddings; recognition becomes a k-nn classification problem; and clustering can be achieved using off-the-shelf techniques such as k-means or agglomerative clustering. previous face recognition approaches based on deep networks use a classification layer trained over a set of known face identities and then take an intermediate bottleneck layer as a representation used to generalize recognition beyond the set of identities used in training. the downsides of this approach are its indirectness and its inefficiency: one has to hope that the bottleneck representation generalizes well to new faces; and by using a bottleneck layer the representation size per face is usually very large (1000s of dimensions). some recent work has reduced this dimensionality using pca, but this is a linear transformation that can be easily learnt in one layer of the network. in contrast to these approaches, facenet directly trains its output to be a compact 128-d embedding using a triplet-based loss function based on lmnn. our triplets consist of two matching face thumbnails and a non-matching face thumbnail and the loss aims to separate the positive pair from the negative by a distance margin. the thumbnails are tight crops of the face area, no 2d or 3d alignment, other than scale and translation is performed. choosing which triplets to use turns out to be very important for achieving good performance and, inspired by curriculum learning, we present a novel online negative exemplar mining strategy which ensures consistently increasing difficulty of triplets as the network trains. to improve clustering accuracy, we also explore hard-positive mining techniques which encourage spherical clusters for the embeddings of a single person. as an illustration of the incredible variability that our method can handle see figure [reference]. shown are image pairs from pie that previously were considered to be very difficult for face verification systems. an overview of the rest of the paper is as follows: in section [reference] we review the literature in this area; section [reference] defines the triplet loss and section [reference] describes our novel triplet selection and training procedure; in section [reference] we describe the model architecture used. finally in section [reference] and [reference] we present some quantitative results of our embeddings and also qualitatively explore some clustering results. section: related work similarly to other recent works which employ deep networks, our approach is a purely data driven method which learns its representation directly from the pixels of the face. rather than using engineered features, we use a large dataset of labelled faces to attain the appropriate invariances to pose, illumination, and other variational conditions. in this paper we explore two different deep network architectures that have been recently used to great success in the computer vision community. both are deep convolutional networks. the first architecture is based on the zeiler& fergus model which consists of multiple interleaved layers of convolutions, non-linear activations, local response normalizations, and max pooling layers. we additionally add several convolution layers inspired by the work of. the second architecture is based on the inception model of szegedy which was recently used as the winning approach for imagenet 2014. these networks use mixed layers that run several different convolutional and pooling layers in parallel and concatenate their responses. we have found that these models can reduce the number of parameters by up to 20 times and have the potential to reduce the number of flops required for comparable performance. there is a vast corpus of face verification and recognition works. reviewing it is out of the scope of this paper so we will only briefly discuss the most relevant recent work. the works of all employ a complex system of multiple stages, that combines the output of a deep convolutional network with pca for dimensionality reduction and an svm for classification. zhenyao employ a deep network to ''warp'' faces into a canonical frontal view and then learn cnn that classifies each face as belonging to a known identity. for face verification, pca on the network output in conjunction with an ensemble of svms is used. taigman propose a multi-stage approach that aligns faces to a general 3d shape model. a multi-class network is trained to perform the face recognition task on over four thousand identities. the authors also experimented with a so called siamese network where they directly optimize the-distance between two face features. their best performance on lfw () stems from an ensemble of three networks using different alignments and color channels. the predicted distances (non-linear svm predictions based on the kernel) of those networks are combined using a non-linear svm. sun propose a compact and therefore relatively cheap to compute network. they use an ensemble of 25 of these network, each operating on a different face patch. for their final performance on lfw () the authors combine 50 responses (regular and flipped). both pca and a joint bayesian model that effectively correspond to a linear transform in the embedding space are employed. their method does not require explicit 2d/ 3d alignment. the networks are trained by using a combination of classification and verification loss. the verification loss is similar to the triplet loss we employ, in that it minimizes the-distance between faces of the same identity and enforces a margin between the distance of faces of different identities. the main difference is that only pairs of images are compared, whereas the triplet loss encourages a relative distance constraint. a similar loss to the one used here was explored in wang for ranking images by semantic and visual similarity. section: method facenet uses a deep convolutional network. we discuss two different core architectures: the zeiler& fergus style networks and the recent inception type networks. the details of these networks are described in section [reference]. given the model details, and treating it as a black box (see figure [reference]), the most important part of our approach lies in the end-to-end learning of the whole system. to this end we employ the triplet loss that directly reflects what we want to achieve in face verification, recognition and clustering. namely, we strive for an embedding, from an image into a feature space, such that the squared distance between all faces, independent of imaging conditions, of the same identity is small, whereas the squared distance between a pair of face images from different identities is large. although we did not directly compare to other losses, the one using pairs of positives and negatives, as used in eq. (2), we believe that the triplet loss is more suitable for face verification. the motivation is that the loss from encourages all faces of one identity to be projected onto a single point in the embedding space. the triplet loss, however, tries to enforce a margin between each pair of faces from one person to all other faces. this allows the faces for one identity to live on a manifold, while still enforcing the distance and thus discriminability to other identities. the following section describes this triplet loss and how it can be learned efficiently at scale. subsection: triplet loss the embedding is represented by. it embeds an image into a-dimensional euclidean space. additionally, we constrain this embedding to live on the-dimensional hypersphere,. this loss is motivated in in the context of nearest-neighbor classification. here we want to ensure that an image (anchor) of a specific person is closer to all other images (positive) of the same person than it is to any image (negative) of any other person. this is visualized in figure [reference]. thus we want, where is a margin that is enforced between positive and negative pairs. is the set of all possible triplets in the training set and has cardinality. the loss that is being minimized is then generating all possible triplets would result in many triplets that are easily satisfied (fulfill the constraint in eq. ([reference])). these triplets would not contribute to the training and result in slower convergence, as they would still be passed through the network. it is crucial to select hard triplets, that are active and can therefore contribute to improving the model. the following section talks about the different approaches we use for the triplet selection. subsection: triplet selection in order to ensure fast convergence it is crucial to select triplets that violate the triplet constraint in eq. ([reference]). this means that, given, we want to select an (hard positive) such that and similarly (hard negative) such that. it is infeasible to compute the and across the whole training set. additionally, it might lead to poor training, as mislabelled and poorly imaged faces would dominate the hard positives and negatives. there are two obvious choices that avoid this issue: generate triplets offline every n steps, using the most recent network checkpoint and computing the and on a subset of the data. generate triplets online. this can be done by selecting the hard positive/ negative exemplars from within a mini-batch. here, we focus on the online generation and use large mini-batches in the order of a few thousand exemplars and only compute the and within a mini-batch. to have a meaningful representation of the anchor-positive distances, it needs to be ensured that a minimal number of exemplars of any one identity is present in each mini-batch. in our experiments we sample the training data such that around 40 faces are selected per identity per mini-batch. additionally, randomly sampled negative faces are added to each mini-batch. instead of picking the hardest positive, we use all anchor-positive pairs in a mini-batch while still selecting the hard negatives. we do n't have a side-by-side comparison of hard anchor-positive pairs versus all anchor-positive pairs within a mini-batch, but we found in practice that the all anchor-positive method was more stable and converged slightly faster at the beginning of training. we also explored the offline generation of triplets in conjunction with the online generation and it may allow the use of smaller batch sizes, but the experiments were inconclusive. selecting the hardest negatives can in practice lead to bad local minima early on in training, specifically it can result in a collapsed model (). in order to mitigate this, it helps to select such that we call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchor-positive distance. those negatives lie inside the margin. as mentioned before, correct triplet selection is crucial for fast convergence. on the one hand we would like to use small mini-batches as these tend to improve convergence during stochastic gradient descent (sgd). on the other hand, implementation details make batches of tens to hundreds of exemplars more efficient. the main constraint with regards to the batch size, however, is the way we select hard relevant triplets from within the mini-batches. in most experiments we use a batch size of around 1, 800 exemplars. subsection: deep convolutional networks in all our experiments we train the cnn using stochastic gradient descent (sgd) with standard backprop and adagrad. in most experiments we start with a learning rate of which we lower to finalize the model. the models are initialized from random, similar to, and trained on a cpu cluster for 1, 000 to 2, 000 hours. the decrease in the loss (and increase in accuracy) slows down drastically after 500h of training, but additional training can still significantly improve performance. the margin is set to. we used two types of architectures and explore their trade-offs in more detail in the experimental section. their practical differences lie in the difference of parameters and flops. the best model may be different depending on the application. a model running in a datacenter can have many parameters and require a large number of flops, whereas a model running on a mobile phone needs to have few parameters, so that it can fit into memory. all our models use rectified linear units as the non-linear activation function. the first category, shown in table [reference], adds convolutional layers, as suggested in, between the standard convolutional layers of the zeiler& fergus architecture and results in a model 22 layers deep. it has a total of 140 million parameters and requires around 1.6 billion flops per image. the second category we use is based on googlenet style inception models. these models have fewer parameters (around 6.6m-7.5 m) and up to fewer flops (between 500m-1.6b). some of these models are dramatically reduced in size (both depth and number of filters), so that they can be run on a mobile phone. one, nns1, has 26 m parameters and only requires 220 m flops per image. the other, nns2, has 4.3 m parameters and 20 m flops. table [reference] describes nn2 our largest network in detail. nn3 is identical in architecture but has a reduced input size of 160x160. nn4 has an input size of only 96x96, thereby drastically reducing the cpu requirements (285 m flops vs 1.6b for nn2). in addition to the reduced input size it does not use 5x5 convolutions in the higher layers as the receptive field is already too small by then. generally we found that the 5x5 convolutions can be removed throughout with only a minor drop in accuracy. figure [reference] compares all our models. section: datasets and evaluation we evaluate our method on four datasets and with the exception of labelled faces in the wild and youtube faces we evaluate our method on the face verification task. given a pair of two face images a squared distance threshold is used to determine the classification of same and different. all faces pairs of the same identity are denoted with, whereas all pairs of different identities are denoted with. we define the set of all true accepts as these are the face pairs that were correctly classified as same at threshold. similarly is the set of all pairs that was incorrectly classified as same (false accept). the validation rate and the false accept rate for a given face distance are then defined as subsection: hold-out test set we keep a hold out set of around one million images, that has the same distribution as our training set, but disjoint identities. for evaluation we split it into five disjoint sets of images each. the and rate are then computed on image pairs. standard error is reported across the five splits. subsection: personal photos this is a test set with similar distribution to our training set, but has been manually verified to have very clean labels. it consists of three personal photo collections with a total of around images. we compute the and rate across all 12k squared pairs of images. subsection: academic datasets labeled faces in the wild (lfw) is the de-facto academic test set for face verification. we follow the standard protocol for unrestricted, labeled outside data and report the mean classification accuracy as well as the standard error of the mean. youtube faces db is a new dataset that has gained popularity in the face recognition community. the setup is similar to lfw, but instead of verifying pairs of images, pairs of videos are used. section: experiments if not mentioned otherwise we use between 100m-200 m training face thumbnails consisting of about 8 m different identities. a face detector is run on each image and a tight bounding box around each face is generated. these face thumbnails are resized to the input size of the respective network. input sizes range from 96x96 pixels to 224x224 pixels in our experiments. subsection: computation accuracy trade-off before diving into the details of more specific experiments we will discuss the trade-off of accuracy versus number of flops that a particular model requires. figure [reference] shows the flops on the x-axis and the accuracy at 0.001 false accept rate () on our user labelled test-data set from section [reference]. it is interesting to see the strong correlation between the computation a model requires and the accuracy it achieves. the figure highlights the five models (nn1, nn2, nn3, nns1, nns2) that we discuss in more detail in our experiments. we also looked into the accuracy trade-off with regards to the number of model parameters. however, the picture is not as clear in that case. for example, the inception based model nn2 achieves a comparable performance to nn1, but only has a 20th of the parameters. the number of flops is comparable, though. obviously at some point the performance is expected to decrease, if the number of parameters is reduced further. other model architectures may allow further reductions without loss of accuracy, just like inception did in this case. subsection: effect of cnn model we now discuss the performance of our four selected models in more detail. on the one hand we have our traditional zeiler& fergus based architecture with convolutions (see table [reference]). on the other hand we have inception based models that dramatically reduce the model size. overall, in the final performance the top models of both architectures perform comparably. however, some of our inception based models, such as nn3, still achieve good performance while significantly reducing both the flops and the model size. the detailed evaluation on our personal photos test set is shown in figure [reference]. while the largest model achieves a dramatic improvement in accuracy compared to the tiny nns2, the latter can be run 30ms/ image on a mobile phone and is still accurate enough to be used in face clustering. the sharp drop in the roc for indicates noisy labels in the test data groundtruth. at extremely low false accept rates a single mislabeled image can have a significant impact on the curve. subsection: sensitivity to image quality table [reference] shows the robustness of our model across a wide range of image sizes. the network is surprisingly robust with respect to jpeg compression and performs very well down to a jpeg quality of 20. the performance drop is very small for face thumbnails down to a size of 120x120 pixels and even at 80x80 pixels it shows acceptable performance. this is notable, because the network was trained on 220x220 input images. training with lower resolution faces could improve this range further. subsection: embedding dimensionality we explored various embedding dimensionalities and selected 128 for all experiments other than the comparison reported in table [reference]. one would expect the larger embeddings to perform at least as good as the smaller ones, however, it is possible that they require more training to achieve the same accuracy. that said, the differences in the performance reported in table [reference] are statistically insignificant. it should be noted, that during training a 128 dimensional float vector is used, but it can be quantized to 128-bytes without loss of accuracy. thus each face is compactly represented by a 128 dimensional byte vector, which is ideal for large scale clustering and recognition. smaller embeddings are possible at a minor loss of accuracy and could be employed on mobile devices. subsection: amount of training data table [reference] shows the impact of large amounts of training data. due to time constraints this evaluation was run on a smaller model; the effect may be even larger on larger models. it is clear that using tens of millions of exemplars results in a clear boost of accuracy on our personal photo test set from section [reference]. compared to only millions of images the relative reduction in error is 60%. using another order of magnitude more images (hundreds of millions) still gives a small boost, but the improvement tapers off. subsection: performance on lfw false accept false reject we evaluate our model on lfw using the standard protocol for unrestricted, labeled outside data. nine training splits are used to select the-distance threshold. classification (same or different) is then performed on the tenth test split. the selected optimal threshold is for all test splits except split eighth (). our model is evaluated in two modes: fixed center crop of the lfw provided thumbnail. a proprietary face detector (similar to picasa) is run on the provided lfw thumbnails. if it fails to align the face (this happens for two images), the lfw alignment is used. figure [reference] gives an overview of all failure cases. it shows false accepts on the top as well as false rejects at the bottom. we achieve a classification accuracy of 98.87%\u00b10.15 when using the fixed center crop described in (1) and the record breaking 99.63%\u00b1 0.09 standard error of the mean when using the extra face alignment (2). this reduces the error reported for deepface in by more than a factor of 7 and the previous state-of-the-art reported for deepid2+ in by 30%. this is the performance of model nn1, but even the much smaller nn3 achieves performance that is not statistically significantly different. subsection: performance on youtube faces db we use the average similarity of all pairs of the first one hundred frames that our face detector detects in each video. this gives us a classification accuracy of 95.12%\u00b10.39. using the first one thousand frames results in 95.18%. compared to 91.4% who also evaluate one hundred frames per video we reduce the error rate by almost half. deepid2+ achieved 93.2% and our method reduces this error by 30%, comparable to our improvement on lfw. subsection: face clustering our compact embedding lends itself to be used in order to cluster a users personal photos into groups of people with the same identity. the constraints in assignment imposed by clustering faces, compared to the pure verification task, lead to truly amazing results. figure [reference] shows one cluster in a users personal photo collection, generated using agglomerative clustering. it is a clear showcase of the incredible invariance to occlusion, lighting, pose and even age. section: summary we provide a method to directly learn an embedding into an euclidean space for face verification. this sets it apart from other methods who use the cnn bottleneck layer, or require additional post-processing such as concatenation of multiple models and pca, as well as svm classification. our end-to-end training both simplifies the setup and shows that directly optimizing a loss relevant to the task at hand improves performance. another strength of our model is that it only requires minimal alignment (tight crop around the face area)., for example, performs a complex 3d alignment. we also experimented with a similarity transform alignment and notice that this can actually improve performance slightly. it is not clear if it is worth the extra complexity. future work will focus on better understanding of the error cases, further improving the model, and also reducing model size and reducing cpu requirements. we will also look into ways of improving the currently extremely long training times, variations of our curriculum learning with smaller batch sizes and offline as well as online positive and negative mining. section: appendix: harmonic embedding in this section we introduce the concept of harmonic embeddings. by this we denote a set of embeddings that are generated by different models v1 and v2 but are compatible in the sense that they can be compared to each other. this compatibility greatly simplifies upgrade paths. in an scenario where embedding v1 was computed across a large set of images and a new embedding model v2 is being rolled out, this compatibility ensures a smooth transition without the need to worry about version incompatibilities. figure [reference] shows results on our 3 g dataset. it can be seen that the improved model nn2 significantly outperforms nn1, while the comparison of nn2 embeddings to nn1 embeddings performs at an intermediate level. subsection: harmonic triplet loss in order to learn the harmonic embedding we mix embeddings of v1 together with the embeddings v2, that are being learned. this is done inside the triplet loss and results in additionally generated triplets that encourage the compatibility between the different embedding versions. figure [reference] visualizes the different combinations of triplets that contribute to the triplet loss. we initialized the v2 embedding from an independently trained nn2 and retrained the last layer (embedding layer) from random initialization with the compatibility encouraging triplet loss. first only the last layer is retrained, then we continue training the whole v2 network with the harmonic loss. figure [reference] shows a possible interpretation of how this compatibility may work in practice. the vast majority of v2 embeddings may be embedded near the corresponding v1 embedding, however, incorrectly placed v1 embeddings can be perturbed slightly such that their new location in embedding space improves verification accuracy. subsection: summary these are very interesting findings and it is somewhat surprising that it works so well. future work can explore how far this idea can be extended. presumably there is a limit as to how much the v2 embedding can improve over v1, while still being compatible. additionally it would be interesting to train small networks that can run on a mobile phone and are compatible to a larger server side model. section: acknowledgments we would like to thank johannes steffens for his discussions and great insights on face recognition and christian szegedy for providing new network architectures like and discussing network design choices. also we are indebted to the distbelief team for their support especially to rajat monga for help in setting up efficient training schemes. also our work would not have been possible without the support of chuck rosenberg, hartwig adam, and simon han. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "labeled faces in the wild",
                        1149
                    ],
                    [
                        "lfw",
                        1177
                    ],
                    [
                        "labelled faces in the wild",
                        16172
                    ],
                    [
                        "lfw thumbnails",
                        22953
                    ],
                    [
                        "lfw alignment",
                        23034
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "facenet",
                        10
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "record accuracy",
                        1216
                    ],
                    [
                        "clustering accuracy",
                        3920
                    ],
                    [
                        "accuracy",
                        14122
                    ],
                    [
                        "mean classification accuracy",
                        17636
                    ],
                    [
                        "computation accuracy trade",
                        18304
                    ],
                    [
                        "classification accuracy",
                        23203
                    ],
                    [
                        "verification accuracy",
                        27616
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "face verification",
                        158
                    ],
                    [
                        "verification",
                        528
                    ],
                    [
                        "classification",
                        6369
                    ],
                    [
                        "verification loss",
                        7738
                    ],
                    [
                        "pure verification task",
                        24404
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "youtube faces db",
                        1246
                    ],
                    [
                        "youtube faces",
                        16203
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "facenet",
                        10
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "record accuracy",
                        1216
                    ],
                    [
                        "clustering accuracy",
                        3920
                    ],
                    [
                        "accuracy",
                        14122
                    ],
                    [
                        "mean classification accuracy",
                        17636
                    ],
                    [
                        "computation accuracy trade",
                        18304
                    ],
                    [
                        "classification accuracy",
                        23203
                    ],
                    [
                        "verification accuracy",
                        27616
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "face verification",
                        158
                    ],
                    [
                        "verification",
                        528
                    ],
                    [
                        "classification",
                        6369
                    ],
                    [
                        "verification loss",
                        7738
                    ],
                    [
                        "pure verification task",
                        24404
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "1c059493904b2244d2280b8b4c0c7d3ca115be73-30",
    "doctext": "document: node2vec: scalable feature learning for networks prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. however, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. in node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. we define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. we demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks. 2 2016 acmlicensed kdd' 16, august 13-17, 2016, san francisco, ca, usa 978-1-4503-4232-2/ 16/ 08$ 15.00 http:// dx.doi.org/ 10.1145/ 2939672.2939754 categories and subject descriptors: h.2.8 [database management]: database applications\u2014 data mining; i.2.6 [artificial intelligence]: learning general terms: algorithms; experimentation. keywords: information networks, feature learning, node embeddings, graph representations. section: introduction many important tasks in network analysis involve predictions over nodes and edges. in a typical node classification task, we are interested in predicting the most probable labels of nodes in a network. for example, in a social network, we might be interested in predicting interests of users, or in a protein-protein interaction network we might be interested in predicting functional labels of proteins. similarly, in link prediction, we wish to predict whether a pair of nodes in a network should have an edge connecting them. link prediction is useful in a wide variety of domains; for instance, in genomics, it helps us discover novel interactions between genes, and in social networks, it can identify real-world friends. any supervised machine learning algorithm requires a set of informative, discriminating, and independent features. in prediction problems on networks this means that one has to construct a feature vector representation for the nodes and edges. a typical solution involves hand-engineering domain-specific features based on expert knowledge. even if one discounts the tedious effort required for feature engineering, such features are usually designed for specific tasks and do not generalize across different prediction tasks. an alternative approach is to learn feature representations by solving an optimization problem. the challenge in feature learning is defining an objective function, which involves a trade-off in balancing computational efficiency and predictive accuracy. on one side of the spectrum, one could directly aim to find a feature representation that optimizes performance of a downstream prediction task. while this supervised procedure results in good accuracy, it comes at the cost of high training time complexity due to a blowup in the number of parameters that need to be estimated. at the other extreme, the objective function can be defined to be independent of the downstream prediction task and the representations can be learned in a purely unsupervised way. this makes the optimization computationally efficient and with a carefully designed objective, it results in task-independent features that closely match task-specific approaches in predictive accuracy. however, current techniques fail to satisfactorily define and optimize a reasonable objective required for scalable unsupervised feature learning in networks. classic approaches based on linear and non-linear dimensionality reduction techniques such as principal component analysis, multi-dimensional scaling and their extensions optimize an objective that transforms a representative data matrix of the network such that it maximizes the variance of the data representation. consequently, these approaches invariably involve eigendecomposition of the appropriate data matrix which is expensive for large real-world networks. moreover, the resulting latent representations give poor performance on various prediction tasks over networks. alternatively, we can design an objective that seeks to preserve local neighborhoods of nodes. the objective can be efficiently optimized using stochastic gradient descent (sgd) akin to backpropogation on just single hidden-layer feedforward neural networks. recent attempts in this direction propose efficient algorithms but rely on a rigid notion of a network neighborhood, which results in these approaches being largely insensitive to connectivity patterns unique to networks. specifically, nodes in networks could be organized based on communities they belong to (i.e., homophily); in other cases, the organization could be based on the structural roles of nodes in the network (i.e., structural equivalence). for instance, in figure [reference], we observe nodes and belonging to the same tightly knit community of nodes, while the nodes and in the two distinct communities share the same structural role of a hub node. real-world networks commonly exhibit a mixture of such equivalences. thus, it is essential to allow for a flexible algorithm that can learn node representations obeying both principles: ability to learn representations that embed nodes from the same network community closely together, as well as to learn representations where nodes that share similar roles have similar embeddings. this would allow feature learning algorithms to generalize across a wide variety of domains and prediction tasks. present work. we propose node2vec, a semi-supervised algorithm for scalable feature learning in networks. we optimize a custom graph-based objective function using sgd motivated by prior work on natural language processing. intuitively, our approach returns feature representations that maximize the likelihood of preserving network neighborhoods of nodes in a-dimensional feature space. we use a 2 order random walk approach to generate (sample) network neighborhoods for nodes. our key contribution is in defining a flexible notion of a node's network neighborhood. by choosing an appropriate notion of a neighborhood, node2vec can learn representations that organize nodes based on their network roles and/ or communities they belong to. we achieve this by developing a family of biased random walks, which efficiently explore diverse neighborhoods of a given node. the resulting algorithm is flexible, giving us control over the search space through tunable parameters, in contrast to rigid search procedures in prior work. consequently, our method generalizes prior work and can model the full spectrum of equivalences observed in networks. the parameters governing our search strategy have an intuitive interpretation and bias the walk towards different network exploration strategies. these parameters can also be learned directly using a tiny fraction of labeled data in a semi-supervised fashion. we also show how feature representations of individual nodes can be extended to pairs of nodes (i.e., edges). in order to generate feature representations of edges, we compose the learned feature representations of the individual nodes using simple binary operators. this compositionality lends node2vec to prediction tasks involving nodes as well as edges. our experiments focus on two common prediction tasks in networks: a multi-label classification task, where every node is assigned one or more class labels and a link prediction task, where we predict the existence of an edge given a pair of nodes. we contrast the performance of node2vec with state-of-the-art feature learning algorithms. we experiment with several real-world networks from diverse domains, such as social networks, information networks, as well as networks from systems biology. experiments demonstrate that node2vec outperforms state-of-the-art methods by up to 26.7% on multi-label classification and up to 12.6% on link prediction. the algorithm shows competitive performance with even 10% labeled data and is also robust to perturbations in the form of noisy or missing edges. computationally, the major phases of node2vec are trivially parallelizable, and it can scale to large networks with millions of nodes in a few hours. overall our paper makes the following contributions: we propose node2vec, an efficient scalable algorithm for feature learning in networks that efficiently optimizes a novel network-aware, neighborhood preserving objective using sgd. we show how node2vec is in accordance with established principles in network science, providing flexibility in discovering representations conforming to different equivalences. we extend node2vec and other feature learning methods based on neighborhood preserving objectives, from nodes to pairs of nodes for edge-based prediction tasks. we empirically evaluate node2vec for multi-label classification and link prediction on several real-world datasets. the rest of the paper is structured as follows. in section [reference], we briefly survey related work in feature learning for networks. we present the technical details for feature learning using node2vec in section [reference]. in section [reference], we empirically evaluate node2vec on prediction tasks over nodes and edges on various real-world networks and assess the parameter sensitivity, perturbation analysis, and scalability aspects of our algorithm. we conclude with a discussion of the node2vec framework and highlight some promising directions for future work in section [reference]. datasets and a reference implementation of node2vec are available on the project page:. section: related work feature engineering has been extensively studied by the machine learning community under various headings. in networks, the conventional paradigm for generating features for nodes is based on feature extraction techniques which typically involve some seed hand-crafted features based on network properties. in contrast, our goal is to automate the whole process by casting feature extraction as a representation learning problem in which case we do not require any hand-engineered features. unsupervised feature learning approaches typically exploit the spectral properties of various matrix representations of graphs, especially the laplacian and the adjacency matrices. under this linear algebra perspective, these methods can be viewed as dimensionality reduction techniques. several linear (e.g., pca) and non-linear (e.g., isomap) dimensionality reduction techniques have been proposed. these methods suffer from both computational and statistical performance drawbacks. in terms of computational efficiency, eigendecomposition of a data matrix is expensive unless the solution quality is significantly compromised with approximations, and hence, these methods are hard to scale to large networks. secondly, these methods optimize for objectives that are not robust to the diverse patterns observed in networks (such as homophily and structural equivalence) and make assumptions about the relationship between the underlying network structure and the prediction task. for instance, spectral clustering makes a strong homophily assumption that graph cuts will be useful for classification. such assumptions are reasonable in many scenarios, but unsatisfactory in effectively generalizing across diverse networks. recent advancements in representational learning for natural language processing opened new ways for feature learning of discrete objects such as words. in particular, the skip-gram model aims to learn continuous feature representations for words by optimizing a neighborhood preserving likelihood objective. the algorithm proceeds as follows: it scans over the words of a document, and for every word it aims to embed it such that the word's features can predict nearby words (i.e., words inside some context window). the word feature representations are learned by optmizing the likelihood objective using sgd with negative sampling. the skip-gram objective is based on the distributional hypothesis which states that words in similar contexts tend to have similar meanings. that is, similar words tend to appear in similar word neighborhoods. inspired by the skip-gram model, recent research established an analogy for networks by representing a network as a\" document\". the same way as a document is an ordered sequence of words, one could sample sequences of nodes from the underlying network and turn a network into a ordered sequence of nodes. however, there are many possible sampling strategies for nodes, resulting in different learned feature representations. in fact, as we shall show, there is no clear winning sampling strategy that works across all networks and all prediction tasks. this is a major shortcoming of prior work which fail to offer any flexibility in sampling of nodes from a network. our algorithm node2vec overcomes this limitation by designing a flexible objective that is not tied to a particular sampling strategy and provides parameters to tune the explored search space (see section [reference]). finally, for both node and edge based prediction tasks, there is a body of recent work for supervised feature learning based on existing and novel graph-specific deep network architectures. these architectures directly minimize the loss function for a downstream prediction task using several layers of non-linear transformations which results in high accuracy, but at the cost of scalability due to high training time requirements. section: feature learning framework we formulate feature learning in networks as a maximum likelihood optimization problem. let be a given network. our analysis is general and applies to any (un) directed, (un) weighted network. let be the mapping function from nodes to feature representaions we aim to learn for a downstream prediction task. here is a parameter specifying the number of dimensions of our feature representation. equivalently, is a matrix of size parameters. for every source node, we define as a network neighborhood of node generated through a neighborhood sampling strategy. we proceed by extending the skip-gram architecture to networks. we seek to optimize the following objective function, which maximizes the log-probability of observing a network neighborhood for a node conditioned on its feature representation, given by: in order to make the optimization problem tractable, we make two standard assumptions: conditional independence. we factorize the likelihood by assuming that the likelihood of observing a neighborhood node is independent of observing any other neighborhood node given the feature representation of the source: symmetry in feature space. a source node and neighborhood node have a symmetric effect over each other in feature space. accordingly, we model the conditional likelihood of every source-neighborhood node pair as a softmax unit parametrized by a dot product of their features: with the above assumptions, the objective in eq. [reference] simplifies to: the per-node partition function,, is expensive to compute for large networks and we approximate it using negative sampling. we optimize eq. [reference] using stochastic gradient ascent over the model parameters defining the features. feature learning methods based on the skip-gram architecture have been originally developed in the context of natural language. given the linear nature of text, the notion of a neighborhood can be naturally defined using a sliding window over consecutive words. networks, however, are not linear, and thus a richer notion of a neighborhood is needed. to resolve this issue, we propose a randomized procedure that samples many different neighborhoods of a given source node. the neighborhoods are not restricted to just immediate neighbors but can have vastly different structures depending on the sampling strategy. subsection: classic search strategies we view the problem of sampling neighborhoods of a source node as a form of local search. figure [reference] shows a graph, where given a source node we aim to generate (sample) its neighborhood. importantly, to be able to fairly compare different sampling strategies, we shall constrain the size of the neighborhood set to nodes and then sample multiple sets for a single node. generally, there are two extreme sampling strategies for generating neighborhood set (s) of nodes: breadth-first sampling (bfs) the neighborhood is restricted to nodes which are immediate neighbors of the source. for example, in figure [reference] for a neighborhood of size, bfs samples nodes,,. depth-first sampling (dfs) the neighborhood consists of nodes sequentially sampled at increasing distances from the source node. in figure [reference], dfs samples,,. the breadth-first and depth-first sampling represent extreme scenarios in terms of the search space they explore leading to interesting implications on the learned representations. in particular, prediction tasks on nodes in networks often shuttle between two kinds of similarities: homophily and structural equivalence. under the homophily hypothesis nodes that are highly interconnected and belong to similar network clusters or communities should be embedded closely together (e.g., nodes and in figure [reference] belong to the same network community). in contrast, under the structural equivalence hypothesis nodes that have similar structural roles in networks should be embedded closely together (e.g., nodes and in figure [reference] act as hubs of their corresponding communities). importantly, unlike homophily, structural equivalence does not emphasize connectivity; nodes could be far apart in the network and still have the same structural role. in real-world, these equivalence notions are not exclusive; networks commonly exhibit both behaviors where some nodes exhibit homophily while others reflect structural equivalence. we observe that bfs and dfs strategies play a key role in producing representations that reflect either of the above equivalences. in particular, the neighborhoods sampled by bfs lead to embeddings that correspond closely to structural equivalence. intuitively, we note that in order to ascertain structural equivalence, it is often sufficient to characterize the local neighborhoods accurately. for example, structural equivalence based on network roles such as bridges and hubs can be inferred just by observing the immediate neighborhoods of each node. by restricting search to nearby nodes, bfs achieves this characterization and obtains a microscopic view of the neighborhood of every node. additionally, in bfs, nodes in the sampled neighborhoods tend to repeat many times. this is also important as it reduces the variance in characterizing the distribution of 1-hop nodes with respect the source node. however, a very small portion of the graph is explored for any given. the opposite is true for dfs which can explore larger parts of the network as it can move further away from the source node (with sample size being fixed). in dfs, the sampled nodes more accurately reflect a macro-view of the neighborhood which is essential in inferring communities based on homophily. however, the issue with dfs is that it is important to not only infer which node-to-node dependencies exist in a network, but also to characterize the exact nature of these dependencies. this is hard given we have a constrain on the sample size and a large neighborhood to explore, resulting in high variance. secondly, moving to much greater depths leads to complex dependencies since a sampled node may be far from the source and potentially less representative. subsection: node2vec building on the above observations, we design a flexible neighborhood sampling strategy which allows us to smoothly interpolate between bfs and dfs. we achieve this by developing a flexible biased random walk procedure that can explore neighborhoods in a bfs as well as dfs fashion. subsubsection: random walks formally, given a source node, we simulate a random walk of fixed length. let denote the th node in the walk, starting with. nodes are generated by the following distribution: where is the unnormalized transition probability between nodes and, and is the normalizing constant. subsubsection: search bias the simplest way to bias our random walks would be to sample the next node based on the static edge weights i.e.,. (in case of unweighted graphs.) however, this does not allow us to account for the network structure and guide our search procedure to explore different types of network neighborhoods. additionally, unlike bfs and dfs which are extreme sampling paradigms suited for structural equivalence and homophily respectively, our random walks should accommodate for the fact that these notions of equivalence are not competing or exclusive, and real-world networks commonly exhibit a mixture of both. we define a 2 order random walk with two parameters and which guide the walk: consider a random walk that just traversed edge and now resides at node (figure [reference]). the walk now needs to decide on the next step so it evaluates the transition probabilities on edges leading from. we set the unnormalized transition probability to, where and denotes the shortest path distance between nodes and. note that must be one of, and hence, the two parameters are necessary and sufficient to guide the walk. intuitively, parameters and control how fast the walk explores and leaves the neighborhood of starting node. in particular, the parameters allow our search procedure to (approximately) interpolate between bfs and dfs and thereby reflect an affinity for different notions of node equivalences. return parameter, p. parameter controls the likelihood of immediately revisiting a node in the walk. setting it to a high value () ensures that we are less likely to sample an already-visited node in the following two steps (unless the next node in the walk had no other neighbor). this strategy encourages moderate exploration and avoids-hop redundancy in sampling. on the other hand, if is low (), it would lead the walk to backtrack a step (figure [reference]) and this would keep the walk\" local\" close to the starting node. in-out parameter, q. parameter allows the search to differentiate between\" inward\" and\" outward\" nodes. going back to figure [reference], if, the random walk is biased towards nodes close to node. such walks obtain a local view of the underlying graph with respect to the start node in the walk and approximate bfs behavior in the sense that our samples comprise of nodes within a small locality. in contrast, if, the walk is more inclined to visit nodes which are further away from the node. such behavior is reflective of dfs which encourages outward exploration. however, an essential difference here is that we achieve dfs-like exploration within the random walk framework. hence, the sampled nodes are not at strictly increasing distances from a given source node, but in turn, we benefit from tractable preprocessing and superior sampling efficiency of random walks. note that by setting to be a function of the preceeding node in the walk, the random walks are 2 order markovian. benefits of random walks. there are several benefits of random walks over pure bfs/ dfs approaches. random walks are computationally efficient in terms of both space and time requirements. the space complexity to store the immediate neighbors of every node in the graph is. for 2 order random walks, it is helpful to store the interconnections between the neighbors of every node, which incurs a space complexity of where is the average degree of the graph and is usually small for real-world networks. the other key advantage of random walks over classic search-based sampling strategies is its time complexity. in particular, by imposing graph connectivity in the sample generation process, random walks provide a convenient mechanism to increase the effective sampling rate by reusing samples across different source nodes. by simulating a random walk of length we can generate samples for nodes at once due to the markovian nature of the random walk. hence, our effective complexity is per sample. for example, in figure [reference] we sample a random walk of length, which results in, and. note that sample reuse can introduce some bias in the overall procedure. however, we observe that it greatly improves the efficiency. subsubsection: the node2vec algorithm [h] learnfeatures (graph, dimensions, walks per node, walk length, context size, return, in-out) preprocessmodifiedweights () initialize to empty to nodes node2vecwalk () append to stochasticgradientdescent (,,) return node2vecwalk (graph, start node, length) inititalize to to getneighbors (,) aliassample () append to return the node2vec algorithm. the pseudocode for node2vec, is given in algorithm [reference]. in any random walk, there is an implicit bias due to the choice of the start node. since we learn representations for all nodes, we offset this bias by simulating random walks of fixed length starting from every node. at every step of the walk, sampling is done based on the transition probabilities. the transition probabilities for the 2 order markov chain can be precomputed and hence, sampling of nodes while simulating the random walk can be done efficiently in time using alias sampling. the three phases of node2vec, i.e., preprocessing to compute transition probabilities, random walk simulations and optimization using sgd, are executed sequentially. each phase is parallelizable and executed asynchronously, contributing to the overall scalability of node2vec. node2vec is available at:. subsection: learning edge features the node2vec algorithm provides a semi-supervised method to learn rich feature representations for nodes in a network. however, we are often interested in prediction tasks involving pairs of nodes instead of individual nodes. for instance, in link prediction, we predict whether a link exists between two nodes in a network. since our random walks are naturally based on the connectivity structure between nodes in the underlying network, we extend them to pairs of nodes using a bootstrapping approach over the feature representations of the individual nodes. given two nodes and, we define a binary operator over the corresponding feature vectors and in order to generate a representation such that where is the representation size for the pair. we want our operators to be generally defined for any pair of nodes, even if an edge does not exist between the pair since doing so makes the representations useful for link prediction where our test set contains both true and false edges (i.e., do not exist). we consider several choices for the operator such that which are summarized in table [reference]. section: experiments the objective in eq. [reference] is independent of any downstream task and the flexibility in exploration offered by node2vec lends the learned feature representations to a wide variety of network analysis settings discussed below. subsection: case study: les mis\u00e9rables network in section [reference] we observed that bfs and dfs strategies represent extreme ends on the spectrum of embedding nodes based on the principles of homophily (i.e., network communities) and structural equivalence (i.e., structural roles of nodes). we now aim to empirically demonstrate this fact and show that node2vec in fact can discover embeddings that obey both principles. we use a network where nodes correspond to characters in the novel les mis\u00e9rables and edges connect coappearing characters. the network has 77 nodes and 254 edges. we set and run node2vec to learn feature representation for every node in the network. the feature representations are clustered using-means. we then visualize the original network in two dimensions with nodes now assigned colors based on their clusters. figure [reference] (top) shows the example when we set. notice how regions of the network (i.e., network communities) are colored using the same color. in this setting node2vec discovers clusters/ communities of characters that frequently interact with each other in the major sub-plots of the novel. since the edges between characters are based on coappearances, we can conclude this characterization closely relates with homophily. in order to discover which nodes have the same structural roles we use the same network but set, use node2vec to get node features and then cluster the nodes based on the obtained features. here node2vec obtains a complementary assignment of node to clusters such that the colors correspond to structural equivalence as illustrated in figure [reference] (bottom). for instance, node2vec embeds blue-colored nodes close together. these nodes represent characters that act as bridges between different sub-plots of the novel. similarly, the yellow nodes mostly represent characters that are at the periphery and have limited interactions. one could assign alternate semantic interpretations to these clusters of nodes, but the key takeaway is that node2vec is not tied to a particular notion of equivalence. as we show through our experiments, these equivalence notions are commonly exhibited in most real-world networks and have a significant impact on the performance of the learned representations for prediction tasks. subsection: experimental setup our experiments evaluate the feature representations obtained through node2vec on standard supervised learning tasks: multi-label classification for nodes and link prediction for edges. for both tasks, we evaluate the performance of node2vec against the following feature learning algorithms: spectral clustering: this is a matrix factorization approach in which we take the top eigenvectors of the normalized laplacian matrix of graph as the feature vector representations for nodes. deepwalk: this approach learns-dimensional feature representations by simulating uniform random walks. the sampling strategy in deepwalk can be seen as a special case of node2vec with and. line: this approach learns-dimensional feature representations in two separate phases. in the first phase, it learns dimensions by bfs-style simulations over immediate neighbors of nodes. in the second phase, it learns the next dimensions by sampling nodes strictly at a 2-hop distance from the source nodes. we exclude other matrix factorization approaches which have already been shown to be inferior to deepwalk. we also exclude a recent approach, grarep, that generalizes line to incorporate information from network neighborhoods beyond 2-hops, but is unable to efficiently scale to large networks. in contrast to the setup used in prior work for evaluating sampling-based feature learning algorithms, we generate an equal number of samples for each method and then evaluate the quality of the obtained features on the prediction task. in doing so, we discount for performance gain observed purely because of the implementation language (c/ c++/ python) since it is secondary to the algorithm. thus, in the sampling phase, the parameters for deepwalk, line and node2vec are set such that they generate equal number of samples at runtime. as an example, if is the overall sampling budget, then the node2vec parameters satisfy. in the optimization phase, all these benchmarks optimize using sgd with two key differences that we correct for. first, deepwalk uses hierarchical sampling to approximate the softmax probabilities with an objective similar to the one use by node2vec. however, hierarchical softmax is inefficient when compared with negative sampling. hence, keeping everything else the same, we switch to negative sampling in deepwalk which is also the de facto approximation in node2vec and line. second, both node2vec and deepwalk have a parameter for the number of context neighborhood nodes to optimize for and the greater the number, the more rounds of optimization are required. this parameter is set to unity for line, but since line completes a single epoch quicker than other approaches, we let it run for epochs. the parameter settings used for node2vec are in line with typical values used for deepwalk and line. specifically, we set,,,, and the optimization is run for a single epoch. we repeat our experiments for random seed initializations, and our results are statistically significant with a p-value of less than 0.01. the best in-out and return hyperparameters were learned using 10-fold cross-validation on 10% labeled data with a grid search over. subsection: multi-label classification in the multi-label classification setting, every node is assigned one or more labels from a finite set. during the training phase, we observe a certain fraction of nodes and all their labels. the task is to predict the labels for the remaining nodes. this is a challenging task especially if is large. we utilize the following datasets: blogcatalog: this is a network of social relationships of the bloggers listed on the blogcatalog website. the labels represent blogger interests inferred through the meta-data provided by the bloggers. the network has 10, 312 nodes, 333, 983 edges, and 39 different labels. protein-protein interactions (ppi): we use a subgraph of the ppi network for homo sapiens. the subgraph corresponds to the graph induced by nodes for which we could obtain labels from the hallmark gene sets and represent biological states. the network has 3, 890 nodes, 76, 584 edges, and 50 different labels. wikipedia: this is a cooccurrence network of words appearing in the first million bytes of the wikipedia dump. the labels represent the part-of-speech (pos) tags inferred using the stanford pos-tagger. the network has 4, 777 nodes, 184, 812 edges, and 40 different labels. all these networks exhibit a fair mix of homophilic and structural equivalences. for example, we expect the social network of bloggers to exhibit strong homophily-based relationships; however, there might also be some\" familiar strangers\", i.e., bloggers that do not interact but share interests and hence are structurally equivalent nodes. the biological states of proteins in a protein-protein interaction network also exhibit both types of equivalences. for example, they exhibit structural equivalence when proteins perform functions complementary to those of neighboring proteins, and at other times, they organize based on homophily in assisting neighboring proteins in performing similar functions. the word cooccurence network is fairly dense, since edges exist between words cooccuring in a 2-length window in the wikipedia corpus. hence, words having the same pos tags are not hard to find, lending a high degree of homophily. at the same time, we expect some structural equivalence in the pos tags due to syntactic grammar patterns such as nouns following determiners, punctuations succeeding nouns etc. experimental results. the node feature representations are input to a one-vs-rest logistic regression classifier with l2 regularization. the train and test data is split equally over 10 random instances. we use the macro-f scores for comparing performance in table [reference] and the relative performance gain is over the closest benchmark. the trends are similar for micro-f and accuracy and are not shown. from the results, it is evident we can see how the added flexibility in exploring neighborhoods allows node2vec to outperform the other benchmark algorithms. in blogcatalog, we can discover the right mix of homophily and structural equivalence by setting parameters and to low values, giving us 22.3% gain over deepwalk and 229.2% gain over line in macro-f scores. line showed worse performance than expected, which can be explained by its inability to reuse samples, a feat that can be easily done using the random walk methods. even in our other two networks, where we have a mix of equivalences present, the semi-supervised nature of node2vec can help us infer the appropriate degree of exploration necessary for feature learning. in the case of ppi network, the best exploration strategy (,) turns out to be virtually indistinguishable from deepwalk's uniform (,) exploration giving us only a slight edge over deepwalk by avoiding redudancy in already visited nodes through a high value, but a convincing 23.8% gain over line in macro-f scores. however, in general, the uniform random walks can be much worse than the exploration strategy learned by node2vec. as we can see in the wikipedia word cooccurrence network, uniform walks can not guide the search procedure towards the best samples and hence, we achieve a gain of 21.8% over deepwalk and 33.2% over line. for a more fine-grained analysis, we also compare performance while varying the train-test split from 10% to 90%, while learning parameters and on 10% of the data as before. for brevity, we summarize the results for the micro-f and macro-f scores graphically in figure [reference]. here we make similar observations. all methods significantly outperform spectral clustering, deepwalk outperforms line, node2vec consistently outperforms line and achieves large improvement over deepwalk across domains. for example, we achieve the biggest improvement over deepwalk of 26.7% on blogcatalog at 70% labeled data. in the worst case, the search phase has little bearing on learned representations in which case node2vec is equivalent to deepwalk. similarly, the improvements are even more striking when compared to line, where in addition to drastic gain (over 200%) on blogcatalog, we observe high magnitude improvements upto 41.1% on other datasets such as ppi while training on just 10% labeled data. [b] 0.70 [b] 0.28 subsection: parameter sensitivity the node2vec algorithm involves a number of parameters and in figure [reference], we examine how the different choices of parameters affect the performance of node2vec on the blogcatalog dataset using a 50-50 split between labeled and unlabeled data. except for the parameter being tested, all other parameters assume default values. the default values for and are set to unity. we measure the macro-f score as a function of parameters and. the performance of node2vec improves as the in-out parameter and the return parameter decrease. this increase in performance can be based on the homophilic and structural equivalences we expect to see in blogcatalog. while a low encourages outward exploration, it is balanced by a low which ensures that the walk does not go too far from the start node. we also examine how the number of features and the node's neighborhood parameters (number of walks, walk length, and neighborhood size) affect the performance. we observe that performance tends to saturate once the dimensions of the representations reaches around 100. similarly, we observe that increasing the number and length of walks per source improves performance, which is not surprising since we have a greater overall sampling budget to learn representations. both these parameters have a relatively high impact on the performance of the method. interestingly, the context size, also improves performance at the cost of increased optimization time. however, the performance differences are not that large in this case. subsection: perturbation analysis for many real-world networks, we do not have access to accurate information about the network structure. we performed a perturbation study where we analyzed the performance of node2vec for two imperfect information scenarios related to the edge structure in the blogcatalog network. in the first scenario, we measure performace as a function of the fraction of missing edges (relative to the full network). the missing edges are chosen randomly, subject to the constraint that the number of connected components in the network remains fixed. as we can see in figure [reference] (top), the decrease in macro-f score as the fraction of missing edges increases is roughly linear with a small slope. robustness to missing edges in the network is especially important in cases where the graphs are evolving over time (e.g., citation networks), or where network construction is expensive (e.g., biological networks). in the second perturbation setting, we have noisy edges between randomly selected pairs of nodes in the network. as shown in figure [reference] (bottom), the performance of node2vec declines slightly faster initially when compared with the setting of missing edges, however, the rate of decrease in macro-f score gradually slows down over time. again, the robustness of node2vec to false edges is useful in several situations such as sensor networks where the measurements used for constructing the network are noisy. subsection: scalability to test for scalability, we learn node representations using node2vec with default parameter values for erdos-renyi graphs with increasing sizes from 100 to 1, 000, 000 nodes and constant average degree of 10. in figure [reference], we empirically observe that node2vec scales linearly with increase in number of nodes generating representations for one million nodes in less than four hours. the sampling procedure comprises of preprocessing for computing transition probabilities for our walk (negligibly small) and simulation of random walks. the optimization phase is made efficient using negative sampling and asynchronous sgd. many ideas from prior work serve as useful pointers in making the sampling procedure computationally efficient. we showed how random walks, also used in deepwalk, allow the sampled nodes to be reused as neighborhoods for different source nodes appearing in the walk. alias sampling allows our walks to generalize to weighted networks, with little preprocessing. though we are free to set the search parameters based on the underlying task and domain at no additional cost, learning the best settings of our search parameters adds an overhead. however, as our experiments confirm, this overhead is minimal since node2vec is semi-supervised and hence, can learn these parameters efficiently with very little labeled data. subsection: link prediction in link prediction, we are given a network with a certain fraction of edges removed, and we would like to predict these missing edges. we generate the labeled dataset of edges as follows: to obtain positive examples, we remove 50% of edges chosen randomly from the network while ensuring that the residual network obtained after the edge removals is connected, and to generate negative examples, we randomly sample an equal number of node pairs from the network which have no edge connecting them. since none of feature learning algorithms have been previously used for link prediction, we additionally evaluate node2vec against some popular heuristic scores that achieve good performance in link prediction. the scores we consider are defined in terms of the neighborhood sets of the nodes constituting the pair (see table [reference]). we test our benchmarks on the following datasets: facebook: in the facebook network, nodes represent users, and edges represent a friendship relation between any two users. the network has 4, 039 nodes and 88, 234 edges. protein-protein interactions (ppi): in the ppi network for homo sapiens, nodes represent proteins, and an edge indicates a biological interaction between a pair of proteins. the network has 19, 706 nodes and 390, 633 edges. arxiv astro-ph: this is a collaboration network generated from papers submitted to the e-print arxiv where nodes represent scientists, and an edge is present between two scientists if they have collaborated in a paper. the network has 18, 722 nodes and 198, 110 edges. experimental results. we summarize our results for link prediction in table [reference]. the best and parameter settings for each node2vec entry are omitted for ease of presentation. a general observation we can draw from the results is that the learned feature representations for node pairs significantly outperform the heuristic benchmark scores with node2vec achieving the best auc improvement on 12.6% on the arxiv dataset over the best performing baseline (adamic-adar). amongst the feature learning algorithms, node2vec outperforms both deepwalk and line in all networks with gain up to 3.8% and 6.5% respectively in the auc scores for the best possible choices of the binary operator for each algorithm. when we look at operators individually (table [reference]), node2vec outperforms deepwalk and line barring a couple of cases involving the weighted-l1 and weighted-l2 operators in which line performs better. overall, the hadamard operator when used with node2vec is highly stable and gives the best performance on average across all networks. section: discussion and conclusion in this paper, we studied feature learning in networks as a search-based optimization problem. this perspective gives us multiple advantages. it can explain classic search strategies on the basis of the exploration-exploitation trade-off. additionally, it provides a degree of interpretability to the learned representations when applied for a prediction task. for instance, we observed that bfs can explore only limited neighborhoods. this makes bfs suitable for characterizing structural equivalences in network that rely on the immediate local structure of nodes. on the other hand, dfs can freely explore network neighborhoods which is important in discovering homophilous communities at the cost of high variance. both deepwalk and line can be seen as rigid search strategies over networks. deepwalk proposes search using uniform random walks. the obvious limitation with such a strategy is that it gives us no control over the explored neighborhoods. line proposes primarily a breadth-first strategy, sampling nodes and optimizing the likelihood independently over only 1-hop and 2-hop neighbors. the effect of such an exploration is easier to characterize, but it is restrictive and provides no flexibility in exploring nodes at further depths. in contrast, the search strategy in node2vec is both flexible and controllable exploring network neighborhoods through parameters and. while these search parameters have intuitive interpretations, we obtain best results on complex networks when we can learn them directly from data. from a practical standpoint, node2vec is scalable and robust to perturbations. we showed how extensions of node embeddings to link prediction outperform popular heuristic scores designed specifically for this task. our method permits additional binary operators beyond those listed in table [reference]. as a future work, we would like to explore the reasons behind the success of hadamard operator over others, as well as establish interpretable equivalence notions for edges based on the search parameters. future extensions of node2vec could involve networks with special structure such as heterogeneous information networks, networks with explicit domain features for nodes and edges and signed-edge networks. continuous feature representations are the backbone of many deep learning algorithms, and it would be interesting to use node2vec representations as building blocks for end-to-end deep learning on graphs. acknowledgements. we are thankful to austin benson, will hamilton, rok sosi\u010d, marinka\u017eitnik as well as the anonymous reviewers for their helpful comments. this research has been supported in part by nsf cns-1010921, iis-1149837, nih bd2 k, aro muri, darpa xdata, darpa simplex, stanford data science initiative, boeing, lightspeed, sap, and volkswagen. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "blogcatalog",
                        33702
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "node2vec",
                        10
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        3604
                    ],
                    [
                        "auc",
                        44827
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "node classification task",
                        1998
                    ],
                    [
                        "classification",
                        11972
                    ],
                    [
                        "nodes",
                        30319
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "blogcatalog",
                        33702
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "node2vec",
                        10
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "macro-f",
                        37684
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "node classification task",
                        1998
                    ],
                    [
                        "classification",
                        11972
                    ],
                    [
                        "nodes",
                        30319
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "wikipedia",
                        34286
                    ],
                    [
                        "wikipedia corpus",
                        35382
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "node2vec",
                        10
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        3604
                    ],
                    [
                        "auc",
                        44827
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "node classification task",
                        1998
                    ],
                    [
                        "classification",
                        11972
                    ],
                    [
                        "nodes",
                        30319
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "wikipedia",
                        34286
                    ],
                    [
                        "wikipedia corpus",
                        35382
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "node2vec",
                        10
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "macro-f",
                        37684
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "node classification task",
                        1998
                    ],
                    [
                        "classification",
                        11972
                    ],
                    [
                        "nodes",
                        30319
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "1d73144204eb1604b26cd57ef75536f0f2753c69-31",
    "doctext": "document: subcategory-aware convolutional neural networks for object proposals and detection in convolutional neural network (cnn)- based object detection methods, region proposal becomes a bottleneck when objects exhibit significant scale variation, occlusion or truncation. in addition, these methods mainly focus on 2d object detection and can not estimate detailed properties of objects. in this paper, we propose subcategory-aware cnns for object detection. we introduce a novel region proposal network that uses subcategory information to guide the proposal generating process, and a new detection network for joint detection and subcategory classification. by using subcategories related to object pose, we achieve state-of-the-art performance on both detection and pose estimation on commonly used benchmarks. section: introduction convolutional neural networks (cnns) have become dominating in solving different recognition problems recently. cnns are powerful due to their capability in both representation and learning. with millions of weights in the contemporary cnns, they are able to learn much richer representations from data. in object detection, we have witnessed the performance boost when cnns are applied to commonly used benchmarks such as pascal voc and imagenet. however, there are two main limitations of the state-of-the-art cnn-based object detection methods. first, they rely on region proposal methods to generate object candidates, which are often based on low-level image features such as superpixels or edges. although these methods work very well on pascal voc and imagenet, however, when it comes to the kitti dataset for autonomous driving where objects have large scale variation, occlusion and truncation, these region proposal methods perform very poor as observed in our experiments. recently, the region proposal network (rpn) in is able to improve over the traditional region proposal methods. however, it still can not efficiently handle the scale change of object, occlusion and truncation. second, the existing cnn-based object detection methods mainly focus on 2d object detection with bounding boxes. as a result, they are not able to estimate detailed information about objects such as 2d segmentation boundary, 3d pose or occlusion relationship between objects, while these information is critical for various applications such as autonomous driving, robotics and augmented reality. in this work, we explore subcategory information, which is widely used in traditional object detection, to tackle the aforementioned two limitations in cnn-based object detection. for region proposal generation, we introduce a new cnn architecture that uses subcategory detections as object candidates. for detection, we modify the network in fast r-cnn for joint detection and subcategory classification. fig. [reference] illustrates our object detection framework. the concept of subcategory is general here. a subcategory can be objects with similar properties or attributes such as 2d appearance, 3d pose or 3d shape. by associating object attributes to subcategories, we are able to estimate these attributes (e.g., 2d segmentation boundary or 3d pose) by conducting subcategory classification. specifically, motivated by the traditional detection methods that train a template or a detector for each subcategory, we introduce a subcategory convolutional (conv) layer in our region proposal network (rpn), where each filter in the conv layer is trained discriminatively for subcategory detection. the subcategory conv layer outputs heat maps about the presence of certain subcategories at a specific location and scale. using these heat maps, our rpn is able to output confident subcategory detections as proposals. for classifying region proposals and refining their locations, we introduce a new object detection network by injecting subcategory information into the network proposed in fast r-cnn. our detection network is able to perform object detection and subcategory classification jointly. by using 3d voxel patterns (3dvps) as subcategories, our method is able to jointly detect the object, estimate its 3d pose, segment its boundary and estimate its occluded or truncated regions. in addition, in both our rpn and our detection cnn, we use image pyramids as input, and we introduce a new feature extrapolating layer to efficiently compute conv features in multiple scales. in this way, our method is able to detect objects with large scale variations. we conduct experiments on the kitti dataset, the pascal3d+ dataset and the pascal voc 2007 dataset. comparisons with the state-of-the-art methods on these benchmarks demonstrate the advantages of our subcategory-aware cnns for object recognition. section: related work subcategory in object detection. subcategory has been widely utilized to facilitate object detection, and different methods of discovering object subcategories have been proposed. in dpm, subcategories are discovered by clustering objects according to the aspect ratio of their bounding boxes. performs clustering according to the viewpoint of the object to discover subcategories. visual subcategories are constructed by clustering in the appearance space of object. 3dvp performs clustering in the 3d voxel space according to the visibility of the voxels. unlike previous works, we utilize subcategory to improve cnn-based detection, and our framework is general to employ different types of object subcategories. cnn-based object detection. we can categorize the state-of-the-art cnn-based object detection methods into two classes: one-stage detection and two-stage detection. in one-stage detection, such as the overfeat framework, a cnn directly processes an input image, and outputs object detections. in two-stage detection, such as r-cnns, region proposals are first generated from an input image, where different region proposal methods can be employed. then these region proposals are fed into a cnn for classification and location refinement. it is debatable which detection paradigm is better. we adopt the two-stage detection framework in this work, and consider the region proposal process to be the coarse detection step in coarse-to-fine detection. we propose a novel rpn motivated by and demonstrate its advantages. section: subcategory-aware rpn ideally, we want to have a region proposal approach that can cover objects in an input image with as few proposals as possible. since objects in images appear at different locations and scales, region proposal itself is a challenging problem. recently, proposed to tackle the region proposal problem with cnns, demonstrating the advantages of using cnns over traditional approaches for region proposal. in this section, we describe our subcategory-aware region proposal network (rpn). subsection: network architecture we introduce a novel network architecture for generating object proposals from images. the architecture is inspired by the traditional sliding-window-based object detectors, such as the aggregated channel feature (acf) detector and the deformable part model (dpm). fig. [reference] illustrates the architecture of our region proposal network. i) to handle different scales of objects, we input into our rpn an image pyramid. this pyramid is processed by several convolutional (conv) and max pooling layers to extract the conv feature maps, with one conv feature map for each scale. ii) in order to speed up the computation of conv features on image pyramids, we introduce the feature extrapolating layer, which generates feature maps for scales that are not covered by the image pyramid via extrapolation. iii) after computing the extrapolated conv feature maps, we specifically design a conv layer for object subcategory detection, where each filter in the conv layer corresponds to an object subcategory. we train these filters to make sure they fire on correct locations and scales of objects in the corresponding subcategories during the network training. the subcategory conv layer outputs a heat map for each scale, where each value in the heat map indicates the confidence of an object in the corresponding location, scale and subcategory. v) using the subcategory heat maps, we design a roi generating layer that generates object candidates (rois) by thresholding the heat maps. vi) the rois are used in a roi pooling layer to pool conv features from the extrapolated conv feature maps. vii) finally, our rpn terminates at two sibling layers: one that outputs softmax probability estimates over object subcategories, and the other layer that refines the roi location with a bounding box regressor. subsection: feature extrapolating layer in our rpn, we use fixed-size conv filters in the subcategory conv layer to localize objects (e.g., conv filters). in order to handle different scales of objects, we resort to image pyramids. an image pyramid consists of images with different resolutions obtained by rescaling the original image according to different sampled scales. after constructing the image pyramid for an input image, multi-resolution conv feature maps can be computed by applying several conv layers and max pooling layers to each image in the pyramid (fig. [reference]). if we perform convolution on every scale explicitly, it is computationally expensive, especially when a finely-sampled image pyramid is needed as in the region proposal process. in, doll\u00e1r et al. demonstrate that multi-resolution image features can be approximated by extrapolation from nearby scales rather than being computed explicitly. inspired by their work, we introduce a feature extrapolating layer to accelerate the computation of conv features on an image pyramid. specifically, a feature extrapolating layer takes as input feature maps that are supplied by the last conv layer for feature extraction, where equals to the number of scales in the input image pyramid. each feature map is a multi-dimensional array of size, with rows, columns, and channels. the width and height of the feature map corresponds to the largest scale in the image pyramid, where images in smaller scales are padded with zeros in order to generate feature maps with the same size. the feature extrapolating layer constructs feature maps at intermediate scales by extrapolating features from the nearest scales among the scales using bilinear interpolation. suppose we add intermediate scales between every th scale and th scale,. the output of the feature extrapolating layer is feature maps, each with size. since extrapolating a multi-dimensional array is much faster than computing a conv feature map explicitly, the feature extrapolating layer speeds up the feature computation with less memory. subsection: subcategory conv layer after computing the conv feature maps, we design a subcategory conv layer for subcategory detection. motivated by the traditional object detection methods that train a classifier or a template for each subcategory, we train a conv filter in the subcategory conv layer to detect a specific subcategory. suppose there are subcategories to be considered. then, the subcategory conv layer consists of conv filters with one additional conv filter for a special\" background\" category. for multi-class detection (e.g., car, pedestrian, cyclist, etc.), the subcategories are the aggregation of all the subcategories from all the classes. these conv filters operate on the extrapolated conv feature maps and output heat maps that indicate the confidences of the presence of objects in the input image. we use fixed-size conv filters in this layer (e.g., conv filters), which are trained to fire on specific scales in the feature pyramid. sec. [reference] explains how we back-propagate errors from the loss layer to train these subcategory conv filters. subsection: roi generating layer the roi generating layer takes as input heat maps and outputs a set of region proposals (rois), where is the number of scales in the feature pyramid after extrapolation. each heat map is a multi-dimensional array of size for subcategories (i.e., for roi generating, we ignore the\" background\" channel in the heat map). the roi generating layer first converts each heat map into a 2d array by performing max operation over the channels for subcategory. then, it thresholds the 2d heat map to generate rois. in this way, we measure the objectness of a region by aggregating information from subcategories. different generating strategies are used in testing and training. in testing, each location in a heat map with a score larger than a predefined threshold is used to generate rois. first, a canonical bounding box is centered on. the width and height of the box are the same as those of the conv filters (e.g.,) in the subcategory conv layer, which have an aspect ratio one. second, a number of boxes centered on with the same areas as the canonical box (e.g.,) but with different aspect ratios are generated. finally, the roi generating layer rescales the generated boxes according to the scale of the heat map, so as to cover objects in different scales and aspect ratios. in training, the roi generating layer outputs hard positive rois and hard negative rois for training the subcategory conv filters, given a budget on batch size in stochastic gradient descent. first, we use the same procedure as described in testing to generate a number of bounding boxes for each location in each heat map. second, according to the ground truth bounding boxes of objects in a training image, we compute the intersection over union (iou) overlap between the generated boxes and the ground truth boxes. bounding boxes with iou overlap larger/ smaller than some threshold (e.g., 0.5) are considered to be positive/ negative. finally, given the number of rois to be generated for each training image (i.e., batch size divided by the number of images in a batch), the roi generating layer outputs hard positives (i.e., positive bounding boxes with lowest scores in the heat maps) and hard negatives (i.e., negative bounding boxes with highest scores in the heat maps), where is the percentage of positive examples. subsection: network training after generating rois, we apply the roi pooling layer proposed in to pool conv features for each roi. then the pooled conv features are used for two tasks: subcategory classification and bounding box regression. as illustrated in fig. [reference], our rpn has two sibling output layers. the first layer outputs a discrete probability distribution, over subcategories, which is computed by applying a softmax function over the outputs of the subcategory conv layer. the second layer outputs bounding box regression offsets for object classes (). we parameterize as in, which specifies a scale-invariant translation and log-space width/ height shift relative to a roi. we employ a multi-task loss as in to train our rpn for subcategory classification and bounding box regression: where and are the truth subcategory label and the true class label respectively, is the standard cross-entropy loss, is the true bounding box regression targets for class, and is the prediction for class. we use the smoothed loss defined in for the bounding box regression loss. the indicator function indicates that bounding box regression is ignored if the roi is background (i.e.,). is a predefined weight to balance the two losses. in training, derivatives from the loss function are back-propagated (see red arrows in fig. [reference]). the two subcategory conv layers in our rpn share their weights. these weights/ conv filters are updated according to the derivatives from the softmax loss function for subcategory classification, so we are able to train these filters for subcategory detection. there is no derivative flow in computing heat maps using the subcategory conv layer and in the roi generating layer. finally, our rpn generates confident subcategory detections as region proposals. section: subcategory-aware detection network after the region proposal process, cnns are utilized to classify these proposals and refine their locations. since region proposal significantly reduces the search space, more powerful cnns can be used in the detection step, which usually contain several fully connected layers with high dimensions. in this section, we introduce our subcategory-aware object detection network for joint detection and subcategory classification. subsection: network architecture fig. [reference] illustrates the architecture of our detection network. the network is constructed based on the fast r-cnn detection network with a number of improvements. i) we use image pyramids to handle the scale variation of objects. after the last conv layer for feature extraction, we add the feature extrapolating layer to increase the number of scales in the conv feature pyramid. ii) given the region proposals generated from our rpn, we employ a roi pooling layer to pool conv features for each roi. each roi is mapped to a scale in the conv feature pyramid such that smaller rois pool features from larger scales. iii) the pooled conv features are fed into three fully connected (fc) layers, where the last fc layer is designed for subcategory classification. for subcategories, the\" subcategory fc\" layer outputs a dimensional vector with one additional dimension for the background class. we consider the output, named roi feature vector, to be an embedding in the subcategory space. iv) finally, the network terminates at three output layers. the first output layer applies a softmax function directly on the output of the\" subcategory fc\" layer for subcategory classification. the other two output layers operate on the roi feature vector and apply fc layers for object class classification and bounding box regression. subsection: network training we train our object detection network with a multi-task loss for joint object class classification, subcategory classification and bounding box regression: where is a probability distribution over subcategories, is a probability distribution over object classes, and are the truth subcategory label and the true class label respectively, and are the predicted vector and the true vector for bounding box regression respectively, and and are predefined weights to balance the losses of different tasks. and are the standard cross-entropy loss, and is the smoothed loss as in our rpn. in back-propagation training, derivatives for the multi-task loss are back-propagated to the previous layers. red arrows in fig. [reference] indicate the route of the derivative flow. section: experiments subsection: experimental settings datasets. we evaluate our object detection framework on the kitti detection benchmark, the pascal3d+ dataset and the pascal voc 2007 dataset. i) the kitti dataset consists of video frames from autonomous driving scenes, with 7, 481 images for training and 7, 518 images for testing. car, pedestrian and cyclist are evaluated for object detection. since the ground truth annotations of the kitti test set are not released, we split the kitti training images into a train set and a validation set for analyses as in. ii) the pascal3d+ dataset augments 12 rigid categories in the pascal voc 2012 with 3d annotations. each object in the 12 categories is registered with a 3d cad model. the train set of pascal voc 2012 is used for training (5, 717 images), while the val set is used for testing (5, 823 images). iii) the pascal voc 2007 dataset contains 5, 011 training images and 4, 952 testing images on 20 categories. evaluation metrics. on kitti, we evaluate our detection framework at three levels of difficulty as suggested by, i.e., easy, moderate and hard, where the difficulty is measured by the minimal scale of object to be considered and the occlusion and truncation of the object. average precision (ap) is used to measure the detection performance, where 70%, 50%, and 50% overlap thresholds are adopted by the kitti benchmark for car, pedestrian and cyclist respectively. to evaluate joint detection and orientation estimation on kitti, introduces average orientation similarity (aos), which evaluates the orientation similarity between detections and ground truths at different detection recalls. introduces average segmentation accuracy (asa) for joint detection and segmentation, and average location precision (alp) for joint detection and 3d location similar to aos. we also use these metrics here. on pascal3d+ and pascal voc 2007, the standard ap with 50% overlap ratio is adopted to evaluate object detection. for joint detection and pose estimation, we use the average viewpoint precision (avp) suggested by, where a detection is considered to be a true positive if its location and viewpoint are both correct. subcategories. we experiment with both 2d subcategories and 3d subcategories. for 2d subcategories, we cluster objects using 2d image features (i.e., aggregated channel features from). only bounding box annotations are needed for 2d subcategories. when additional annotations are available, we can obtain 3d subcategories. we adopt the 3d voxel pattern (3dvp) representation for rigid objects (i.e., car in kitti and the 12 categories in pascal3d+), which jointly models object pose, occlusion and truncation in the clustering process. each 3dvp is considered to be a subcategory. for pedestrian and cyclist in kitti, we perform clustering according to the object orientation, and each cluster is considered to be a subcategory. in this way, by subcategory classification, we can transfer the meta data carried by 3dvps (3d pose, segmentation boundary and occluded regions) to the detected object. for validation on kitti (3, 682 images for training, 3, 799 images for testing), we use 173 subcategories (125 3dvps for car, 24 poses for pedestrian and cyclist each), while for testing on kitti (7, 481 images for training, 7, 518 images for testing), we use 275 subcategories (227 3dvps for car, 24 poses for pedestrian and cyclist each). 3dvps are discovered with affinity propagation clustering, which automatically discovers the number of clusters from the data. for pascal3d+, 337 3dvps are discovered among the 12 categories. for pascal voc 2007, we use 240 2d subcategories, with 12 for each class. correspondingly, the output number of the subcategory conv layer in our rpn and that of the subcategory fc layer in our detection network equal to the number of subcategory plus one. region proposal network hyper-parameters. in our rpn, we use 5 scales for kitti in the input image pyramid and 4 scales for pascal (both pascal3d+ and pascal voc 2007), where each number indicates the rescaling factor with respect to the original image size. objects in pascal have smaller scale variation compared to objects in kitti. adding larger scales for pascal only results in marginal improvement but significantly increases the computation. the feature extrapolating layer extrapolates 4 scales with equal intervals between every two input scales, so the final conv feature pyramid has 21 scales for kitti and 16 scales for pascal. in the roi generating layer, each location in a heat map generates 7 boxes with 7 different aspect ratios for kitti and 5 aspect ratios for pascal, where each number indicates the ratio between the height and the width of the bounding box. in training the rpn, each sgd mini-batch is constructed from a single image, chosen uniformly at random. a mini-batch has size 128, with 64 positive rois and 64 negative rois, where the iou threshold is 70% for both kitti and pascal. detection network hyper-parameters. in our detection network, we use 4 scales in the input image pyramid for kitti and 2 scales for pascal, both with 4 scales extrapolated between every two scales. each sgd mini-batch is constructed from 2 images. a mini-batch has size 128, with 64 rois from each image. 25% of the rois are positive, where the iou threshold is 70% for car in kitti, and 50% for the other categories. the same sgd hyper-parameters are used as in for region proposal and detection. fine-tuning pre-trained networks. our framework is implemented in caffe. we initialize the conv layers for feature extraction in both networks and the two fc layers before subcategory fc layer in the detection network with pre-trained networks on imagenet. on kitti, we experiment with the alexnet, the vgg16 network and the googlenet. on pascal, we fine-tune the vgg16 network. subsection: analysis on kitti validation set region proposal evalutaion on recall. we evaluate the detection recall of our rpn and compare it with the state-of-the-art methods in table [reference] on the kitti validation set. for each image, we use 2k proposals for all the methods. first, two popular methods that work well on pascal voc, selective search and edge boxes, do not perform well on kitti, mainly because objects in kitti exhibit more significant scale variation, occlusion and truncation. it is challenging for a bottom-up proposal method to achieve high recall under a small budget (i.e, 2k boxes per image). second, the rpn in faster r-cnn performs much better than selective search and edge boxes, which demonstrates the ability of discriminatively trained cnns for region proposal. but we have to increase its parameter setting from 3 scales and 3 aspect ratios in to 10 scales and 7 aspect ratios in order to make it work on kitti. finally, our rpn performs on par with faster r-cnn on car, and outperforms it on pedestrian and cyclist using the same number of proposals per image. our new architecture can better handle scale variation using image pyramid. it also benefits from data mining hard training examples in our roi generating layer. region proposal evalutaion on detection and oritentaion estimation. detection recall measures the coverage of region proposals, which can not demonstrate the quality of the region proposals for detection. in this experiment, we directly measure the detection and orientation estimation performance using different region proposals. table [reference] presents the detection and orientation estimation results using rpn in faster r-cnn and the rpn we propose, while keeping the detection network the same as described in sec. [reference]. we compare our rpn with two variations of the rpn in faster r-cnn. for the first model, the rpn and the detection network are trained independently to each other (\" unshared\"). for the second model, the rpn and the detection network share their conv layers for feature extraction in order to save computation on convolution (\" shared\"). the sharing is achieved by the four-step alternating optimization training algorithm described in. by comparing the two models in table [reference], we find that sharing conv layers hurts the performance on car and pedestrian, but improves the performance on cyclist. car and pedestrian have much more training examples available than cyclist. with enough training data, the rpn and the detection network trained independently can develop conv features suitable for its own task. in this case, shared conv features degrade the performance. however, when the training data is insufficient, sharing conv features can help. in table [reference], by using region proposals from our rpn, we achieve better performance on detection and orientation estimation across all the three categories. the experimental results demonstrate the advantages of our rpn. we also tried to share the conv layers in our rpn and our detection network. however, since the architecture of our rpn after the conv layers for feature extraction is quite different from that of the detection network, we found that the training can not converge, which verifies our observation that the rpn and the detection network have developed their own conv features that are suitable for its own task. detection network evalutaion. in table [reference], we first show that our rpn achieves significantly better performance than the rpn in when the two rpns are used with fast r-cnn on the kitti validation set respectively. then, we use region proposals from our rpn and compare different variations of the network architecture for detection. i)\" ours w/ o pose\" indicates using 2d subcategories from clustering on 2d appearances of objects without using additional pose information. as we can see, our method still outperforms fatser r-cnn in this case. ii) by using pose information to obtain subcategories, our detection network is also able to estimate the orientation of the object.\" ours w/ o extra\" refers to a network without feature extrapolating. by augmenting the network with the feature extrapolating layer, our full model (\" ours full\" in table [reference]) further boosts the performance, except for a minor drop on orientation estimation of pedestrian. evaluation on 2d segmentation and 3d localization. 3dvps enable us to transfer the meta data to the detect objects, so our method is able to segment the boundary of object. in addition, after detecting the objects and estimating their 3d poses, we can back-project them into 3d using the camera parameters provided in kitti, so as to evaluate the 3d localization performance. in table [reference], we compare our method on 2d segmentation and 3d localization of car with dpm and 3dvp on the kitti validation set. we have significantly improve the segmentation accuracy and 3d location accuracy when the 2-meter threshold is used (i.e., a detection within 2 meters from the ground truth location is considered to be correct). surprisingly, obtains better 3d localization accuracy with the 1-meter threshold, which indicates that more detections from are within the 1-meter distance from the ground truth. subsection: kitti test set evaluation to compare with the state-of-the-art methods on the kitti detection benchmark, we train our rpn and detection network with all the kitti training data, and then test our method on the kitti test set by submitting our results to. table [reference] presents the detection and orientation estimation results on the three categories, where we compare our method (subcnn) with different methods evaluated on kitti. we have experimented fine-tuning both the vgg16 network and the googlenet for the detection network. our method ranks on top among all the published methods. the experimental results demonstrate the ability of our cnns in using subcategory information for detection and orientation estimation. fig. [reference] presents some examples of our detection and 3d localization results on kitti. subsection: evaluation on pascal3d+ and pascal voc we also evaluate our detection framework on the 12 categories in pascal3d+. table [reference] presents the detection results in ap and the joint detection and pose estimation results in avp. after generating region proposals from our rpn, we experiment with our detection networks with and without feature extrapolation. first, in terms of detection, our method improves over r-cnn on all 12 categories. second, in terms of join detection and pose estimation, our method significantly outperforms two state-of-the-art methods: vdpm and dpm-voc+ vp. third, feature extrapolation helps both detection and pose estimation on pascal3d+. it is worth mentioning that pascal3d+ has much fewer training examples in each subcategory compared to kitti. our pose estimation performance is limited by the number of training examples available in pascal3d+. we also note that the two recent methods achieve very appealing pose estimation results on pascal3d+. however, both of them utilize additional training images (imagenet images in and synthetic images in) and conduct detection and pose estimation with separate cnns, where a cnn is specifically designed for pose estimation. our method is capable of simultaneous object detection and viewpoint estimation even in the presence of limited training examples per viewpoint. fig. [reference] shows some detection results from our method. we again transfer segmentation masks of 3dvps to the detected objects according to the subcategory classification results. please see supplementary material for more examples. to demonstrate that our method also works on datasets with bounding box annotations only, we have conducted experiments on the pascal voc 2007 dataset, where subcategories are obtained by clustering on image features. in table [reference], we compare with fast r-cnn and faster r-cnn. we have achieved comparable performance to the state-of-the-arts. region proposal on pascal voc is relatively easy compared to kitti. so we do not see much improvement with our rpn on pascal voc 2007. section: conclusion in this work, we explore how subcategory information can be exploited in cnn-based object detection. we have proposed a novel region proposal network, and a novel object detection network, where we explicitly employ subcategory information to improve region proposal generation, object detection and object pose estimation. our subcategory-aware cnns can also handle the scale variation of objects using image pyramids in an efficient way. we have conducted extensive experiments on the kitti detection benchmark, the pascal3d+ dataset and pascal voc 2007 dataset. our method achieves the state-of-the-art results on these benchmarks. acknowledgments. we acknowledge the support of nissan grant 1188371-1-udarq and muri grant 1186514-1-tbcje. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "pascal voc",
                        1263
                    ],
                    [
                        "pascal voc 2007",
                        20483
                    ],
                    [
                        "pascal",
                        22594
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "subcategory-aware convolutional neural networks",
                        10
                    ],
                    [
                        "subcategory-aware cnns",
                        418
                    ],
                    [
                        "subcategory convolutional (conv) layer",
                        3366
                    ],
                    [
                        "subcnn",
                        30128
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "detection",
                        83
                    ],
                    [
                        "object detection",
                        445
                    ],
                    [
                        "two-stage detection",
                        5629
                    ],
                    [
                        "one-stage detection",
                        5653
                    ],
                    [
                        "object detections",
                        5759
                    ],
                    [
                        "object subcategory detection",
                        7755
                    ],
                    [
                        "detection network",
                        30260
                    ],
                    [
                        "join detection",
                        31042
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "1e5b1bf032059caddb4ef0d6bfec4706d8f3a946-32",
    "doctext": "geometric matrix completion with recurrent multi-graph neural networks section: abstract matrix completion models are among the most common formulations of recommender systems. recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/ items in the form of graphs, and imposing smoothness priors on these graphs. however, such techniques do not fully exploit the local stationarity structures of user/ item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. we propose a novel approach to overcome these limitations by using geometric deep learning on graphs. our matrix completion architecture combines graph convolutional neural networks and recurrent neural networks to learn meaningful statistical graph-structured patterns and the non-linear diffusion process that generates the known ratings. this neural network system requires a constant number of parameters independent of the matrix size. we apply our method on both synthetic and real datasets, showing that it outperforms state-of-the-art techniques. section: introduction recommender systems have become a central part of modern intelligent systems. recommending movies on netflix, friends on facebook, furniture on amazon, jobs on linkedin are a few examples of the main purpose of these systems. two major approach to recommender systems are collaborative [reference] and content [reference] use of similarities between products and customers to recommend new products. hybrid systems combine collaborative and content techniques. matrix completion. mathematically, a recommendation method can be posed as a matrix completion problem [reference], where columns and rows represent users and items, respectively, and matrix values represent a score determining whether a user would like an item or not. given a small subset of known elements of the matrix, the goal is to fill in the rest. a famous example is the netflix challenge [reference]) offered in 2009 and carrying a 1m$ prize for the algorithm that can best predict user ratings for movies based on previous ratings. the size of the netflix is 480k movies\u00d7 18k users (8.5b entries), with only 0.011% known entries. recently, there have been several attempts to incorporate geometric structure into matrix completion problems [reference][reference][reference][reference], e.g. in the form of column and row graphs representing similarity of users and items, respectively. such additional information makes well-defined e.g. the notion of smoothness of data and was shown beneficial for the performance of recommender systems. these approaches can be generally related to the field of signal processing on graphs [reference], extending classical harmonic analysis methods to non-euclidean domains. geometric deep learning. of key interest to the design of recommender systems are deep learning approaches. in the recent years, deep neural networks and, in particular, convolutional neural networks (cnns) [reference] have been applied with great success to numerous computer vision-related applications. however, original cnn models can not be directly applied to the recommendation problem to extract meaningful patterns in users, items and ratings because these data are not euclidean structured, i.e. they do not lie on regular lattices like images but irregular domains like graphs or manifolds. this strongly motivates the development of geometric deep learning [reference] techniques that can mathematically deal with graph-structured data, which arises in numerous applications, ranging from computer graphics and vision [reference][reference][reference][reference] to chemistry [reference]. the earliest attempts to apply neural networks to graphs are due to [reference] (see more recent formulation [reference][reference]). [reference][reference] formulated cnn-like deep neural architectures on graphs in the spectral domain, employing the analogy between the classical fourier transforms and projections onto the eigenbasis of the graph laplacian operator [reference]. in a follow-up work, proposed an efficient filtering scheme using recurrent chebyshev polynomials, which reduces the complexity of cnns on graphs to the same complexity of standard cnns (on grids). this model was later extended to deal with dynamic data [reference] main contribution. in this work, we treat matrix completion problem as deep learning on graph-structured data. we introduce a novel neural network architecture that is able to extract local stationary patterns from the highdimensional spaces of users and items, and use these meaningful representations to infer the non-linear temporal diffusion mechanism of ratings. the spatial patterns are extracted by a new cnn architecture designed to work on multiple graphs. the temporal dynamics of the rating diffusion is produced by a long-short term memory (lstm) recurrent neural network (rnn) [reference]. to our knowledge, our work is the first application of graph-based deep learning to matrix completion problem. the rest of the paper is organized as follows. section 2 reviews the matrix completion models. section 3 presents the proposed approach. section 4 presents experimental results demonstrating the efficiency of our techniques on synthetic and real-world datasets, and section 5 concludes the paper. section: background section: matrix completion matrix completion problem. recovering the missing values of a matrix given a small fraction of its entries is an ill-posed problem without additional mathematical constraints on the space of solutions. a well-posed problem is to assume that the variables lie in a smaller subspace, i.e., that the matrix is of low rank, where x denotes the matrix to recover,\u03c9 is the set of the known entries and y ij are their values. to make (1) robust against noise and perturbation, the equality constraint can be replaced with a penalty where\u03c9 is the indicator matrix of the known entries\u03c9 and\u2022 denotes the hadamard pointwise product. unfortunately, rank minimization turns out an np-hard combinatorial problem that is computationally intractable in practical cases. the tightest possible convex relaxation of the previous problem is where\u00b7 is the nuclear norm of a matrix equal to the sum of its singular values [reference]). [reference] proved that the 1 relaxation of the svd lead to solutions that recover almost exactly the original low-rank matrix. geometric matrix completion an alternative relaxation of the rank operator in (1) is to constraint the space of solutions to be smooth w.r.t. some geometric structure of the matrix rows and columns [reference][reference][reference][reference]. the simplest model is proximity structure represented as an undirected weighted column graph g c= ({1,..., n}, e c, w c) with in our notation, the column graph could be thought of as a social network capturing relations between users and the similarity of their tastes. the row graph g r= ({1,..., m}, e r, w r) representing the similarities of the items is defined in a similar manner. on each of these graphs one can construct the (unnormalized) graph laplacian, an n\u00d7 n symmetric positivesemidefinite matrix w ij is the degree matrix. we denote the laplacian associated with row and column graphs by\u2206 r and\u2206 c, respectively. considering the columns (respectively, rows) of matrix x as vector-valued functions on the column graph g c (respectively, row graph g r), their smoothness can be expressed as the dirichlet norm x 2 gr= trace (x\u2206 r x) (respecitvely, x gc= trace (x\u2206 c x)). the geometric matrix completion problem thus boils down to minimizing n users factorized models. matrix completion algorithms introduced in the previous section are well-posed as convex optimization problems, guaranteeing existence, uniqueness and robustness of solutions. besides, fast algorithms have been developed in the context of compressed sensing to solve the non-differential nuclear norm problem. however, the variables in this formulation are the full m\u00d7 n matrix x, making such methods hard to scale up to large matrices such as the notorious netflix challenge. a solution is to use a factorized representation [reference][reference][reference][reference][reference][reference]) x= wh, where w, h are m\u00d7 r and n\u00d7 r matrices, respectively, with r min (m, n). the use of factors w, h reduce the number of degrees of freedom from o (mn) to o (m+ n); this representation is also attractive as solving the matrix completion problem often assumes the original matrix to be low-rank, and rank (wh)\u2264 r by construction. figure 1 shows the full and factorized settings of the matrix completion problem. the nuclear norm minimization problem in the previous section can be equivalently rewritten in a factorized form as [reference]: and the factorized formulation of the graph-based minimization problem (4) as the limitation of model (6) is to decouple the regularization process applied simultaneously on the rows and columns of x in (4), but the advantage is linear instead of quadratic complexity. section: deep learning on graphs the key idea to our work is geometric deep learning, an extension of the popular cnns to graphs. a graph laplacian admits a spectral eigendecomposition of the form\u2206=\u03c6\u03bb\u03c6, where\u03c6= (\u03c6 1,...\u03c6 n) denotes the matrix of orthonormal eigenvectors and\u03bb= diag (\u03bb 1,...,\u03bb n) is the diagonal matrix of the corresponding eigenvalues. the eigenvectors play the role of fourier atoms in classical harmonic analysis and the eigenvalues can be interpreted as frequencies. given a function x= (x 1,..., x n) on the vertices of the graph, its graph fourier transform is given byx=\u03c6 x. the spectral convolution of two functions x, y can be defined as the element-wise product of the respective fourier transforms, bruna et al. 2013 used the spectral definition of convolution (7) to generalize cnns on graphs. a spectral convolutional layer has the form where q, q denote the number of input and output channels, respectively,\u0177 ll= diag (\u0177 ll, 1,...,\u0177 ll, n) is a diagonal matrix of spectral multipliers representing a learnable filter in the spectral domain, and\u03be is a nonlinearity (e.g. relu) applied on the vertex-wise function values. unlike classical convolutions carried out efficiently in the spectral domain using fft, the computations of the forward and inverse graph fourier transform incur expensive o (n 2) multiplication by the matrices\u03c6,\u03c6, as there are no fftlike algorithms on general graphs. furthermore, there is no guarantee that the filters represented in the spectral domain are localized in the spatial domain, which is an important property of classical cnns. to address these issues, [reference] proposed using an explicit expansion in the chebyshev polynomial basis to represent the spectral filters row+ column filtering figure 2. recurrent gcnn (rgcnn) architecture using the full matrix completion model and operating simultaneously on the rows and columns of the matrix x. the output of the multi-graph cnn (mgcnn) module is a q-dimensional feature vector for each element of the input matrix. the number of parameters to learn is o (1) and the learning complexity is o (mn). n\u2206\u2212 i is the rescaled laplacian such that its eigenvalues\u03bb= 2\u03bb\u03b8 is the p-dimensional vector of polynomial coefficients parametrizing the filter, and t j (\u03bb)= 2\u03bbt j\u22121 (\u03bb)\u2212t j\u22122 (\u03bb) denotes the chebyshev polynomial of degree j defined in a recursive manner with t 1 (\u03bb)=\u03bb and t 0 (\u03bb)= 1. 1 this approach benefits from several advantages. first, it does not require an explicit computation of the laplacian eigenvectors, and due to the recursive definition of the chebyshev polynomials, the computation of the filter incurs applying the laplacian p times. multiplication by laplacian has the cost of o (|e|), and assuming the graph has|e|= o (n) edges (which is the case for k-nearest neighbors graphs and most real-world networks), the overall complexity is o (n) rather than o (n 2) operations, which is the same complexity than standard cnns. moreover, since the laplacian is a local operator affecting only 1-hop neighbors of a vertex and accordingly its (p\u2212 1) st power affects the p-hop neighborhood, the resulting filters are spatially localized. section: our approach in this paper, we propose formulating matrix completion as a learnable diffusion process applied to the score values. the deep learning architecture considered for this purpose consists of a spatial part extracting spatial features from the matrix (we consider two different approaches working on the full and factorized matrix models), and a temporal part using a recurrent lstm network. the two architectures are 1 tj (\u03bb)= cos (j cos\u22121 (\u03bb)) is an oscillating function on [\u22121, 1] with j roots, j+ 1 equally spaced extrema, and a frequency linearly dependent on j. chebyshev polynomials form an orthogonal basis for the space of smooth functions on [\u22121, 1] and are thus convenient to compactly represent spectral filters. summarized in figures 2 and 3 and described in details in the following. section: multi-graph cnns multi-graph convolution. our first goal is to extend the notion of the aforementioned graph fourier transform to matrices whose rows and columns are defined on row-and column-graphs. we recall that a classical two-dimensional fourier transform of an image (matrix) can be thought of as applying a one-dimensional fourier transform to its rows and columns. in our setting, the analogy of the twodimensional fourier transform has the form where\u03c6 c,\u03c6 r and\u03bb c,\u03bb r denote the n\u00d7 n and m\u00d7 m eigenvector-and eigenvalue matrices of the column-and row-graph laplacians\u2206 c,\u2206 r, respectively. the multigraph version of the spectral convolution (7) is given by representing the filters as their spectral multipliers\u0177 would yield o (mn) parameters, prohibitive in any practical application. to overcome this limitation, we resort to the representation of the filters in chebychev polynomial bases of degree p, where\u03b8= (\u03b8 jj) is the (p+ 1)\u00d7 (p+ 1) matrix of coefficients, i.e., o (1) parameters. the application of such filters to the matrix x a multi-graph cnn (mgcnn) using this parametrization of filters (13) in the convolutional layer is applied to the m\u00d7n matrix x (single input channel), producing q outputs (i.e., a tensor of size m\u00d7 n\u00d7 q). separable convolution. a simplification of the multigraph convolution is obtained considering the factorized form of the matrix x= wh and applying onedimensional convolution on the respective graph to each factor, w with 2 (p+ 1) qq parameters in total. section: matrix diffusion with rnn the next step of our approach is to feed the features extracted from the matrix by the mgcnn (or alternatively, the row-and column-gcnns) to a recurrent neural network (rnn) implementing the score diffusion process. we use the classical long-short term memory (lstm) rnn architecture [reference], which has demonstrated to be highly efficient to learn the dynamical property of data sequences as lstm is able to keep long-term internal states (in particular, avoiding the vanishing gradient issue). the input of the lstm gate is given by the static features extracted from the mgcnn, which can be seen as a projection or dimensionality reduction of the original matrix in the space of the most meaningful and representative information (the disentanglement effect). this representation coupled with lstm appears particularly well-suited to keep a long term internal state, which allows to predict accurate small changes dx of the matrix x (or dw, dh of the factors w, h) that can propagate through the full temporal steps. figures 2 and 3 provides an illustration of the proposed matrix completion model. we also give a precise description of the two settings of our model in algorithms 1 and 2. we refer to the whole architecture combining the mgcnn and rnn in the full matrix completion setting as recurrent graph cnn (rgcnn). the factorized version with two gcnns and rnn is referred to as separable recurrent graph cnn (srgcnn). the complexity of algorithm 1 scales quadratically as o (mn) due to the use of mgcnn. for large matrices, we can opt for algorithm 2 that processes the rows and columns separately with standard gcnns and scales linearly as o (m+ n). section: algorithm 1 full matrix completion model using rgcnn input m\u00d7 n matrix x (0) containing initial values 1: for t= 0: t do 2: apply the multi-graph cnn (13) on x (t) producing an m\u00d7n\u00d7q outputx (t) containing a q-dimensional feature vector for each matrix element. section: 3: for all elements (i, j) do section: 4: apply rnn to feature vectorx ijq) producing the predicted incremental value dx (t) ij 5: 7: end for algorithm 2 factorized matrix completion model using srgcnn input m\u00d7r factor h (0) and n\u00d7r factor w (0) representing the matrix x apply the graph cnn on h (t) producing an n\u00d7 q outputh (t). 3: apply rnn to feature vectorh jq) producing the predicted incremental value dh update apply the graph cnn on w (t) producing an m\u00d7 q outputw (t). 8: apply rnn to feature vectorw iq) producing the predicted incremental value dw 12: end for section: training training of the networks is performed by minimizing the loss here, t denotes the number of diffusion iterations (applications of the rnn), and we use the notation x (t)\u03b8,\u03c3 to emphasize that the matrix depends on the parameters of the mgcnn (chebyshev polynomial coefficients\u03b8) and those of the lstm (denoted by\u03c3). in the factorized setting, we use the loss where\u03b8 c,\u03b8 r are the parameters of the two gcnns. section: results experimental settings. we closely followed the experimental setup of [reference], using five standard datasets: synthetic dataset from [reference], movielens [reference], flixster [reference], douban [reference], and yahoomusic [reference]. classical matrix completion (mc) [reference], inductive matrix completion (imc) [reference][reference], geometric matrix completion (gmc) [reference], and graph regularized alternating least squares (grals) [reference] were used as baseline methods. in all the experiments, we used the following settings for our rgcnns: chebyshev polynomials of order p= 5, outputting k= 32-dimensional features, lstm cells with 32 features and t= 10 diffusion steps. all the models were implemented in google tensorflow and trained using the adam stochastic optimization algorithm [reference] with learning rate 10\u22123. in factorized models, rank r= 15 and 10 was used for the synthetic and real datasets, respectively. for all methods, hyperparameters were chosen by cross-validation. section: synthetic data we start our experimental evaluation showing the performance of our approach on a small synthetic dataset, in which the user and item graphs have strong communities structure. though rather simple, such a dataset allows to study the behavior of different algorithms in controlled settings. the performance of different matrix completion methods is reported in table 1, along with their theoretical complexity. our rgcnn model achieves the best accuracy, followed by the separable rgcnn. different diffusion time steps of these two models are visualized in figure 4. figure 5 shows the convergence rates of different methods. figures 6 and 7 depict the spectral filters learnt by the mgcnn and row-and column-gcnns. we repeated the same experiment considering only the column (users) graph to be given. in this setting, the rgcnn can not be applied, while the srgcnn has only one gcnn applied on the factor h, and the other factor w is free. figure 4. evolution of the matrix x (t) with our architecture using full matrix completion model rgcnn (top) and factorized matrix completion model srgcnn (bottom). numbers indicate the rms error. figure 5. convergence rates of the tested algorithms over the synthetic netflix dataset. section: real data following [reference], we evaluated the proposed approach on the movielens, flixster, douban and yahoomusic datasets. for the movielens dataset we constructed the user and item (movie) graphs as unweighted 10-nearest neighbor graphs in the space of user and movie features, respectively. for flixster, the user and item graphs were constructed from the scores of the original matrix. on this dataset, we also performed an experiment using only the users graph. for the douban dataset, we used only the user graph (the provided social network of the user). for the yahoomusic dataset, we used only the item graph, constructed with unweighted 10-nearest neighbors in the space of item features (artists, albums, and genres). for the latter three datasets, we used a sub-matrix of 3000\u00d7 3000 entries for evaluating the performance. tables 3 and 4 summarize the performance of different methods. rgcnn outperforms the competitors in all the experiments. section: conclusion in this paper, we presented a new deep learning approach for matrix completion based on a specially designed multigraph convolutional neural network architecture. among the key advantages of our approach compared to traditional methods is its low computational complexity and constant number of degrees of freedom independent of the matrix size. we showed that the use of deep learning for matrix completion allows to beat current state-of-the-art recommender system methods. to our knowledge, our work is the first application of deep learning on graphs to this class of problems. we believe that it shows the potential of the nascent field of geometric deep learning on non-euclidean domains, and will encourage future works in this direction. section: section: acknowledgments section:",
    "templates": [
        {
            "Material": [
                [
                    [
                        "douban",
                        17799
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "separable recurrent graph cnn",
                        16038
                    ],
                    [
                        "srgcnn",
                        16070
                    ],
                    [
                        "separable rgcnn",
                        19110
                    ],
                    [
                        "factorized matrix completion model srgcnn",
                        19694
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "rms error",
                        19767
                    ]
                ]
            ],
            "Task": []
        },
        {
            "Material": [
                [
                    [
                        "netflix",
                        2167
                    ],
                    [
                        "flixster",
                        17777
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "separable recurrent graph cnn",
                        16038
                    ],
                    [
                        "srgcnn",
                        16070
                    ],
                    [
                        "separable rgcnn",
                        19110
                    ],
                    [
                        "factorized matrix completion model srgcnn",
                        19694
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "rms error",
                        19767
                    ]
                ]
            ],
            "Task": []
        },
        {
            "Material": [
                [
                    [
                        "movielens",
                        17754
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "separable recurrent graph cnn",
                        16038
                    ],
                    [
                        "srgcnn",
                        16070
                    ],
                    [
                        "separable rgcnn",
                        19110
                    ],
                    [
                        "factorized matrix completion model srgcnn",
                        19694
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "rms error",
                        19767
                    ]
                ]
            ],
            "Task": []
        }
    ]
}
{
    "docid": "1efdad6f91e830fd64306e4625f74191b05ef9c4-33",
    "doctext": "numerous important problems can be framed as learning from graph data. we propose a framework for learning convolutional neural networks for arbitrary graphs. these graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient. section: introduction with this paper we aim to bring convolutional neural networks to bear on a large class of graph-based learning problems. we consider the following two problems. given a collection of graphs, learn a function that can be used for classification and regression problems on unseen graphs. the nodes of any two graphs are not necessarily in correspondence. for instance, each graph of the collection could model a chemical compound and the output could be a function mapping unseen compounds to their level of activity against cancer cells. given a large graph, learn graph representations that can be used to infer unseen graph properties such as node types and missing edges. we propose a framework for learning representations for classes of directed and undirected graphs. the graphs may have nodes and edges with multiple discrete and continuous attributes and may have multiple types of edges. similar to convolutional neural network for images, we construct locally connected neighborhoods from the input graphs. these neighborhoods are generated efficiently and serve as the receptive fields of a convolutional architecture, allowing the framework to learn effective graph representations. the proposed approach builds on concepts from convolutional neural networks (cnns) for images and extends them to arbitrary graphs. figure [reference] illustrates the locally connected receptive fields of a cnn for images. an image can be represented as a square grid graph whose nodes represent pixels. now, a cnn can be seen as traversing a node sequence (nodes-in figure [reference] (a)) and generating fixed-size neighborhood graphs (the x grids in figure [reference] (b)) for each of the nodes. the neighborhood graphs serve as the receptive fields to read feature values from the pixel nodes. due to the implicit spatial order of the pixels, the sequence of nodes for which neighborhood graphs are created, from left to right and top to bottom, is uniquely determined. the same holds for nlp problems where each sentence (and its parse-tree) determines a sequence of words. however, for numerous graph collections a problem-specific ordering (spatial, temporal, or otherwise) is missing and the nodes of the graphs are not in correspondence. in these instances, one has to solve two problems: (i) determining the node sequences for which neighborhood graphs are created and (ii) computing a normalization of neighborhood graphs, that is, a unique mapping from a graph representation into a vector space representation. the proposed approach, termed patchy-san, addresses these two problems for arbitrary graphs. for each input graph, it first determines nodes (and their order) for which neighborhood graphs are created. for each of these nodes, a neighborhood consisting of exactly nodes is extracted and normalized, that is, it is uniquely mapped to a space with a fixed linear order. the normalized neighborhood serves as the receptive field for a node under consideration. finally, feature learning components such as convolutional and dense layers are combined with the normalized neighborhood graphs as the cnn's receptive fields. figure [reference] illustrates the patchy-san architecture which has several advantages over existing approaches: first, it is highly efficient, naively parallelizable, and applicable to large graphs. second, for a number of applications, ranging from computational biology to social network analysis, it is important to visualize learned network motifs. patchy-san supports feature visualizations providing insights into the structural properties of graphs. third, instead of crafting yet another graph kernel, patchy-san learns application dependent features without the need to feature engineering. our theoretical contributions are the definition of the normalization problem on graphs and its complexity; a method for comparing graph labeling approaches for a collection of graphs; and a result that shows that patchy-san generalizes cnns on images. using standard benchmark data sets, we demonstrate that the learned cnns for graphs are both efficient and effective compared to state of the art graph kernels. section: related work graph kernels allow kernel-based learning approaches such as svms to work directly on graphs. kernels on graphs were originally defined as similarity functions on the nodes of a single graph. two representative classes of kernels are the skew spectrum kernel and kernels based on graphlets. the latter is related to our work, as it builds kernels based on fixed-sized subgraphs. these subgraphs, which are often called motifs or graphlets, reflect functional network properties. however, due to the combinatorial complexity of subgraph enumeration, graphlet kernels are restricted to subgraphs with few nodes. an effective class of graph kernels are the weisfeiler-lehman (wl) kernels. wl kernels, however, only support discrete features and use memory linear in the number of training examples at test time. patchy-san uses wl as one possible labeling procedure to compute receptive fields. deep graph kernels and graph invariant kernels compare graphs based on the existence or count of small substructures such as shortest paths, graphlets, subtrees, and other graph invariants. in contrast, patchy-san learns substructures from graph data and is not limited to a predefined set of motifs. moreover, while all graph kernels have a training complexity at least quadratic in the number of graphs, which is prohibitive for large-scale problems, patchy-san scales linearly with the number of graphs. graph neural networks (gnns) are a recurrent neural network architecture defined on graphs. gnns apply recurrent neural networks for walks on the graph structure, propagating node representations until a fixed point is reached. the resulting node representations are then used as features in classification and regression problems. gnns support only discrete labels and perform as many backpropagation operations as there are edges and nodes in the graph per learning iteration. gated graph sequence neural networks modify gnns to use gated recurrent units and to output sequences. recent work extended cnns to topologies that differ from the low-dimensional grid structure. all of these methods, however, assume one global graph structure, that is, a correspondence of the vertices across input examples. perform convolutional type operations on graphs, developing a differentiable variant of one specific graph feature. section: background we provide a brief introduction to the required background in convolutional networks and graph theory. subsection: convolutional neural networks cnns were inspired by earlier work that showed that the visual cortex in animals contains complex arrangements of cells, responsible for detecting light in small local regions of the visual field. cnns were developed in the s and have been applied to image, speech, text, and drug discovery problems. a predecessor to cnns was the neocognitron. a typical cnn is composed of convolutional and dense layers. the purpose of the first convolutional layer is the extraction of common patterns found within local regions of the input images. cnns convolve learned filters over the input image, computing the inner product at every image location in the image and outputting the result as tensors whose depth is the number of filters. subsection: graphs a graph is a pair with the set of vertices and the set of edges. let be the number of vertices and the number of edges. each graph can be represented by an adjacency matrix of size, where if there is an edge from vertex to vertex, and otherwise. in this case, we say that vertex has position in. moreover, if we say and are adjacent. node and edge attributes are features that attain one value for each node and edge of a graph. we use the term attribute value instead of label to avoid confusion with the graph-theoretical concept of a labeling. a walk is a sequence of nodes in a graph, in which consecutive nodes are connected by an edge. a path is a walk with distinct nodes. we write to denote the distance between and, that is, the length of the shortest path between and. is the-neighborhood of a node, that is, all nodes that are adjacent to. labeling and node partitions. patchy-san utilizes graph labelings to impose an order on nodes. a graph labeling is a function from the set of vertices to an ordered set such as the real numbers and integers. a graph labeling procedure computes a graph labeling for an input graph. when it is clear from the context, we use labeling to refer to both, the graph labeling and the procedure to compute it. a ranking (or coloring) is a function. every labeling induces a ranking with if and only if. if the labeling of graph is injective, it determines a total order of's vertices and a unique adjacency matrix of where vertex has position in. moreover, every graph labeling induces a partition on with if and only if. examples of graph labeling procedures are node degree and other measures of centrality commonly used in the analysis of networks. for instance, the betweeness centrality of a vertex computes the fractions of shortest paths that pass through. the weisfeiler-lehman algorithm is a procedure for partitioning the vertices of a graph. it is also known as color refinement and naive vertex classification. color refinement has attracted considerable interest in the ml community since it can be applied to speed-up inference in graphical models and as a method to compute graph kernels. patchy-san applies these labeling procedures, among others (degree, page-rank, eigenvector centrality, etc.), to impose an order on the nodes of graphs, replacing application-dependent orders (temporal, spatial, etc.) where missing. isomorphism and canonicalization. the computational problem of deciding whether two graphs are isomorphic surfaces in several application domains. the graph isomorphism (gi) problem is in np but not known to be in p or np-hard. under several mild restrictions, gi is known to be in p. for instance, gi is in p for graphs of bounded degree. a canonicalization of a graph is a graph with a fixed vertex order which is isomorphic to and which represents its entire isomorphism class. in practice, the graph canonicalization tool nauty has shown remarkable performance. section: learning cnns for arbitrary graphs when cnns are applied to images, a receptive field (a square grid) is moved over each image with a particular step size. the receptive field reads the pixels' feature values, for each channel once, and a patch of values is created for each channel. since the pixels of an image have an implicit arrangement- a spatial order- the receptive fields are always moved from left to right and top to bottom. moreover, the spatial order uniquely determines the nodes of each receptive field and the way these nodes are mapped to a vector space representation (see figure [reference] (b)). consequently, the values read from two pixels using two different locations of the receptive field are assigned to the same relative position if and only if the pixels' structural roles (their spatial position within the receptive field) are identical. to show the connection between cnns and patchy-san, we frame cnns on images as identifying a sequence of nodes in the square grid graph representing the image and building a normalized neighborhood graph- a receptive field- for each node in the identified sequence. for graph collections where an application-dependent node order is missing and where the nodes of any two graphs are not yet aligned, we need to determine for each graph (i) the sequences of nodes for which we create neighborhoods, and (ii) a unique mapping from the graph representation to a vector representation such that nodes with similar structural roles in the neighborhood graphs are positioned similarly in the vector representation. we address these problems by leveraging graph labeling procedures that assigns nodes from two different graphs to a similar relative position in their respective adjacency matrices if their structural roles within the graphs are similar. given a collection of graphs, patchy-san (select-assemble-normalize) applies the following steps to each graph: (1) select a fixed-length sequence of nodes from the graph; (2) assemble a fixed-size neighborhood for each node in the selected sequence; (3) normalize the extracted neighborhood graph; and (4) learn neighborhood representations with convolutional neural networks from the resulting sequence of patches. in the following, we describe methods that address the above-mentioned challenges. [t!] selnodeseq: select node sequence {algorithmic} [1] graph labeling procedure\u2113, graph= g (v, e), stride s, width w, receptive field size k= top w elements of v according to\u2113 f to each input channel= j+ j1 subsection: node sequence selection node sequence selection is the process of identifying, for each input graph, a sequence of nodes for which receptive fields are created. algorithm [reference] lists one such procedure. first, the vertices of the input graph are sorted with respect to a given graph labeling. second, the resulting node sequence is traversed using a given stride and for each visited node, algorithm [reference] is executed to construct a receptive field, until exactly receptive fields have been created. the stride determines the distance, relative to the selected node sequence, between two consecutive nodes for which a receptive field is created. if the number of nodes is smaller than, the algorithm creates all-zero receptive fields for padding purposes. several alternative methods for vertex sequence selection are possible. for instance, a depth-first traversal of the input graph guided by the values of the graph labeling. we leave these ideas to future work. [t!] neighassemb: neighborhood assembly {algorithmic} [1] vertex v, receptive field size k set of neighborhood nodes n for v and>|l|0 the set of vertices n subsection: neighborhood assembly for each of the nodes identified in the previous step, a receptive field has to be constructed. algorithm [reference] first calls algorithm [reference] to assembles a local neighborhood for the input node. the nodes of the neighborhood are the candidates for the receptive field. algorithm [reference] lists the neighborhood assembly steps. given as inputs a node and the size of the receptive field, the procedure performs a breadth-first search, exploring vertices with an increasing distance from, and adds these vertices to a set. if the number of collected nodes is smaller than, the-neighborhood of the vertices most recently added to are collected, and so on, until at least vertices are in, or until there are no more neighbors to add. note that at this time, the size of is possibly different to. subsection: graph normalization the receptive field for a node is constructed by normalizing the neighborhood assembled in the previous step. illustrated in figure [reference], the normalization imposes an order on the nodes of the neighborhood graph so as to map from the unordered graph space to a vector space with a linear order. the basic idea is to leverage graph labeling procedures that assigns nodes of two different graphs to a similar relative position in the respective adjacency matrices if and only if their structural roles within the graphs are similar. to formalize this intuition, we define the optimal graph normalization problem which aims to find a labeling that is optimal relative to a given collection of graphs. theorem: (optimal graph normalization). let g be a collection of unlabeled graphs with k nodes, let\u2113 be an injective graph labeling procedure, let dg be a distance measure on graphs with k nodes, and let da be a distance measure on\u00d7kk matrices. find^\u2113 such that the problem amounts to finding a graph labeling procedure, such that, for any two graphs drawn uniformly at random from, the expected difference between the distance of the graphs in vector space (with respect to the adjacency matrices based on) and the distance of the graphs in graph space is minimized. the optimal graph normalization problem is a generalization of the classical graph canonicalization problem. a canonical labeling algorithm, however, is optimal only for isomorphic graphs and might perform poorly for graphs that are similar but not isomorphic. in contrast, the smaller the expectation of the optimal normalization problem, the better the labeling aligns nodes with similar structural roles. note that the similarity is determined by. [t!] receptivefield: create receptive field {algorithmic} [1] vertex v, graph labeling\u2113, receptive field size k gnorm we have the following result concerning the complexity of the optimal normalization problem. theorem:. optimal graph normalization is np-hard. proof: proof. by reduction from subgraph isomorphism.\u220e patchy-san does not solve the above optimization problem. instead, it may compare different graph labeling methods and choose the one that performs best relative to a given collection of graphs. [t!] normalizegraph: graph normalization {algorithmic} [1] subset of vertices u from original graph g, vertex v, graph labeling\u2113, receptive field size k receptive field for v ranking r of u using\u2113, subject to:\u2208\u2200u, wu\u2062d (u, v)<\u2062d (w, v)\u21d2\u2062r (u)<\u2062r (w) top k vertices in u according to r ranking r of n using\u2113, subject to:\u2208\u2200u, wn\u2062d (u, v)<\u2062d (w, v)\u21d2\u2062r (u)<\u2062r (w) and-k|u| dummy nodes the subgraph\u2062g [n] for the vertices n\u2062g [n], respecting the prior coloring r\u2062g [n] theorem:. let g be a collection of graphs and let (g1, g1\u2032),\u2026, (gn, gn\u2032) be a sequence of pairs of graphs sampled independently and uniformly at random from g. let:=^\u03b8\u2113\u2211=i1n/\u2062da (\u2062a\u2113 (gi),\u2062a\u2113 (gi\u2032)) n and:=\u03b8\u2113\u2062eg [| -\u2062da (\u2062a\u2113 (g),\u2062a\u2113 (g\u2032))\u2062dg (g, g\u2032)|]. if\u2265dadg, then<\u2062eg [^\u03b8\u21131]\u2062eg [^\u03b8\u21132] if and only if<\u03b8\u21131\u03b8\u21132. theorem [reference] enables us to compare different labeling procedures in an unsupervised manner via a comparison of the corresponding estimators. under the assumption, the smaller the estimate the smaller the absolute difference. therefore, we can simply choose the labeling for which is minimal. the assumption holds, for instance, for the edit distance on graphs and the hamming distance on adjacency matrices. finally, note that all of the above results can be extended to directed graphs. the graph normalization problem and the application of appropriate graph labeling procedures for the normalization of local graph structures is at the core of the proposed approach. within the patchy-san framework, we normalize the neighborhood graphs of a vertex. the labeling of the vertices is therefore constrained by the graph distance to: for any two vertices, if is closer to than, then is always ranked higher than. this definition ensures that has always rank, and that the closer a vertex is to in, the higher it is ranked in the vector space representation. since most labeling methods are not injective, it is necessary to break ties between same-label nodes. to do so, we use nauty. nauty accepts prior node partitions as input and breaks remaining ties by choosing the lexicographically maximal adjacency matrix. it is known that graph isomorphism is in ptime for graphs of bounded degree. due to the constant size of the neighborhood graphs, the algorithm runs in time polynomial in the size of the original graph and, on average, in time linear in. our experiments verify that computing a canonical labeling of the graph neigborhoods adds a negligible overhead. algorithm [reference] lists the normalization procedure. if the size of the input set is larger than, it first applies the ranking based on to select the top nodes and recomputes a ranking on the smaller set of nodes. if the size of is smaller than, it adds disconnected dummy nodes. finally, it induces the subgraph on the vertices and canonicalizes the graph taking the ranking as prior coloring. we can relate patchy-san to cnns for images as follows. theorem:. given a sequence of pixels taken from an image. applying patchy-san with receptive field size (-\u20622m1) 2, stride s, no zero padding, and 1-wl normalization to the sequence is identical (up to a fixed permutation of the receptive field) to the first layer of a cnn with receptive field size -\u20622m1, stride s, and no zero padding. proof: proof. it is possible to show that if an input graph is a square grid, then the-wl normalized receptive field constructed for a vertex is always a square grid graph with a unique vertex order.\u220e subsection: convolutional architecture patchy-san is able to process both vertex and edge attributes (discrete and continuous). let be the number of vertex attributes and let be the number of edge attributes. for each input graph, it applies normalized receptive fields for vertices and edges which results in one and one tensor. these can be reshaped to a and a tensors. note that and are the number of input channels. we can now apply a-dimensional convolutional layer with stride and receptive field size to the first and to the second tensor. the rest of the architecture can be chosen arbitrarily. we may use merge layers to combine convolutional layers representing nodes and edges, respectively. section: complexity and implementation patchy-san's algorithm for creating receptive fields is highly efficient and naively parallelizable because the fields are generated independently. we can show the following asymptotic worst-case result. theorem:. let n be the number of graphs, let k be the receptive field size, w the width, and\u2062o (\u2062f (n, m)) the complexity of computing a given labeling\u2113 for a graph with n vertices and m edges. patchy-san has a worst-case complexity of\u2062o (\u2062nw (+\u2062f (n, m)\u2062nlog (n) exp (k))) for computing the receptive fields for n graphs. proof: proof. node sequence selection requires the labeling of each input graph and the retrieval of the highest ranked nodes. for the creation of normalized graph patches, most computational effort is spent applying the labeling procedure to a neighborhood whose size may be larger than. let be the maximum degree of the input graph, and the neighborhood returned by algorithm [reference]. we have. the term comes from the worst-case complexity of the graph canonicalization algorithm nauty on a node graph.\u220e for instance, for the weisfeiler-lehman algorithm, which has a complexity of, and constants and, the complexity of patchy-san is linear in and quasi-linear in and. section: experiments we conduct three types of experiments: a runtime analysis, a qualitative analysis of the learned features, and a comparison to graph kernels on benchmark data sets. [subfigure] labelformat= empty subsection: runtime analysis we assess the efficiency of patchy-san by applying it to real-world graphs. the objective is to compare the rates at which receptive fields are generated to the rate at which state of the art cnns perform learning. all input graphs are part of the collection of the python module graph-toolhttps:// graph-tool.skewed.de/. for a given graph, we used patchy-san to compute a receptive field for all nodes using the-dimensional weisfeiler-lehman (1-wl) algorithm for the normalization. torus is a periodic lattice with nodes; random is a random undirected graph with nodes and a degree distribution and; power is a network representing the topology of a power grid in the us; polbooks is a co-purchasing network of books about us politics published during the presidential election; preferential is a preferential attachment network model where newly added vertices have degree; astro-ph is a coauthorship network between authors of preprints posted on the astrophysics arxiv; email-enron is a communication network generated from about half a million sent emails. all experiments were run on commodity hardware with 64 g ram and a single 2.8 ghz cpu. figure [reference] depicts the receptive fields per second rates for each input graph. for receptive field size and patchy-san creates fields at a rate of more than except for email-enron with a rate of and, respectively. for, the largest tested size, fields are created at a rate of at least. a cnn with convolutional and dense layers learns at a rate of about-training examples per second on the same machine. hence, the speed at which receptive fields are generated is sufficient to saturate a downstream cnn. subsection: feature visualization the visualization experiments' aim is to qualitatively investigate whether popular models such as the restricted boltzman machine (rbm) can be combined with patchy-san for unsupervised feature learning. for every input graph, we have generated receptive fields for all nodes and used these as input to an rbm. the rbm had hidden nodes and was trained for epochs with contrastive divergence and a learning rate of. we visualize the features learned by a single-layer rbm for-dimensional weisfeiler-lehman (1-wl) normalized receptive fields of size. note that the features learned by the rbm correspond to reoccurring receptive field patterns. figure [reference] depicts some of the features and samples drawn from it for four different graphs. subsection: graph classification graph classification is the problem of assigning graphs to one of several categories. data sets. we use standard benchmark data sets to compare run-time and classification accuracy with state of the art graph kernels: mutag, pct, nci1, nci109, protein, and d& d. mutag is a data set of nitro compounds where classes indicate whether the compound has a mutagenic effect on a bacterium. ptc consists of chemical compounds where classes indicate carcinogenicity for male and female rats. nci1 and nci109 are chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer cell lines. proteins is a graph collection where nodes are secondary structure elements and edges indicate neighborhood in the amino-acid sequence or in 3d space. graphs are classified as enzyme or non-enzyme. d& d is a data set of protein structures classified into enzymes and non-enzymes. experimental set-up. we compared patchy-san with the shortest-path kernel (sp), the random walk kernel (rw), the graphlet count kernel (gk), and the weisfeiler-lehman subtree kernel (wl). similar to previous work, we set the height parameter of wl to, the size of the graphlets for gk to, and chose the decay factor for rw from. we performed-fold cross-validation with lib-svm, using folds for training and for testing, and repeated the experiments times. we report average prediction accuracies and standard deviations. for patchy-san (referred to as pscn), we used-dimensional wl normalization, a width equal to the average number of nodes (see table [reference]), and receptive field sizes of and. for the experiments we only used node attributes. in addition, we ran experiments for where we combined receptive fields for nodes and edges using a merge layer (). to make a fair comparison, we used a single network architecture with two convolutional layers, one dense hidden layer, and a softmax layer for all experiments. the first convolutional layer had output channels (feature maps). the second conv layer has output channels, a stride of, and a field size of. the convolutional layers have rectified linear units. the dense layer has rectified linear units with a dropout rate of. dropout and the relatively small number of neurons are needed to avoid overfitting on the smaller data sets. the only hyperparameter we optimized is the number of epochs and the batch size for the mini-batch gradient decent algorithm rmsprop. all of the above was implemented with the theano wrapper keras. we also applied a logistic regression (pslr) classifier on the patches for. moreover, we ran experiments with the same set-up on larger social graph data sets (up to graphs each, with an average of nodes), and compared patchy-san with previously reported results for the graphlet count (gk) and the deep graphlet count kernel (dgk). we used the normalized node degree as attribute for patchy-san, highlighting one of its advantages: it can easily incorporate continuous features. results. table [reference] lists the results of the experiments. we omit the results for nci109 as they are almost identical to nci1. despite using a one-fits-all cnn architecture, the cnns accuracy is highly competitive with existing graph kernels. in most cases, a receptive field size of results in the best classification accuracy. the relatively high variance can be explained with the small size of the benchmark data sets and the fact that the cnns hyperparameters (with the exception of epochs and batch size) were not tuned to individual data sets. similar to the experience on image and text data, we expect patchy-san to perform even better for large data sets. moreover, patchy-san is between and times more efficient than the most efficient graph kernel (wl). we expect the performance advantage to be much more pronounced for data sets with a large number of graphs. results for betweeness centrality normalization are similar with the exception of the runtime which increases by about%. logistic regression applied to patchy-san's receptive fields performs worse, indicating that patchy-san works especially well in conjunction with cnns which learn non-linear feature combinations and which share weights across receptive fields. patchy-san is also highly competitive on the social graph data. it significantly outperforms the other two kernels on four of the six data sets and achieves ties on the rest. table [reference] lists the results of the experiments. section: conclusion and future work we proposed a framework for learning graph representations that are especially beneficial in conjunction with cnns. it combines two complementary procedures: (a) selecting a sequence of nodes that covers large parts of the graph and (b) generating local normalized neighborhood representations for each of the nodes in the sequence. experiments show that the approach is competitive with state of the art graph kernels. directions for future work include the use of alternative neural network architectures such as rnns; combining different receptive field sizes; pretraining with rbms and autoencoders; and statistical relational models based on the ideas of the approach. section: acknowledgments many thanks to the anonymous icml reviewers who provided tremendously helpful comments. the research leading to these results has received funding from the european union's horizon 2020 innovation action program under grant agreement no 653449-types. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "graph data",
                        59
                    ],
                    [
                        "random",
                        23962
                    ],
                    [
                        "power grid",
                        24090
                    ],
                    [
                        "image and text data",
                        29468
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "gated graph sequence neural networks",
                        6722
                    ],
                    [
                        "patchy-san (select-assemble-normalize",
                        12878
                    ],
                    [
                        "pscn",
                        27355
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        26068
                    ],
                    [
                        "average prediction accuracies",
                        27269
                    ],
                    [
                        "cnns accuracy",
                        29066
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "classification and regression problems",
                        898
                    ],
                    [
                        "graphs",
                        4738
                    ],
                    [
                        "naive vertex classification",
                        10014
                    ],
                    [
                        "graph classification",
                        25890
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "nci1",
                        26141
                    ],
                    [
                        "nci109",
                        26147
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "gated graph sequence neural networks",
                        6722
                    ],
                    [
                        "patchy-san (select-assemble-normalize",
                        12878
                    ],
                    [
                        "pscn",
                        27355
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        26068
                    ],
                    [
                        "average prediction accuracies",
                        27269
                    ],
                    [
                        "cnns accuracy",
                        29066
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "classification and regression problems",
                        898
                    ],
                    [
                        "graphs",
                        4738
                    ],
                    [
                        "naive vertex classification",
                        10014
                    ],
                    [
                        "graph classification",
                        25890
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "1f8f0abfe4689aa93f2f6cc7ec4fd4c6adc2c2d6-34",
    "doctext": "document: semantic instance segmentation with a discriminative loss function semantic instance segmentation remains a challenging task. in this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a representation of the image that can easily be clustered into instances with a simple post-processing step. the loss function encourages the network to map each pixel to a point in feature space so that pixels belonging to the same instance lie close together while different instances are separated by a wide margin. our approach of combining an off-the-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and distinct from recent efforts in instance segmentation. in contrast to previous works, our method does not rely on object proposals or recurrent mechanisms. a key contribution of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform on-par with more complex methods. moreover, we show that it does not suffer from some of the limitations of the popular detect-and-segment approaches. we achieve competitive performance on the cityscapes and cvppp leaf segmentation benchmarks. section: introduction semantic instance segmentation has recently gained in popularity. as an extension of regular semantic segmentation, the task is to generate a binary segmentation mask for each individual object along with a semantic label. it is considered a fundamentally harder problem than semantic segmentation-where overlapping objects of the same class are segmented as one-and is closely related to the tasks of object counting and object detection. one could also say instance segmentation is a generalization of object detection, with the goal of producing a segmentation mask rather than a bounding box for each object. pinheiro obtain bounding boxes from instance segmentations by simply drawing the tightest bounding box around each segmentation mask, and show that their system reaches state-of-the-art performance on an object detection benchmark. the relation between instance segmentation and semantic segmentation is less clear. intuitively, the two tasks feel very closely related, but it turns out not to be obvious how to apply the network architectures and loss functions that are successful in semantic segmentation to this related instance task. one key factor that complicates the naive application of the popular softmax cross-entropy loss function to instance segmentation, is the fact that an image can contain an arbitrary number of instances and that the labeling is permutation-invariant: it does not matter which specific label an instance gets, as long as it is different from all other instance labels. one possible solution is to set an upper limit to the number of detectable instances and to impose extra constraints on the labeling, but this may unnecessarily limit the representational power of the network and introduce unwanted biases, leading to unsatisfying results. most recent works on instance segmentation with deep networks go a different route. two popular approaches introduce a multi-stage pipeline with object proposals, or train a recurrent network end-to-end with a custom loss function that outputs instances sequentially. another line of research is to train a network to transform the image into a representation that is clustered into individual instances with a post-processing step. our method belongs to this last category, but takes a more principled (less ad-hoc) approach than previous works and reduces the post-processing step to a minimum. inspired by the success of siamese networks and the triplet loss in image classification, we introduce a discriminative loss function to replace the pixel-wise softmax loss that is commonly used in semantic segmentation. our loss function enforces the network to map each pixel in the image to an n-dimensional vector in feature space, such that feature vectors of pixels that belong to the same instance lie close together while feature vectors of pixels that belong to different instances lie far apart. the output of the network can easily be clustered with a fast and simple post-processing operation. with this mechanism, we optimize an objective that avoids the aforementioned problems related to variable number of instances and permutation-invariance. our work mainly focuses on the loss function, as we aim to be able to re-use network architectures that were designed for semantic segmentation: we plug in an off-the-shelf architecture and retrain the system with our discriminative loss function. in our framework, the tasks of semantic and instance segmentation can be treated in a consistent and similar manner and do not require changes on the architecture side. the rest of this paper is structured as follows. first we give an extensive overview of the related work in section [reference]. in section [reference] we discuss our proposed method in detail. in section [reference] we set up experiments on two instance segmentation benchmarks and show that we get a performance that is competitive with the state-of-the-art. section: related work in the last few years, deep networks have achieved impressive results in semantic and instance segmentation. all top-performing methods across different benchmarks use a deep network in their pipeline. here we discuss these prior works and situate our model between them. proposal-based many instance segmentation approaches build a multi-stage pipeline with a separate object proposal and classification step. hariharan and chen use mcg to generate category-independent region proposals, followed by a classification step. pinheiro use the same general approach, but their work focuses on generating segmentation proposals with a deep network. dai won the 2015 ms-coco instance segmentation challenge with a cascade of networks (mnc) to merge bounding boxes, segmentation masks and category information. many works were inspired by this approach and also combine an object detector with a semantic segmentation network to produce instances. in contrast to these works, our method does not rely on object proposals or bounding boxes but treats the image holistically, which we show to be beneficial for handling certain tasks with complex occlusions as discussed in section [reference]. recurrent methods other recent works employ recurrent networks to generate the individual instances sequentially. stewart train a network for end-to-end object detection using an lstm. their loss function is permutation-invariant as it incorporates the hungarian algorithm to match candidate hypotheses to ground-truth instances. inspired by their work, romera propose an end-to-end recurrent network with convolutional lstms that sequentially outputs binary segmentation maps for each instance. ren improve upon by adding a box network to confine segmentations within a local window and skip connections instead of graphical models to restore the resolution at the output. their final framework consists of four major components: an external memory and networks for box proposal, segmentation and scoring. we argue that our proposed method is conceptually simpler and easier to implement than these methods. our method does not involve recurrent mechanisms and can work with any off-the-shelf segmentation architecture. moreover, our loss function is permutation-invariant by design, without the need to resort to a hungarian algorithm. clustering another approach is to transform the image into a representation that is subsequently clustered into discrete instances. silberman produce a segmentation tree and use a coverage loss to cut it into non-overlapping regions. zhang impose an ordering on the individual instances based on their depth, and use a mrf to merge overlapping predicted patches into a coherent segmentation. two earlier works also use depth information to segment instances. uhrig train a network to predict each pixel's direction towards its instance center, along with monocular depth and semantic labels. they use template matching and proposal fusion techniques to extract the individual instances from this representation. liang predict pixel-wise feature vectors representing the ground truth bounding box of the instance it belongs to. with the help of a sub-network that predicts an object count, they cluster the output of the network into individual instances. our work is similar to these works in that we have a separate clustering step, but our loss does not constrain the output of the network to a specific representation like instance center coordinates or depth ordering; it is less ad-hoc in that sense. other bai use deep networks to directly learn the energy of the watershed transform. a drawback of this bottom-up approach is that they can not handle occlusions where instances are separated into multiple pieces. kirillov use a crf, but with a novel multicut formulation to combine semantic segmentations with edge maps to extract instances as connected regions. a shortcoming of this method is that, although they reason globally about instances, they also can not handle occlusions. arnab combine an object detector with a semantic segmentation module using a crf model. by considering the image holistically it can handle occlusions and produce more precise segmentations. loss function our loss function is inspired by earlier works on distance metric learning, discriminative loss functions and siamese networks. most similar to our loss function, weinberger propose to learn a distance metric for large margin nearest neighbor classification. kostinger further explore a similar lda based objective. more recently schroff, building on sun, introduced the triplet loss for face recognition. the triplet loss enforces a margin between each pair of faces from one person, to all other faces. xie propose a clustering objective for unsupervised learning. whereas these works employ a discriminative loss function to optimize distances between images in a dataset, our method operates at the pixel level, optimizing distances between individual pixels in an image. to our knowledge, we are the first to successfully use a discriminative loss based on distance metric learning principles for the task of instance segmentation with deep networks. section: method subsection: discriminative loss function consider a differentiable function that maps each pixel in an input image to a point in n-dimensional feature space, referred to as the pixel embedding. the intuition behind our loss function is that embeddings with the same label (same instance) should end up close together, while embeddings with a different label (different instance) should end up far apart. weinberger propose a loss function with two competing terms to achieve this objective: a term to penalize large distances between embeddings with the same label, and a term to penalize small distances between embeddings with a different label. in our loss function we keep the first term, but replace the second term with a more tractable one: instead of directly penalizing small distances between every pair of differently-labeled embeddings, we only penalize small distances between the mean embeddings of different labels. if the number of different labels is smaller than the number of inputs, this is computationally much cheaper than calculating the distances between every pair of embeddings. this is a valid assumption for instance segmentation, where there are orders of magnitude fewer instances than pixels in an image. we now formulate our discriminative loss in terms of push (i.e. repelling) and pull forces between and within clusters. a cluster is defined as a group of pixel embeddings sharing the same label, e.g. pixels belonging to the same instance. our loss consists of three terms: variance term: an intra-cluster pull-force that draws embeddings towards the mean embedding, i.e. the cluster center. distance term: an inter-cluster push-force that pushes clusters away from each other, increasing the distance between the cluster centers. regularization term: a small pull-force that draws all clusters towards the origin, to keep the activations bounded. the variance and distance terms are hinged: their forces are only active up to a certain distance. embeddings within a distance of from their cluster centers are no longer attracted to it, which means that they can exist on a local manifold in feature space rather than having to converge to a single point. analogously, cluster centers further apart than are no longer repulsed and can move freely in feature space. hinging the forces relaxes the constraints on the network, giving it more representational power to achieve its goal. the interacting forces in feature space are illustrated in figure [reference]. the loss function can also be written down exactly. we use the following definitions: is the number of clusters in the ground truth, is the number of elements in cluster, is an embedding, is the mean embedding of cluster (the cluster center), is the l1 or l2 distance, and denotes the hinge. and are respectively the margins for the variance and distance loss. the loss can then be written as follows: in our experiments we set and. the loss is minimized by stochastic gradient descent. comparison with softmax loss we discuss the relation of our loss function with the popular pixel-wise multi-class cross-entropy loss, often referred to as the softmax loss. in the case of a softmax loss with classes, each pixel embedding is driven to a one-hot vector, i.e. the unit vector on one of the axes of an-dimensional feature space. because the softmax function has the normalizing property that its outputs are positive and sum to one, the embeddings are restricted to lie on the unit simplex. when the loss reaches zero, all embeddings lie on one of the unit vectors. by design, the dimensions of the output feature space (which correspond to the number of feature maps in the last layer of the network) must be equal to the number of classes. to add a class after training, the architecture needs to be updated too. in comparison, our loss function does not drive the embeddings to a specific point in feature space. the network could for example place similar clusters (e.g. two small objects) closer together than dissimilar ones (e.g. a small and a large object). when the loss reaches zero, the system of push and pull forces has minimal energy and the clusters have organized themselves in n-dimensional space. most importantly, the dimensionality of the feature space is independent of the number of instances that needs to be segmented. figure [reference] depicts the convergence of our loss function when overfitting on a single image with instances, in a 2-dimensional feature space. subsection: post-processing when the variance and distance terms of the loss are zero, the following is true: all embeddings are within a distance of from their cluster center all cluster centers are at least apart if, then each embedding is closer to its own cluster center than to any other cluster center. it follows that during inference, we can threshold with a bandwidth around a cluster center to select all embeddings belonging to that cluster. thresholding in this case means selecting all embeddings that lie within a hypersphere with radius around the cluster center: for the tasks of classification and semantic segmentation, with a fixed set of classes, this leads to a simple strategy for post-processing the output of the network into discrete classes: after training, calculate the cluster centers of each class over the entire training set. during inference, threshold around each of the cluster centers to select all pixels belonging to the corresponding semantic class. this requires that the cluster centers of a specific class are the same in each image, which can be accomplished by coupling the cluster centers across a mini-batch. for the task of instance segmentation things are more complicated. as the labeling is permutation invariant, we can not simply record cluster centers and threshold around them during inference. we could follow a different strategy: if we set, then each embedding is closer to all embeddings of its own cluster than to any embedding of a different cluster. it follows that we can threshold around any embedding to select all embeddings belonging to the same cluster. the procedure during inference is to select an unlabeled pixel, threshold around its embedding to find all pixels belonging to the same instance, and assign them all the same label. then select another pixel that does not yet belong to an instance and repeat until all pixels are labeled. increasing robustness in a real-world problem the loss on the test set will not be zero, potentially causing our clustering algorithm for instance segmentation to make mistakes. if a cluster is not compact and we accidentally select an outlier to threshold around, it could happen that a real cluster gets predicted as two sub-clusters. to avoid this issue, we make the clustering more robust against outliers by applying a fast variant of the mean-shift algorithm. as before, we select a random unlabeled pixel and threshold around its embedding. next however, we calculate the mean of the selected group of embeddings and use the mean to threshold again. we repeat this process until mean convergence. this has the effect of moving to a high-density area in feature space, likely corresponding to a true cluster center. in the experiments section, we investigate the effect of this clustering algorithm by comparing against ground truth clustering, where the thresholding targets are calculated as an average embedding over the ground truth instance labels. subsection: pros and cons our proposed method has some distinctive advantages and disadvantages compared to other methods that we now discuss. one big limitation of detect-and-segment approaches that is not immediately apparent from their excellent results on popular benchmarks, is that they rely on the assumption that an object's segmentation mask can be unambiguously extracted from its bounding box. this is an implicit prior that is very effective for datasets like ms coco and pascal voc, which contain relatively blobby objects that do not occlude each other in complex ways. however, the assumption is problematic for tasks where an object's bounding box conveys insufficient information to recover the object's segmentation mask. consider the synthetic scattered sticks dataset shown in figure [reference] as an example to illustrate the issue. when two sticks overlap like two crossed swords, their bounding boxes are highly overlapping. given only a detection in the form of a bounding box, it is exceedingly hard to unambigously extract a segmentation mask of the indicated object. methods that rely on bounding boxes in their pipeline all suffer from this issue. in contrast, our method can handle such complex occlusions without problems as it treats the image holistically and learns to reason about occlusions, but does not employ a computationally expensive crf like. many real-world industrial or medical applications (conveyor belt sorting systems, overlapping cell and chromosome segmentation, etc.) exhibit this kind of occlusions. to the best of our knowledge no sufficiently large datasets for such tasks are publicly available, which unfortunately prevents us from showcasing this particular strength of our method to the full. on the other hand, our method also has some drawbacks. due to the holistic treatment of the image, our method performs well on datasets with a lot of similarity across the images (traffic scenes in cityscapes or leaf configurations in cvppp), but underperforms on datasets where objects can appear in random constellations and diverse settings, like pascal voc and mscoco. a sliding-window detection-based approach with non-max suppression is more suited for such datasets. for example, if our method were trained on images with only one object, it would perform badly on an image that unexpectedly contained many of these objects. a detection-based approach has no trouble with this. section: experiments we test our loss function on two instance segmentation datasets: the cvppp leaf segmentation dataset and the cityscapes instance-level semantic labeling task. these datasets contain a median number of more than 15 instances per image. we also study the influence of the different components of our method and point out where there is room for improvement. subsection: datasets the lsc competition of the cvppp workshop is a small but challenging benchmark. the task is to individually segment each leaf of a plant. the dataset was developed to encourage the use of computer vision methods to aid in the study of plant phenotyping. we use the a1 subset which consists of 128 labeled images and 33 test images. gives an overview of results on this dataset. we compare our performance with some of these works and two other recent approaches. we report two metrics defined in: symmetric best dice (), which denotes the accuracy of the instance segmentation and absolute difference in count () which is the absolute value of the mean of the difference between the predicted number of leaves and the ground truth over all images. the large-scale cityscapes dataset focuses on semantic understanding of urban street scenes. it has a benchmark for pixel-level and instance-level semantic segmentation. we test our method on the latter, using only the fine-grained annotations. the dataset is split up in 2975 training images, 500 validation images, and 1525 test images. we tune hyperparameters using the validation set and only use the train set to train our final model. we compare our results with the published works in the official leaderboard. we report accuracy using 4 metrics defined in: mean average precision (ap), mean average precision with overlap of 50% (ap0.5), ap50 m and ap100 m, where evaluation is restricted to objects within 50 m and 100 m distance, respectively. subsection: setup model architecture and general setup since we want to stress the fact that our loss can be used with an off-the-shelf network, we use the resnet-38 network, designed for semantic segmentation. we finetune from a model that was pre-trained on cityscapes semantic segmentation. in the following experiments, all models are trained using adam, with a learning rate of 1e-4 on a nvidia titan x gpu. leaf segmentation since this dataset only consists of 128 images, we use online data augmentation to prevent the model from overfitting and to increase the overall robustness. we apply random left-right flip, random rotation with and random scale deformation with. all images are rescaled to 512x512 and concatenated with an x-and y-coordinate map with values between-1 and 1. we train the network with margins,, and 16 output dimensions. foreground masks are included with the test set, since this challenge only focuses on instance segmentation. cityscapes our final model is trained on the training images, downsampled to. because of the size and variability of the dataset, there is no need for extra data augmentation. we train the network with margins,, and 8 output dimensions. in contrast to the cvppp dataset, cityscapes is a multi-class instance segmentation challenge. therefore, we run our loss function independently on every semantic class, so that instances belonging to the same class are far apart in feature space, whereas instances from different classes can occupy the same space. for example, the cluster centers of a pedestrian and a car that appear in the same image are not pushed away from each other. we use a pretrained resnet-38 network to generate segmentation masks for the semantic classes. subsection: analysis of the individual components the final result of the semantic instance segmentation is influenced by three main components: the performance of the network architecture with our loss function, the quality of the semantic labels, and the post-processing step. to disentangle the effects of the semantic segmentation and the post-processing and to point out potential for improvement, we run two extra experiments on the cityscapes validation set: semantic segmentation vs ground truth for the cityscapes challenge, we rely on semantic segmentation masks to make a distinction between the different classes. since our instance segmentation will discard regions not indicated in the semantic segmentation labels, the results will be influenced by the quality of the semantic segmentation masks. to measure the size of this influence, we also report performance with the ground truth semantic segmentation masks. mean shift clustering vs ground truth clustering since the output of our network needs to be clustered into discrete instances, the clustering method can potentially influence the accuracy of the overall instance segmentation. in this experiment, we measure this influence by clustering with our adapted mean shift clustering algorithm versus thresholding around the mean embeddings over the ground truth instance masks, as explained in section [reference]). subsection: results and discussion figure [reference] shows some results of our method on the validation set of the cvppp dataset. the network makes very few mistakes: only the segmentation of the smallest leafs and the leaf stalks sometimes show a small error. table [reference] contains the numerical results. we achieve competitive results (sbd score of 84.2) that are on-par with the state-of-the art (sbd score of 84.9). we outperform all non-deep learning methods and also the recurrent instance segmentation of, with a method that is arguably less complex. figure [reference] shows some visual results on the cityscapes validation set. we see that even in difficult scenarios, with street scenes containing a lot of cars or pedestrians, our method often manages to identify the individual objects. failure cases mostly involve the splitting up of a single object into multiple instances or incorrect merging of neighboring instances. this happens in the lower left example, where the two rightmost cars are merged. another failure mode is incorrect semantic segmentation: in the lower right example, the semantic segmentation network accidentally mistakes an empty bicycle storage for actual bikes. the instance segmentation network is left no choice but to give it a shot, and tries to split up the imaginary bikes into individual objects. nevertheless, we achieve competitive results on the cityscapes leaderboard, outperforming all but one unpublished work. note that we perform on-par with the mnc-based method sais on this dataset. see table [reference] for a complete overview. a video of the results is available at. as discussed in section [reference], we are interested to know the influence of the quality of the semantic segmentations and the clustering algorithm on the overall performance. the results of these experiments can be found in table [reference]. as expected, the performance increases when we switch out a component with its ground truth counterpart. the effect of the semantic segmentation is the largest: comparing the first row (our method) to the third row, we see a large performance increase when replacing the resnet-38 semantic segmentation masks with the ground truth masks. this can be explained by the fact that the average precision metric is an average over the semantic classes. some classes like tram, train and bus almost never have more than one instance per image, causing the semantic segmentation to have a big influence on this metric. it is clear that the overall performance can be increased by having better semantic segmentations. the last two entries of the table show the difference between ground truth clustering and mean shift clustering, both using ground truth segmentation masks. here also, there is a performance gap. the main reason is that the loss on the validation set is not zero which means the constraints imposed by the loss function are not met. clustering using mean-shift will therefore not lead to perfect results. the effect is more pronounced for small instances, as also noticeable in the shown examples. subsection: speed-accuracy trade-off to investigate the trade-off between speed, accuracy and memory requirements, we train four different network models on different resolutions and evaluate them on the car class of the cityscapes validation set. this also illustrates the benefit that our method can be used with any off-the-shelf network designed for semantic segmentation. table [reference] shows the results. we can conclude that resnet-38 is best for accuracy, but requires some more memory. if speed is important, enet would favor over segnet since it is much faster with almost the same accuracy. it also shows that running on a higher resolution than 768x384 does n't increase accuracy much for the tested networks. note that the post-processing step can be implemented efficiently, causing only a negligible overhead. section: conclusion in this work we have proposed a discriminative loss function for the task of instance segmentation. after training, the output of the network can be clustered into discrete instances with a simple post-processing thresholding operation that is tailored to the loss function. furthermore, we showed that our method can handle complex occlusions as opposed to popular detect-and-segment approaches. our method achieves competitive performance on two benchmarks. in this paper we still used a pretrained network to produce the semantic segmentation masks. we will investigate the joint training of instance and semantic segmentation with our loss function in future work. acknowledgement: this work was supported by toyota, and was carried out at the trace lab at ku leuven (toyota research on automated cars in europe-leuven). bibliography: references",
    "templates": [
        {
            "Material": [],
            "Method": [],
            "Metric": [
                [
                    [
                        "ap",
                        22099
                    ]
                ]
            ],
            "Task": []
        },
        {
            "Material": [],
            "Method": [
                [
                    [
                        "discriminative loss function",
                        48
                    ],
                    [
                        "discriminative loss",
                        10326
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        21300
                    ]
                ]
            ],
            "Task": []
        }
    ]
}
{
    "docid": "22634b09c3c83f4a959f4f732b03ec3c92808094-35",
    "doctext": "document: deepmatching: hierarchical deformable dense matching we introduce a novel matching algorithm, called deepmatching, to compute dense correspondences between images. deepmatching relies on a hierarchical, multi-layer, correlational architecture designed for matching images and was inspired by deep convolutional approaches. the proposed matching algorithm can handle non-rigid deformations and repetitive textures and efficiently determines dense correspondences in the presence of significant changes between images. we evaluate the performance of deepmatching, in comparison with state-of-the-art matching algorithms, on the mikolajczyk mikolajczyk2005, the mpi-sintel sintel and the kitti kitti datasets. deepmatching outperforms the state-of-the-art algorithms and shows excellent results in particular for repetitive textures. we also propose a method for estimating optical flow, called deepflow, by integrating deepmatching in the large displacement optical flow (ldof) approach of []. compared to existing matching algorithms, additional robustness to large displacements and complex motion is obtained thanks to our matching approach. deepflow obtains competitive performance on public benchmarks for optical flow estimation. section: introduction computing correspondences between related images is a central issue in many computer vision problems, ranging from scene recognition to optical flow estimation forsyth2011computer, szeliski2010. the goal of a matching algorithm is to discover shared visual content between two images, and to establish as many as possible precise point-wise correspondences, called matches. an essential aspect of matching approaches is the amount of rigidity they assume when computing the correspondences. in fact, matching approaches range between two extreme cases: stereo matching, where matching hinges upon strong geometric constraints, and matching ''in the wild'', where the set of possible transformations from the source image to the target one is large and the problem is basically almost unconstrained. effective approaches have been designed for matching rigid objects across images in the presence of large viewpoint changes lowe2004, barnes2010, hacohen2011. however, the performance of current state-of-the-art matching algorithms for images ''in the wild'', such as consecutive images in real-world videos featuring fast non-rigid motion, still calls for improvement mdpof, chen2013. in this paper, we aim at tackling matching in such a general setting. matching algorithms for images ''in the wild'' need to accommodate several requirements, that turn out to be often in contradiction. on one hand, matching objects necessarily requires rigidity assumptions to some extent. it is also mandatory that these objects have sufficiently discriminative textures to make the problem well-defined. on the other hand, many objects or regions are not rigid objects, like humans or animals. furthermore, large portions of an image are usually occupied by weakly-to-no textured regions, often with repetitive textures, like sky or bucolic background. descriptor matching approaches, such as sift lowe2004 or hog dalal2005, bro11a matching, compute discriminative feature representations from rectangular patches. however, while these approaches succeed in case of rigid motion, they fail to match regions with weak or repetitive textures, as local patches are poorly discriminative. furthermore, matches are usually poor and imprecise in case of non-rigid deformations, as these approaches rely on rigid patches. discriminative power can be traded against increased robustness to non-rigid deformations. indeed, propagation-based approaches, such as generalized patchmatch barnes2010 or non-rigid dense correspondences hacohen2011, compute simple feature representations from small patches and propagate matches to neighboring patches. they yield good performance in case of non-rigid deformations. however, matching repetitive textures remains beyond the reach of these approaches. in this paper we propose a novel approach, called deepmatching, that gracefully combines the strengths of these two families of approaches. deepmatching is computed using a multi-layer architecture, which breaks down patches into a hierarchy of sub-patches. this architecture allows to work at several scales and handle repetitive textures. furthermore, within each layer, local matches are computed assuming a restricted set of feasible rigid deformations. local matches are then propagated up the hierarchy, which progressively discard spurious incorrect matches. we called our approach deepmatching, as it is inspired by deep convolutional approaches. in summary, we make three contributions: dense matching: we propose a matching algorithm, deepmatching, that allows to robustly determine dense correspondences between two images. it explicitly handles non-rigid deformations, with bounds on the deformation tolerance, and incorporates a multi-scale scoring of the matches, making it robust to repetitive or weak textures. furthermore, our approach is based on gradient histograms, and thus robust to appearance changes caused by illumination and color variations. fast, scale/ rotation-invariant matching: we propose a computationally efficient version of deepmatching, which performs almost as well as exact deepmatching, but at a much lower memory cost. furthermore, this fast version of deepmatching can be extended to a scale and rotation-invariant version, making it an excellent competitor to state-of-the-art descriptor matching approaches. large-displacement optical flow: we propose an optical flow approach which uses deepmatching in the matching term of the large displacement variational energy minimization of []. we show that deepmatching is a better choice compared to the hog descriptor used by [] and other state-of-the-art matching algorithms. the approach, named deepflow, obtains competitive results on public optical flow benchmarks. this paper is organized as follows. after a review of previous works (section [reference]), we start by presenting the proposed matching algorithm, deepmatching, in section [reference]. then, section [reference] describes several extensions of deepmatching. in particular, we propose an optical flow approach, deepflow, in section [reference]. finally, we present experimental results in section [reference]. a preliminary version of this article has appeared in. this version adds (1) an in-depth presentation of deepmatching; (2) an enhanced version of deepmatching, which improves the match scoring and the selection of entry points for backtracking; (3) proofs on time and memory complexity of deepmatching as well as its deformation tolerance; (4) a discussion on the connection between deep convolutional neural networks and deepmatching; (5) a fast approximate version of deepmatching; (6) a scale and rotation invariant version of deepmatching; and (7) an extensive experimental evaluation of deepmatching on several state-of-the-art benchmarks. the code for deepmatching as well as deepflow are available at and. note that we provide a gpu implementation in addition to the cpu one. section: related work in this section we review related work on ''general'' image matching, that is matching without prior knowledge and constraints, and on matching in the context of optical flow estimation, that is matching consecutive images in videos. subsection: general image matching image matching based on local features has been extensively studied in the past decade. it has been applied successfully to various domains, such as wide baseline stereo matching furukawa2010 and image retrieval philbin2010. it consists of two steps, i.e., extracting local descriptors and matching them. image descriptors are extracted in rigid (generally square) local frames at sparse invariant image locations mikolajczyk2005, szeliski2010. matching then equals nearest neighbor search between descriptors, followed by an optional geometric verification. note that a confidence value can be obtained by computing the uniqueness of a match, i.e., by looking at the distance of its nearest neighbors while this class of techniques is well suited for well-textured rigid objects, it fails to match non-rigid objects and weakly textured regions. in contrast, the proposed matching algorithm, called deepmatching, is inspired by non-rigid 2d warping and deep convolutional networks lecun98, uchida1998, keysers2007. this family of approaches explicitly models non-rigid deformations. we employ a novel family of feasible warpings that does not enforce monotonicity nor continuity constraints, in contrast to traditional 2d warping uchida1998, keysers2007. this makes the problem computationally much less expensive. it is also worthwhile to mention the similarity with non-rigid matching approaches developed for a broad range of applications. proposed a similar pipeline to ours (albeit more complex) to measure the similarity of small images. however, their method lacks a way of merging correspondences belonging to objects with contradictory motions, e.g., on different focal planes. for the purpose of establishing dense correspondences between images, estimated a non-rigid matching by robustly fitting smooth parametric models (homography and splines) to local descriptor matches. in contrast, our approach is non-parametric and model-free. recently, fast algorithms for dense patch matching have taken advantage of the redundancy between overlapping patches barnes2010, korman2011, kpm, daisyff. the insight is to propagate good matches to their neighborhood in a loose fashion, yielding dense non-rigid matches. in practice, however, the lack of a smoothness constraint leads to highly discontinuous matches. several works have proposed ways to fix this. reinforce neighboring matches using an iterative multiscale expansion and contraction strategy, performed in a coarse-to-fine manner. include a guided filtering stage on top of patchmatch, which obtains smooth correspondence fields by locally approximating a mrf. finally, propose a hierarchical matching to obtain dense correspondences, using a coarse-to-fine (top-down) strategy. loopy belief propagation is used to perform inference. in contrast to these approaches, deepmatching proceeds bottom-up and, then, top-down. due to its hierarchical nature, deepmatching is able to consider patches at several scales, thus overcoming the lack of distinctiveness that affects small patches. yet, the multi-layer construction allows to efficiently perform matching allowing semi-rigid local deformations. in addition, deepmatching can be computed efficiently, and can be further accelerated to satisfy low-memory requirements with negligible loss in accuracy. subsection: matching for flow estimation variational energy minimization is currently the most popular framework for optical flow estimation. since the pioneering work of, research has focused on alleviating the drawbacks of this approach. a series of improvements were proposed over the years black1996, werlberger2009, bruhn2005, papenberg2006, middlebury, sun2014, vogel2013data. the variational approach of combines most of these improvements in a unified framework. the energy decomposes into several terms, resp. the data-fitting and the smoothness terms. energy minimization is performed by solving the euler-lagrange equations, reducing the problem to solving a sequence of large and structured linear systems. more recently, the addition of a descriptor matching term in the energy to be minimized was proposed by. following this idea, several papers tola2008, bro11a, siftflow, siftscales show that dense descriptor matching improves performance. strategies such as reciprocal nearest-neighbor verification allow to prune most of the false matches. however, a variational energy minimization approach that includes such a descriptor matching term may fail at locations where matches are missing or wrong. related approaches tackle the problem of dense scene correspondence. sift-flow one of the most famous method in this context, also formulates the matching problem in a variational framework. [] improve over sift-flow by using multi-scale patches. however, this decreases performance in cases where scale invariance is not required. integrate matching of sift lowe2004 and patchmatch barnes2010 to refine the flow initialization at each level. excellent results are obtained for optical flow estimation, yet at the cost of expensive fusion steps. extends sparse matches with locally affine constraints to dense matches and, then, uses a total variation algorithm to refine the flow estimation. we present here a computationally efficient and competitive approach for large displacement optical flow by integrating the proposed deepmatching algorithm into the approach of. section: deepmatching this section introduces our matching algorithm deepmatching. deepmatching is a matching algorithm based on correlations at the patch-level, that proceeds in a multi-layer fashion. the multi-layer architecture relies on a quadtree-like patch subdivision scheme, with an extra degree of freedom to locally re-optimize the positions of each quadrant. in order to enhance the contrast of the spatial correlation maps output by the local correlations, a nonlinear transformation is applied after each layer. we first give an overview of deepmatching in section [reference] and show that it can be decomposed in a bottom-up pass followed by a top-down pass. we, then, present the bottom-up pass in section [reference] and the top-down one in section [reference]. finally, we analyze deepmatching in section [reference]. subsection: overview of the approach a state-of-the-art approach for matching regions between two images is based on the sift descriptor lowe2004. sift is a histogram of gradients with spatial and 8 orientation bins, yielding a robust descriptor that effectively encodes a square image region. note that its cell grid can also be viewed as 4 so-called ''quadrants'' of cells, see figure [reference]. we can, then, rewrite with. let and be the sift descriptors of the corresponding regions in the source and target image. in order to remove the effect of non-rigid motion, we propose to optimize the positions of the quadrants of the target descriptor (rather than keeping them fixed), in order to maximize where is the descriptor of a single quadrant extracted at position and a similarity function. now, is able to handle situations such as the one presented in figure [reference], where a region contains multiple objects moving in different directions. furthermore, if the four quadrants can move independently (of course, within some extent), it can be calculated more efficiently as: when applied recursively to each quadrant by subdivided it into 4 sub-quadrants until a minimum patch size is reached (atomic patches), this strategy allows for accurate non-rigid matching. such a recursive decomposition can be represented as a quad-tree, see figure [reference]. given an initial pair of two matching regions, retrieving atomic patch correspondences is then done in a top-down fashion (i.e. by recursively applying eq. ([reference]) to the quadrant's positions). nevertheless, in order to first determine the set of matching regions between the two images, we need to compute beforehand the matching scores (i.e. similarity) of all large-enough patches in the two images (as in figure [reference]), and keep the pairs with maximum similarity. as indicated by eq. ([reference]), the score is formed by averaging the max-pooled scores of the quadrants. hence, the process of computing the matching scores is bottom-up. in the following, we call correlation map the matching scores of a single patch from the first image at every position in the second image. selecting matching patches then corresponds to finding local maxima in the correlation maps. to sum-up, the algorithm can be decomposed in two steps: (i) first, correlation maps are computed using a bottom-up algorithm, as shown in figure [reference]. correlation maps of small patches are first computed and then aggregated to form correlation maps of larger patches; (ii) next, a top-down method estimates the motion of atomic patches starting from matches of large patches. in the remainder of this section, we detail the two steps described above (section [reference] and section [reference]), before analyzing the properties of deepmatching in section [reference]. subsection: bottom-up correlation pyramid computation let and be two images of resolution and. paragraph: bottom level. we use patches of size pixels as atomic patches. we split into non-overlapping atomic patches, and compute the correlation map with image for each of them, see figure [reference]. the score between two atomic patches and is defined as the average pixel-wise similarity: where each pixel is represented as a histogram of oriented gradients pooled over a local neighborhood. we detail below how the pixel descriptor is computed. paragraph: pixel descriptor: we rely on a robust pixel representation that is similar in spirit to sift and daisy given an input image i, we first apply a gaussian smoothing of radius\u03bd1 in order to denoise i from potential artifacts caused for example by jpeg compression. we then extract the gradient (\u2062\u03b4x,\u2062\u03b4y) at each pixel and compute its non-negative projection onto 8 orientations {(cos\u2062i\u03c04, sin\u2062i\u03c04)}=i\u20621\u2026 8. at this point, we obtain 8 oriented gradient maps. we smooth each map with a gaussian filter of radius\u03bd2. next we cap strong gradients using a sigmoid\u21a6x -/ 2 (+ 1exp (-\u2062\u03c2x)) 1, to help canceling out effects of varying illumination. we smooth gradients one more time for each orientation with a gaussian filter of radius\u03bd3. finally, the descriptor for each pixel is obtained by the\u21132-normalized concatenation of 8 oriented gradients and a ninth small constant value\u03bc. appending\u03bc amounts to adding a regularizer that will reduce the importance of small gradients (i.e. noise) and ensures that two pixels lying in areas without gradient information will still correlate positively. pixel descriptors ri, j are compared using dot-product and the similarity function takes value in the interval [0, 1]. in section, we evaluate the impact of the parameters of this pixel descriptor. paragraph: bottom-level correlation map: we can express the correlation map computation obtained from eq. ([reference]) more conveniently in a convolutional framework. let be a patch of size from the first image centered at (is a power of 2). let be a grid with step pixels. is the set of the centers of the atomic patches. for each, we convolve the flipped patch over to get the correlation map, where.f denotes an horizontal and vertical flipthis amounts to the cross-correlation of the patch and i\u2032.. for any pixel of, is a measure of similarity between and. examples of such correlation maps are shown in figure [reference] and figure [reference]. without surprise we can observe that atomic patches are not discriminative. recursive aggregation of patches in subsequent stages will be the key to create discriminative responses. paragraph: iteration. we then compute the correlation maps of larger patches by aggregating those of smaller patches. as shown in figure [reference], a patch is the concatenation of patches of size: they correspond respectively to the bottom-left, top-left, bottom-right and top-right quadrants. the correlation map of can thus be computed using its children's correlation maps. for the sake of clarity, we define the short-hand notation= sn, i\u2062n4oi describing the positional shift of a children patch\u2208i [0, 3] relatively to its parent patch (see figure). using the above notations, we rewrite eq. () by replacing=\u2062def\u2062sim (r, r\u2032)\u2062cn, p (p\u2032) (i.e. assuming here that patch= rin, p and that r\u2032 is centered at\u2208p\u2032i\u2032). similarly, we replace the similarity between children patches\u2062sim (ri,\u2062ri\u2032 (p\u2032i)) by\u2062cn2,+ psn, i (pi\u2032). for each child, we retain the maximum similarity over a small neighborhood\u03b8i of width and height n8 centered at+ p\u2032sn, i. we then obtain: we now explain how we can break down eq. () into a succession of simple operations. first, let us assume that= n\u00d742\u2113, where\u2265\u21131 is the current iteration. during iteration\u2113, we want to compute the correlation maps cn, p of every patch in, p from the first image for which correlation maps of its children have been computed in the previous iteration. formally, the position gn of such patches is defined according to the position of children patches gn2 according to eq. (): we observe that the larger a patch is (i.e. after several iterations), the smaller the spatial variation of its correlation map (see figure [reference]). this is due to the statistics of natural images, in which low frequencies significantly dominate over high frequencies. as a consequence, we choose to subsample each map cn, p by a factor 2. we express this with an operator: the subsampling reduces by the area of the correlation maps and, as a direct consequence, the computational requirements. instead of computing the subsampling on top of eq. (), it is actually more efficient to propagate it towards the children maps and perform it jointly with max-pooling. it also makes the max-pooling domain become independent from in the subsampled maps, as it exactly cancels out the effect of doubling at each iteration. we call the max-pooling operator with the iteration-independent domain: for the same reason, the shift applied to the correlation maps in's definition becomes simply after subsampling. let be the shift (or translation) operator on the correlation map: finally, we incorporate an additional non-linear mapping at each iteration on top of eq. ([reference]) by applying a power transform malikperona90, lecun98: this step, commonly referred to as rectification, is added in order to better propagate high correlations after each level, or, in other words, to counterbalance the fact that max-pooling tends to retain only high scores. indeed, its effect is to decrease the correlation values (which are in [0, 1]) as we use>\u03bb1. such post-processing is commonly used in deep convolutional networks lecun-98b, bengio09. in practice, good performance is obtained with, see section [reference]. the final expression of eq. ([reference]) is: figure [reference] illustrates the computation of correlation maps for different patch sizes and algorithm [reference] summarizes our approach. the resulting set of correlation maps across iterations is referred to as multi-level correlation pyramid. paragraph: boundary effects: in practice, a patch in, p can overlap with the image boundary, as long as its center p remains inside the image (from eq. ()). for instance, a patch in, p0 with center at p0= (0, 0)\u2208gn has only a single valid child (the one for which= i3 as\u2208+ p0sn, 3i). in such degenerate cases, the average sum in eq. () is carried out on valid children only. for in, p0, it thus only comprises one term weighted by 1 instead of 14. note that eq. ([reference]) implicitly defines the set of possible displacements of the approach, see figures [reference] and [reference]. given the position of a parent patch, each child patch can move only within a small extent, equal to the quarter of its own size. figure [reference] shows the correlation maps for patches of size, and. clearly, correlation maps for larger patch are more and more discriminative, while still allowing non-rigid matching. input: images, for do (convolution, eq. ([reference])) (rectification, eq. ([reference])) while do for do (max-pooling and subsampling) for do (shift and average) (rectification, eq. ([reference])) return the multi-level correlation pyramid computing the multi-level correlation pyramid. subsection: top-down correspondence extraction a score in the multi-level correlation pyramid represents the deformation-tolerant similarity of two patches and. since this score is built from the similarity of 4 matching sub-patches at the lower pyramid level, we can thus recursively backtrack a set of correspondences to the bottom level (corresponding to matches of atomic patches). in this section, we first describe this backtracking. we, then, present the procedure for merging atomic correspondences backtracked from different entry points in the multi-level pyramid, which constitute the final output of deepmatching. compared to our initial version of deepmatching deepflow, we have updated match scoring and entry point selection to optimize the execution time and the matching accuracy. a quantitative comparison is provided in section [reference]. paragraph: backtracking atomic correspondences. given an entry point in the pyramid (i.e. a match between two patches and), we retrieve atomic correspondences by successively undoing the steps used to aggregate correlation maps during the pyramid construction, see figure [reference]. the entry patch is itself composed of four moving quadrants,. due to the subsampling, the quadrant matches with where for the sake of clarity, we define the short-hand notations and. let be the function that assigns to a tuple, representing a correspondence between pixel and for patch of size with a score, the set of the correspondences of children patches: given a set of such tuples, let be the union of the sets for all. note that if all candidate correspondences corresponds to atomic patches, then. thus, the algorithm for backtracking correspondences is the following. consider an entry match. we repeatedly apply on. after calls, we get one correspondence for each of the atomic patches. furthermore, their score is equal to the sum of all patch similarities along their backtracking path. paragraph: merging correspondences. we have shown how to retrieve atomic correspondences from a match between two deformable (potentially large) patches. despite this flexibility, a single match is unlikely to explain the complex set of motions that can occur, for example, between two adjacent frames in a video, i.e., two objects moving independently with significantly different motions exceeds the deformation range of deepmatching. we quantitatively specify this range in the next subsection. we thus merge atomic correspondences gathered from different entry points (matches) in the pyramid. in the initial version of deepmatching deepflow, entry points were local maxima over all correlation maps. this is now replaced by a faster procedure, that starts with all possible matches in the top pyramid level (i.e.). using this level only results in significantly less entry points than starting from all maxima in the entire pyramid. we did not observe any impact on the matching performance, see section [reference]. because contains a lot of overlapping patches, most of the computation during repeated calls to can be factorized. in other words, as soon as two tuples in are equal in terms of, and, the one with the lowest score is simply eliminated. we thus obtain a set of atomic correspondences: that we filter with reciprocal match verification. the final set of correspondences is obtained as: where (resp.) returns the best match in a small vicinity of pixels around in (resp. around in) from. subsection: discussion and analysis of deepmatching paragraph: multi-size patches and repetitive textures. during the bottom-up pass of the algorithm, we iteratively aggregate correlation maps of smaller patches to form the correlation maps of larger patches. doing so, we effectively consider patches of different sizes (), in contrast to most existing matching methods. this is a key feature of our approach when dealing with repetitive textures. as one moves up to upper levels, the matching problem gets less ambiguous. hence, our method can correctly match repetitive patterns, see for instance figure [reference]. paragraph: quasi-dense correspondences. our method retrieves dense correspondences for every single match between large regions (i.e. entry point for the backtracking in the top-level correlation maps), even in weakly textured areas; this is in contrast to correspondences obtained when matching descriptors (e.g. sift). a quantitative assessment, which compares the coverage of matches obtained with several matching schemes, is given in section [reference]. paragraph: non-rigid deformations. our matching algorithm is able to cope with various sources of image deformations: object-induced or camera-induced. the set of feasible deformations, explicitly defined by eq. ([reference]), theoretically allows to deal with a scaling factor in the range and rotations approximately in the range. note also that deepmatching is translation-invariant by construction, thanks to the convolutional nature of the processing. given a patch of size located at level, eq. ([reference]) allows each of its children patches to move by at most pixels from their ideal location in. by recursively summing the displacements at each level, the maximal displacements for an atomic patch is. an example is given in figure [reference] with and. relatively to, we thus have and. for a rotation, the rationale is similar, see figure [reference]. note that the displacement tolerance in\u03b8i from eq. () could be extended to/\u00d7xn8 pixels with\u2208x{2, 3,\u2026} (instead of= x1). then the above formula for computing the lower bound on the scale factor of deepmatching generalizes to=\u2062lb (x) lim\u2192n\u221e/(- n\u20622xdn) n. hence, for\u2265x2 we obtain<\u2062lb (x) 0 instead of=\u2062lb (1) 12. this implies that the deformation range is extended to a point where any patch can be matched to a single pixel, i.e., this results in unrealistic deformations. for this reason, we choose to not expand the deformation range of deepmatching. paragraph: built-in smoothing. furthermore, correspondences generated through backtracking of a single entry point in the correlation maps are naturally smooth. indeed, feasible deformations can not be too ''far'' from the identity deformation. to verify this assumption, we conduct the following experiment. we artificially generate two types of correspondences between two images of size\u00d7128128. the first one is completely random, i.e. for each atomic patch in the first image we assign randomly a match in the second image. the second one respects the backtracking constraints. starting from a single entry point in the top level we simulate the backtracking procedure from section by replacing in eq. () the max operation by a random sampling over {- 1, 0, 1}2. by generating 10, 000 sets of possible atomic correspondences, we simulate a set which respects the deformations allowed by deepmatching. figure [reference] compares the smoothness of these two types of artificial correspondences. smoothness is measured by interpreting the correspondences as flow and measuring the gradient flow norm, see eq. (). clearly, the two types of warpings are different by orders of magnitude. furthermore, the one which respects the built-in constraints of deepmatching is close to the identity warping. paragraph: relation to deep convolutional neural networks (cnns). deepmatching relies on a hierarchical, multi-layer, correlational architecture designed for matching images and was inspired by deep convolutional approaches in the following we describe the major similarities and differences. deep networks learn from data the weights of the convolutions. in contrast, deepmatching does not learn any feature representations and instead directly computes correlations at the patch level. it uses patches from the first image as convolution filters for the second one. however, the bottom-up pipeline of deepmatching is similar to cnns. it alternates aggregating channels from the previous layer with channel-wise max-pooling and subsampling. as in cnns, max-pooling in deepmatching allows for invariance w.r.t. small deformations. likewise, the algorithm propagates pairwise patch similarity scores through the hierarchy using non-linear rectifying stages in-between layers. finally, deepmatching includes a top-down pass which is not present in cnns. paragraph: time and space complexity. deepmatching has a complexity in memory and time, where and are the number of pixels per image. computing the initial correlations is a operation. then, at each level of the pyramid, the process is repeated while the complexity is divided by a factor due to the subsampling step in the target image (since the cardinality of remains approximately constant). thus, the total complexity of the correlation maps computation is, at worst,. during the top-down pass, most backtracking paths can be pruned as soon as they cross a concurrent path with a higher score (see section [reference]). thus, all correlations will be examined at most once, and there are values in total. however, this analysis is worst-case. in practice, only correlations lying on maximal paths are actually examined. section: extensions of deepmatching subsection: approximate deepmatching as a consequence of its space complexity, deepmatching requires an amount of ram that is orders of magnitude above other state-of-the-art matching methods. this could correspond to several gigabytes for images of moderate size (800 600 pixels); see section [reference]. this section introduces an approximation of deepmatching that allows to trade matching quality for reduced time and memory usage. as shown in section [reference], near-optimal results can be obtained at a fraction of the original cost. our approximation proposes to compress the representation of atomic patches. atomic patches carry little information, and thus are highly redundant. for instance, in uniform regions, all patches are nearly identical (i.e., gradient-wise). to exploit this property, we index atomic patches with a small set of patch prototypes. we substitute each patch with its closest neighbor in a fixed dictionary of prototypes. hence, we need to perform and store only convolutions at the first level, instead of (with). this significantly reduces both memory and time complexity. note that higher pyramid levels also benefit from this optimization. indeed, two parent patches at the second level have the exact same correlation map in case their children are assigned the same prototypes. the same reasoning also holds for all subsequent levels, but the gains rapidly diminish due to statistical unlikeliness of the required condition. this is not really an issue, since the memory and computational cost mostly rests on the initial levels; see section [reference]. in practice, we build the prototype dictionary using k-means, as it is designed to minimize the approximation error between input descriptors and resulting centroids (i.e. prototypes). given a pair of images to match, we perform on-line clustering of all descriptors of atomic patches in the first image. since the original descriptors lie on an hypersphere (each pixel descriptor has norm 1), we modify the k-means approach so as to project the estimated centroids on the hypersphere at each iteration. we find experimentally that this is important to obtain good results. subsection: scale and rotation invariant deepmatching for a variety of tasks, objects to be matched can appear under image rotations or at different scales lowe2004, mikolajczyk2005, szeliski2010, hacohen2011. as discussed above, deepmatching (dm) is only robust to moderate scale changes and rotations. we now present a scale and rotation invariant version. the approach is straightforward: we apply dm to several rotated and scaled versions of the second image. according to the invariance range of dm, we use steps of for image rotation and power of for scale changes. while iterating over all combinations of scale changes and rotations, we maintain a list of all atomic correspondences obtained so far, i.e. corresponding positions and scores. as before, the final output correspondences consists of the reciprocal matches in. storing all matches and finally choosing the best ones based on reciprocal verification permits to capture distinct motions possibly occurring together in the same scene (e.g. one object could have undergone a rotation, while the rest of the scene did not move). the steps of the approach are described in algorithm [reference]. since we iterate sequentially over a fixed list of rotations and scale changes, the space and time complexity of the algorithm remains unchanged (i.e.). in practice, the run-time compared to dm is multiplied by a constant approximately equal to 25, see section [reference]. note that the algorithm permits a straightforward parallelization. input:, are the images to be matched initialize an empty set of correspondences for do# either downsize image 1# or downsize image 2 for\u2208\u03b8{0,\u2062\u03c04\u2026,\u20627\u03c04} do# get raw atomic correspondences (eq. ())# geometric rectification to the input image space:# concatenate results:# keep reciprocal matches (eq. ()) return scale and rotation invariant version of deepmatching (dm). i\u03c3 denotes the image i downsized by a factor\u03c3, and r\u03b8 denotes rotation by an angle\u03b8. subsection: deepflow we now present our approach for optical flow estimation, deepflow. we adopt the method introduced by [], where a matching term penalizes the differences between optical flow and input matches, and replace their matching approach by deepmatching. in addition, we make a few minor modifications introduced recently in the state of the art: (i) we add a normalization in the data term to downweight the impact of locations with high spatial image derivatives (ii) we use a different weight at each level to downweight the matching term at finer scales and (iii) the smoothness term is locally weighted let be two consecutive images defined on with channels. the goal is to estimate the flow. we assume that the images are already smoothed using a gaussian filter of standard deviation. the energy we optimize is a weighted sum of a data term, a smoothness term and a matching term: for the three terms, we use a robust penalizer with which has shown excellent results sun2014. paragraph: data term. the data term is a separate penalization of the color and gradient constancy assumptions with a normalization factor as proposed by. we start from the optical flow constraint assuming brightness constancy: the spatio-temporal gradient. a basic way to build a data term is to penalize it, i.e. with the tensor defined by. as highlighted by, such a data term adds a higher weight in locations corresponding to high spatial image derivatives. we normalize it by the norm of the spatial derivatives plus a small factor to avoid division by zero, and to reduce a bit the influence in tiny gradient locations zimmer2011. let be the normalized tensor with. we set in the following. to deal with color images, we consider the tensor defined for a channel denoted by upper indices and we penalize the sum over channels:. we consider images in the rgb color space. we separately penalize the gradient constancy assumption bruhn2005. let and be the derivatives of the images with respect to the and axis respectively. let be the tensor for the channel including the normalization the data term is the sum of two terms, balanced by two weights and: paragraph: smoothness term. the smoothness term is a robust penalization of the gradient flow norm: the smoothness weight\u03b1 is locally set according to image derivatives with=\u2062\u03b1 (x) exp (-\u2062\u03ba\u22072i (x)) where\u03ba is experimentally set to=\u03ba5. paragraph: matching term. the matching term encourages the flow estimation to be similar to a precomputed vector field. to this end, we penalize the difference between and using the robust penalizer. since the matching is not totally dense, we add a binary term which is equal to if and only if a match is available at. we also multiply each matching penalization by a weight, which is low in uniform regions where matching is ambiguous and when matched patches are dissimilar. to that aim, we rely on, the minimum eigenvalue of the autocorrelation matrix multiplied by. we also compute the visual similarity between matches as. we then compute the score as a gaussian kernel on weighted by with a parameter, experimentally set to. more precisely, we define at each point with a match as: the matching term is then. paragraph: minimization. this energy objective is non-convex and non-linear. to solve it, we use a numerical optimization algorithm similar as. an incremental coarse-to-fine warping strategy is used with a downsampling factor. the remaining equations are still non-linear due to the robust penalizers. we apply 5 inner fixed point iterations where the non-linear weights and the flow increments are iteratively updated while fixing the other. to approximate the solution of the linear system, we use 25 iterations of the successive over relaxation (sor) method young: sor. to downweight the matching term on fine scales, we use a different weight at each level as proposed by. we set where k is the current level of computation, kmax the coarsest level and b a parameter which is optimized together with the other parameters, see section. section: experiments this section presents an experimental evaluation of deepmatching and deepflow. the datasets and metrics used to evaluate deepmatching and deepflow are introduced in section [reference]. experimental results are given in sections [reference] and [reference] respectively. subsection: datasets and metrics in this section we briefly introduce the matching and flow datasets used in our experiments. since consecutive frames of a video are well-suited to evaluate a matching approach, we use several optical flow datasets for evaluating both the quality of matching and flow, but we rely on different metrics. paragraph: the mikolajczyk dataset was originally proposed by to evaluate and compare the performance of keypoint detectors and descriptors. it is one of the standard benchmarks for evaluating matching approaches. the dataset consists of 8 sequences of 6 images each viewing a scene under different conditions, such as illumination changes or viewpoint changes. the images of a sequence are related by homographies. during the evaluation, we comply to the standard procedure in which the first image of each scene is matched to the 5 remaining ones. since our goal is to study robustness of deepmatching to geometric distortions, we follow and restrict our evaluation to the 4 most difficult sequences with viewpoint changes: bark, boat, graf and wall. paragraph: the mpi-sintel dataset sintel is a challenging evaluation benchmark for optical flow estimation, constructed from realistic computer-animated films. the dataset contains sequences with large motions and specular reflections. in the training set, more than of the pixels have a motion over pixels, approximately over pixels. we use the ''final'' version, featuring rendering effects such as motion blur, defocus blur and atmospheric effects. note that ground-truth optical flows for the test set are not publicly available. paragraph: the middlebury dataset middlebury has been extensively used for evaluating optical flow methods. the dataset contains complex motions, but most of the motions are small. less than of the pixels have a motion over pixels, and no motion exceeds pixels (training set). ground-truth optical flows for the test set are not publicly available. paragraph: the kitti dataset contains real-world sequences taken from a driving platform. the dataset includes non-lambertian surfaces, different lighting conditions, a large variety of materials and large displacements. more than 16% of the pixels have motion over 20 pixels. again, ground-truth optical flows for the test set are not publicly available. paragraph: performance metric for matching. choosing a performance measure for matching approaches is delicate. matching approaches typically do not return dense correspondences, but output varying numbers of matches. furthermore, correspondences might be concentrated in different areas of the image. most matching approaches, including deepmatching, are based on establishing correspondences between patches. given a pair of matching patches, it is possible to obtain a list of pixel correspondences for all pixels within the patches. we introduce a measure based on the number of correctly matched pixels compared to the overall number of pixels. we define ''accuracy@'' as the proportion of ''correct'' pixels from the first image with respect to the total number of pixels. a pixel is considered correct if its pixel match in the second image is closer than pixels to ground-truth. in practice, we use a threshold of pixels, as this represents a sufficiently precise estimation (about 1% of image diagonal for all datasets), while allowing some tolerance in blurred areas that are difficult to match exactly. if a pixel belongs to several matches, we choose the one with the highest score to predict its correspondence. pixels which do not belong to any patch have an infinite error. paragraph: performance metric for optical flow. to evaluate optical flow, we follow the standard protocol and measure the average endpoint error over all pixels, denoted as ''epe''. the ''s10-40'' variant measures the epe only for pixels with a ground-truth displacement between 10 and 40 pixels, and likewise for ''s0-10'' and ''s40+''. in all cases, scores are averaged over all image pairs to yield the final result for a given dataset. subsection: matching experiments in this section, we evaluate deepmatching (dm). we present results for all datasets presented above but middlebury, which does not feature long-range motions, the main difficulty in image matching. when evaluating on the mikolajczyk dataset, we employ the scale and rotation invariant version of dm presented in section [reference]. for all the matching experiments reported in this section, we use the mikolajczyk dataset and the training sets of mpi-sintel and kitti. subsubsection: impact of the parameters we optimize the different parameters of dm jointly on all datasets. to prevent overfitting, we use the same parameters across all datasets. paragraph: pixel descriptor parameters: we first optimize the parameters of the pixel representation (section):\u03bd1,\u03bd2,\u03bd3 (different smoothing stages),\u03c2 (sigmoid slope) and\u03bc (regularization constant). after performing a grid search, we find that good results are obtained at\u03bd1=\u03bd2=\u03bd3=1,=\u03c20.2 and=\u03bc0.3 across all datasets. figure shows the accuracy@10 in the neighborhood of these values for all parameters. image pre-smoothing seems to be crucial for jpeg images (mikolajczyk dataset), as it smooths out compression artifacts, whereas it slightly degrades performance for uncompressed png images (mpi-sintel and kitti). as expected, similar findings are observed for the regularization constant\u03bc since it acts as a regularizer that reduces the impact of small gradients (i.e. noise). in the following, we thus use low values of\u03bd1 and\u03bc when dealing with png images (we set=\u03bd10 and=\u03bc0.1, other parameters are unchanged). paragraph: non-linear rectification: we also evaluate the impact of the parameter of the non-linear rectification obtained by applying power normalization, see eq. ([reference]). figure [reference] displays the accuracy@10 for various values of. we can observe that the optimal performance is achieved at for all datasets. we use this value in the remainder of our experiments. subsubsection: evaluation of the backtracking and scoring schemes we now evaluate two improvements of dm with respect to the previous version published in [], referred to as dm*: backtracking (bt) entry points: in dm* we select as entry points local maxima in the correlation maps from all pyramid levels. the new alternative is to start from all possible points in the top pyramid level. scoring scheme: in dm* we scored atomic correspondences based on the correlation values of start and end point of the backtracking path. the new scoring scheme is the sum of correlation values along the full backtracking path. we report results for the different variants in table on each dataset. the first two rows for each dataset correspond to the exact settings used for dm* (i.e. with an image resolution of 1/ 4 and 1/ 2). we observe a steady increase in performance on all datasets when we add the new scoring and backtracking approach. we can observe that starting from all possible entry points in the top pyramid level (i.e. considering all possible translations) yields slightly better results than starting from local maxima. this demonstrates that some ground-truth matches are not covered by any local maximum. by enumerating all possible patch translations from the top-level, we instead ensure to fully explore the space of all possible matches. furthermore, it is interesting to note that memory usage and run-time significantly decreases when using the new options. this is because (1) searching and storing local maxima (which are exponentially more numerous in lower pyramid levels) is not necessary anymore, and (2) the new scoring scheme allows for further optimization, i.e. early pruning of backtracking paths (section). subsubsection: approximate deepmatching we now evaluate the performance of approximate deepmatching (section [reference]) and report its run-time and memory usage. we evaluate and compare two different ways of reducing the computational load. the first one simply consists in downsizing the input images, and upscaling the resulting matches accordingly. the second option is the compression scheme proposed in section [reference]. we evaluate both schemes jointly by varying the input image size (expressed as a fraction of the original resolution) and the size of the prototype dictionary (i.e. parameter of k-means in section [reference]). corresponds to the original dataset image size (no downsizing). we display the results in terms of matching accuracy (accuracy@10) against memory consumption in figure [reference] and as a function of in figure [reference]. figure [reference] shows that deepmatching can be computed in an approximate manner for any given memory budget. unsurprisingly, too low settings (e.g.,) result in a strong loss of performance. it should be noted that that we were unable to compute deepmatching at full resolution (for, as the memory consumption explodes. as a consequence, all subsequent experiments in the paper are done at. in figure [reference], we observe that good trades-off are achieved for dictionary sizes comprised in. for instance, on mpi-sintel, at, 94% of the performance of the uncompressed case () is reached for half the computation time and one third of the memory usage. detailed timings of the different stages of deepmatching are given in table. as expected, only the bottom-up pass is affected by the approximation, with a run-time of the different operations involved (patch correlations, max-pooling, subsampling, aggregation and non-linear rectification) roughly proportional to d (or to|g4|, the actual number of atomic patches, if= d\u221e). the overhead of clustering the dictionary prototypes with k-means appears negligible, with the exception of the largest dictionary size () for which it induces a slightly longer run-time than in the uncompressed case. overall, the proposed method for approximating deepmatching is highly effective. paragraph: gpu implementation. we have implemented dm on gpu in the caffe framework caffe. using existing caffe layers like convolutionlayer and poolinglayer, the implementation is straightforward for most layers. we had to specifically code a few layers which are not available in caffe (e.g. the backtracking pass). for the aggregation layer which consists in selecting and averaging 4 children channels out of many channels, we relied on the sparse matrix multiplication in the cusparse toolbox. detailed timings are given in table [reference] on a geforce titan x. our code runs in about 0.2s for a pair of mpi-sintel image. as expected, the computation bottleneck essentially lies in the computation of bottom-level patch correlations and the backtracking pass. note that computing patch descriptors takes significantly more time, in proportion, than on cpu: it takes about 0.024s= 11% of total time (not shown in table). this is because it involves a succession of many small layers (image smoothing, gradient extraction and projection, etc.), which causes overhead and is rather inefficient. subsubsection: comparison to the state of the art we compare dm with several baselines and state-of-the-art matching algorithms, namely: sift keypoints extracted with dog detector lowe2004 and matched with flann flann, referred to as sift-nn, dense hog matching, followed by nearest-neighbor matching with reciprocal verification as done in ldof bro11a, referred to as hog-nn, generalized patchmatch (gpm) barnes2010, with default parameters, 32x32 patches and 20 iterations (best settings in our experiments), kd-tree patchmatch (kpm) kpm, an improved version of patchmatch based on better patch descriptors and kd-trees optimized for correspondence propagation, non-rigid dense correspondences (nrdc) hacohen2011, an improved version of gpm based on a multiscale iterative expansion/ contraction strategy, sift-flow a dense matching algorithm based on an energy minimization where pixels are represented as sift features and a smoothness term is incorporated to explicitly preserve spatial discontinuities scale-less sift (sls) an improvement of sift-flow to handle scale changes (multiple sized sifts are extracted and combined to form a scale-invariant pixel representation) daisyfilterflow (daisyff) a dense matching approach that combines filter-based efficient flow inference and the patch-match fast search algorithm to match pixels described using the daisy representation deformable pyramid matching (dsp) a dense matching approach based on a coarse-to-fine (top-down) strategy where inference is performed with (inexact) loopy belief propagation sift-nn, hog-nn and dm output sparse matches, whereas the other methods output fully dense correspondence fields. sift keypoints, gpm, nrdc and daisyff are scale and rotation invariant, whereas hog-nn, kpm, sift-flow, sls and dsp are not. we, therefore, do not report results for these latter methods on the mikolajczyk dataset which includes image rotations and scale changes. statistics about each method (average number of matches per image and their coverage) are reported in table [reference]. coverage is computed as the proportion of points on a regular grid with 10 pixel spacing for which there exists a correspondence (in the raw output of the considered method) within a 10 pixel neighborhood. thus, it measures how well matches ''cover'' the image. table [reference] shows that deepmatching outputs 2 to 7 times more matches than sift-nn and a comparable number to hog-nn. yet, the coverage for dm matches is much higher than for hog-nn and sift-nn. this shows that dm matches are well distributed over the entire image, which is not the case for hog-nn and sift-nn, as they have difficulties estimating matches in regions with weak or repetitive textures. quantitative results are listed in table [reference], and qualitative results in figures [reference], [reference] and [reference]. overall, dm significantly outperforms all other methods, even when reduced settings are used (e.g. for image resolution and prototypes). as expected, sift-nn performs rather well in presence of global image transformation (mikolajczyk dataset), but yields the worst result for the case of more complex motions (flow datasets). figures [reference] and [reference] illustrate the reason: sift's large patches are way too coarse to follow motion boundaries precisely. the same issue also holds for hog-nn. methods predicting dense correspondence fields return a more precise estimate, yet most of them (kpm, gpm, sift-flow, dsp) are not robust to repetitive textures in the kitti dataset (figure) as they rely on weakly discriminative small patches. despite this limitation, sift-flow and dsp are still able to perform well on mpi-sintel as this dataset contains little scale changes. other dense methods, nrdc, sls and daisyff, can handle patches of different sizes and thus perform better on kitti. but in turn this is at the cost of reduced performance on the mpi-sintel or mikolajczyk datasets (qualitative results are in figure). in conclusion, dm outperforms all other methods on the 3 datasets, including dsp which also relies on a hierarchical matching. in terms of computing resources, deepmatching with full settings (,) is one of the most costly method (only sls and daisyff require the same order of memory and longer run-time). the scale and rotation invariant version of dm, used for the mikolajczyk dataset, is slow compared to most other approaches, due to its sequential processing (i.e. treating each combination of rotation and scaling sequentially), yet yields near perfect results. however, running dm with reduced settings is very competitive to the other approaches. on mpi-sintel and kitti, for instance, dm with a quarter resolution has a run-time comparable to the fastest method, sift-nn, with a reasonable memory usage, while still outperforming nearly all methods in terms of the accuracy@10 measure. ground-truth sift-nn gpm nrdc daisyff deepmatching gt sift-nn hog-nn kpm gpm sift-flow sls daisyff dsp dm gt sift-nn hog-nn kpm gpm sift-flow sls daisyff dsp dm subsection: optical flow experiments we now present experimental results for the optical flow estimation. optical flow is predicted using the variational framework presented in section [reference] that takes as input a set of matches. in the following, we evaluate the impact of deepmatching against other matching methods, and compare to the state of the art. subsubsection: optimization of the parameters we optimize the parameters of deepflow on a subset of the mpi-sintel training set (20%), called ''small'' set, and report results on the remaining image pairs (80%, called ''validation set'') and on the training sets of kitti and middlebury. ground-truth optical flows for the three test sets are not publicly available, in order to prevent parameter tuning on the test set. we first optimize the different flow parameters (,,, and) by employing a gradient descent strategy with multiple initializations followed by a local grid search. for the data term, we find an optimum at, which is equivalent to removing the color constancy assumption. this can be explained by the fact that the ''final'' version contains atmospheric effects, reflections, blurs, etc. the remaining parameters are optimal at,,,. these parameters are used in the remaining of the experiments for deepflow, i.e. using matches obtained with deepmatching, except when reporting results on kitti and middlebury test sets in section. in this case the parameters are optimized on their respective training set. subsubsection: impact of the matches on the flow we examine the impact of different matching methods on the flow, i.e., different matches are used in deepflow, see section [reference]. for all matching approaches evaluated in the previous section, we use their output as matching term in eq. ([reference]). because these approaches may output matches with statistics different from dm, we separately optimize the flow parameters for each matching approach on the small training set of mpi-sintel. table [reference] shows the endpoint error, averaged over all pixels. clearly, a sufficiently dense and accurate matching like dm allows to considerably improve the flow estimation on datasets with large displacements (mpi-sintel, kitti). in contrast, none of the methods presented have a tangible effect on the middlebury dataset, where the displacements are small. the relatively small gains achieved by sift-nn and hog-nn on mpi-sintel and kitti are due to the fact that a lot of regions with large displacements are not covered by any matches, such as the sky or the blurred character in the first and second column of figure [reference]. hence, sift-nn and hog-nn have only a limited impact on the variational approach. on the other hand, the gains are also small (or even negative) for the dense methods despite the fact that they output significantly more correspondences. we observe for these methods that the weight of the matching term tends to be small after optimizing the parameters, thus indicating that the matches are found unreliable and noisy during training. the cause is clearly visible in figure [reference], where large portions containing repetitive textures (e.g. road, trees) are incorrectly matched. the poor quality of these matches even leads to a significant drop in performance on the kitti dataset. in contrast, deepmatching generates accurate matches well covering the image that enable to boost the optical flow accuracy in case of large displacements. namely, we observe a relative improvement of 30% on mpi-sintel and of 50% on kitti. it is interesting to observe that dm is able to effectively prune false matches arising in occluded areas (black areas in figures and). this is due to the reciprocal verification filtering incorporated in dm (eq. ()). when using the approximation with 1024 prototypes, however, a significant drop is observed on the kitti dataset, while the performance remains good on mpi-sintel. this indicates that approximating deepmatching can result in a significant loss of robustness when matching repetitive textures, that are more frequent in kitti than in mpi-sintel. subsubsection: comparison to the state of the art in this section, we compare deepflow to the state of the art on the test sets of mpi-sintel, kitti and middlebury datasets. for theses datasets, the results are submitted to a dedicated server which performs the evaluation. prior to submitting our results for kitti and middlebury test sets, we have optimized the parameters on the respective training set. paragraph: results on mpi-sintel. images ground-truth deepmatching deepflow mdp-flow2 ldof table [reference] compares our method to state-of-the-art algorithms on the mpi-sintel test set. a comparison with the preliminary version of deepflow deepflow, referred to as deepflow*, is also provided. in this early version, we used a constant smoothness weight instead of a local one here (see section [reference]) and used dm* as input matches. we can see that deepflow is among the best performing methods on mpi-sintel, particularly for large displacements. this is due to the use of a reliable matching term in the variational approach, and this property is shared by all top performing approaches, e.g. epicflow, leordeanu2013. furthermore, it is interesting to note that among the top performers on mpi-sintel, 3 methods out of 6 actually employ deepmatching. in particular, the top-3 method epicflow epicflow relies on the output of deepmatching to produce a piece-wise affine flow, and sparseflowfused sparseflowfused combines matches obtained with deepmatching and another algorithm. we refer to the webpage of the mpi-sintel dataset for complete results including the ''clean'' version. paragraph: timings. as mentioned before, deepmatching at half the resolution takes 15 seconds to compute on cpu and 0.2 second on gpu. the variational part requires 10 additional seconds on cpu. note that by implementing it on gpu, we could obtain a significant speed-up as well. deepflow consequently takes 25 seconds in total on a single cpu core@ 3.6 ghz or 10.2s with gpu+ cpu. this is in the same order of magnitude as the fastest among the best competitors, epicflow epicflow. paragraph: results on kitti. table [reference] summarizes the main results on the kitti benchmark (see official website for complete results), when optimizing the parameters on the kitti training set. epe-noc is the epe computed only in non-occluded areas. ''out 3'' corresponds to the proportion of incorrect pixel correspondences for an error threshold of 3 pixels, i.e. it corresponds to, and likewise for ''out-noc 3'' for non-occluded areas. in terms of epe-noc, deepflow is on par with the best approaches, but performs somewhat worse in the occluded areas. this is due to a specificity of the kitti dataset, in which motion is mostly homographic (especially on the image borders, where most surfaces like roads and walls are planar). in such cases, flow is better predicted using an affine motion prior, which locally well approximates homographies (a constant motion prior is used in deepflow). as a matter of facts, all top performing methods in terms of total epe output piece-wise affine optical flow, either due to affine regularizers (btf-illum demetz2014learning, nltgb-sc ranftl2014non, tgv2adcsift braux-zin_2013_iccv) or due to local affine estimators (epicflow epicflow). note that the learned parameters on kitti and mpi-sintel are close. in particular, running the experiments with the same parameters as mpi-sintel decreases epe-noc by only pixels on the training set. this shows that our method does not suffer from overfitting. paragraph: results on middlebury. we optimize the parameters on the middlebury training set by minimizing the average angular error with the same strategy as for mpi-sintel. we find weights quasi-zero for the matching term due to the absence of large displacements. deepflow obtained an average endpoint error of on the test which is competitive with the state of the art. section: conclusion we have introduced a dense matching algorithm, termed deepmatching. the proposed algorithm gracefully handles complex non-rigid object deformations and repetitive textured regions. deepmatching yields state-of-the-art performance for image matching, on the mikolajczyk mikolajczyk2005, the mpi-sintel sintel and the kitti kitti datasets. integrated in a variational energy minimization approach bro11a, the resulting approach for optical flow estimation, termed deepflow, shows competitive performance on optical flow benchmarks. future work includes incorporating a weighting of the patches in eq. ([reference]) instead of weighting all patches equally to take into account that different parts of a large patch may belong to different objects. this could improve the performance of deepmatching for thin objects, such as human limbs. bibliography: references",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "deepmatching",
                        10
                    ],
                    [
                        "deepmatching algorithm",
                        12841
                    ],
                    [
                        "dm",
                        35434
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "average endpoint error",
                        45219
                    ],
                    [
                        "epe",
                        45273
                    ],
                    [
                        "endpoint error",
                        59599
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "dense correspondences between images",
                        136
                    ],
                    [
                        "dense correspondences",
                        10160
                    ],
                    [
                        "dense scene correspondence",
                        12056
                    ],
                    [
                        "top-down correspondence extraction",
                        23872
                    ],
                    [
                        "correspondence propagation",
                        53180
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc-36",
    "doctext": "imagenet classification with deep convolutional neural networks we trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the imagenet lsvrc-2010 contest into the 1000 different classes. on the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. the neural network, which has 60 million parameters and 650, 000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. to make training faster, we used non-saturating neurons and a very efficient gpu implementation of the convolution operation. to reduce overfitting in the fully connected layers we employed a recently developed regularization method called\" dropout\" that proved to be very effective. we also entered a variant of this model in the ilsvrc-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. we trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the imagenet lsvrc-2010 contest into the 1000 different classes. on the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. the neural network, which has 60 million parameters and 650, 000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. to make training faster, we used non-saturating neurons and a very efficient gpu implementation of the convolution operation. to reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called\" dropout\" that proved to be very effective. we also entered a variant of this model in the ilsvrc-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1 introduction current approaches to object recognition make essential use of machine learning methods. to improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. until recently, datasets of labeled images were relatively small\u2014 on the order of tens of thousands of images (e.g., norb [16], caltech-101/ 256 [8, 9], and cifar-10/ 100 [12]). simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. for example, the currentbest error rate on the mnist digit-recognition task (< 0.3%) approaches human performance [4]. but objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. and indeed, the shortcomings of small image datasets have been widely recognized (e.g., pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images. the new larger datasets include labelme [23], which consists of hundreds of thousands of fully-segmented images, and imagenet [6], which consists of over 15 million labeled high-resolution images in over 22, 000 categories. to learn about thousands of objects from millions of images, we need a model with a large learning capacity. however, the immense complexity of the object recognition task means that this problem can not be specified even by a dataset as large as imagenet, so our model should also have lots of prior knowledge to compensate for all the data we do n't have. convolutional neural networks (cnns) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). thus, compared to standard feedforward neural networks with similarly-sized layers, cnns have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse. despite the attractive qualities of cnns, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images. luckily, current gpus, paired with a highly-optimized implementation of 2d convolution, are powerful enough to facilitate the training of interestingly-large cnns, and recent datasets such as imagenet contain enough labeled examples to train such models without severe overfitting. the specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of imagenet used in the ilsvrc-2010 and ilsvrc-2012 competitions [2] and achieved by far the best results ever reported on these datasets. we wrote a highly-optimized gpu implementation of 2d convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly1. our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in section 3. the size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in section 4. our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model's parameters) resulted in inferior performance. in the end, the network's size is limited mainly by the amount of memory available on current gpus and by the amount of training time that we are willing to tolerate. our network takes between five and six days to train on two gtx 580 3 gb gpus. all of our experiments suggest that our results can be improved simply by waiting for faster gpus and bigger datasets to become available. 2 the dataset imagenet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22, 000 categories. the images were collected from the web and labeled by human labelers using amazon's mechanical turk crowd-sourcing tool. starting in 2010, as part of the pascal visual object challenge, an annual competition called the imagenet large-scale visual recognition challenge (ilsvrc) has been held. ilsvrc uses a subset of imagenet with roughly 1000 images in each of 1000 categories. in all, there are roughly 1.2 million training images, 50, 000 validation images, and 150, 000 testing images. ilsvrc-2010 is the only version of ilsvrc for which the test set labels are available, so this is the version on which we performed most of our experiments. since we also entered our model in the ilsvrc-2012 competition, in section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. on imagenet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model. imagenet consists of variable-resolution images, while our system requires a constant input dimensionality. therefore, we down-sampled the images to a fixed resolution of 256\u21e5 256. given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256\u21e5 256 patch from the resulting image. we did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. so we trained our network on the (centered) raw rgb values of the pixels. 3 the architecture the architecture of our network is summarized in figure 2. it contains eight learned layers\u2014 five convolutional and three fully-connected. below, we describe some of the novel or unusual features of our network's architecture. sections 3.1-3.4 are sorted according to our estimation of their importance, with the most important first. 1http:// code.google.com/ p/ cuda-convnet/ 3.1 relu nonlinearity the standard way to model a neuron's output f as a function of its input x is with f (x)= tanh (x) or f (x)= (1+ e x) 1. in terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity f (x)= max (0, x). following nair and hinton [20], we refer to neurons with this nonlinearity as rectified linear units (relus). deep convolutional neural networks with relus train several times faster than their equivalents with tanh units. this is demonstrated in figure 1, which shows the number of iterations required to reach 25% training error on the cifar-10 dataset for a particular four-layer convolutional network. this plot shows that we would not have been able to experiment with such large neural networks for this work if we had used traditional saturating neuron models. we are not the first to consider alternatives to traditional neuron models in cnns. for example, jarrett et al. [11] claim that the nonlinearity f (x)=|tanh (x)| works particularly well with their type of contrast normalization followed by local average pooling on the caltech-101 dataset. however, on this dataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using relus. faster learning has a great influence on the performance of large models trained on large datasets. 3.2 training on multiple gpus a single gtx 580 gpu has only 3 gb of memory, which limits the maximum size of the networks that can be trained on it. it turns out that 1.2 million training examples are enough to train networks which are too big to fit on one gpu. therefore we spread the net across two gpus. current gpus are particularly well-suited to cross-gpu parallelization, as they are able to read from and write to one another's memory directly, without going through host machine memory. the parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each gpu, with one additional trick: the gpus communicate only in certain layers. this means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2. however, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same gpu. choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation. the resultant architecture is somewhat similar to that of the\" columnar\" cnn employed by cires\u0327an et al. [5], except that our columns are not independent (see figure 2). this scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one gpu. the two-gpu net takes slightly less time to train than the one-gpu net2. 2the one-gpu net actually has the same number of kernels as the two-gpu net in the final convolutional layer. this is because most of the net's parameters are in the first fully-connected layer, which takes the last convolutional layer as input. so to make the two nets have approximately the same number of parameters, we did not halve the size of the final convolutional layer (nor the fully-conneced layers which follow). therefore this comparison is biased in favor of the one-gpu net, since it is bigger than\" half the size\" of the two-gpu net. 3.3 local response normalization relus have the desirable property that they do not require input normalization to prevent them from saturating. if at least some training examples produce a positive input to a relu, learning will happen in that neuron. however, we still find that the following local normalization scheme aids generalization. denoting by aix, y the activity of a neuron computed by applying kernel i at position (x, y) and then applying the relu nonlinearity, the response-normalized activity bix, y is given by the expression bix, y= a i x, y/ 0@k+\u21b5 min (n 1, i+ n/ 2) x j= max (0, i n/ 2) (ajx, y) 2 1 a where the sum runs over n\" adjacent\" kernel maps at the same spatial position, and n is the total number of kernels in the layer. the ordering of the kernel maps is of course arbitrary and determined before training begins. this sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. the constants k, n,\u21b5, and are hyper-parameters whose values are determined using a validation set; we used k= 2, n= 5,\u21b5= 10 4, and= 0.75. we applied this normalization after applying the relu nonlinearity in certain layers (see section 3.5). this scheme bears some resemblance to the local contrast normalization scheme of jarrett et al. [11], but ours would be more correctly termed\" brightness normalization\", since we do not subtract the mean activity. response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. we also verified the effectiveness of this scheme on the cifar-10 dataset: a four-layer cnn achieved a 13% test error rate without normalization and 11% with normalization3. 3.4 overlapping pooling pooling layers in cnns summarize the outputs of neighboring groups of neurons in the same kernel map. traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]). to be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z\u21e5 z centered at the location of the pooling unit. if we set s= z, we obtain traditional local pooling as commonly employed in cnns. if we set s< z, we obtain overlapping pooling. this is what we use throughout our network, with s= 2 and z= 3. this scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme s= 2, z= 2, which produces output of equivalent dimensions. we generally observe during training that models with overlapping pooling find it slightly more difficult to overfit. 3.5 overall architecture now we are ready to describe the overall architecture of our cnn. as depicted in figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fullyconnected. the output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution. the kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same gpu (see figure 2). the kernels of the third convolutional layer are connected to all kernel maps in the second layer. the neurons in the fullyconnected layers are connected to all neurons in the previous layer. response-normalization layers follow the first and second convolutional layers. max-pooling layers, of the kind described in section 3.4, follow both response-normalization layers as well as the fifth convolutional layer. the relu non-linearity is applied to the output of every convolutional and fully-connected layer. the first convolutional layer filters the 224\u21e5 224\u21e5 3 input image with 96 kernels of size 11\u21e5 11\u21e5 3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring 3we can not describe this network in detail due to space constraints, but it is specified precisely by the code and parameter files provided here: http:// code.google.com/ p/ cuda-convnet/. neurons in a kernel map). the second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5\u21e5 5\u21e5 48. the third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. the third convolutional layer has 384 kernels of size 3\u21e5 3\u21e5 256 connected to the (normalized, pooled) outputs of the second convolutional layer. the fourth convolutional layer has 384 kernels of size 3\u21e5 3\u21e5 192, and the fifth convolutional layer has 256 kernels of size 3\u21e5 3\u21e5 192. the fully-connected layers have 4096 neurons each. 4 reducing overfitting our neural network architecture has 60 million parameters. although the 1000 classes of ilsvrc make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. below, we describe the two primary ways in which we combat overfitting. 4.1 data augmentation the easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations (e.g., [25, 4, 5]). we employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. in our implementation, the transformed images are generated in python code on the cpu while the gpu is training on the previous batch of images. so these data augmentation schemes are, in effect, computationally free. the first form of data augmentation consists of generating image translations and horizontal reflections. we do this by extracting random 224\u21e5 224 patches (and their horizontal reflections) from the 256\u21e5 256 images and training our network on these extracted patches4. this increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent. without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks. at test time, the network makes a prediction by extracting five 224\u21e5 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network's softmax layer on the ten patches. the second form of data augmentation consists of altering the intensities of the rgb channels in training images. specifically, we perform pca on the set of rgb pixel values throughout the imagenet training set. to each training image, we add multiples of the found principal components, 4this is the reason why the input images in figure 2 are 224\u21e5 224\u21e5 3-dimensional. with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a gaussian with mean zero and standard deviation 0.1. therefore to each rgb image pixel ixy= [irxy, i g xy, i b xy] t we add the following quantity: [p1, p2, p3] [\u21b5 1 1,\u21b5 2 2,\u21b5 3 3] t where pi and i are ith eigenvector and eigenvalue of the 3\u21e5 3 covariance matrix of rgb pixel values, respectively, and\u21b5 i is the aforementioned random variable. each\u21b5 i is drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is re-drawn. this scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination. this scheme reduces the top-1 error rate by over 1%. 4.2 dropout combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. there is, however, a very efficient version of model combination that only costs about a factor of two during training. the recently-introduced technique, called\" dropout\" [10], consists of setting to zero the output of each hidden neuron with probability 0.5. the neurons which are\" dropped out\" in this way do not contribute to the forward pass and do not participate in backpropagation. so every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. this technique reduces complex co-adaptations of neurons, since a neuron can not rely on the presence of particular other neurons. it is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. at test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks. we use dropout in the first two fully-connected layers of figure 2. without dropout, our network exhibits substantial overfitting. dropout roughly doubles the number of iterations required to converge. 5 details of learning we trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. we found that this small amount of weight decay was important for the model to learn. in other words, weight decay here is not merely a regularizer: it reduces the model's training error. the update rule for weight w was vi+ 1:= 0.9\u00b7 vi 0.0005\u00b7\u270f\u00b7 wi\u270f\u00b7\u2327@l@w wi di wi+ 1:= wi+ vi+ 1 where i is the iteration index, v is the momentum variable,\u270f is the learning rate, and d@l@w wi e di is the average over the ith batch di of the derivative of the objective with respect to w, evaluated at wi. we initialized the weights in each layer from a zero-mean gaussian distribution with standard deviation 0.01. we initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. this initialization accelerates the early stages of learning by providing the relus with positive inputs. we initialized the neuron biases in the remaining layers with the constant 0. we used an equal learning rate for all layers, which we adjusted manually throughout training. the heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. the learning rate was initialized at 0.01 and reduced three times prior to termination. we trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two nvidia gtx 580 3 gb gpus. 6 results our results on ilsvrc-2010 are summarized in table 1. our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%5. the best performance achieved during the ilsvrc2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on fisher vectors (fvs) computed from two types of densely-sampled features [24]. we also entered our model in the ilsvrc-2012 competition and report our results in table 2. since the ilsvrc-2012 test set labels are not publicly available, we can not report test error rates for all the models that we tried. in the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see table 2). the cnn described in this paper achieves a top-5 error rate of 18.2%. averaging the predictions of five similar cnns gives an error rate of 16.4%. training one cnn, with an extra sixth convolutional layer over the last pooling layer, to classify the entire imagenet fall 2011 release (15 m images, 22 k categories), and then\" fine-tuning\" it on ilsvrc-2012 gives an error rate of 16.6%. averaging the predictions of two cnns that were pre-trained on the entire fall 2011 release with the aforementioned five cnns gives an error rate of 15.3%. the second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on fvs computed from different types of densely-sampled features [7]. finally, we also report our error rates on the fall 2009 version of imagenet with 10, 184 categories and 8.9 million images. on this dataset we follow the convention in the literature of using half of the images for training and half for testing. since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably. our top-1 and top-5 error rates on this dataset are 67.4% and 40.9%, attained by the net described above but with an additional, sixth convolutional layer over the last pooling layer. the best published results on this dataset are 78.1% and 60.9% [19]. 6.1 qualitative evaluations figure 3 shows the convolutional kernels learned by the network's two data-connected layers. the network has learned a variety of frequency-and orientation-selective kernels, as well as various colored blobs. notice the specialization exhibited by the two gpus, a result of the restricted connectivity described in section 3.5. the kernels on gpu 1 are largely color-agnostic, while the kernels on on gpu 2 are largely color-specific. this kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the gpus). 5the error rates without averaging predictions over ten patches as described in section 4.1 are 39.0% and 18.3%. in the left panel of figure 4 we qualitatively assess what the network has learned by computing its top-5 predictions on eight test images. notice that even off-center objects, such as the mite in the top-left, can be recognized by the net. most of the top-5 labels appear reasonable. for example, only other types of cat are considered plausible labels for the leopard. in some cases (grille, cherry) there is genuine ambiguity about the intended focus of the photograph. another way to probe the network's visual knowledge is to consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. if two images produce feature activation vectors with a small euclidean separation, we can say that the higher levels of the neural network consider them to be similar. figure 4 shows five images from the test set and the six images from the training set that are most similar to each of them according to this measure. notice that at the pixel level, the retrieved training images are generally not close in l2 to the query images in the first column. for example, the retrieved dogs and elephants appear in a variety of poses. we present the results for many more test images in the supplementary material. computing similarity by using euclidean distance between two 4096-dimensional, real-valued vectors is inefficient, but it could be made efficient by training an auto-encoder to compress these vectors to short binary codes. this should produce a much better image retrieval method than applying autoencoders to the raw pixels [14], which does not make use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar. 7 discussion our results show that a large, deep convolutional neural network is capable of achieving recordbreaking results on a highly challenging dataset using purely supervised learning. it is notable that our network's performance degrades if a single convolutional layer is removed. for example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. so the depth really is important for achieving our results. to simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help, especially if we obtain enough computational power to significantly increase the size of the network without obtaining a corresponding increase in the amount of labeled data. thus far, our results have improved as we have made our network larger and trained it longer but we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system. ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar-10",
                        2494
                    ],
                    [
                        "cifar-10 dataset",
                        9028
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "deep convolutional neural networks",
                        29
                    ],
                    [
                        "deep convolutional neural network",
                        84
                    ],
                    [
                        "deep convolutional nets",
                        28871
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "imagenet classification",
                        0
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "2347efd31b723a636902ccaa7b5bc351788e793f-37",
    "doctext": "document: attending to characters in neural sequence labeling models sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. we investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. by using an attention mechanism, the model is able to dynamically decide how much information to use from a word-or character-level component. we evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. in addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters. section: introduction this work is licenced under a creative commons attribution 4.0 international licence. licence details: many nlp tasks, including named entity recognition (ner), part-of-speech (pos) tagging and shallow parsing can be framed as types of sequence labeling. the development of accurate and efficient sequence labeling models is thereby useful for a wide range of downstream applications. work in this area has traditionally involved task-specific feature engineering- for example, integrating gazetteers for named entity recognition, or using features from a morphological analyser in pos-tagging. recent developments in neural architectures and representation learning have opened the door to models that can discover useful features automatically from the data. such sequence labeling systems are applicable to many tasks, using only the surface text as input, yet are able to achieve competitive results. current neural models generally make use of word embeddings, which allow them to learn similar representations for semantically or functionally similar words. while this is an important improvement over count-based models, they still have weaknesses that should be addressed. the most obvious problem arises when dealing with out-of-vocabulary (oov) words- if a token has never been seen before, then it does not have an embedding and the model needs to back-off to a generic oov representation. words that have been seen very infrequently have embeddings, but they will likely have low quality due to lack of training data. the approach can also be sub-optimal in terms of parameter usage- for example, certain suffixes indicate more likely pos tags for these words, but this information gets encoded into each individual embedding as opposed to being shared between the whole vocabulary. in this paper, we construct a task-independent neural network architecture for sequence labeling, and then extend it with two different approaches for integrating character-level information. by operating on individual characters, the model is able to infer representations for previously unseen words and share information about morpheme-level regularities. we propose a novel architecture for combining character-level representations with word embeddings using a gating mechanism, also referred to as attention, which allows the model to dynamically decide which source of information to use for each word. in addition, we describe a new objective for model training where the character-level representations are optimised to mimic the current state of word embeddings. we evaluate the neural models on 8 datasets from the fields of ner, pos-tagging, chunking and error detection in learner texts. our experiments show that including a character-based component in the sequence labeling model provides substantial performance improvements on all the benchmarks. in addition, the attention-based architecture achieves the best results on all evaluations, while requiring a smaller number of parameters. section: bidirectional lstm for sequence labeling we first describe a basic word-level neural network for sequence labeling, following the models described by lample2016 and rei2016, and then propose two alternative methods for incorporating character-level information. figure [reference] shows the general architecture of the sequence labeling network. the model receives a sequence of tokens as input, and predicts a label corresponding to each of the input tokens. the tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings. next, the embeddings are given as input to two lstm components moving in opposite directions through the text, creating context-specific representations. the respective forward-and backward-conditioned representations are concatenated for each word position, resulting in representations that are conditioned on the whole sequence: we include an extra narrow hidden layer on top of the lstm, which proved to be a useful modification based on development experiments. an additional hidden layer allows the model to detect higher-level feature combinations, while constraining it to be small forces it to focus on more generalisable patterns: where is a weight matrix between the layers, and the size of is intentionally kept small. finally, to produce label predictions, we use either a softmax layer or a conditional random field (crf, lafferty2001). the softmax calculates a normalised probability distribution over all the possible labels for each word: where is the probability of the label of the-th word () being, is the set of all possible labels, and is the-th row of output weight matrix. to optimise this model, we minimise categorical crossentropy, which is equivalent to minimising the negative log-probability of the correct labels: following huang2015, we can also use a crf as the output layer, which conditions each prediction on the previously predicted label. in this architecture, the last hidden layer is used to predict confidence scores for the word having each of the possible labels. a separate weight matrix is used to learn transition probabilities between different labels, and the viterbi algorithm is used to find an optimal sequence of weights. given that is a sequence of labels, then the crf score for this sequence can be calculated as: where shows how confident the network is that the label on the-th word is. shows the likelihood of transitioning from label to label, and these values are optimised during training. the output from the model is the sequence of labels with the largest score, which can be found efficiently using the viterbi algorithm. in order to optimise the crf model, the loss function maximises the score for the correct label sequence, while minimising the scores for all other sequences: where is the set of all possible label sequences. section: character-level sequence labeling distributed embeddings map words into a space where semantically similar words have similar vector representations, allowing the models to generalise better. however, they still treat words as atomic units and ignore any surface-or morphological similarities between different words. by constructing models that operate over individual characters in each word, we can take advantage of these regularities. this can be particularly useful for handling unseen words- for example, if we have never seen the word cabinets before, a character-level model could still infer a representation for this word if it has previously seen the word cabinet and other words with the suffix-s. in contrast, a word-level model can only represent this word with a generic out-of-vocabulary representation, which is shared between all other unseen words. research into character-level models is still in fairly early stages, and models that operate exclusively on characters are not yet competitive to word-level models on most tasks. however, instead of fully replacing word embeddings, we are interested in combining the two approaches, thereby allowing the model to take advantage of information at both granularity levels. the general outline of our approach is shown in figure [reference]. each word is broken down into individual characters, these are then mapped to a sequence of character embeddings, which are passed through a bidirectional lstm: we then use the last hidden vectors from each of the lstm components, concatenate them together, and pass the result through a separate non-linear layer. where is a weight matrix mapping the concatenated hidden vectors from both lstms into a joint word representation, built from individual characters. we now have two alternative feature representations for each word- from section [reference] is an embedding learned on the word level, and is a representation dynamically built from individual characters in the-th word of the input text. following lample2016, one possible approach is to concatenate the two vectors and use this as the new word-level representation for the sequence labeling model: this approach, also illustrated in figure [reference], assumes that the word-level and character-level components learn somewhat disjoint information, and it is beneficial to give them separately as input to the sequence labeler. section: attention over character features alternatively, we can have the word embedding and the character-level component learn the same semantic features for each word. instead of concatenating them as alternative feature sets, we specifically construct the network so that they would learn the same representations, and then allow the model to decide how to combine the information for each specific word. we first construct the word representation from characters using the same architecture- a bidirectional lstm operates over characters, and the last hidden states are used to create vector for the input word. instead of concatenating this with the word embedding, the two vectors are added together using a weighted sum, where the weights are predicted by a two-layer network: where, and are weight matrices for calculating, and is the logistic function with values in the range. the vector has the same dimensions as or, acting as the weight between the two vectors. it allows the model to dynamically decide how much information to use from the character-level component or from the word embedding. this decision is done for each feature separately, which adds extra flexiblity- for example, words with regular suffixes can share some character-level features, whereas irregular words can store exceptions into word embeddings. furthermore, previously unknown words are able to use character-level regularities whenever possible, and are still able to revert to using the generic oov token when necessary. the main benefits of character-level modeling are expected to come from improved handling of rare and unseen words, whereas frequent words are likely able to learn high-quality word-level embeddings directly. we would like to take advantage of this, and train the character component to predict these word embeddings. our attention-based architecture requires the learned features in both word representations to align, and we can add in an extra constraint to encourage this. during training, we add a term to the loss function that optimises the vector to be similar to the word embedding: equation [reference] maximises the cosine similarity between and. importantly, this is done only for words that are not out-of-vocabulary- we want the character-level component to learn from the word embeddings, but this should exclude the oov embedding, as it is shared between many words. we use to set this cost component to for any oov tokens. while the character component learns general regularities that are shared between all the words, individual word embeddings provide a way for the model to store word-specific information and any exceptions. therefore, while we want the character-based model to shift towards predicting high-quality word embeddings, it is not desireable to optimise the word embeddings towards the character-level representations. this can be achieved by making sure that the optimisation is performed only in one direction; in theano, the disconnected_grad function gives the desired effect. section: datasets we evaluate the sequence labeling models and character architectures on 8 different datasets. table [reference] contains information about the number of labels and dataset sizes for each of them. conll00: the conll-2000 dataset is a frequently used benchmark for the task of chunking. wall street journal sections 15-18 from the penn treebank are used for training, and section 20 as the test data. as there is no official development set, we separated some of the training set for this purpose. conll03: the conll-2003 corpus was created for the shared task on language-independent ner. we use the english section of the dataset, containing news stories from the reuters corpus. ptb-pos: the penn treebank pos-tag corpus contains texts from the wall street journal, annotated for part-of-speech tags. the ptb label set includes 36 main tags and an additional 12 tags covering items such as punctuation. fcepublic: the publicly released subset of the first certificate in english (fce) dataset contains short essays written by language learners and manual corrections by examiners. we use a version of this corpus converted into a binary error detection task, where each token is labeled as being correct or incorrect in the given context. bc2gm: the biocreative ii gene mention corpus consists of 20, 000 sentences from biomedical publication abstracts and is annotated for mentions of the names of genes, proteins and related entities using a single ne class. chemdner: the biocreative iv chemical and drug ner corpus consists of 10, 000 abstracts annotated for mentions of chemical and drug names using a single class. we make use of the official splits provided by the shared task organizers. jnlpba: the jnlpba corpus consists of 2, 404 biomedical abstracts and is annotated for mentions of five entity types: cell line, cell type, dna, rna, and protein. the corpus was derived from genia corpus entity annotations for use in the shared task organized in conjuction with the bionlp 2004 workshop. genia-pos: the genia corpus is one of the most widely used resources for biomedical nlp and has a rich set of annotations including parts of speech, phrase structure syntax, entity mentions, and events. here, we make use of the genia pos annotations, which cover 2, 000 pubmed abstracts (approx. 20, 000 sentences). we use the same 210-document test set as tsuruoka2005, and additionally split off a sample of 210 from the remaining documents as a development set. section: experiment settings for data prepocessing, all digits were replaced with the character' 0'. any words that occurred only once in the training data were replaced by the generic oov token for word embeddings, but were still used in the character-level components. the word embeddings were initialised with publicly available pretrained vectors, created using word2vec, and then fine-tuned during model training. for the general-domain datasets we used 300-dimensional vectors trained on google news; for the biomedical datasets we used 200-dimensional vectors trained on pubmed and pmc. the embeddings for characters were set to length and initialised randomly. the lstm layer size was set to in each direction for both word-and character-level components. the hidden layer has size, and the combined representation has the same length as the word embeddings. crf was used as the output layer for all the experiments- we found that this gave most benefits to tasks with larger numbers of possible labels. parameters were optimised using adadelta with default learning rate and sentences were grouped into batches of size. performance on the development set was measured at every epoch and training was stopped if performance had not improved for 7 epochs; the best-performing model on the development set was then used for evaluation on the test set. in order to avoid any outlier results due to randomness in the model initialisation, we trained each configuration with 10 different random seeds and present here the averaged results. when evaluating on each dataset, we report the measures established in previous work. token-level accuracy is used for ptb-pos and genia-pos; score over the erroneous words for fcepublic; the official evaluation script for bc2gm which allows for alternative correct entity spans; and microaveraged mention-level score for the remaining datasets. section: results while optimising the hyperparameters for each dataset separately would likely improve individual performance, we conduct more controlled experiments on a task-independent model. therefore, we use the same hyperparameters from section [reference] on all datasets, and the development set is only used for the stopping condition. with these experiments, we wish to determine 1) on which sequence labeling tasks do character-based models offer an advantange, and 2) which character-based architecture performs better. results for the different model architectures on all 8 datasets are shown in table [reference]. as can be seen, including a character-based component in the sequence labeling architecture improves performance on every benchmark. the ner datasets have the largest absolute improvement- the model is able to learn character-level patterns for names, and also improve the handling of any previously unseen tokens. compared to concatenating the word-and character-level representations, the attention-based character model outperforms the former on all evaluations. the mechanism for dynamically deciding how much character-level information to use allows the model to better handle individual word representations, giving it an advantage in the experiments. visualisation of the attention values in figure [reference] shows that the model is actively using character-based features, and the attention areas vary between different words. the results of this general tagging architecture are competitive, even when compared to previous work using hand-crafted features. the network achieves 97.27% on ptb-pos compared to 97.55% by huang2015, and 72.70% on jnlpba compared to 72.55% by zhou2004. in some cases, we are also able to beat the previous best results- 87.99% on bc2gm compared to 87.48% by campos2015, and 41.88% on fcepublic compared to 41.1% by rei2016. lample2016 report a considerably higher result of 90.94% on conll03, indicating that the chosen hyperparameters for the baseline system are suboptimal for this specific task. compared to the experiments presented here, their model used the iobes tagging scheme instead of the original iob, and embeddings pretrained with a more specialised method that accounts for word order. it is important to also compare the parameter counts of alternative neural architectures, as this shows their learning capacity and indicates their time requirements in practice. table [reference] contains the parameter counts on three representative datasets. while keeping the model hyperparameters constant, the character-level models require additional parameters for the character composition and character embeddings. however, the attention-based model uses fewer parameters compared to the concatenation approach. when the two representations are concatenated, the overall word representation size is increased, which in turn increases the number of parameters required for the word-level bidirectional lstm. therefore, the attention-based character architecture achieves improved results even with a smaller parameter footprint. section: related work there is a wide range of previous work on constructing and optimising neural architectures applicable to sequence labeling. collobert2011 described one of the first task-independent neural tagging models using convolutional neural networks. they were able to achieve good results on pos tagging, chunking, ner and semantic role labeling, without relying on hand-engineered features. irsoy2014a experimented with multi-layer bidirectional elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. huang2015 described a bidirectional lstm model with a crf layer, which included hand-crafted features specialised for the task of named entity recognition. rei2016 evaluated a range of neural architectures, including convolutional and recurrent networks, on the task of error detection in learner writing. the word-level sequence labeling model described in this paper follows the previous work, combining useful design choices from each of them. in addition, we extended the model with two alternative character-level architectures, and evaluated its performance on 8 different datasets. character-level models have the potential of capturing morpheme patterns, thereby improving generalisation on both frequent and unseen words. in recent years, there has been an increase in research into these models, resulting in several interesting applications. ling2015b described a character-level neural model for machine translation, performing both encoding and decoding on individual characters. kim2016 implemented a language model where encoding is performed by a convolutional network and lstm over characters, whereas predictions are given on the word-level. cao2016 proposed a method for learning both word embeddings and morphological segmentation with a bidirectional recurrent network over characters. there is also research on performing parsing and text classification with character-level neural models. ling2015a proposed a neural architecture that replaces word embeddings with dynamically-constructed character-based representations. we applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. lample2016 described a model where the character-level representation is combined with word embeddings through concatenation. in this work, we proposed an alternative architecture, where the representations are combined using an attention mechanism, and evaluated both approaches on a range of tasks and datasets. recently, miyamoto2016 have also described a related method for the task of language modelling, combining characters and word embeddings using gating. section: conclusion developments in neural network research allow for model architectures that work well on a wide range of sequence labeling datasets without requiring hand-crafted data. while word-level representation learning is a powerful tool for automatically discovering useful features, these models still come with certain weaknesses- rare words have low-quality representations, previously unseen words can not be modeled at all, and morpheme-level information is not shared with the whole vocabulary. in this paper, we investigated character-level model components for a sequence labeling architecture, which allow the system to learn useful patterns from sub-word units. in addition to a bidirectional lstm operating over words, a separate bidirectional lstm is used to construct word representations from individual characters. we proposed a novel architecture for combining the character-based representation with the word embedding by using an attention mechanism, allowing the model to dynamically choose which information to use from each information source. in addition, the character-level composition function is augmented with a novel training objective, optimising it to predict representations that are similar to the word embeddings in the model. the evaluation was performed on 8 different sequence labeling datasets, covering a range of tasks and domains. we found that incorporating character-level information into the model improved performance on every benchmark, indicating that capturing features regarding characters and morphmes is indeed useful in a general-purpose tagging system. in addition, the attention-based model for combining character representations outperformed the concatenation method used in previous work in all evaluations. even though the proposed method requires fewer parameters, the added ability of controlling how much character-level information is used for each word has led to improved performance on a range of different tasks. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "fcepublic",
                        13039
                    ],
                    [
                        "first certificate in english",
                        13086
                    ],
                    [
                        "fce",
                        13117
                    ]
                ]
            ],
            "Method": [],
            "Metric": [],
            "Task": [
                [
                    [
                        "error detection",
                        3468
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "penn treebank",
                        12464
                    ],
                    [
                        "ptb-pos",
                        12815
                    ],
                    [
                        "wall street journal",
                        12881
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "token-level accuracy",
                        16231
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "part-of-speech",
                        967
                    ],
                    [
                        "pos",
                        984
                    ],
                    [
                        "pos-tagging",
                        1388
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "25ace72fc3e483d5d52c9209f76b04f0fdd08f9b-38",
    "doctext": "document: fused text segmentation networks for multi-oriented scene text detection in this paper, we introduce a novel end-end framework for multi-oriented scene text detection from an instance-aware semantic segmentation perspective. we present fused text segmentation networks, which combine multi-level features during the feature extracting as text instance may rely on finer feature expression compared to general objects. it detects and segments the text instance jointly and simultaneously, leveraging merits from both semantic segmentation task and region proposal based object detection task. not involving any extra pipelines, our approach surpasses the current state of the art on multi-oriented scene text detection benchmarks: icdar2015 incidental scene text and msra-td500 reaching hmean 84.1% and 82.0% respectively. morever, we report a baseline on total-text containing curved text which suggests effectiveness of the proposed approach. section: introduction recently, scene text detection has drawn great attention from computer vision and machine learning community. driven by many content-based image applications such as photo translation and receipt content recognition, it has become a promising and challenging research area both in academia and industry. detecting text in natural images is difficult, because both text and background may be complex in the wild and it often suffers from disturbance such as occlusion and uncontrollable lighting conditions. previous text detection methods have achieved promising results on several benchmarks. the essential problem in text detection is to represent text region using discriminative features. conventionally, hand-crafted features are designed to capture the properties of text region such as texture and shape, while in the past few years, deep learning based approaches directly learn hierarchical features from training data, demonstrating more accurate and efficient performance in various benchmarks such as icdar series contests. existing methods have obtained decent performance for detecting horizontal or near-horizontal text. while horizontal text detection has constraints of axis-aligned bounding-box ground truth, the multi-oriented text is not restrictive to a particular orientation and usually uses quadrilaterals for annotations. therefore, it reports relatively lower accuracies in icdar 2015 competition challenge 4\u00e2\u0080\u009cincidental scene text localization\u00e2\u0080\u009d compared to horizontal scene text detection benchmarks. recently, a few approaches have been proposed to address the multi-oriented text detection. in general, there are currently four different types of methods. region based methods leverage advanced object detection techniques such as faster rcnn and ssd. segmentation-based methods mainly utilize fully convolutional neural networks (fcn) for generating text score maps, which often need several stages and components to achieve final detections. direct regression based method regresses the position and size of an object from a given point. finally, hybrid method combines text scores map and rotated/ quadrangle bounding boxes generation to collaboratively obtain the efficient and accurate performance in multi-oriented text detection. inspired by recent advance of instance-aware semantic segmentation, we present a novel perspective to handle the task of multi-oriented text detection. in this work, we leverage the merits from accurate region proposal based methods, and flexible segmentation based methods which can easily generate arbitrary-shaped text mask. it is an end-to-end trainable framework excluding redundant and low-efficient pipelines such as the use of text/ nontext salient map and text-line generation. based on region proposal network (rpn), our approach detects and segments text instance simultaneously, followed by non-maximum suppression (nms) to suppress overlapping instances. finally, a minimum quadrangle bounding box to fit each instance area is generated as the result of the whole detection process. our main contributions are summarized as follows: we present an end-end efficient and trainable solution for multi-oriented text detection from an instance aware segmentation perspective, excluding any redundant pipelines. during feature extraction, feature maps are composed in a fused fashion to adaptively satisfy the finer representation of text instance. mask-nms is introduced to improve the standard nms when facing heavily inclined or line-level text instances. without many bells and whistles, our approach outperforms state of the art on current multi-oriented text detection benchmarks. section: related work detecting text in natural images has been widely studied in past few years, motivated by many text-related real-world applications such as photo ocr and blind navigation. one of the mainstream traditional methods for scene text detection are connected components (ccs) based methods, which consider text as a group of individual components such as characters. within these methods, stroke width transform (swt) and maximally stable extremal region (mser) are usually used to seek character candidates. finally, these candidates are combined to obtain text objects. although these bottom up approaches may be accurate on some benchmarks, they often suffer from too many pipelines, which may cause inefficiency. another mainstream traditional methods are sliding window based. these methods often use a fixed-size or multi-scale window to slide through the image searching the region which most likely contains text. however, the process of sliding window may involve large computational cost which results in inefficiency. generally, traditional methods often require several steps to obtain final detections, and hand-designed features are usually used to represent properties of text. therefore, they may suffer from inefficiency and low generalization ability against complex situations such as non-uniform illumination. recent progress on deep learning based approaches for object detection and semantic segmentation has provided new techniques for reading text in the wild, which can be also seen as an instance of general object detection. driven by the advance of object detection frameworks such as faster rcnn and ssd, these methods achieved state of the art by either using a region proposal network to first classify some text region proposals, or directly regress text bounding boxes coordinates from a set of default boxes. these methods are able to achieve leading performance on horizontal or multi-oriented scene text detection benchmarks. however, they may also be restricted to rectangular bounding box constraints even with appropriate rotation. different from these methods, fcn based approaches generate text/ non-text map which classifies text at the pixel level. though it may be suited well for arbitrary shape of text in natural images, it often involves several pipelines which leads to inefficiency. inspired by recent advance on instance-aware semantic segmentation, we present an end-end trainable framework called fused text segmentation networks (ftsn) to handle arbitrary-shape text detection with no extra pipelines involved. it inherits merits from both object detection and semantic segmentation architecture which efficiently detects and segments an text instance simultaneously and accurately gives predictions in the pixel level. as text may rely on finer feature representation, a fused structure formed by multi-level feature maps is set to fit this property. section: methods the proposed framework for multi-oriented scene text detection is diagrammed in fig.2. it is a deep cnn model which mainly consists of three parts. feature representations of each image are extracted through resnet-101 backbone, then multi-level feature maps are fused as fusedmapa which is fed to the region proposed network (rpn) for text region of interest (roi) generation and fusedmapb for later rois' psroipooling. finally the rois are sent to the detection, segmentation and box regression branches to output text instances in pixel level along with their corresponding bounding boxes. the post-processing part includes nms and minimal quadrilateral generation. subsection: network architecture the convolutional feature representation is designed in a fusion fashion. the text instance is not like the general object such as people and cars which have relatively strong semantics. on the contrary, texts often vary tremendously in intra-class geometries. consequently, low-level features should be taken into consideration. basically, resnet-101 consists of five stages. before region proposing, stage3 and upsampled stage4 feature maps are combined to form fusedmapa through element-wise adding, then upsampled feature maps from stage5 are fused with fusedmapa to form fusedmapb. it is noted that downsampling is not involved during stage5. instead, we use the\u00e2\u0080\u009chole algorithm\u00e2\u0080\u009d to keep the feature stride and maintain the receptive field. the reason for this is that both text properties and the segmentation task may require finer features and involving final downsampling may lose some useful information. because using feature stride of stage3 may cause millions of anchors in original rpn which makes model training hard, so we add a with stride 2 convolution to reduce such huge number of anchors. followed fcis, we use joint mask prediction and classification to simultaneously classify and mask the text instance on inside/ outside score maps generated through psroipooling on conv-cls-seg feature maps, and box regression branch utilizes feature maps from conv-box after psroipooling (\"\" means one class is for text and the other for background). we use shown in fig.2 in our experiments by default. it is noted that after psroipooling, the resolution of feature maps becomes. therefore, we use global average pooling for classification (after pixel-wise max) and box regression branches, and pixel-wise softmax on mask branch. subsection: ground truth and loss function the whole multi-task loss can be interpreted as the full loss consists of two sub stage losses: rpn loss where is for region proposal classification and is for box regression, and text instance loss based on each roi, where represent losses for instance classification, mask and box regression task respectively. is the hyper-parameter to control the balance among each loss term. they are set as in our experiments. classification and mask task both use cross-entropy as loss function, whereas we use smooth-l1 for box regression task formulated as is set to 3 in our experiments which makes the box regression loss less sensitive to outliers. ground truth of each text instance is presented by bounding boxes and masks shown in fig.3. in most multi-oriented text detection dataset, annotations are given in quadrilaterals such as ic15 or can be converted to quadrilaterals such as td500. for each instance, we directly generate mask from quadrilateral coordinates and use the minimal rectangle containing the mask as the bounding box. subsection: post processing mask-nms to obtain final detection results, we use non-maximum suppression mechanism (nms) to filter overlapped text instances and preserve those with highest scores. after nms, we generate a minimum quadrilateral for each text instance covering the mask as shown in fig.1. standard nms computes iou among bounding boxes, which may be fine for word-level and near-horizontal results' filtering. however, it may filter some correct line-level detections when they are close and heavily inclined as shown in fig.4 or when words stay close in the same line as shown in fig.5. consequently, we propose a modified nms called mask-nms to handle such situations. mask-nms mainly changes bounding box iou computation to so-called mask-maximum-intersection (mmi) as formulated: are mask areas of two text instances to be computed, is the intersection area between the masks. maximum intersection over the mask areas are used to replace original iou for the reason that detections may easily involve line-level and word-level text instances simultaneously at the same line as shown in fig.5. the proposed mask-nms has significantly improved performance for multi-oriented scene text detection as shown in section.5. section: experiments to evaluate the proposed framework, we conduct quantitative experiments on three public benchmarks: icdar2015, msra-td500 and total-text. subsection: datasets icdar 2015 incidental text (ic15) the challenge 4 of icdar 2015 robust reading competition. ic15 contains 1000 training and 500 testing incidental images taken by google glasses without paying attention to viewpoint and image quality. therefore, large variations in text scale, orientation and resolution lead to difficulty for text detection. annotations of the dataset are given in word-level quadrilaterals. msra-td500 (td500) is early presented in. the dataset is multi-oriented and multi-lingual including both chinese and english text\u00ef\u00bc\u008c which consists of 300 training and 200 testing images. different from ic15, annotations of td500 are at line level which are rotated rectangles. total-text is presented in icdar2017. it consists of 1555 images with more than 3 different text orientations: horizontal, multi-oriented, and curved. synthtext in the wild (synthtext) the dataset contains 800, 000 synthetic images, text with random color, fonts, scale and orientation are rendered on natural images carefully to have a realistic look. annotations are given in character, word and line level. subsection: implementation details training we pretrain the proposed ftsn on a subset of synthtext containing 160, 000 images, then finetune on ic15, td500 and total-text. for optimization, standard sgd is used during training with learning rate for first 5 epochs and for the last epoch, and we also apply online hard example mining (ohem) for balancing the positive and negative samples. different from original rpn anchor ratios and scales setting for object detection, anchor scales of [] and ratios of [1/ 3, 1/ 2, 1, 2, 3, 5, 7] are set because text often has a large aspect ratio and a small scale. data augmentation multi-scale training, rotation and color jittering are applied during training. scales are randomly chosen from [600, 720, 960, 1100] and each number represents the short edge of input images. rotation with, and are applied with horizontal flip. consequently, it enlarges 8x dataset size than the original one. random brightness, contrast and saturation jittering are applied for input images. testing input images are resized to when testing. after nms, mask voting is used to obtain an ensemble text instance mask by averaging all reasonable detections. experiments are conducted on mxnet and run on a server with intel i7 6700 k cpu, 64 gb ram, gtx 1080 and ubuntu 14.04 os. subsection: results tabel.1 shows results of the proposed ftsn on ic15 compared with previous state of art published methods. snms and mnms represent standard nms and mask-nms respectively. our ftsn with mask-nms outperforms former best result by 5.3% in precision and 3.1% in hmean. it is evaluated by the official submission server. results on td500 are shown in table.2 along with other state of art methods. it is shown that our methods outperform the current state of art approaches by a large margin in hmean and recall, without adding extra real-world training images. our method also shows great flexibility on the total-text dataset containing curved text. as the dataset is new to the community, experiments are seldom conducted on it which makes our results as a baseline shown in table3. the evaluation metric uses iou of 0.5 between each instance masks. outperforming the current state of the art, our approach runs about 4 fps on images and 2.5 fps when using mask-nms, which presents efficiency and accuracy. it is noted that the proposed mask-nms significantly improved hmean by 0.7 and 0.3 percent on ic15 and td500, which mainly target the situations in fig.4 and fig.5. fig.6 shows example results of ftsn. from left to right, it illustrates results on ic15, td500 and total-text dataset. the decent performance for word-level, line-level and curved text detection with large variation in resolution, view point, scale and linguistics suggests excellent generalization ability. section: conclusion we present ftsn, an end-end efficient and accurate multi-oriented scene text detection framework. it has outperformed previous state of the art approaches on word-level line-level annotated benchmarks and report a baseline on total-text demonstrating decent generalization ability and flexibility. section: acknowlegements the research is supported by the national key research and development program of china under grant 2017yfb1002401. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "icdar2015",
                        740
                    ],
                    [
                        "incidental scene text",
                        750
                    ],
                    [
                        "ic15",
                        10900
                    ],
                    [
                        "icdar 2015",
                        12519
                    ],
                    [
                        "incidental text",
                        12530
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "fused text segmentation networks",
                        10
                    ],
                    [
                        "ftsn",
                        7139
                    ],
                    [
                        "feature representation",
                        7454
                    ],
                    [
                        "feature representations",
                        7725
                    ],
                    [
                        "convolutional feature representation",
                        8283
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "multi-oriented scene text detection",
                        47
                    ],
                    [
                        "scene text detection",
                        986
                    ],
                    [
                        "detecting text",
                        1280
                    ],
                    [
                        "text detection",
                        1595
                    ],
                    [
                        "horizontal text detection",
                        2118
                    ],
                    [
                        "multi-oriented text detection",
                        2568
                    ],
                    [
                        "detections",
                        2940
                    ],
                    [
                        "detection process",
                        4022
                    ],
                    [
                        "object detection",
                        6038
                    ],
                    [
                        "general object detection",
                        6180
                    ],
                    [
                        "arbitrary-shape text detection",
                        7154
                    ],
                    [
                        "detection",
                        11158
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "25b9b1ede6519eeef3e67657a97e2f551940b0d6-39",
    "doctext": "document: segmental recurrent neural networks for end-to-end speech recognition we study the segmental recurrent neural network for end-to-end acoustic modelling. this model connects the segmental conditional random field (crf) with a recurrent neural network (rnn) used for feature extraction. compared to most previous crf-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. instead, this model marginalises out all the possible segmentations, and features are extracted from the rnn trained together with the segmental crf. essentially, this model is self-contained and can be trained end-to-end. in this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. we performed experiments on the timit dataset. we achieved 17.3% phone error rate (per) from the first-pass decoding\u2014 the best reported result using crfs, despite the fact that we only used a zeroth-order crf and without using any language model., theuniversityofedinburgh, edinburgh, uk schoolofcomputerscience, carnegiemellonuniversity, pittsburgh, usa computerscience& engineering, theuniversityofwashington, seattle, usa {liang.lu, s.renals}@ed.ac.uk, {lingpenk, cdyer}@cs.cmu.edu, nasmith@cs.washington.edu index terms: end-to-end speech recognition, segmental crf, recurrent neural networks. section: introduction speech recognition is a typical sequence to sequence transduction problem, i.e., given a sequence of acoustic observations, the speech recognition engine decodes the corresponding sequence of words or phonemes. a key component in a speech recognition system is the acoustic model, which computes the conditional probability of the output sequence given the input sequence. however, directly computing this conditional probability is challenging due to many factors including the variable lengths of the input and output sequences. the hidden markov model (hmm) converts this sequence-level classification task into a frame-level classification problem, where each acoustic frame is classified into one of the hidden states, and each output sequence corresponds to a sequence of hidden states. to make it computationally tractable, hmms usually rely on the conditional independence assumption and the first-order markov rule\u2014 the well-known weaknesses of hmms. furthermore, the hmm-based pipeline is composed of a few relatively independent modules, which makes the joint optimisation nontrivial. there has been a consistent research effort to seek architectures to replace hmms and overcome their limitation for acoustic modelling, e.g.,; however these approaches have not yet improved speech recognition accuracy over hmms. in the past few years, several neural network based approaches have been proposed and demonstrated promising results. in particular, the connectionist temporal classification (ctc) approach defines the loss function directly to maximise the conditional probability of the output sequence given the input sequence, and it usually uses a recurrent neural network to extract features. however, ctc simplifies the sequence-level error function by a product of the frame-level error functions (i.e., independence assumption), which means it essentially still does frame-level classification. it also requires the lengths of the input and output sequence to be the same, which is inappropriate for speech recognition. ctc deals with this problem by replicating the output labels so that a consecutive frames may correspond to the same output label or a blank token. attention-based rnns have been demonstrated to be a powerful alternative sequence-to-sequence transducer, e.g., in machine translation, and speech recognition. a key difference of this model from hmms and ctcs is that the attention-based approach does not apply the conditional independence assumption to the input sequence. instead, it maps the variable-length input sequence into a fixed-size vector representation at each decoding step by an attention-based scheme (see for further explanation). it then generates the output sequence using an rnn conditioned on the vector representation from the source sequence. the attentive scheme suits the machine translation task well, because there may be no clear alignment between the source and target sequence for many language pairs. however, this approach does not naturally apply to the speech recognition task, as each output token only corresponds to a small size window of acoustic spectrum. in this paper, we study segmental rnns for acoustic modelling. this model is similar to ctc and attention-based rnn in the sense that an rnn encoder is also used for feature extraction, but it differs in the sense that the sequence-level conditional probability is defined using an segmental (semi-markov) crf, which is an extension on the standard crf. there have been numerous works on crfs and their variants for speech recognition, e.g, (see for an overview). in particular, feed-forward neural networks have been used with segmental crfs for speech recognition. however, segmental rnns are different in that they are end-to-end models\u2014 they do not depend on external systems to provide segmentation boundaries and features, instead, they are trained by marginalising out all possible segmentations, while the features are derived from the encoder rnns, which are trained jointly with the segmental crfs. our experiments were performed on the timit dataset, and we achieved 17.3% per from first-pass decoding with zeroth-order crf and without using any language model\u2014 the best reported result using crfs. section: segmental recurrent neural networks subsection: segmental conditional random fields given a sequence of acoustic frames and its corresponding sequence of output labels, where, segmental (or semi-markov) conditional random field defines the sequence-level conditional probability with the auxiliary segment labels as where is a tuple of the beginning () and the end () time tag for the segment of, and while; and denotes the vocabulary set; is the normaliser that that sums over all the possible pairs, i.e., here, we only consider the zeroth-order crf, while the extension to higher order models is straightforward. similar to other crf-based models, the function is defined as where denotes the feature function, and is the weight vector. previous works on crf-based acoustic models mainly use heuristically handcrafted feature function. they also usually rely on an external system to provide the segment labels. in this paper, we define using neural networks, and the segmentation is marginalised out during training, which makes our model self-contained. subsection: feature representations we use neural networks to define the feature function, which maps the acoustic segment and its corresponding label into a joint feature space. more specifically, is firstly represented as a one-hot vector, and it is then mapped into a continuous space by a linear embedding matrix as given the segment label, we use an rnn to map the acoustic segment to a fixed-dimensional vector representation, i.e., where denotes the initial hidden state, denotes the duration of the segment and is a non-linear function. we take the final hidden state as the segment embedding vector, then can be represented as where corresponds to one layer or multiple layers of linear or non-linear transformation. in fact, it is flexible to include other relevant features as additional inputs to the function, e.g., the duration feature which can be obtained by converting into another embedding vector. in practice, multiple rnn layers can be used transform the acoustic signal before extracting the segment embedding vector as figure [reference]. subsection: conditional maximum likelihood training for speech recognition, the segmentation labels are usually unknown, training the model by maximising the conditional probability as eq. ([reference]) is therefore not practical. the problem can be addressed by defining the loss function as the negative marginal log-likelihood as where denotes the set of model parameters, and denotes the summation over all the possible segmentations when only is observed. to simplify notations, the objective function is define with only one training utterance. however, the number of possible segmentations is exponential with the length of, which makes the naive computation of both and impractical. fortunately, this can be addressed by using the following dynamic programming algorithm as proposed in: in eq. ([reference]), the first summation is over all the possible segmentation up to timestep, and the second summation is over all the possible labels from the vocabulary. the computation cost of this algorithm is, where is the size of the vocabulary. the cost can be further reduced by introducing an upper bound of the segment length, in which case eq. ([reference]) can be rewritten as where denotes the maximum value of the segment length. the cost is then reduced to, and for long sequences like speech signals where, the computational savings are substantial. the term can be computed similarly. in this case, since the label is now observed, the summation over all the possible labels in eq. ([reference]) is not necessary, i.e., again, we can limit the length of the possible segments as eq. ([reference]). given and, the loss function can be minimised using the stochastic gradient decent (sgd) algorithm similar to training other neural network models. other losses, for example, hinge, can be considered in future work. subsection: viterbi decoding during decoding, we need to search the target label sequence that yields the highest posterior probability given by marginalising out all the possible segmentations: this involves minor modification of the recursive algorithm in eq. ([reference]) that instead of summing over all the possible labels, the viterbi path up to the timestep is however, marginalising out all the possible segmentations is still expensive. the computational cost can be further reduced by greedy searching the most likely segmentation, i.e., which corresponds to the decoding objective as this joint maximization algorithm may yield high search error, because it only considers one segmentation. in the future, we shall investigate the beam search algorithm which may yield a lower search error. subsection: further speedup it is computationally expensive for rnns to model long sequences, and the number of possible segmentations is exponential with the length of the input sequence as mentioned before. the computational cost can be significantly reduced by using the hierarchical subsampling rnn to shorten the input sequences, where the subsampling layer takes a window of hidden states from the lower layer as input as shown in figure [reference]. in this work, we consider three variants: a) concatenate- the hidden states in the subsampling window are concatenated before been fed into the next layer; b) add- the hidden states are added into one vector for the next layer; c) skip- only the last hidden state in the window is kept and all the others are skipped. the last two schemes are computationally cheaper as they do not introduce extra model parameters. section: experiments subsection: system setup we used the timit dataset to evaluate the segmental rnn acoustic models. this dataset was preferred for the rapid evaluation of different system settings, and for the comparison to other crf and end-to-end systems. we followed the standard protocol of the timit dataset, and our experiments were based on the kaldi recipe. we used the core test set as our evaluation set, which has 192 utterances. we used 24 dimensional log fiterbanks (fbanks) with delta and double-delta coefficients, yielding 72 dimensional feature vectors. our models were trained with 48 phonemes, and their predictions were converted to 39 phonemes before scoring. the dimension of was fixed to be 32. for all our experiments, we used the long short-term memory (lstm) networks as the implementation of rnns, and the networks were always bi-directional. we set the initial sgd learning rate to be 0.1, and we exponentially decay the learning rate by a factor of 2 when the validation error stopped decreasing. our models were trained with dropout regularisation, using an specific implementation for recurrent networks. the dropout rate was 0.2 unless specified otherwise. our models were randomly initialised with the same random seed. subsection: results of hierarchical subsampling we first demonstrate the results of the hierarchical subsampling recurrent network, which is the key to speed up our experiments. we set the size of the subsampling window to be 2, therefore each subsampling layer reduced the time resolution by a factor of 2. we set the maximum segment length in eq. ([reference]) to be 300 milliseconds, which corresponded to 30 frames of fbanks (sampled at the rate of 10 milliseconds). with two layers of subsampling recurrent networks, the time resolution was reduced by a factor of 4, and the value of was reduced to be 8, yielding around 10 times speedup as shown in table [reference]. table [reference] compares the three implementations of the recurrent subsampling network detailed in section [reference]. we observed that concatenating all the hidden states in the subsampling window did not yield lower phone error rate (per) than using the simple skipping approach, which may be due to the fact that the timit dataset is small and it prefers a smaller model. on the other hand, adding the hidden states in the subsampling window together worked even worse, possibly due to that the sequential information in the subsampling window was flattened. in the following experiments, we sticked to the skipping method, and using two subsampling layers. subsection: hyperparameters and different features we then evaluated the model by tuning the hyperparameters, and the results are given in table [reference]. we tuned the number of lstm layers, and the dimension of lstm cells, as well as the dimensions of and the segment vector. in general, larger models with dropout regularisation yielded higher recognition accuracy. our best result was obtained using 6 layers of 250-dimensional lstms. however, without the dropout regularisation, the model can be easily overfit due to the small size of training set. in the future, we shall evaluate this model with a large dataset. we then evaluated another two types of features using the same system configuration that achieved the best result in table [reference]. we increased the number of fbanks from 24 to 40, which yielded slightly lower per. we also evaluated the standard kaldi features\u2014 39 dimensional mfccs spliced by a context window of 7, followed by lda and mllt transform and with feature-space speaker-dependent mllr, which were the same features used in the hmm-dnn baseline in table [reference]. the well-engineered features improved the accuracy of our system by more than 1% absolute. subsection: comparison to related works in table [reference], we compare our result to other reported results using segmental crfs as well as recent end-to-end systems. previous state-of-the-art result using segmental crfs on the timit dataset is reported in, where the first-pass decoding was used to prune the search space, and the second-pass was used to re-score the hypothesis using various features including neural network features. besides, the ground-truth segmentation was used in. we achieved considerably lower per with first-pass decoding, despite the fact that our crf was zeroth-order, and we did not use any language model. furthermore, our results are also comparable to that from the ctc and attention-based rnn end-to-end systems. the accuracy of segmental rnns may be further improved by using higher-order crfs or incorporating a language model into the decode step, and using beam search to reduce the search error. section: conclusions in this paper, we present the segmental rnn\u2014 a novel acoustic model that combines the segmental crf with an encoder rnn for end-to-end speech recognition. we discuss the practical training and decoding algorithms of this model for speech recognition, and the subsampling network to reduce the computational cost. our experiments were performed on the timit dataset, and we achieved strong recognition accuracy using zeroth-order crf, and without using any language model. in the future, we shall investigate discriminative training criteria, and incorporating a language model into the decoding step. future works also include implementing a weighted finite sate transducer (wfst) based decoder and scaling this model to large vocabulary datasets. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "timit dataset",
                        840
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "phone error rate",
                        873
                    ],
                    [
                        "per",
                        892
                    ],
                    [
                        "search error",
                        10272
                    ],
                    [
                        "validation error",
                        12295
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "end-to-end speech recognition",
                        50
                    ],
                    [
                        "speech recognition",
                        788
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "26d4ab9b60b91bb610202b58fa1766951fedb9e9-40",
    "doctext": "this paper introduces the deep recurrent attentive writer (draw) neural network architecture for image generation. draw networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. the system substantially improves on the state of the art for generative models on mnist, and, when trained on the street view house numbers dataset, it generates images that can not be distinguished from real data with the naked eye. draw: section: introduction a person asked to draw, paint or otherwise recreate a visual scene will naturally do so in a sequential, iterative fashion, reassessing their handiwork after each modification. rough outlines are gradually replaced by precise forms, lines are sharpened, darkened or erased, shapes are altered, and the final picture emerges. most approaches to automatic image generation, however, aim to generate entire scenes at once. in the context of generative neural networks, this typically means that all the pixels are conditioned on a single latent distribution. as well as precluding the possibility of iterative self-correction, the\" one shot\" approach is fundamentally difficult to scale to large images. the deep recurrent attentive writer (draw) architecture represents a shift towards a more natural form of image construction, in which parts of a scene are created independently from others, and approximate sketches are successively refined. the core of the draw architecture is a pair of recurrent neural networks: an encoder network that compresses the real images presented during training, and a decoder that reconstitutes images after receiving codes. the combined system is trained end-to-end with stochastic gradient descent, where the loss function is a variational upper bound on the log-likelihood of the data. it therefore belongs to the family of variational auto-encoders, a recently emerged hybrid of deep learning and variational inference that has led to significant advances in generative modelling. where draw differs from its siblings is that, rather than generating images in a single pass, it iteratively constructs scenes through an accumulation of modifications emitted by the decoder, each of which is observed by the encoder. an obvious correlate of generating images step by step is the ability to selectively attend to parts of the scene while ignoring others. a wealth of results in the past few years suggest that visual structure can be better captured by a sequence of partial glimpses, or foveations, than by a single sweep through the entire image. the main challenge faced by sequential attention models is learning where to look, which can be addressed with reinforcement learning techniques such as policy gradients. the attention model in draw, however, is fully differentiable, making it possible to train with standard backpropagation. in this sense it resembles the selective read and write operations developed for the neural turing machine. the following section defines the draw architecture, along with the loss function used for training and the procedure for image generation. section [reference] presents the selective attention model and shows how it is applied to reading and modifying images. section [reference] provides experimental results on the mnist, street view house numbers and cifar-10 datasets, with examples of generated images; and concluding remarks are given in section [reference]. lastly, we would like to direct the reader to the video accompanying this paper () which contains examples of draw networks reading and generating images. section: the draw network the basic structure of a draw network is similar to that of other variational auto-encoders: an encoder network determines a distribution over latent codes that capture salient information about the input data; a decoder network receives samples from the code distribuion and uses them to condition its own distribution over images. however there are three key differences. firstly, both the encoder and decoder are recurrent networks in draw, so that a sequence of code samples is exchanged between them; moreover the encoder is privy to the decoder's previous outputs, allowing it to tailor the codes it sends according to the decoder's behaviour so far. secondly, the decoder's outputs are successively added to the distribution that will ultimately generate the data, as opposed to emitting this distribution in a single step. and thirdly, a dynamically updated attention mechanism is used to restrict both the input region observed by the encoder, and the output region modified by the decoder. in simple terms, the network decides at each time-step\" where to read\" and\" where to write\" as well as\" what to write\". the architecture is sketched in fig. [reference], alongside a feedforward variational auto-encoder. subsection: network architecture let be the function enacted by the encoder network at a single time-step. the output of at time is the encoder hidden vector. similarly the output of the decoder at is the hidden vector. in general the encoder and decoder may be implemented by any recurrent neural network. in our experiments we use the long short-term memory architecture (lstm;) for both, in the extended form with forget gates. we favour lstm due to its proven track record for handling long-range dependencies in real sequential data. throughout the paper, we use the notation to denote a linear weight matrix with bias from the vector to the vector. at each time-step, the encoder receives input from both the image and from the previous decoder hidden vector. the precise form of the encoder input depends on a operation, which will be defined in the next section. the output of the encoder is used to parameterise a distribution over the latent vector. in our experiments the latent distribution is a diagonal gaussian: bernoulli distributions are more common than gaussians for latent variables in auto-encoders; however a great advantage of gaussian latents is that the gradient of a function of the samples with respect to the distribution parameters can be easily obtained using the so-called reparameterization trick. this makes it straightforward to back-propagate unbiased, low variance stochastic gradients of the loss function through the latent distribution. at each time-step a sample drawn from the latent distribution is passed as input to the decoder. the output of the decoder is added (via a operation, defined in the sequel) to a cumulative canvas matrix, which is ultimately used to reconstruct the image. the total number of time-steps consumed by the network before performing the reconstruction is a free parameter that must be specified in advance. for each image presented to the network, are initialised to learned biases, and the draw network iteratively computes the following equations for: where is the error image, is the concatenation of vectors and into a single vector, and denotes the logistic sigmoid function:. note that, and hence, depends on both and the history of previous latent samples. we will sometimes make this dependency explicit by writing, as shown in fig. [reference]. can also be passed as input to the operation; however we did not find that this helped performance and therefore omitted it. subsection: loss function the final canvas matrix is used to parameterise a model of the input data. if the input is binary, the natural choice for is a bernoulli distribution with means given by. the reconstruction loss is defined as the negative log probability of under: the latent loss for a sequence of latent distributions is defined as the summed kullback-leibler divergence of some latent prior from: note that this loss depends upon the latent samples drawn from, which depend in turn on the input. if the latent distribution is a diagonal gaussian with, as defined in eqs [reference] and [reference], a simple choice for is a standard gaussian with mean zero and standard deviation one, in which case eq. [reference] becomes the total loss for the network is the expectation of the sum of the reconstruction and latent losses: which we optimise using a single sample of for each stochastic gradient descent step. can be interpreted as the number of nats required to transmit the latent sample sequence to the decoder from the prior, and (if is discrete) is the number of nats required for the decoder to reconstruct given. the total loss is therefore equivalent to the expected compression of the data by the decoder and prior. subsection: stochastic data generation an image can be generated by a draw network by iteratively picking latent samples from the prior, then running the decoder to update the canvas matrix. after repetitions of this process the generated image is a sample from: note that the encoder is not involved in image generation. section: read and write operations the draw network described in the previous section is not complete until the and operations in eqs. [reference] and [reference] have been defined. this section describes two ways to do so, one with selective attention and one without. subsection: reading and writing without attention in the simplest instantiation of draw the entire input image is passed to the encoder at every time-step, and the decoder modifies the entire canvas matrix at every time-step. in this case the and operations reduce to however this approach does not allow the encoder to focus on only part of the input when creating the latent distribution; nor does it allow the decoder to modify only a part of the canvas vector. in other words it does not provide the network with an explicit selective attention mechanism, which we believe to be crucial to large scale image generation. we refer to the above configuration as\" draw without attention\". subsection: selective attention model to endow the network with selective attention without sacrificing the benefits of gradient descent training, we take inspiration from the differentiable attention mechanisms recently used in handwriting synthesis and neural turing machines. unlike the aforementioned works, we consider an explicitly two-dimensional form of attention, where an array of 2d gaussian filters is applied to the image, yielding an image 'patch' of smoothly varying location and zoom. this configuration, which we refer to simply as\" draw\", somewhat resembles the affine transformations used in computer graphics-based autoencoders. as illustrated in fig. [reference], the grid of gaussian filters is positioned on the image by specifying the co-ordinates of the grid centre and the stride distance between adjacent filters. the stride controls the 'zoom' of the patch; that is, the larger the stride, the larger an area of the original image will be visible in the attention patch, but the lower the effective resolution of the patch will be. the grid centre and stride (both of which are real-valued) determine the mean location of the filter at row, column in the patch as follows: two more parameters are required to fully specify the attention model: the isotropic variance of the gaussian filters, and a scalar intensity that multiplies the filter response. given an input image, all five attention parameters are dynamically determined at each time step via a linear transformation of the decoder output: where the variance, stride and intensity are emitted in the log-scale to ensure positivity. the scaling of, and is chosen to ensure that the initial patch (with a randomly initialised network) roughly covers the whole input image. given the attention parameters emitted by the decoder, the horizontal and vertical filterbank matrices and (dimensions and respectively) are defined as follows: where is a point in the attention patch, is a point in the input image, and are normalisation constants that ensure that and. subsection: reading and writing with attention given, and intensity determined by, along with an input image and error image, the read operation returns the concatenation of two patches from the image and error image: note that the same filterbanks are used for both the image and error image. for the write operation, a distinct set of attention parameters, and are extracted from, the order of transposition is reversed, and the intensity is inverted: where is the writing patch emitted by. for colour images each point in the input and error image (and hence in the reading and writing patches) is an rgb triple. in this case the same reading and writing filters are used for all three channels. section: experimental results we assess the ability of draw to generate realistic-looking images by training on three datasets of progressively increasing visual complexity: mnist, street view house numbers (svhn) and cifar-10. the images generated by the network are always novel (not simply copies of training examples), and are virtually indistinguishable from real data for mnist and svhn; the generated cifar images are somewhat blurry, but still contain recognisable structure from natural scenes. the binarized mnist results substantially improve on the state of the art. as a preliminary exercise, we also evaluate the 2d attention module of the draw network on cluttered mnist classification. for all experiments, the model of the input data was a bernoulli distribution with means given by. for the mnist experiments, the reconstruction loss from eq [reference] was the usual binary cross-entropy term. for the svhn and cifar-10 experiments, the red, green and blue pixel intensities were represented as numbers between 0 and 1, which were then interpreted as independent colour emission probabilities. the reconstruction loss was therefore the cross-entropy between the pixel intensities and the model probabilities. although this approach worked well in practice, it means that the training loss did not correspond to the true compression cost of rgb images. network hyper-parameters for all the experiments are presented in table [reference]. the adam optimisation algorithm was used throughout. examples of generation sequences for mnist and svhn are provided in the accompanying video (). subsection: cluttered mnist classification to test the classification efficacy of the draw attention mechanism (as opposed to its ability to aid in image generation), we evaluate its performance on the cluttered translated mnist task. each image in cluttered mnist contains many digit-like fragments of visual clutter that the network must distinguish from the true digit to be classified. as illustrated in fig. [reference], having an iterative attention model allows the network to progressively zoom in on the relevant region of the image, and ignore the clutter outside it. our model consists of an lstm recurrent network that receives a 'glimpse' from the input image at each time-step, using the selective read operation defined in section [reference]. after a fixed number of glimpses the network uses a softmax layer to classify the mnist digit. the network is similar to the recently introduced recurrent attention model (ram), except that our attention method is differentiable; we therefore refer to it as\" differentiable ram\". the results in table [reference] demonstrate a significant improvement in test error over the original ram network. moreover our model had only a single attention patch at each time-step, whereas ram used four, at different zooms. subsection: mnist generation we trained the full draw network as a generative model on the binarized mnist dataset. this dataset has been widely studied in the literature, allowing us to compare the numerical performance (measured in average nats per image on the test set) of draw with existing methods. table [reference] shows that draw without selective attention performs comparably to other recent generative models such as darn, nade and dbms, and that draw with attention considerably improves on the state of the art. once the draw network was trained, we generated mnist digits following the method in section [reference], examples of which are presented in fig. [reference]. fig. [reference] illustrates the image generation sequence for a draw network without selective attention (see section [reference]). it is interesting to compare this with the generation sequence for draw with attention, as depicted in fig. [reference]. whereas without attention it progressively sharpens a blurred image in a global way, with attention it constructs the digit by tracing the lines\u2014 much like a person with a pen. subsection: mnist generation with two digits the main motivation for using an attention-based generative model is that large images can be built up iteratively, by adding to a small part of the image at a time. to test this capability in a controlled fashion, we trained draw to generate images with two mnist images chosen at random and placed at random locations in a black background. in cases where the two digits overlap, the pixel intensities were added together at each point and clipped to be no greater than one. examples of generated data are shown in fig. [reference]. the network typically generates one digit and then the other, suggesting an ability to recreate composite scenes from simple pieces. subsection: street view house number generation mnist digits are very simplistic in terms of visual structure, and we were keen to see how well draw performed on natural images. our first natural image generation experiment used the multi-digit street view house numbers dataset. we used the same preprocessing as, yielding a house number image for each training example. the network was then trained using patches extracted at random locations from the preprocessed images. the svhn training set contains 231, 053 images, and the validation set contains 4, 701 images. the house number images generated by the network are highly realistic, as shown in figs. [reference] and [reference]. fig. [reference] reveals that, despite the long training time, the draw network underfit the svhn training data. s subsection: generating cifar images the most challenging dataset we applied draw to was the cifar-10 collection of natural images. cifar-10 is very diverse, and with only 50, 000 training examples it is very difficult to generate realistic-looking objects without overfitting (in other words, without copying from the training set). nonetheless the images in fig. [reference] demonstrate that draw is able to capture much of the shape, colour and composition of real photographs. section: conclusion this paper introduced the deep recurrent attentive writer (draw) neural network architecture, and demonstrated its ability to generate highly realistic natural images such as photographs of house numbers, as well as improving on the best known results for binarized mnist generation. we also established that the two-dimensional differentiable attention mechanism embedded in draw is beneficial not only to image generation, but also to image classification. section: acknowledgments of the many who assisted in creating this paper, we are especially thankful to koray kavukcuoglu, volodymyr mnih, jimmy ba, yaroslav bulatov, greg wayne, andrei rusu and shakir mohamed. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar-10 datasets",
                        3434
                    ],
                    [
                        "cifar-10",
                        12878
                    ],
                    [
                        "cifar images",
                        13068
                    ],
                    [
                        "cifar-10 experiments",
                        13590
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "deep recurrent attentive writer",
                        26
                    ],
                    [
                        "draw",
                        60
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "image generation",
                        97
                    ],
                    [
                        "generating cifar images",
                        18178
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "27e98e09cf09bc13c913d01676e5f32624011050-41",
    "doctext": "document: u-net: machine reading comprehension with unanswerable questions machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. a key subtask is to reliably predict whether the question is unanswerable. in this paper, we propose a unified model, called u-net, with three important components: answer pointer, no-answer pointer, and answer verifier. we introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. the universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the u-net. different from the state-of-art pipeline models, u-net can be learned in an end-to-end fashion. the experimental results on the squad 2.0 dataset show that u-net can effectively predict the unanswerability of questions and achieves an f1 score of 71.7 on squad 2.0. section: introduction machine reading comprehension (mrc) is a challenging task in natural language processing, which requires that machine can read, understand, and answer questions about a text. benefiting from the rapid development of deep learning techniques and large-scale benchmarks, the end-to-end neural methods have achieved promising results on mrc task. the best systems have even surpassed human performance on the stanford question answering dataset (squad), one of the most widely used mrc benchmarks. however, one of the limitations of the squad task is that each question has a correct answer in the context passage, therefore most models just need to select the most relevant text span as the answer, without necessarily checking whether it is indeed the answer to the question. to remedy the deficiency of squad, squad2.0 squad2.0 developed squad 2.0 that combines squad with new unanswerable questions. table [reference] shows two examples of unanswerable questions. the new dataset requires the mrc systems to know what they do n't know. paragraph:\"\u2026 other legislation followed, including the migratory bird conservation act of 1929, a 1937 treaty prohibiting the hunting of right and gray whales, and the bald eagle protection act of 1940. these later laws had a low cost to society\u00e2\u0080\u0094the species were relatively rare\u00e2\u0080\u0094and little opposition was raised.\u00e2\u0080\u009d to do well on mrc with unanswerable questions, the model needs to comprehend the question, reason among the passage, judge the unanswerability and then identify the answer span. since extensive work has been done on how to correctly predict the answer span when the question is answerable (e.g., squad 1.1), the main challenge of this task lies in how to reliably determine whether a question is not answerable from the passage. there are two kinds of approaches to model the answerability of a question. one approach is to directly extend previous mrc models by introducing a no-answer score to the score vector of the answer span. but this kind of approaches is relatively simple and can not effectively model the answerability of a question. another approach introduces an answer verifier to determine whether the question is unanswerable. however, this kind of approaches usually has a pipeline structure. the answer pointer and answer verifier have their respective models, which are trained separately. intuitively, it is unnecessary since the underlying comprehension and reasoning of language for these components is the same. in this paper, we decompose the problem of mrc with unanswerable questions into three sub-tasks: answer pointer, no-answer pointer, and answer verifier. since these three sub-tasks are highly related, we regard the mrc with unanswerable questions as a multi-task learning problem by sharing some meta-knowledge. we propose the u-net to incorporate these three sub-tasks into a unified model: 1) an answer pointer to predict a candidate answer span for a question; 2) a no-answer pointer to avoid selecting any text span when a question has no answer; and 3) an answer verifier to determine the probability of the\" unanswerability\" of a question with candidate answer information. additionally, we also introduce a universal node and process the question and its context passage as a single contiguous sequence of tokens, which greatly improves the conciseness of u-net. the universal node acts on both question and passage to learn whether the question is answerable. different from the previous pipeline models, u-net can be learned in an end-to-end fashion. our experimental results on the squad 2.0 dataset show that u-net effectively predicts the unanswerability of questions and achieves an f1 score of 72.6. the contributions of this paper can be summarized as follows. we decompose the problem of mrc with unanswerable questions into three sub-tasks and combine them into a unified model, which uses the shared encoding and interaction layers. thus, the three-tasks can be trained simultaneously in an end-to-end fashion. we introduce a universal node to encode the common information of the question and passage. thus, we can use a unified representation to model the question and passage, which makes our model more condensed. u-net is very easy to implement yet effective. section: proposed model formally, we can represent the mrc problem as: given a set of tuples, where is the question with words, is the context passage with words, and is the answer with and indicating the start and end points, the task is to estimate the conditional probability. the architecture of our proposed u-net is illustrated in figure [reference]. u-net consists of four major blocks: unified encoding, multi-level attention, final fusion, and prediction. as shown in figure [reference], we first combine the embedded representation of the question and passage with a universal node and pass them through a bilstm to encode the whole text. we then use the encoded representation to do the information interaction. then we use the encoded and interacted representation to fuse the full representation and feed them into the final prediction layers to do the multi-task training. we will describe our model in details in the following. subsection: (a) unified encoding paragraph: embedding following the successful models on squad 1.1, we first embed both the question and the passage with the following features. glove embedding and elmo embedding are used as basic embeddings. besides, we use pos embedding, ner embedding, and a feature embedding that includes the exact match, lower-case match, lemma match, and a tf-idf feature. now we get the question representation and the passage representation, where each word is represented as a-dim embedding by combining the features/ embedding described above. paragraph: universal node we create a universal node, which is a key factor in our model and has several roles in predicting the unanswerability of question. we expect this node to learn universal information from both passage and question. this universal node is added and connects the passage and question at the phase of embedding, and then goes along with the whole representation, so it is a key factor in information representation. since the universal node is in between and later shared between passage and question, it has an abstract semantic meaning rather than just a word embedding. also, the universal node is later shared in the attention interaction mechanism and used in both the answer boundary detection and classification tasks, so this node carries massive information and has several important roles in our whole model construction. the universal node is first represented by a-dim randomly-initialized vector. we concatenated question representation, universal node representation, passage representation together as: is a joint representation of question and passage. paragraph: word-level fusion then we first use two-layer bidirectional lstm (bilstm) to fuse the joint representation of question, universal node, and passage. where is the hidden states of the first bilstm, representing the low-level semantic information, and is the hidden states of the second bilstm, representing the high-level semantic information. finally, we concatenate and together and pass them through the third bilstm and obtain a full representation. thus, represents the deep fusion information of the question and passage on word-level. when a bilstm is applied to encode representations, it learns the semantic information bi-directionally. since the universal node is between the question and passage, its hidden states can learn both question and passage information. when the passage-question pair was encoded as a unified representation and information flows via the bilstm, the universal node has an important role in information representation. subsection: (b) multi-level attention to fully fuse the semantic representation of the question and passage, we use the attention mechanism to capture their interactions on different levels. we expected that we could simply use self-attention on the encoded representation for interaction between question and passage, which contains both bi-attention and self-attention of the question and passage. but we found that it performed slightly worse than the traditional bi-directional attention with the universal node included. therefore, we use a bi-directional attention between the question and passage. we first divide into two representations: attached passage and attached question, and let the universal node representation attached to both the passage and question, i.e., note is shared by and. here the universal node works as a special information carrier, and both passage and question can focus attention information on this node so that the connection between them is closer than a traditional bi-attention interaction. since both and are concatenated by three-level representations, we followed previous work fusionnet to construct their iterations on three levels. take the first level as an example. we first compute the affine matrix of and by where; and are learnable parameters. next, a bi-directional attention is used to compute the interacted representation and. where is column-wise normalized function. we use the same attention layer to model the interactions for all the three levels, and get the final fused representation for the question and passage respectively. note that while dealing with the attention output of the universal node, we added two outputs from passage-to-question attention and question-to-passage attention. so after the interaction, the fused representation still have the same length as the encoded representation, and. subsection: (c) final fusion after the three-level attentive interaction, we generate the final fused information for the question and passage. we concatenate all the history information: we first concatenate the encoded representation and the representation after attention (again, we use, and to represent 3 different levels of representation for the two previous steps respectively). following the success of densenet, we concatenate the input and output of each layer as the input of the next layer. first, we pass the concatenated representation through a bilstm to get. where the representation is a fusion of information from different levels. then we concatenate the original embedded representation and for better representation of the fused information of passage, universal node, and question. finally, we use a self-attention layer to get the attention information within the fused information. the self-attention layer is constructed the same way as: where is the representation after self-attention of the fused information. next we concatenated representation and and pass them through another bilstm layer: now is the final fused representation of all the information. at this point, we divide into two parts:,, representing the fused information of the question and passage respectively. note for the final representation, we attach the universal node only in the passage representation. this is because we need the universal node as a focus for the pointer when the question is unanswerable. these will be fed into the next decoder prediction layer. subsection: (d) prediction the prediction layer receives fused information of passage and question, and tackles three prediction tasks: (1) answer pointer, (2) no-answer pointer and (3) answer verifier. first, we use a function shown below to summarize the question information into a fixed-dim representation. where is a learnable weight matrix and represents the word in the question representation. then we feed into the answer pointer to find boundaries of answers, and the classification layer to distinguish whether the question is answerable. paragraph: (i) answer pointer we use this answer pointer to detect the answer boundaries from the passage when the question is answerable (i.e., the answer is a span in the passage). this layer is a classic pointer net structure. we use two trainable matrices and to estimate the probability of the answer start and end boundaries of the word in the passage, and. note here when the question is answerable, we do not consider the universal node in answer boundary detection, so we have (is the universal node in the passage representation). the loss function for the answerable question pairs is: where and are the ground-truth of the start and end boundary of the answer. paragraph: (ii) no-answer pointer then we use the same pointer for questions that are not answerable. here the loss is: and correspond to the position of the universal node, which is at the front of the passage representation. for this scenario, the loss is calculated for the universal node. additionally, since there exits a plausible answer for each unanswerable question in squad 2.0, we introduce an auxiliary plausible answer pointer to predict the boundaries of the plausible answers. the plausible answer pointer has the same structure as the answer pointer, but with different parameters. thus, the total loss function is: where and are the output of the plausible answer pointer; and are the start and end boundary of the unanswerable answer. the no-answer pointer and plausible answer pointer are removed at test phase. paragraph: (iii) answer verifier we use the answer verifier to distinguish whether the question is answerable. answer verifier applies a weighted summary layer to summarize the passage information into a fixed-dim representation (as shown in eq. ([reference])). and we use the weight matrix obtained from the answer pointer to get two representations of the passage. then we use the universal node and concatenate it with the summary of question and passage to make a fixed vector this fixed includes the representation representing the question information, and and representing the passage information. since these representations are highly summarized specially for classification, we believe that this passage-question pair contains information to distinguish whether this question is answerable. in addition, we include the universal node as a supplement. since the universal node is pointed at when the question is unanswerable and this node itself already contains information collected from both the passage and question during encoding and information interaction, we believe that this node is important in distinguishing whether the question is answerable. finally, we pass this fixed vector through a linear layer to obtain the prediction whether the question is answerable. where is a sigmoid function, is a learnable weight matrix. here we use the cross-entropy loss in training. where indicates whether the question has an answer in the passage. compared with other relatively complex structures developped for this mrc task, our u-net model passes the original question and passage pair through embedding and encoding, which then interacts with each other, yielding fused information merged from all the levels. the entire architecture is very easy to construct. after we have the fused representation of the question and passage, we pass them through the pointer layer and a fused information classification layer in a multi-task setup. section: training we jointly train the three tasks by combining the three loss functions. the final loss function is: where indicates whether the question has an answer in the passage,, and are the three loss functions of the answer pointer, no-answer pointer, and answer verifier. although the three tasks could have different weights in the final loss function and be further fine-tuned after joint training, here we just consider them in the same weight and do not fine-tune them individually. at the test phase, we first use the answer pointer to find a potential answer to the question, while the verifier layer judges whether the question is answerable. if the classifier predicts the question is unanswerable, we consider the answer extracted by the answer pointer as plausible. in this way, we get the system result. section: experiment subsection: datasets recently, machine reading comprehension and question answering have progressed rapidly, owing to the computation ability and publicly available high-quality datasets such as squad. now new research efforts have been devoted to the newly released answer extraction test with unanswerable questions, squad 2.0. it is constructed by combining question-answer pairs selected from squad 1.0 and newly crafted unanswerable questions. these unanswerable questions are created by workers that were asked to pose questions that can not be answered based on the paragraph alone but are similar to the answerable questions. it is very difficult to distinguish these questions from the answerable ones. we evaluate our model using this data set. it contains over 100, 000+ questions on 500+ wikipedia articles. subsection: implementation details we use spacy to process each question and passage to obtain tokens, pos tags, ner tags and lemmas tags of each text. we use 12 dimensions to embed pos tags, 8 for ner tags. we use 3 binary features: exact match, lower-case match and lemma match between the question and passage. we use 100-dim glove pretrained word embeddings and 1024-dim elmo embeddings. all the lstm blocks are bi-directional with one single layer. we set the hidden layer dimension as 125, attention layer dimension as 250. we added a dropout layer over all the modeling layers, including the embedding layer, at a dropout rate of 0.3. we use adam optimizer with a learning rate of 0.002. during training, we omit passage with over 400 words and question with more than 50 words. for testing, when the passage has over 600 words and the question is over 100 words, we simply label these questions as unanswerable. subsection: main results our model achieves an f1 score of 74.0 and an em score of 70.3 on the development set, and an f1 score of 72.6 and an em score of 69.2 on test set, as shown in table [reference]. our model outperforms most of the previous approaches. comparing to the best-performing systems, our model has a simple architecture and is an end-to-end model. in fact, among all the end-to-end models, we achieve the best f1 scores. we believe that the performance of the u-net can be boosted with an additional post-processing step to verify answers using approaches such as. subsection: ablation study we also do an ablation study on the squad 2.0 development set to further test the effectiveness of different components in our model. in table [reference], we show four different configurations. first, we remove the universal node. we let the negative examples focus on the plausible answer spans instead of focusing on the universal node. this results in a loss of 2.6% f1 score on the development set, showing that the universal node indeed learns information about whether the question is answerable. we also tried to make the universal node only attached to the passage representation when passing the attention layer. our results showed that when node is shared, as it is called 'universal', it learns information interaction between the question and passage, and when it is not shared, the performance slightly degraded. as for the approaches to encode the representations, we pass both the question and passage through a shared bilstm. to test the effectiveness of this, we ran the experiment using separate bilstms on embedded question and passage representations. results show that the performance dropped slightly, suggesting sharing bilstm is an effective method to improve the quality of the encoder. after removing the plausible answer pointer, the performance also dropped, indicating the plausible answers are useful to improve the model even though they are incorrect. after removing the answer verifier, the performance dropped greatly, indicating it is vital for our model. lastly, we run a test using a more concise configuration. in the second block (multi-level attention) of the u-net, we do not split the output of the encoded presentation and let it pass through a self-attention layer. the bidirectional attention is removed. in this way, our model uses only one unified representation of the question and passage at all time. we simply pass this representation layer by layer to get the final result. compared to the bi-attention model, the f1-score decreases 0.5%. subsection: multi-task study we also run an experiment to test the performance of our multi-task model. we select different losses that participate in the training procedure to observe the performance affected by answer boundary detect or classification. table [reference] shows the performance. here we use and to represent the em and f1 score when the classification is not part of the task, which makes it very much like the task in squad 1.1. to test our classifier performance, we do not use backward propagation over the loss of answer boundary detection and simply run a classification task. results (the first two rows in table [reference]) show that there is a large gain when using the multi-task model. the answer boundary detection task helps the encoder learn information between the passage and question and also feed information into the universal node, therefore we can use a summarized representation of the passage and question as well as the universal node to distinguish whether the question is answerable, i.e., help improve classification. for the answer boundary detection task, we find that the multi-task setup (i.e., the classification layer participates in the training process) does not help its performance. since the classifier and pointer layer shared the encoding process, we originally expected that classification information can help detect answer boundaries. but this is not the case. we think this is also reasonable since distinguishing whether the question is answerable is mainly focusing on the interactions between the passage-question pair, so once the question is predicted as answerable or not, it has nothing to do with the answer boundaries. this is consistent with how human-beings do this classification task. we also run the test over squad 1.1 development test to evaluate the performance. due to a condensed structure, our model achieves an score of less than 86%, which is not a very competitive score on squad 1.1 test. but as shown above, our model achieves a good score in squad 2.0 test, which shows this model has the potential to achieve higher performance by making progress on both the answer detection and classification tasks. overall, we can conclude that our multi-task model works well since the performance of unanswerability classification improves significantly when the answer pointer and answer verifier work simultaneously. subsection: study on the different thresholds of unanswerability classification the output of the answer verifier is the probability of a question being unanswerable. the smaller the output, the lower the probability of unanswerability is. in squad 2.0, the proportions of unanswerable questions are different in the training and test sets. the default threshold is optimized on the training set, but not suitable for the test set. therefore, it is reasonable to set a proper threshold to manually adapt to the test set. as mentioned in squad 2.0 paper, different thresholds for answerability prediction result in fluctuated scores between answerable and unanswerable questions. here we show the variation of the f1 score with different thresholds in figure. the threshold between is used to decide whether a question can be answered. when the threshold is set to, all questions are considered as answerable. as we can see, when the threshold is set to 0.5, f1 score of answerable questions is similar to that of unanswerable questions. when we increase the threshold (i.e., more likely to predict the question as unanswerable), performance for answerable questions degrades, and improves for unanswerable questions. this is as expected. we can see that the overall score is slightly better, which is consistent with the idea from squad 2.0. in addition, we find that for larger thresholds, the variance between and is narrowed since and scores for unanswerable questions are the same. finally, we set the threshold to be for the submission system to squad evaluation. section: related work subsection: end-to-end models for mrc currently, end-to-end neural network models have achieved great successes for machine reading comprehension. most of these models consist of three components: encoder, interaction, and pointer. the bilstm is widely used for encoding the embedded representation. for the interaction, bidirectional attention mechanism is very effective to fuse information of the question and passage. finally, a pointer network is used to predict the span boundaries of the answer. specifically, in squad test, there are approaches to combine match-lstm and pointer networks to produce boundaries of the answer and employ variant bidirectional attention mechanism to match the question and passage mutually. in our model, we learn from previous work and develop a condensed end-to-end model for the squad 2.0 task. different from the previous models, we use a unified representation to encode the question and passage simultaneously, and introduce a universal node to encode the fused information of the question and passage, which also plays an important role to predict the unanswerability of a question. subsection: mrc with unanswerable questions mrc with unanswerable questions is a more challenging task. previous work levy2017zero, clark2017simple levy2017zero, clark2017simple has attempted to normalize a no-answer score depending on the probability of all answer spans and still detect boundaries at the same time. but the scores of the answer span predictions are not very discriminative in distinguishing whether the question is answerable. therefore, this kind of approaches, though relatively simple, can not effectively deal with the answerability of a question. hu2018read, tan2018know hu2018read, tan2018know introduced an answer verifier idea to construct a classification layer. however, this kind of approaches usually has a pipeline structure. the answer pointer and answer verifier have their respective models that are trained separately. paragraph: multi-task models different from existing work, we regard the mrc with unanswerable questions as a multi-task learning problem by sharing some meta-knowledge. intuitively, answer prediction and answer verification are related tasks since the underlying comprehension and reasoning of language for these components is the same. therefore, we construct a multi-task model to solve three sub-tasks: answer pointer, no-answer pointer, and answer verifier. section: conclusion and future work in this paper, we regard the mrc with unanswerable questions as multi-task learning problems and propose the u-net, a simple end-to-end model for mrc challenges. u-net has good performance on squad 2.0. we first add a universal node to learn a fused representation from both the question and passage, then use a concatenated representation to pass through encoding layers. we only treat question and passage differently during attention interactions. in the rest blocks of u-net, we still use the unified representation containing both the question and passage representation. finally, we train the u-net as a multi-task framework to determine the final answer boundaries as well as whether the question is answerable. our model has very simple structure yet achieves good results on squad 2.0 test. our future work is to reconstruct the structure of u-net by replacing the current multi-level attention block with a simpler self-attention mechanism, which we believe can capture the question and passage information, and intuitively is also coherent with the rest of our u-net model. in addition, we will improve the answer boundary detection performance based on some of the previous successful models. since our model actually does not achieve very competitive performance in the boundary detection task yet still has a good overall performance on squad 2.0 test, we are optimistic that our u-net model is potentially capable of achieving better performance. furthermore, our model has a simple structure and is easy to implement, therefore we believe that our model can be easily modified for various datasets. section: acknowledgement we would like to thank robin jia, pranav rajpurkar for their help with squad 2.0 submissions. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "squad 2.0 dataset",
                        883
                    ],
                    [
                        "squad 2.0",
                        1010
                    ],
                    [
                        "squad2.0",
                        1853
                    ],
                    [
                        "squad 2.0 test",
                        23551
                    ],
                    [
                        "squad 2.0 task",
                        26329
                    ],
                    [
                        "squad 2.0 submissions",
                        29702
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "em score",
                        18992
                    ],
                    [
                        "em",
                        21851
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "mrc with unanswerable questions",
                        3578
                    ],
                    [
                        "unanswerability of questions",
                        4686
                    ],
                    [
                        "answerable questions",
                        25063
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "squad 2.0 dataset",
                        883
                    ],
                    [
                        "squad 2.0",
                        1010
                    ],
                    [
                        "squad2.0",
                        1853
                    ],
                    [
                        "squad 2.0 test",
                        23551
                    ],
                    [
                        "squad 2.0 task",
                        26329
                    ],
                    [
                        "squad 2.0 submissions",
                        29702
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "f1 score",
                        990
                    ],
                    [
                        "f1 scores",
                        19348
                    ],
                    [
                        "f1-score",
                        21497
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "mrc with unanswerable questions",
                        3578
                    ],
                    [
                        "unanswerability of questions",
                        4686
                    ],
                    [
                        "answerable questions",
                        25063
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "289e3e6b84982eb65aea8e3a64f2f6916c98e87e-42",
    "doctext": "document: grammar as a foreign language syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. as a result, the most accurate parsers are domain specific, complex, and inefficient. in this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. it also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. our parser is also fast, processing over a hundred sentences per second with an unoptimized cpu implementation. section: introduction syntactic constituency parsing is a fundamental problem in linguistics and natural language processing that has a wide range of applications. this problem has been the subject of intense research for decades, and as a result, there exist highly accurate domain-specific parsers. the computational requirements of traditional parsers are cubic in sentence length, and while linear-time shift-reduce constituency parsers improved in accuracy in recent years, they never matched state-of-the-art. furthermore, standard parsers have been designed with parsing in mind; the concept of a parse tree is deeply ingrained into these systems, which makes these methods inapplicable to other problems. recently, sutskever et al. introduced a neural network model for solving the general sequence-to-sequence problem, and bahdanau et al. proposed a related model with an attention mechanism that makes it capable of handling long sequences well. both models achieve state-of-the-art results on large scale machine translation tasks (e.g.,). syntactic constituency parsing can be formulated as a sequence-to-sequence problem if we linearize the parse tree (cf. figure [reference]), so we can apply these models to parsing as well. our early experiments focused on the sequence-to-sequence model of sutskever et al.. we found this model to work poorly when we trained it on standard human-annotated parsing datasets (1 m tokens), so we constructed an artificial dataset by labelling a large corpus with the berkeleyparser. to our surprise, the sequence-to-sequence model matched the berkeleyparser that produced the annotation, having achieved an f1 score of 90.5 on the test set (section 23 of the wsj). we suspected that the attention model of bahdanau et al. might be more data efficient and we found that it is indeed the case. we trained a sequence-to-sequence model with attention on the small human-annotated parsing dataset and were able to achieve an f1 score of 88.3 on section 23 of the wsj without the use of an ensemble and 90.5 with an ensemble, which matches the performance of the berkeleyparser (90.4) when trained on the same data. finally, we constructed a second artificial dataset consisting of only high-confidence parse trees, as measured by the agreement of two parsers. we trained a sequence-to-sequence model with attention on this data and achieved an f1 score of 92.5 on section 23 of the wsj- a new state-of-the-art. this result did not require an ensemble, and as a result, the parser is also very fast. an ensemble further improves the score to 92.8. section: lstm+ a parsing model [xscale=0.64, yscale=0.75] (a) at (0,-1.7).; (b) at (2,-1.7) go; [thick,-\u00bf] (a)- (0,-1.05); [thick,-\u00bf] (b)- (2,-1.05); (- 0.8,-1.0) rectangle (2.8, 0.0); (lstm) at (1,-0.5) lstm; [thick,-\u00bf] (0, 0.05)- (0, 0.45); [thick,-\u00bf] (2, 0.05)- (2, 0.45); (- 0.8, 0.5) rectangle (2.8, 1.5); (lstm) at (1, 1) lstm; [thick,-\u00bf] (0, 1.55)- (0, 1.95); [thick,-\u00bf] (2, 1.55)- (2, 1.95); (- 0.8, 2) rectangle (2.8, 3); (lstm) at (1, 2.5) lstm; [thick,-\u00bf] (2.85,-0.5)- (4.15,-0.5); [thick,-\u00bf] (2.85, 1)- (4.15, 1); [thick,-\u00bf] (2.85, 2.5)- (4.15, 2.5); [xshift=1 cm] (end) at (4,-1.7) end; (s) at (6,-1.7) (s; (vp) at (8,-1.7) (vp; (vb) at (10,-1.7) xx; (cvp) at (12,-1.7)); (dot) at (14,-1.7).; (cs) at (16,-1.7)); [thick,-\u00bf] (end)- (4,-1.05); [thick,-\u00bf] (s)- (6,-1.05); [thick,-\u00bf] (vp)- (8,-1.05); [thick,-\u00bf] (vb)- (10,-1.05); [thick,-\u00bf] (cvp)- (12,-1.05); [thick,-\u00bf] (dot)- (14,-1.05); [thick,-\u00bf] (cs)- (16,-1.05); (3.2,-1.0) rectangle (16.8, 0.0); (lstm) at (10,-0.5) lstm; [thick,-\u00bf] (4, 0.05)- (4, 0.45); [thick,-\u00bf] (6, 0.05)- (6, 0.45); [thick,-\u00bf] (8, 0.05)- (8, 0.45); [thick,-\u00bf] (10, 0.05)- (10, 0.45); [thick,-\u00bf] (12, 0.05)- (12, 0.45); [thick,-\u00bf] (14, 0.05)- (14, 0.45); [thick,-\u00bf] (16, 0.05)- (16, 0.45); (3.2, 0.5) rectangle (16.8, 1.5); (lstm) at (10, 1) lstm; [thick,-\u00bf] (4, 1.55)- (4, 1.95); [thick,-\u00bf] (6, 1.55)- (6, 1.95); [thick,-\u00bf] (8, 1.55)- (8, 1.95); [thick,-\u00bf] (10, 1.55)- (10, 1.95); [thick,-\u00bf] (12, 1.55)- (12, 1.95); [thick,-\u00bf] (14, 1.55)- (14, 1.95); [thick,-\u00bf] (16, 1.55)- (16, 1.95); (3.2, 2) rectangle (16.8, 3); (lstm) at (10, 2.5) lstm; (s) at (4, 3.7) (s; (vp) at (6, 3.7) (vp; (vb) at (8, 3.7) xx; (cvp) at (10, 3.7)); (dot) at (12, 3.7).; (cs) at (14, 3.7)); (end) at (16, 3.7) end; [thick,-\u00bf] (4, 3.05)- (s); 0, 3.05); [thick,-\u00bf] (6, 3.05)- (vp); 0, 3.05); [thick,-\u00bf] (8, 3.05)- (vb); 0, 3.05); [thick,-\u00bf] (10, 3.05)- (cvp); 0, 3.05); [thick,-\u00bf] (12, 3.05)- (dot); 0, 3.05); [thick,-\u00bf] (14, 3.05)- (cs); 0, 3.05); [thick,-\u00bf] (16, 3.05)- (end); 0, 3.05); let us first recall the sequence-to-sequence lstm model. the long short-term memory model of is defined as follows. let,, and be the input, control state, and memory state at timestep. given a sequence of inputs, the lstm computes the-sequence and the-sequence as follows. the operator denotes element-wise multiplication, the matrices and the vector are the parameters of the model, and all the nonlinearities are computed element-wise. in a deep lstm, each subsequent layer uses the-sequence of the previous layer for its input sequence. the deep lstm defines a distribution over output sequences given an input sequence: the above equation assumes a deep lstm whose input sequence is, so denotes-th element of the-sequence of topmost lstm. the matrix consists of the vector representations of each output symbol and the symbol is a kronecker delta with a dimension for each output symbol, so is precisely the' th element of the distribution defined by the softmax. every output sequence terminates with a special end-of-sequence token which is necessary in order to define a distribution over sequences of variable lengths. we use two different sets of lstm parameters, one for the input sequence and one for the output sequence, as shown in figure [reference]. stochastic gradient descent is used to maximize the training objective which is the average over the training set of the log probability of the correct output sequence given the input sequence. subsection: attention mechanism an important extension of the sequence-to-sequence model is by adding an attention mechanism. we adapted the attention model from which, to produce each output symbol, uses an attention mechanism over the encoder lstm states. similar to our sequence-to-sequence model described in the previous section, we use two separate lstms (one to encode the sequence of input words, and another one to produce or decode the output symbols). recall that the encoder hidden states are denoted and we denote the hidden states of the decoder by. to compute the attention vector at each output time over the input words we define: the vector and matrices are learnable parameters of the model. the vector has length and its-th item contains a score of how much attention should be put on the-th hidden encoder state. these scores are normalized by softmax to create the attention mask over encoder hidden states. in all our experiments, we use the same hidden dimensionality (256) at the encoder and the decoder, so is a vector and and are square matrices. lastly, we concatenate with, which becomes the new hidden state from which we make predictions, and which is fed to the next time step in our recurrent model. in section [reference] we provide an analysis of what the attention mechanism learned, and we visualize the normalized attention vector for all in figure [reference]. subsection: linearizing parsing trees to apply the model described above to parsing, we need to design an invertible way of converting the parse tree into a sequence (linearization). we do this in a very simple way following a depth-first traversal order, as depicted in figure [reference]. [growth parent anchor= north, level distance=2em, xscale=1.5, yscale=1.3] (tokens) at (- 2, 0) john has a dog.; (arrow) at (- 0.7, 0); (s) at (2, 0.5) s child node np child node nnp child node vp child node vbz child node np child node dt child node nn child node.; (tokens) at (- 2,-1.7) john has a dog.; (arrow) at (- 0.7,-1.7); [anchor= west] (result) at (0,-1.7) (s (np nnp) (vp vbz (np dt nn)).); we use the above model for parsing in the following way. first, the network consumes the sentence in a left-to-right sweep, creating vectors in memory. then, it outputs the linearized parse tree using information in these vectors. as described below, we use 3 lstm layers, reverse the input sentence and normalize part-of-speech tags. an example run of our lstm+ a model on the sentence\" go.\" is depicted in figure [reference] (top gray edges illustrate attention). subsection: parameters and initialization paragraph: sizes. in our experiments we used a model with 3 lstm layers and 256 units in each layer, which we call lstm+ a. our input vocabulary size was 90 k and we output 128 symbols. paragraph: dropout. training on a small dataset we additionally used 2 dropout layers, one between lstm and lstm, and one between lstm and lstm. we call this model lstm+ a+ d. paragraph: pos-tag normalization. since part-of-speech (pos) tags are not evaluated in the syntactic parsing f1 score, we replaced all of them by\" xx\" in the training data. this improved our f1 score by about 1 point, which is surprising: for standard parsers, including pos tags in training data helps significantly. all experiments reported below are performed with normalized pos tags. paragraph: input reversing. we also found it useful to reverse the input sentences but not their parse trees, similarly to. not reversing the input had a small negative impact on the f1 score on our development set (about absolute). all experiments reported below are performed with input reversing. paragraph: pre-training word vectors. the embedding layer for our 90 k vocabulary can be initialized randomly or using pre-trained word-vector embeddings. we pre-trained skip-gram embeddings of size 512 using word2vec on a 10b-word corpus. these embeddings were used to initialize our network but not fixed, they were later modified during training. we discuss the impact of pre-training in the experimental section. we do not apply any other special preprocessing to the data. in particular, we do not binarize the parse trees or handle unaries in any specific way. we also treat unknown words in a naive way: we map all words beyond our 90 k vocabulary to a single unk token. this potentially underestimates our final results, but keeps our framework task-independent. section: experiments subsection: training data we trained the model described above on 2 different datasets. for one, we trained on the standard wsj training dataset. this is a very small training set by neural network standards, as it contains only 40 k sentences (compared to 60 k examples even in mnist). still, even training on this set, we managed to get results that match those obtained by domain-specific parsers. to exceed the previous state-of-the-art, we created another, larger training set of 11 m parsed sentences (250 m tokens). first, we collected all publicly available treebanks. we used the ontonotes corpus version 5, the english web treebank and the updated and corrected question treebank. note that the popular wall street journal section of the penn treebank is part of the ontonotes corpus. in total, these corpora give us 90 k training sentences (we held out certain sections for evaluation, as described below). in addition to this gold standard data, we use a corpus parsed with existing parsers using the\" tri-training\" approach of. in this approach, two parsers, our reimplementation of berkeleyparser and a reimplementation of zpar, are used to process unlabeled sentences sampled from news appearing on the web. we select only sentences for which both parsers produced the same parse tree and re-sample to match the distribution of sentence lengths of the wsj training corpus. re-sampling is useful because parsers agree much more often on short sentences. we call the set of 11 million sentences selected in this way, together with the 90 k golden sentences described above, the high-confidence corpus. in earlier experiments, we only used one parser, our reimplementation of berkeleyparser, to create a corpus of parsed sentences. in that case we just parsed 7 million senteces from news appearing on the web and combined these parsed sentences with the 90 k golden corpus described above. we call this the berkeleyparser corpus. subsection: evaluation we use the standard evalb tool for evaluation and report f1 scores on our developments set (section 22 of the penn treebank) and the final test set (section 23) in table [reference]. first, let us remark that our training setup differs from those reported in previous works. to the best of our knowledge, no standard parsers have ever been trained on datasets numbering in the hundreds of millions of tokens, and it would be hard to do due to efficiency problems. we therefore cite the semi-supervised results, which are analogous in spirit but use less data. table [reference] shows performance of our models on the top and results from other papers at the bottom. we compare to variants of the berkeleyparser that use self-training on unlabeled data, or built an ensemble of multiple parsers, or combine both techniques. we also include the best linear-time parser in the literature, the transition-based parser of. it can be seen that, when training on wsj only, a baseline lstm does not achieve any reasonable score, even with dropout and early stopping. but a single attention model gets to and an ensemble of 5 lstm+ a+ d models achieves matching a single-model berkeleyparser on wsj 23. when trained on the large high-confidence corpus, a single lstm+ a model achieves and so outperforms not only the best single model, but also the best ensemble result reported previously. an ensemble of 5 lstm+ a models further improves this score to. paragraph: generating well-formed trees. the lstm+ a model trained on wsj dataset only produced malformed trees for 25 of the 1700 sentences in our development set (% of all cases), and the model trained on full high-confidence dataset did this for 14 sentences (%). in these few cases where lstm+ a outputs a malformed tree, we simply add brackets to either the beginning or the end of the tree in order to make it balanced. it is worth noting that all 14 cases where lstm+ a produced unbalanced trees were sentences or sentence fragments that did not end with proper punctuation. there were very few such sentences in the training data, so it is not a surprise that our model can not deal with them very well. paragraph: score by sentence length. an important concern with the sequence-to-sequence lstm was that it may not be able to handle long sentences well. we determine the extent of this problem by partitioning the development set by length, and evaluating berkeleyparser, a baseline lstm model without attention, and lstm+ a on sentences of each length. the results, presented in figure [reference], are surprising. the difference between the f1 score on sentences of length upto 30 and that upto 70 is for the berkeleyparser, for the baseline lstm, and for lstm+ a. so already the baseline lstm has similar performance to the berkeleyparser, it degrades with length only slightly. surprisingly, lstm+ a shows less degradation with length than berkeleyparser- a full chart parser that uses a lot more memory. [xscale=0.18, yscale=0.5] [xstep=10 cm, ystep=1 cm, color= lightgray, thin] (10, 90) grid (71, 96); in 10, 20,\u2026, 70 (x) at (, 89.8); in 90,\u2026, 96 (x) at (8,); (bottom) at (40, 89.3) sentence length; [rotate=90] (left) at (5, 92.5) f1 score; [color= blue, fill= blue] (72, 93.85) rectangle (73, 94.15); [anchor= west] (l) at (73, 94) berkeleyparser; [color= red, fill= red] (72, 93.35) rectangle (73, 93.65); [anchor= west] (l) at (73, 93.5) baseline lstm; [color= orange, fill= orange] (72, 92.85) rectangle (73, 93.15); [anchor= west] (l) at (73, 93) lstm+ a; [thick, color= blue] plot coordinates (10, 92.7) (11, 92.94) (12, 92.95) (13, 93.49) (14, 93.69) (15, 93.73) (16, 93.75) (17, 93.83) (18, 93.69) (19, 93.57) (20, 93.33) (21, 93.17) (22, 93.12) (23, 93.34) (24, 93.2) (25, 93.05) (26, 92.78) (27, 92.74) (28, 92.64) (29, 92.54) (30, 92.37) (31, 92.34) (32, 92.3) (33, 92.29) (34, 92.15) (35, 92.04) (36, 91.99) (37, 91.97) (38, 91.91) (39, 91.89) (40, 91.8) (41, 91.72) (42, 91.69) (43, 91.67) (44, 91.55) (45, 91.55) (46, 91.47) (47, 91.48) (48, 91.45) (49, 91.47) (50, 91.43) (51, 91.37) (52, 91.34) (53, 91.3) (54, 91.3) (55, 91.3) (56, 91.26) (57, 91.14) (58, 91.15) (59, 91.15) (60, 91.15) (61, 91.14) (62, 91.14) (63, 91.14) (64, 91.09) (65, 91.1) (66, 91.1) (67, 91.1) (68, 91.1) (69, 91.1) (70, 91.1) (71, 91.1); [thick, color= red] plot coordinates (10, 92.36) (11, 93.05) (12, 93.33) (13, 93.47) (14, 93.88) (15, 94.12) (16, 93.77) (17, 93.83) (18, 93.76) (19, 93.84) (20, 93.67) (21, 93.53) (22, 93.63) (23, 93.76) (24, 93.38) (25, 93.3) (26, 93.21) (27, 92.98) (28, 92.81) (29, 92.72) (30, 92.69) (31, 92.54) (32, 92.57) (33, 92.61) (34, 92.57) (35, 92.44) (36, 92.31) (37, 92.33) (38, 92.3) (39, 92.18) (40, 92.09) (41, 92.04) (42, 92.01) (43, 91.99) (44, 91.92) (45, 91.9) (46, 91.75) (47, 91.74) (48, 91.71) (49, 91.7) (50, 91.67) (51, 91.57) (52, 91.53) (53, 91.52) (54, 91.52) (55, 91.52) (56, 91.46) (57, 91.23) (58, 91.14) (59, 91.11) (60, 91.11) (61, 91.06) (62, 91.06) (63, 90.99) (64, 90.99) (65, 90.99) (66, 90.99) (67, 90.99) (68, 90.99) (69, 90.99) (70, 90.99) (71, 90.99); [thick, color= orange] plot coordinates (10, 95.41) (11, 95.07) (12, 95.57) (13, 95.16) (14, 95.39) (15, 95.24) (16, 95.44) (17, 95.5) (18, 95.5) (19, 95.39) (20, 95.42) (21, 95.1) (22, 94.85) (23, 94.76) (24, 94.83) (25, 94.58) (26, 94.54) (27, 94.37) (28, 94.18) (29, 94.06) (30, 94.03) (31, 94.03) (32, 93.91) (33, 93.9) (34, 93.99) (35, 93.94) (36, 93.85) (37, 93.86) (38, 93.9) (39, 93.87) (40, 93.87) (41, 93.84) (42, 93.77) (43, 93.77) (44, 93.77) (45, 93.75) (46, 93.72) (47, 93.62) (48, 93.61) (49, 93.59) (50, 93.6) (51, 93.6) (52, 93.54) (53, 93.49) (54, 93.49) (55, 93.49) (56, 93.5) (57, 93.43) (58, 93.42) (59, 93.42) (60, 93.43) (61, 93.43) (62, 93.42) (63, 93.42) (64, 93.42) (65, 93.36) (66, 93.37) (67, 93.37) (68, 93.37) (69, 93.37) (70, 93.37) (71, 93.37); paragraph: beam size influence. our decoder uses a beam of a fixed size to calculate the output sequence of labels. we experimented with different settings for the beam size. it turns out that it is almost irrelevant. we report report results that use beam size 10, but using beam size 2 only lowers the f1 score of lstm+ a on the development set by, and using beam size 1 lowers it by (to). beam sizes above 10 do not give any additional improvements. paragraph: dropout influence. we only used dropout when training on the small wsj dataset and its influence was significant. a single lstm+ a model only achieved an f1 score of on our development set, that is over points lower than the of a lstm+ a+ d model. paragraph: pre-training influence. as described in the previous section, we initialized the word-vector embedding with pre-trained word vectors obtained from word2vec. to test the influence of this initialization, we trained a lstm+ a model on the high-confidence corpus, and a lstm+ a+ d model on the wsj corpus, starting with randomly initialized word-vector embeddings. the f1 score on our development set was lower for the lstm+ a model (vs) and lower for the lstm+ a+ d model (vs). so the effect of pre-training is consistent but small. paragraph: performance on other datasets. the wsj evaluation set has been in use for 20 years and is commonly used to compare syntactic parsers. but it is not representative for text encountered on the web. even though our model was trained on a news corpus, we wanted to check how well it generalizes to other forms of text. to this end, we evaluated it on two additional datasets: 1000 held-out sentences from the question treebank; the first half of each domain from the english web treebank (8310 sentences). lstm+ a trained on the high-confidence corpus (which only includes text from news) achieved an f1 score of on qtb and on web. our score on web is higher both than the best score reported in () and the best score we achieved with an in-house reimplementation of berkeleyparser trained on human-annotated data (). we managed to achieve a slightly higher score () with the in-house berkeleyparser trained on a large corpus. on qtb, the score of lstm+ a is also lower than the best score of our in-house berkeleyparser (). still, taking into account that there were only few questions in the training data, these scores show that lstm+ a managed to generalize well beyond the news language it was trained on. paragraph: parsing speed. our lstm+ a model, running on a multi-core cpu using batches of 128 sentences on a generic unoptimized decoder, can parse over 120 sentences from wsj per second for sentences of all lengths (using beam-size 1). this is better than the speed reported for this batch size in figure 4 of at 100 sentences per second, even though they run on a gpu and only on sentences of under 40 words. note that they achieve f1 score on this subset of sentences of section 22, while our model at beam-size 1 achieves a score of on this subset. section: analysis as shown in this paper, the attention mechanism was a key component especially when learning from a relatively small dataset. we found that the model did not overfit and learned the parsing function from scratch much faster, which resulted in a model which generalized much better than the plain lstm without attention. one of the most interesting aspects of attention is that it allows us to visualize to interpret what the model has learned from the data. for example, in it is shown that for translation, attention learns an alignment function, which certainly should help translating from english to french. figure [reference] shows an example of the attention model trained only on the wsj dataset. from the attention matrix, where each column is the attention vector over the inputs, it is clear that the model focuses quite sharply on one word as it produces the parse tree. it is also clear that the focus moves from the first word to the last monotonically, and steps to the right deterministically when a word is consumed. on the bottom of figure [reference] we see where the model attends (black arrow), and the current output being decoded in the tree (black circle). this stack procedure is learned from data (as all the parameters are randomly initialized), but is not quite a simple stack decoding. indeed, at the input side, if the model focuses on position, that state has information for all words after (since we also reverse the inputs). it is worth noting that, in some examples (not shown here), the model does skip words. [width=1.1] attention_figure section: related work the task of syntactic constituency parsing has received a tremendous amount of attention in the last 20 years. traditional approaches to constituency parsing rely on probabilistic context-free grammars (cfgs). the focus in these approaches is on devising appropriate smoothing techniques for highly lexicalized and thus rare events or carefully crafting the model structure. partially alleviate the heavy reliance on manual modeling of linguistic structure by using latent variables to learn a more articulated model. however, their model still depends on a cfg backbone and is thereby potentially restricted in its capacity. early neural network approaches to parsing, for example by also relied on strong linguistic insights. introduced incremental sigmoid belief networks for syntactic parsing. by constructing the model structure incrementally, they are able to avoid making strong independence assumptions but inference becomes intractable. to avoid complex inference methods, propose a recurrent neural network where parse trees are decomposed into a stack of independent levels. unfortunately, this decomposition breaks for long sentences and their accuracy on longer sentences falls quite significantly behind the state-of-the-art. used a tree-structured neural network to score candidate parse trees. their model however relies again on the cfg assumption and furthermore can only be used to score candidate trees rather than for full inference. our lstm model significantly differs from all these models, as it makes no assumptions about the task. as a sequence-to-sequence prediction model it is somewhat related to the incremental parsing models, pioneered by and extended by. such linear time parsers however typically need some task-specific constraints and might build up the parse in multiple passes. relatedly, present excellent parsing results with a single left-to-right pass, but require a stack to explicitly delay making decisions and a parsing-specific transition strategy in order to achieve good parsing accuracies. the lstm in contrast uses its short term memory to model the complex underlying structure that connects the input-output pairs. recently, researchers have developed a number of neural network models that can be applied to general sequence-to-sequence problems. was the first to propose a differentiable attention mechanism for the general problem of handwritten text synthesis, although his approach assumed a monotonic alignment between the input and output sequences. later, introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and applied the same model to speech recognition. used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used a rnn to produce the output sentence. essentially the same model has been used by to successfully learn to generate image captions. finally, already in 1990 experimented with applying recurrent neural networks to the problem of syntactic parsing. section: conclusions in this work, we have shown that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing with relatively little effort or tuning. in addition, while we found the model of sutskever et al. to not be particularly data efficient, the attention model of bahdanau et al. was found to be highly data efficient, as it has matched the performance of the berkeleyparser when trained on a small human-annotated parsing dataset. finally, we showed that synthetic datasets with imperfect labels can be highly useful, as our models have substantially outperformed the models that have been used to create their training data. we suspect it is the case due to the different natures of the teacher model and the student model: the student model has likely viewed the teacher's errors as noise which it has been able to ignore. this approach was so successful that we obtained a new state-of-the-art result in syntactic constituency parsing with a single attention model, which also means that the model is exceedingly fast. this work shows that domain independent models with excellent learning algorithms can match and even outperform domain specific models. acknowledgement. we would like to thank amin ahmad, dan bikel and jonni kanerva. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "penn treebank",
                        12154
                    ],
                    [
                        "question treebank",
                        20891
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "f1 score",
                        2567
                    ],
                    [
                        "f1 scores",
                        13429
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "syntactic constituency parsing",
                        40
                    ],
                    [
                        "parsing",
                        1482
                    ],
                    [
                        "constituency parsing",
                        23997
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "28bf3aee9eecc2f7a7b4ac71bfe89534d3fe5f19-43",
    "doctext": "document: occlusion-aware r-cnn: detecting pedestrians in a crowd pedestrian detection in crowded scenes is a challenging problem since the pedestrians often gather together and occlude each other. in this paper, we propose a new occlusion-aware r-cnn (or-cnn) to improve the detection accuracy in the crowd. specifically, we design a new aggregation loss to enforce proposals to be close and locate compactly to the corresponding objects. meanwhile, we use a new part occlusion-aware region of interest (poroi) pooling unit to replace the roi pooling layer in order to integrate the prior structure information of human body with visibility prediction into the network to handle occlusion. our detector is trained in an end-to-end fashion, which achieves state-of-the-art results on three pedestrian detection datasets, i.e., citypersons, eth, and inria, and performs on-pair with the state-of-the-arts on caltech. section: introduction pedestrian detection is an important research topic in computer vision field with various applications, such as autonomous driving, video surveillance, and robotics, which aims to predict a series of bounding boxes enclosing pedestrian instances in an image. recent advances in object detection are driven by the success of deep convolutional neural networks (cnns), which uses the bounding box regression techniques to accurately localize the objects based on the deep features. actually, in real life complex scenarios, occlusion is one of the most significant challenges in detecting pedestrian, especially in the crowded scenes. for example, as pointed out in, annotated pedestrians are occluded by other pedestrians in the citypersons dataset. previous methods only require each predicted bounding box to be close to its designated ground truth, without considering the relations among them. thus, they make the detectors sensitive to the threshold of non-maximum suppression (nms) in the crowded scenes, wherein filling with occlusions. to that end, wang et al. [] design a repulsion loss, which not only pushes each proposal to approach its designated target, but also to keep it away from the other ground truth objects and their corresponding designated proposals. however, it is difficult to control the balance between the repulsion and attraction terms in the loss function to handle the overlapping pedestrians. in this paper, we propose a new occlusion-aware r-cnn (or-cnn) based on the faster r-cnn detection framework to mitigate the impact of occlusion challenge. specifically, to reduce the false detections of the adjacent overlapping pedestrians, we expect the proposals to be close and locate compactly to the corresponding objects. thus, inspired by the herd behavior in psychology, we design a new loss function, called aggregation loss (aggloss), not only to enforce proposals to be close to the corresponding objects, but also to minimize the internal region distances of proposals associated with the same objects. meanwhile, to effectively handle partial occlusion, we propose a new part occlusion-aware region of interest (poroi) pooling unit to replace the original roi pooling layer in the second stage fast r-cnn module of the detector, which integrates the prior structure information of human body with visibility prediction into the network. that is, we first partition the pedestrian region into five parts, and pool the features under each part's projection as well as the whole proposal's projection onto the feature map into fixed-length feature vectors by adaptively-sized pooling bins. after that, we use the learned sub-network to predict the visibility score of each part to combine the extracted features for pedestrian detection. several experiments are carried out on four pedestrian detection datasets, i.e., citypersons, caltech, eth and inria, to demonstrate the superiority of the proposed method, especially for the crowded scenes. notably, the proposed or-cnn method achieves the state-of-the-art results with mr on the citypersons dataset, mr on the eth dataset, and mr on the inria dataset. the main contributions of this work are summarized as follows. we propose a new occlusion-aware r-cnn method, which uses a new designed aggloss to enforce proposals to be close to the corresponding objects, as well as minimize the internal region distances of proposals associated with the same objects. we design a new poroi pooling unit to replace the roi pooling layer in the second fast r-cnn module to integrate the prior structure information of human body with visibility prediction into the network. several experiments are carried out on four challenging pedestrian detection datasets, i.e., citypersons, caltech, eth, and inria, to demonstrate the superiority of the proposed method. section: related work generic object detection. early generic object detectors rely on the sliding window paradigm based on the hand-crafted features and classifiers to find the objects of interest. in recent years, with the advent of deep convolutional neural network (cnn), a new generation of more effective object detection methods based on cnn significantly improve the state-of-the-art performances, which can be roughly divided into two categories, i.e., the one-stage approach and the two-stage approach. the one-stage approach directly predicts object class label and regresses object bounding box based on the pre-tiled anchor boxes using deep cnns. the main advantage of the one-stage approach is its high computational efficiency. in contrast to the one-stage approach, the two-stage approach always achieves top accuracy on several benchmarks, which first generates a pool of object proposals by a separated proposal generator (e.g., selective search, edgeboxes, and rpn), and then predicts the class label and accurate location and size of each proposal. pedestrian detection. even as one of the long-standing problems in computer vision field with an extensive literature, pedestrian detection still receives considerable interests with a wide range of applications. a common paradigm to address this problem is to train a pedestrian detector that exhaustively operates on the sub-images across all locations and scales. dalal and triggs design the histograms of oriented gradient (hog) descriptors and support vector machine (svm) classifier for human detection. doll\u00e1r et al. [] demonstrate that using features from multiple channels can significantly improve the performance. zhang et al. [] provide a systematic analysis for the filtered channel features, and find that with the proper filter bank, filtered channel features can reach top detection quality. paisitkriangkrai et al. [] design a new features built on the basis of low-level visual features and spatial pooling, and directly optimize the partial area under the roc curve for better performance. recently, pedestrian detection is dominated by the cnn-based methods (e.g.,). sermanet et al. [] present an unsupervised method using the convolutional sparse coding to pre-train cnn for pedestrian detection. in, a complexity-aware cascaded detector is proposed for an optimal trade-off between accuracy and speed. angelova et al. [] combine the ideas of fast cascade and a deep network to detect pedestrian. yang et al. [] use scale-dependent pooling and layer-wise cascaded rejection classifiers to detect objects efficiently. zhang et al. [] present an effective pipeline for pedestrian detection via using rpn followed by boosted forests. to jointly learn pedestrian detection with the given extra features, a novel network architecture is presented in. li et al. [] use multiple built-in sub-networks to adaptively detect pedestrians across scales. brazil et al. [] exploit weakly annotated boxes via a segmentation infusion network to achieve considerable performance gains. however, occlusion still remains one of the most significant challenges in pedestrian detection, which increases the difficulty in pedestrian localization. several methods use part-based model to describe the pedestrian in occlusion handling, which learn a series of part detectors and design some mechanisms to fuse the part detection results to localize partially occluded pedestrians. besides the part-based model, leibe et al. [] propose an implicit shape model to generate a set of pedestrian hypotheses that are further refined to obtain the visible regions. wang et al. [] divide the template of pedestrian into a set of blocks and conduct occlusion reasoning by estimating the visibility status of each block. ouyang et al. [] exploit multi-pedestrian detectors to aid single-pedestrian detectors to handle partial occlusions, especially when the pedestrians gather together and occlude each other in real-world scenarios. in, a set of occlusion patterns of pedestrians are discovered to learn a mixture of occlusion-specific detectors. zhou et al. [] propose to jointly learn part detectors so as to exploit part correlations and reduce the computational cost. wang et al. [] introduce a novel bounding box regression loss to detect pedestrians in the crowd scenes. although numerous pedestrian detection methods are presented in literature, how to robustly detect each individual pedestrian in crowded scenarios is still one of the most critical issues for pedestrian detectors. section: occlusion-aware r-cnn our occlusion-aware r-cnn detector follows the adaptive faster r-cnn detection framework for pedestrian detection, with the new designed aggregation loss (section [reference]), and the poroi pooling unit (section [reference]). specifically, faster r-cnn consists of two modules, i.e., the first region proposal network (rpn) module and the second fast r-cnn module. the rpn module is designed to generate high-quality region proposals, and the fast r-cnn module is used to classify and regress the accurate locations and sizes of objects, based on the generated proposals. to effectively generate accurate region proposals in the first rpn module, we design the aggloss term to enforce the proposals locate closely and compactly to the ground-truth object, which is defined as where is the index of anchor in a mini-batch, and are the predicted confidence of the-th anchor being a pedestrian and the predicted coordinates of the pedestrian, and are the associated ground truth class label and coordinates of the-th anchor, is the hyperparameters used to balance the two loss terms, is the classification loss, and is the aggloss (see section [reference]). we use the log loss to calculate the classification loss over two classes (pedestrian vs. background), i.e., where is the total number of anchors in classification. subsection: aggregation loss to reduce the false detections of the adjacent overlapping pedestrians, we enforce proposals to be close and locate compactly to the corresponding ground truth objects. to that end, we design a new aggregation loss (aggloss) for both the region proposal network (rpn) and fast r-cnn modules in the faster r-cnn algorithm, which is a multi-task loss pushing proposals to be close to the corresponding ground truth object, while minimizing the internal region distances of proposals associated with the same objects, i.e., where is the regression loss which requires each proposal to approach the designated ground truth, and is the compactness loss which enforces proposals locate compactly to the designated ground truth object, and is the hyper-parameters used to balance the two loss terms. similar to fast r-cnn, we use the smooth l1 loss as the regression loss to measure the accuracy of predicted bounding boxes, i.e., where is the total number of anchors in regression, and is the smooth l1 loss of the predicted bounding box. the compactness term is designed to consider the attractiveness among proposals associated with the same ground truth object. in this way, we can make the proposals to locate compactly around the ground truth to reduce the false detections of adjacent overlapping objects. specifically, we set to be the ground truth set associated with more than one anchor, and to be the index sets of the associated anchors corresponding to the ground truth objects, i.e., the anchors indexed by are associated to the ground truth, where is the total number of ground-truth object associated with more than one anchor. thus, we have, for, and. we use the smooth l1 loss to measure the difference between the average predictions of the anchors indexed by each set in and the corresponding ground truth object, describing the compactness of predicted bounding boxes with respect to the ground truth object, i.e., where is the total number of ground truth object associated with more than one anchor (i.e.,), and is the number of anchors associated with the-th ground truth object. subsection: part occlusion-aware roi pooling unit in real life complex scenarios, occlusion is ubiquitous challenging the accuracy of detectors, especially in crowded scenes. as indicated in, the part-based model is effective in handling occluded pedestrians. in contrast to the aforementioned methods, we design a new part occlusion-aware roi pooling unit to integrate the prior structure information of human body with visibility prediction into the fast r-cnn module of the detector, which assembles a micro neural network to estimate the part occlusion status. as shown in figure [reference] (a), we first divide the pedestrian region into five parts with the empirical ratio in. for each part, we use the roi pooling layer to pool the features into a small feature map with a fixed spatial extent of (e.g.,). we introduce an occlusion process unit, shown in figure [reference] (b), to predict the visibility score of the corresponding part based on the pooled features. specifically, the occlusion process unit is constructed by three convolutional layers followed by a softmax layer with the log loss in training. symbolically, indicates the-th part of the-th proposal, represents its predicted visibility score, and is the corresponding ground truth visibility score. if half of the part is visible,, otherwise. mathematically, if the intersection between and the visible region of ground truth object divided by the area of is larger than the threshold,, otherwise. that is where is the area computing function, is the region of, is the visible region of the ground truth object, and is the intersection operation between two regions. then, the loss function of the occlusion process unit is calculated as. after that, we apply the element-wise multiplication operator to multiply the pooled features of each part and the corresponding predicted visibility score to generate the final features with the dimensions. the element-wise summation operation is further used to combine the extracted features of the five parts and the whole proposal for classification and regression in the fast r-cnn module (see figure [reference]). to further improve the regression accuracy, we also use aggloss in the fast r-cnn module, which is defined as: where and are used to balance the three loss terms, and are the classification and aggregation losses, defined the same as that in the rpn module, and is the occlusion process loss. section: experiments several experiments are conducted on four datasets: citypersons, caltech-usa, eth, and inria, to demonstrate the performance of the proposed or-cnn method. subsection: experimental setup our or-cnn detector follows the adaptive faster r-cnn framework and uses vgg-16 as the backbone network, pre-trained on the ilsvrc cls-loc dataset. to improve the detection accuracy of pedestrians with small scale, we use the method presented in to dense the anchor boxes with the height less than pixels two times, and use the matching strategy in to associate the anchors and the ground truth objects. all the parameters in the newly added convolutional layers are randomly initialized by the\" xavier\" method. we optimize the or-cnn detector using the stochastic gradient descent (sgd) algorithm with momentum and weight decay, which is trained on titan x gpus with the mini-batch involving image per gpu. for the citypersons dataset, we set the learning rate to for the first iterations, and decay it to for another iterations. for the caltech-usa dataset, we train the network for iterations with the initial learning rate and decrease it by a factor of after the first iterations. all the hyperparameters, and are empirically set to. subsection: citypersons dataset the citypersons dataset is built upon the semantic segmentation dataset cityscapes to provide a new dataset of interest for pedestrian detection. it is recorded across different cities in germany with different seasons and various weather conditions. the dataset includes images (for training, for validation, and for testing) with manually annotated persons plus ignore region annotations. both the bounding boxes and visible parts of pedestrians are provided and there are approximately pedestrians in average per image. following the evaluation protocol in citypersons, we train our or-cnn detector on the training set, and evaluate it on both the validation and the testing sets. the log miss rate averaged over the false positive per image (fppi) range of () is used to measure the detection performance (lower score indicates better performance). we use the adaptive faster r-cnn method trained by ourselves as the baseline detector, which achieves on the validation set with scale, sightly better than the reported result () in. subsubsection: ablation study on aggloss to demonstrate the effectiveness of aggloss, we construct a detector, denoted as or-cnn-a, that use aggloss instead of the original regression loss in the baseline detector, and evaluate it on the validation set of citypersons in table [reference]. for a fair comparison, we use the same setting of parameters of or-cnn-a and our or-cnn detector in both training and testing. all of the experiments are conducted on the reasonable train/ validation sets for training and testing. comparing the detection results between the baseline and or-cnn-a in table [reference], we find that using the newly proposed aggloss can reduce the by (i.e., vs.) with scale. it is worth noting that the or-cnn-a detector achieves with scale, surpassing the state-of-the-art method using repulsion loss (), which demonstrates that aggloss is more effective than repulsion loss for detecting the pedestrians in a crowd. in addition, we also show some visual comparison results of the predicted bounding boxes before nms of the baseline and or-cnn-a detectors in figure [reference] (a). as shown in figure [reference] (a), the predictions of or-cnn-a locate more compactly than that of the baseline detector, and there are fewer predictions of or-cnn-a lying in between two adjacent ground-truth objects than the baseline detector. this phenomenon demonstrates that aggloss can push the predictions lying compactly to the ground-truth objects, making the detector less sensitive to the nms threshold with better performance in the crowd scene. to further validate this point, we also present the results with aggloss across various nms threshold at in figure [reference] (b). a high nms threshold may lead to more false positives, while a low nms threshold may lead to more false negatives. as shown in figure [reference] (b), we find that the curve of or-cnn-a is smoother than that of baseline (i.e., the variances of the miss rates are vs.), which indicates that the former is less sensitive to the nms threshold. it is worth noting that across various nms thresholds at, the or-cnn-a method always produces lower miss rate, which is due to the nms operation filtering out more false positives in the predictions of or-cnn-a than that of baseline, implying that the predicted bounding boxes of or-cnn-a locate compactly than baseline. subsubsection: ablation study on poroi pooling to validate the effectiveness of the poroi pooling unit, we construct a detector, denoted as or-cnn-p, that use the poroi pooling unit instead of the roi pooling layer in baseline, and evaluate it on the validation set of citypersons in table [reference]. for a fair comparison, we use the same parameter settings of or-cnn-p and our or-cnn detector in both training and testing. all of the ablation experiments involved citypersons are conducted on the reasonable train/ validation sets for training and testing. as shown in table [reference], comparing to baseline, or-cnn-p reduces with scale (i.e., vs.), which demonstrates the effectiveness of the poroi pooling unit in pedestrian detection. meanwhile, we also present some qualitative results of the predictions with the visibility scores of the corresponding parts in figure [reference]. notably, we find that the visibility scores predicted by the poroi pooling unit are in accordance with the human visual system. as shown in figure [reference] (a) and (b), if the pedestrian is not occluded, the visibility score of each part of the pedestrian approaches. however, if some parts of the pedestrians are occluded by the background obstacles or other pedestrians, the scores of the corresponding parts decrease, such as the occluded thigh and calf in figure [reference] (c)-(f). besides, if two pedestrians gather together and occlude each other, our poroi pooling unit successfully detects the occluded human parts that can help lower the contributions of the occluded parts in pedestrian detection, see figure [reference] (g) and (h). notably, the detection accuracy of the or-cnn detector can not be improved if we fix the visibility score of each part to instead of using the predictions of the occlusion process unit (see figure [reference]). thus, the occlusion process unit is the key component to detection accuracy, since it enables our poroi pooling unit to detect the occluded parts of pedestrians, which is useful to help extract effective features for detection. subsubsection: evaluation results we compare the proposed or-cnn method with the state-of-the-art detectors on both the validation and testing sets of citypersons in table [reference] and table [reference], respectively. our or-cnn achieves the state-of-the-art results on the validation set of citypersons by reducing (i.e., vs. of) with scale and (i.e., vs. of) with scale, surpassing all published approaches, which demonstrates the superiority of the proposed method in pedestrian detection. to demonstrate the effectiveness of or-cnn under various occlusion levels, we follow the strategy in to divide the reasonable subset in the validation set (occlusion) into the reasonable-partial subset (occlusion), denoted as partial subset, and the reasonable-bare subset (occlusion), denoted as bare subset. meanwhile, we denote the annotated pedestrians with the occlusion ratio larger than (that are not included in the reasonable set) as heavy subset. we report the results of the proposed or-cnn method and other state-of-the-art methods on these three subsets in table [reference]. as shown in table [reference], or-cnn outperforms the state-of-the-art methods consistently across all three subsets, i.e., reduces on the bare subset, on the partial subset, and on the heavy subset. notably, when the occlusion becomes severely (i.e., from bare subset to heavy subset), the performance improvement of our or-cnn is more obvious compared to the state-of-the-art methods, which demonstrates that the aggloss and poroi pooling unit are extremely effective to address the occlusion challenge. in addition, we also evaluate the proposed or-cnn method on the testing set of citypersons. following its evaluation protocol, we submit the detection results of or-cnn to the authors for evaluation and report the results in table [reference]. the proposed or-cnn method achieves the top accuracy with only scale. although the second best detector repulsion loss uses much bigger input images (i.e., scale of vs. scale of or-cnn) and stronger backbone network (i.e., resnet-50 of vs. vgg-16 of or-cnn), it still produces higher on the reasonable subset and higher on the reasonable-small subset. we believe the performance of or-cnn can be further improved by using bigger input images and stronger backbone network. subsection: caltech-usa dataset the caltech-usa dataset is one of the most popular and challenging datasets for pedestrian detection, which comes from approximately hours hz vga video recorded by a car traversing the streets in the greater los angeles metropolitan area. we use the new high quality annotations provided by to evaluate the proposed or-cnn method. the training and testing sets contains and frames, respectively. following, the log-average miss rate over points ranging from to fppi is used to evaluate the performance of the detectors. we directly fine-tune the detection models pre-trained on citypersons of the proposed or-cnn method on the training set in caltech-usa. similar to, we evaluate the or-cnn method on the reasonable subset of the caltech-usa dataset, and compare it to other state-of-the-art methods (e.g.,) in figure [reference]. notably, the reasonable subset (occlusion) only includes the pedestrians with at least pixels tall, which is widely used to evaluate the pedestrian detectors. as shown in figure [reference], the or-cnn method performs competitively with the state-of-the-art method by producing. subsection: eth dataset to verify the generalization capacity of the proposed or-cnn detector, we directly use the model trained on the citypersons dataset to detect the pedestrians in the eth dataset without fine-tuning. that is, all frames in three video clips of the eth dataset are used to evaluate the performance of the or-cnn detector. we use to evaluate the performance of the detectors, and compare the proposed or-cnn method with other state-of-the-art methods (i.e.,) in figure [reference]. our or-cnn detector achieves the top accuracy by reducing comparing to the state-of-the-art results (i.e., of or-cnn vs. rfn-bf). the results on the eth dataset not only demonstrates the superiority of the proposed or-cnn method in pedestrian detection, but also verifies its generalization capacity to other scenarios. subsection: inria dataset the inria dataset contains images of high resolution pedestrians collected mostly from holiday photos, which consists of images, including images for training and images. specifically, there are positive images and negative images in the training set. we use the positive images in the training set to fine-tune our model pre-trained on citypersons for iterations, and test it on the testing images. figure [reference] shows that our or-cnn method achieves an of, better than the other available competitors (i.e.,), which demonstrates the effectiveness of the proposed method in pedestrian detection. section: conclusions in this paper, we present a new occlusion-aware r-cnn method to improve the pedestrian detection accuracy in crowded scenes. specifically, we design a new aggregation loss to reduce the false detections of the adjacent overlapping pedestrians, by simultaneously enforcing the proposals to be close to the associated objects, and locate compactly. meanwhile, to effectively handle partial occlusion, we propose a new part occlusion-aware roi pooling unit to replace the roi pooling layer in the fast r-cnn module of the detector, which integrates the prior structure information of human body with visibility prediction into the network to handle occlusion. our method is trained in an end-to-end fashion and achieves the state-of-the-art accuracy on three pedestrian detection datasets, i.e., citypersons, eth, and inria, and performs on-pair with the state-of-the-arts on caltech. in the future, we plan to improve the method in two aspects. first, we would like to redesign the poroi pooling unit to jointly estimate the location, size, and occlusion status of the object parts in the network, instead of using the empirical ratio. and then, we plan to extend the proposed method to detect other kinds of objects, e.g., car, bicycle, tricycle, etc. section: acknowledgments this work was supported by the national key research and development plan (grant no.2016yfc0801002), the chinese national natural science foundation projects,,,, the science and technology development fund of macau (no. 0025/ 2018/ a1, 151/ 2017/ a, 152/ 2017/ a), jdgrapevine plan and authenmetric r& d funds. we also thank nvidia for gpu donations through their academic program. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "caltech",
                        907
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "miss rates",
                        19490
                    ],
                    [
                        "miss rate",
                        19683
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "detecting pedestrians",
                        33
                    ],
                    [
                        "pedestrian detection",
                        66
                    ],
                    [
                        "detecting pedestrian",
                        1515
                    ],
                    [
                        "pedestrian",
                        7251
                    ],
                    [
                        "pedestrians",
                        9077
                    ],
                    [
                        "pedestrian detectors",
                        9302
                    ],
                    [
                        "detection",
                        17298
                    ],
                    [
                        "pedestrian detection accuracy",
                        26981
                    ],
                    [
                        "pedestrian detection datasets",
                        27661
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "citypersons",
                        827
                    ],
                    [
                        "citypersons dataset",
                        1666
                    ],
                    [
                        "semantic segmentation dataset cityscapes",
                        16553
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "occlusion-aware r-cnn",
                        10
                    ],
                    [
                        "or-cnn",
                        254
                    ],
                    [
                        "or-cnn method",
                        3942
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "detecting pedestrians",
                        33
                    ],
                    [
                        "pedestrian detection",
                        66
                    ],
                    [
                        "detecting pedestrian",
                        1515
                    ],
                    [
                        "pedestrian",
                        7251
                    ],
                    [
                        "pedestrians",
                        9077
                    ],
                    [
                        "pedestrian detectors",
                        9302
                    ],
                    [
                        "detection",
                        17298
                    ],
                    [
                        "pedestrian detection accuracy",
                        26981
                    ],
                    [
                        "pedestrian detection datasets",
                        27661
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "2af864bc38fd1116b4af32f16b44f5b3b156f5a3-44",
    "doctext": "finding remo (related memory object): a simple neural architecture for text based reasoning section: abstract to solve the text-based question and answering task that requires relational reasoning, it is necessary to memorize a large amount of information and find out the question relevant information from the memory. most approaches were based on external memory and four components proposed by memory network. the distinctive component among them was the way of finding the necessary information and it contributes to the performance. recently, a simple but powerful neural network module for reasoning called relation network (rn) has been introduced. we analyzed rn from the view of memory network, and realized that its mlp component is able to reveal the complicate relation between question and object pair. motivated from it, we introduce relation memory network (rmn) which uses mlp to find out relevant information on memory network architecture. it shows new state-of-the-art results in jointly trained babi-10k story-based question answering tasks and babi dialog-based question answering tasks. section: introduction neural network has made an enormous progress on the two major challenges in artificial intelligence: seeing and reading. in both areas, embedding methods have served as the main vehicle to process and analyze text and image data for solving classification problems. as for the task of logical reasoning, however, more complex and careful handling of features is called for. a reasoning task requires the machine to answer a simple question upon the delivery of a series of sequential information. for example, imagine that the machine is given the following three sentences:\" mary got the milk there.\",\" john moved to the bedroom.\", and\" mary traveled to the hallway.\" once prompted with the question,\" where is the milk?\", the machine then needs to sequentially focus on the two supporting sentences,\" mary got the milk there.\" and\" mary traveled to the hallway.\" in order to successfully determine that the milk is located in the hallway. inspired by this reasoning mechanism, j. has introduced the memory network (memnn), which consists of an external memory and four components: input feature map (i), generalization (g), output feature map (o), and response (r). the external memory enables the model to deal with a knowledge base without loss of information. input feature map embeds the incoming sentences. generalization updates old memories given the new input and output feature map finds relevant information from the memory. finally, response produces the final output. based on the memory network architecture, neural network based models like end-to-end memory network (memn2n) [reference], gated end-to-end memory network (gmemn2n) [reference], dynamic memory network (dmn) [reference], and dynamic memory network+ (dmn+) [reference] are proposed. since strong reasoning ability depends on whether the model is able to sequentially catching the right supporting sentences that lead to the answer, the most important thing that discriminates those models is the way of constructing the output feature map. as the output feature map becomes more complex, it is able to learn patterns for more complicate relations. for example, memn2n, which has the lowest performance among the four models, measures the relatedness between question and sentence by the inner product, while the best performing dmn+ uses inner product and absolute difference with two embedding matrices. recently, a new architecture called relation network (rn) [reference] has been proposed as a general solution to relational reasoning. the design philosophy behind it is to directly capture the supporting relation between the sentences through the multi-layer perceptron (mlp). despite its simplicity, rn achieves better performance than previous models without any catastrophic failure. the interesting thing we found is that rn can also be interpreted in terms of memnn. it is composed of o and r where each corresponds to mlp which focuses on the related pair and another mlp which infers the answer. rn does not need to have g because it directly finds all the supporting sentences at once. in this point of view, the significant component would be mlp-based output feature map. as mlp is enough to recognize highly non-linear pattern, rn could find the proper relation better than previous models to answer the given question. however, as rn considers a pair at a time unlike memnn, the number of relations that rn learns is n 2 when the number of input sentence is n. when n is small, the cost of learning relation is reduced by n times compared to memnn based models, which enables more data-efficient learning [reference]. however, when n increases, the performance becomes worse than the previous models. in this case, the pair-wise operation increases the number of non-related sentence pairs more than the related sentence pair, thereby confuses rn's learning. [reference] has suggested attention mechanisms as a solution to filter out unimportant relations; however, since it interrupts the reasoning operation, it may not be the most optimal solution to the problem. our proposed model,\" relation memory network\" (rmn), is able to find complex relation even when a lot of information is given. it uses mlp to find out relevant information with a new generalization which simply erase the information already used. in other words, rmn inherits rn's mlp-based output feature map on memory network architecture. experiments show its state-ofthe-art result on the text-based question answering tasks. relation memory network (rmn) is composed of four components-embedding, attention, updating, and reasoning. it takes as the inputs a set of sentences x 1, x 2,..., x n and its related question u, and outputs an answer a. each of the x i, u, and a is made up of one-hot representation of words, for example, section: relation memory network.., n i), v= vocabulary size, n i= number of words in sentence i). section: embedding component we first embed words in each x i= {x i1, x i2, x i3,..., x ini} and u to a continuous space multiplying an embedding matrix a\u2208 r d\u00d7v. then, the embedded sentence is stored and represented as a memory object m i while question is represented as q. any of the following methods are available for embedding component: simple sum (equation 1), position encoding (j. (equation 2), concatenation (equation 3), lstm, and gru. in case of lstm or gru, m i is the final hidden state of it. as the following attention component takes the concatenation of m i and q, it is not necessarily the case that sentence and question have the same dimensional embedding vectors unlike previous memory-augmented neural networks. section: attention component attention component can be applied more than once depending on the problem; figure 1 illustrates 2 hop version of rmn. we refer to the i th embedded sentence on the t th hop as m section: updating component to forget the information already used, we use intuitive updating component to renew the memory. it is replaced by the amount of unconsumed from the old one: contrary to other components, updating is not a mandatory component. when it is considered to have 1 hop, there is no need to use this. section: reasoning component similar to attention component, reasoning component is also made up of mlp, represented as f\u03c6. it receives both q and the final result of attention component r f and then takes a softmax to produce the model answer\u00e2: to answer the question from a given set of facts, the model needs to memorize these facts from the past. long short term memory (lstm) [reference], one of the variants of recurrent neural network (rnn), is inept at remembering past stories because of their small internal memory [reference]. to cope with this problem, j. has proposed a new class of memory-augmented model called memory network (memnn). memnn comprises an external memory m and four components: input feature map (i), generalization (g), output feature map (o), and response (r). i encodes the sentences which are stored in memory m. g updates the memory, whereas o reads output feature o from the memory. finally, r infers an answer from o. memn2n, gmemn2n, dmn, and dmn+ all follow the same structure of memnn from a broad perspective, however, output feature map is composed in slightly different way. the relation between question and supporting sentences is realized from its cooperation. memn2n first calculates the relatedness of sentences in the question and memory by taking the inner product, and the sentence with the highest relatedness is selected as the first supporting sentence for the given question. the first supporting sentence is then added with the question and repeat the same operation with the updated memory to find the second supporting sentence. gmemn2n selects the supporting sentence in the same way as memn2n, but uses the gate to selectively add the the question to control the influence of the question information in finding the supporting sentence in the next step. dmn and dmn+ use output feature map based on various relatedness such as absolute difference, as well as inner product, to understand the relation between sentence and question at various points. the more difficult the task, the more complex the output feature map and the generalization component to get the correct answer. for a dataset experimenting the text-based reasoning ability of the model, the overall accuracy could be increased in order of memn2n, gmemn2n, dmn, and dmn+, where the complexity of the component increases. section: relation network relation network (rn) has emerged as a new and simpler framework for solving the general reasoning problem. rn takes in a pair of objects as its input and simply learns from the compositions of two mlps represented as g\u03b8 and f\u03c6. the role of each mlp is not clearly defined in the original paper, but from the view of memnn, it can be understood that g\u03b8 corresponds to o and f\u03c6 corresponds to r. table 1 summarizes the interpretation of rn compared to memn2n and our model, rmn. to verify the role of g\u03b8, we compare the output when pairs are made with supporting sentences and when made with unrelated sentences. figure 2 shows the visualization result of each output. when we focus on whether the value is activated or not, we can see that g\u03b8 distinguishes supporting sentence pair from non-supporting sentence pair as output feature map examines how relevant the sentence is to the question. therefore, we can comprehend the output of g\u03b8 reveals the relation between the object pair and the question and f\u03c6 aggregates all these outputs to infer the answer. babi story-based qa dataset babi story-based qa dataset is composed of 20 different types of tasks for testing natural language reasoning ability. each task requires different methods to infer the answer. the dataset includes a set of statements comprised of multiple sentences, a question and answer. a statement can be as short as two sentences and as long as 320 sentences. to answer the question, it is necessary to find relevant one or more sentences to a given question and derive answer from them. answer is typically a single word but in a few tasks, answers are a set of words. each task is regarded as success when the accuracy is greater than 95%. there are two versions of this dataset, one that has 1k training examples and the other with 10k examples. most of the previous models test their accuracy on 10k dataset with trained jointly. babi dialog dataset babi dialog dataset [reference]) is a set of 5 tasks within the goal-oriented context of restaurant reservation. it is designed to test if model can learn various abilities such as performing dialog management, querying knowledge bases (kbs), and interpreting the output of such queries. the kb can be queried using api calls and 4 fields (a type of cuisine, a location, a price range, and a party size). they should be filled to issue an api call. task 1 tests the capacity of interpreting a request and asking the right questions to issue an api call. task 2 checks the ability to modify an api call. task 3 and 4 test the capacity of using outputs from an api call to propose options in the order of rating and to provide extra-information of what user asks for. task 5 combines everything. the maximum length of the dialog for each task is different: 14 for task 1, 20 for task 2, 78 for task 3, 13 for task 4, and 96 for task 5. as restaurant name, locations, and cuisine types always face new entities, there are normal and oov test sets to assess model's generalization ability. training sets consist fo 1k examples, which is not a large amount of creating realistic learning conditions. section: training details babi story-based qa dataset we trained 2 hop rmn jointly on all tasks using 10k dataset for model to infer the solution suited to each type of tasks. we limited the input to the last 70 stories for all tasks except task 3 for which we limited input to the last 130 stories, similar to [reference] which is the hardest condition among previous models. then, we labeled each sentence with its relative position. embedding component is similar to [reference], where story and question are embedded through different lstms; 32 unit word-lookup embeddings; 32 unit lstm for story and question. for attention component, as we use 2 hop rmn, there are g 1\u03b8 and g 2\u03b8; both are three-layer mlp consisting of 256, 128, 1 unit with relu activation function [reference]. f\u03c6 is composed of 512, 512, and 159 units (the number of words appearing in babi dataset is 159) of three-layer mlp with relu non-linearities where the final layer was a linear that produced logits for a softmax over the answer vocabulary. for regularization, we use batch normalization [reference] for all mlps. the softmax output was optimized with a cross-entropy loss function using the adam optimizer [reference]) with a learning rate of 2e\u22124. babi dialog dataset we trained on full dialog scripts with every model response as answer, all previous dialog history as sentences to be memorized, and the last user utterance as question. model selects the most probable response from 4, 212 candidates which are ranked from a set of all bot utterances appearing in training, validation and test sets (plain and oov) for all tasks combined. we also report results when we use match type features for dialog. match type feature is an additional label on the candidates indicating if word is found on the dialog history. for example, if the concentrate on the same sentences which are all critical to answer the question, and sometimes g 1\u03b8 finds a fact related to the given question and with this information g 2\u03b8 chooses the key fact to answer. while trained jointly, rmn learns these different solutions for each task. for the task 3, the only failed task, attention component still functions well; it focuses sequentially on the supporting sentences. however, the reasoning component, f\u03c6, had difficulty catching the word' before'. we could easily figure out' before' implies' just before' the certain situation, whereas rmn confused its meaning. as shown in table 3c, our model found all previous locations before the garden. still, it is remarkable that the simple mlp carried out all of these various roles. section: babi dialog the results in the table 4 show that the rmn has the best results in any conditions. without any match type, rn and rmn outperform previous memory-augmented models on both normal and oov tasks. this is mainly attributed to the impressive result on task 4 which can be interpreted as an effect of mlp based output feature map. to solve task 4, it is critical to understand the relation between' phone number' of user input and' r phone' of previous dialog as shown in table 8c. we assumed that inner product was not sufficient to capture their implicit similarity and performed an supporting experiment. we converted rmn's attention component to inner product based attention, and the results revealed the error rate increased to 11.3%. for the task 3 and task 5 where the maximum length is especially longer than the others, rn performs worse than memn2n, gmemn2n and rmn. the number of unnecessary object pairs created by the rn not only increases the processing time but also decreases the accuracy. with the match type feature, all models other than rmn have significantly improved their performance except for task 3 compared to the plain condition. rmn was helped by the match type only on the oov tasks and this implies rmn is able to find relation in the with match condition for the normal tasks. when we look at the oov tasks more precisely, rmn failed to perform well on the oov task 1 and 2 even though g 1\u03b8 properly focused on the related object as shown in table 8a. we state that this originated from the fact that the number of keywords in task 1 and 2 is bigger than that in task 4. in task 1 and 2, all four keywords (cuisine, location, number and price) must be correctly aligned from the supporting sentence in order to make the correct api call which is harder than task 4. consider the example in table 8a and table 8c. supporting sentence of task 4 have one keyword out of three words, whereas supporting sentences of task 1 and 2 consist of four keywords (cuisine, location, number and price) out of sixteen words. different from other tasks, rmn yields the same error rate 25.1% with memn2n and gmemn2n on the task 3. the main goal of task 3 is to recommend restaurant from knowledge base in the order of rating. all failed cases are displaying restaurant where the user input is< silence> which is somewhat an ambiguous trigger to find the input relevant previous utterance. as shown in table 8b, there are two different types of response to the same user input. one is to check whether all the required fields are given from the previous utterances and then ask user for the missing fields or send a\" ok let me look into some options for you.\" message. the other type is to recommend restaurant starting from the highest rating. all models show lack of ability to discriminate these two types of silences so that concluded to the same results. to verify our statement, we performed an additional experiment on task 3 and checked the performance gain (extra result is given in table 10 of appendix b). section: model analysis effectiveness of the mlp-based output feature map the most important feature that distinguishes memnn based models is the output feature map. table 5 summarizes the experimental results for the babi story-based qa dataset when replacing the rmn's mlp-based output feature map with the idea of the previous models. inner product was used in memn2n, inner product with gate was used in gmemn2n, and inner product and absolute difference with two embedding matrices was used in dmn and dmn+. from the table 5, the more complex the output feature map, the better the overall performance. in this point of view, mlp is the effective output feature map. performance of rn and rmn according to memory size additional experiments were conducted with the babi story-based qa dataset to see how memory size affects both performance and training time of rn and rmn. test errors with training time written in parentheses are summarized in table 6. when memory size is small, we could observe the data-effeciency of rn. it shows similar performance to rmn in less time. however, when the memory size increases, performance is significantly reduced compared to rmn, even though it has been learned for a longer time. it is even lower than itself when the memory size is 20. on the other hand, rmn maintains high performance even when the memory size increases. effectiveness of the number of hops babi story based qa dataset differs in the number of supporting sentences by each task that need to be referenced to solve problems. for example, task 1, 2, and 3 require single, two, and three supporting facts, respectively. the result of the mean error rate for each task according to the number of hops is in table 7. overall, the number of hops is correlated with the number of supporting sentences. in this respect, when the number of relations increases, rmn could reason across increasing the number of hops to 3, 4 or more. section: conclusion our work, rmn, is a simple and powerful architecture that effectively handles text-based question answering tasks when large size of memory and high reasoning ability is required. multiple access to the external memory to find out necessary information through a multi-hop approach is similar to most existing approaches. however, by using a mlp that can effectively deal with complex relatedness when searching for the right supporting sentences among a lot of sentences, rmn raised the state-of-the-art performance on the story-based qa and goal-oriented dialog dataset. when comparing rn which also used mlp to understand relations, rmn was more effective in the case of large memory. future work will apply rmn to image based reasoning task (e.g., clevr, daquar, vqa etc.). to extract features from the image, vgg net [reference]) is used in convention and outputs 196 objects of 512 dimensional vectors which also require large sized memory. an important direction will be to find an appropriate way to focus sequentially on related object which was rather easy in text-based reasoning. 5 number in parentheses indicates the number of supporting sentences to solve the task a model details we modify the user input from< silence> to< silence><silence> when looking for restaurant recommendations. this makes model to distinguish two different situations whether to ask for additional fields or to recommend restaurant. section:",
    "templates": [
        {
            "Material": [
                [
                    [
                        "babi story-based qa dataset",
                        10730
                    ],
                    [
                        "babi dialog dataset",
                        11581
                    ],
                    [
                        "babi dataset",
                        13657
                    ],
                    [
                        "hops babi story based qa dataset",
                        19844
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "remo",
                        8
                    ],
                    [
                        "related memory object",
                        15
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "mean error rate",
                        20093
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "text-based question and answering task",
                        123
                    ],
                    [
                        "babi-10k story-based question answering tasks",
                        1016
                    ],
                    [
                        "babi dialog-based question answering tasks",
                        1066
                    ],
                    [
                        "text-based question answering tasks",
                        5590
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "2b507f659b341ed0f23106446de8e4322f4a3f7e-45",
    "doctext": "document: deep identity-aware transfer of facial attributes this paper presents a deep convolutional network model for identity-aware transfer (diat) of facial attributes. given the source input image and the reference attribute, diat aims to generate a facial image that owns the reference attribute as well as keeps the same or similar identity to the input image. in general, our model consists of a mask network and an attribute transform network which work in synergy to generate photo-realistic facial image with the reference attribute. considering that the reference attribute may be only related to some parts of the image, the mask network is introduced to avoid the incorrect editing on attribute irrelevant region. then the estimated mask is adopted to combine the input and transformed image for producing the transfer result. for joint training of transform network and mask network, we incorporate the adversarial attribute loss, identity aware adaptive perceptual loss and vgg-face based identity loss. furthermore, a denoising network is presented to serve for perceptual regularization to suppress the artifacts in transfer result, while an attribute ratio regularization is introduced to constrain the size of attribute relevant region. our diat can provide a unified solution for several representative facial attribute transfer tasks, e. g., expression transfer, accessory removal, age progression and gender transfer, and can be extended for other face enhancement tasks such as face hallucination. the experimental results validate the effectiveness of the proposed method. even for the identity-related attribute (e. g., gender), our diat can obtain visually impressive results by changing the attribute while retaining most identity-aware features. facial attribute transfer, generative adversarial nets, convolutional networks, perceptual loss. section: introduction face attributes, e. g., gender and expression, can not only provide a natural description of facial images, but also offer a unified viewpoint for understanding many facial animation and manipulation tasks. for example, the goal of facial avatar and reenactment is to transfer the facial expression attributes of a source actor to a target actor. in most applications such as expression transfer, accessory removal and age progression, the animation only modifies the related attribute without changing the identity. but for some other tasks, the change of some attributes, e. g., gender and ethnicity, will inevitably alter the identity of the source image. in recent years, a variety of methods have been developed for specific facial attribute transfer tasks, and have achieved impressive results. for expression transfer, approaches have been suggested to create 3d or image-based avatars from hand-held video, while face trackers and expression modeling have been investigated for offline and online facial reenactment. for age progression, explicit and implicit synthesis methods have been proposed for different image models. hair style generation and replacement have also been studied in literatures. convolutional neural network (cnn)- based models have also been investigated for human face generation with attributes. kulkarni et al. propose deep convolution inverse graphic network (dg-ign). this method requires a large number of faces of a single person for training, and can only generate faces with different pose and light. gauthier developes a conditional generative adversarial network (cgan) to generate facial image from a noise distribution and conditional attributes. yan et al. suggest an attribute-conditioned deep variational auto-encoder which extracts the latent variables from a reference image and combines them with attributes to produce the generated image with a generative model. oord et al. propose a conditional image generation model based on pixelcnn decoder for image generation conditioned on an arbitrary feature vector. however, the identity of the generated face is not emphasized in, making them not directly applicable to attribute transfer. motivated by the strong capability of cnn in modeling complex transformation and capturing perceptual similarity, several approaches have also been suggested for facial attribute transfer. li et al. suggest a cnn-based attribute transfer model from the optimization perspective, but both run time and transfer quality is far from satisfying. considering that it is impracticable to collect labeled data for supervised learning, the generative adversarial net (gan) framework usually is adopted for handling this task. however, visible artifacts and over-smoothing usually are inevitable in the transfer result for these methods. in this paper, we present a novel deep cnn model for identity-aware transfer (diat) of facial attributes which can provide a unified solution to several facial animation and manipulation tasks, e. g., expression transfer, accessory removal, age progression, and gender transfer. for each reference attribute label, we train a cnn model for the transfer of the input image to the desired attribute. note that the reference attribute may be only related to some parts of the image. to avoid the the incorrect editing on attribute irrelevant region, our model consists of a mask network and an attribute transform network. the attribute transform network is presented to edit the input image for generating the desired attribute. while the mask network is adopted to estimate a mask of the attribute relevant region for guiding the combination of the input image and transformed image. then attribute transform network and mask network work collaboratively to generate final photo-realistic transfer result. for attribute transfer, the ground truth transfer results generally are very difficult or even impossible to obtain. therefore, we follow the gan framework to train the model. as for training data, we only consider the binary attribute labels presented in the large-scale celebfaces attributes (celeba) dataset. to capture the convolutional feature distribution of each attribute, we construct an attribute guided set using all the images with the desired attribute in celeba. then, the input set is defined as a set of input images without the reference attribute. due to the infeasibility of ground truth transfer results, two alternative losses, i. e., adversarial attribute loss and identity-aware perceptual loss, are incorporated for unsupervised training of our diat model. furthermore, two regularizers, i. e., perceptual regularization and attribute ratio regularization, are also introduced in the learning of diat. in terms of attribute transfer, the generated image should have the desired attribute label. following the gan framework, we define the adversarial attribute loss on the attribute discriminator to require the generated image to have the desired attribute. as for identity-aware transfer, our diat requires that the generated image should keep the same or similar identity to the input image. to this end, the identity-aware perceptual loss is introduced on the convolutional feature map of a cnn model to model the content similarity between the reference face and the generated face. instead of adopting any pre-trained cnns, we suggest to define the perceptual loss on the attribute discriminator, which can be adaptively trained along with the learning procedure, and is named as adaptive perceptual loss. compared with conventional perceptual loss, ours is more effective in computation, tailored to our specific attribute transfer task, and can serve as a kind of hidden-layer supervision or regularization to ease the training of the diat model. to further encourage the identity keeping property, we add an identity loss by minimizing the distance between feature representations of the generated face and the reference. pre-trained vgg-face is used to extract the identity related features for face verification. the model objective of diat also consider two regularizers, i. e., perceptual regularization and attribute ratio regularization. to suppress the artifacts, we propose a denoising network to serve for a perceptual regularization on the generated image. to guide the learning of mask network, an attribute ratio regularization is introduced to constrain the size of attribute relevant region. finally, our diat model can be learned from training data by incorporating adversarial attribute loss, adaptive perceptual loss with perceptual regularization and attribute ratio regularization. extensive experiments are conducted on celeba and real images from the website istock. as illustrated in fig. [reference], our diat performs favorably in attribute transfer with minor or no modification on the identity of the input faces. even for some identity-related attributes (e. g., gender), our diat can obtain visually impressive transfer result while retaining most identity-relevant features. computational efficiency is also a prominent merit of our method. in the testing stage, our diat can process more than one hundred of images within one second. furthermore, our model can be extended to face hallucination, and is effective in generating photo-realistic high resolution images. a preliminary report of this work is given in 2016. to sum up, our contribution is three-fold: a novel diat model is developed for facial attribute transfer. for better preserving of attribute irrelevant feature, our model comprises a mask network and an attribute transform network, which collaborate to generate the transfer result and can be jointly learned from training data. adversarial attribute loss, adaptive perceptual loss, identity loss, perceptual regularization, and attribute ratio regularization are incorporated for training our diat model. the adversarial attribute loss is adopted to make the transfer result exhibit the desired attribute, and the adaptive perceptual loss is defined on the discriminator for identity-aware transfer while improving training efficiency. moreover, perceptual regularization and attribute ratio regularization are further introduced for suppressing the artifacts and constraining the mask network. experimental results validate the effectiveness and efficiency of our method for identity-aware attribute transfer. our diat can be used for the transfer of either local (e. g., mouth), global (e. g., age progression) or identity-related (e. g., gender) attributes, and can be extended to face hallucination. the remainder of the paper is organized as follows. section [reference] gives a brief survey on relevant work. section [reference] describes the model and learning of our diat method. section [reference] reports the experimental results on facial attribute transfer and face hallucination. finally, section [reference] ends this work with several concluding remarks. section: related work deep convolutional neural networks (cnns) not only have achieved unprecedented success in versatile high level vision problems, but also exhibited their remarkable power in understanding, generating, and recovering images. in this section, we focus on the task of facial attribute transfer, and briefly survey the cnn models for image generation and face generation. subsection: cnn for image generation generative image modeling is a critical issue for image generation and many low level vision problems. conventional sparse, low rank, frame and non-local similarity based models usually are limited in capturing highly complex and long-range dependence between pixels. for better image modeling, a number of cnn-based methods have been proposed, including convolutional auto-encoder, pixelcnn and pixelrnn, and they have been applied to image completion and generation. several cnn architectures have been developed for image generation. fully convolutional networks can be trained in the supervised learning manner to generate an image from an input image. the generative cnn model stacks four convolution layers upon five fully connected layers to generate images from object description. kulkarni et al. suggest the deep convolution inverse graphics network (dc-ign), which follows the variational autoencoder architecture to transform the input image into different pose and lighting condition. however, both generative cnn and dc-ign require many labeled images in training. to visualize and understand cnn features, several methods have been proposed to reconstruct images by inverting deep representation or maximizing class score. subsequently, gatys et al. suggest to combine content and style losses defined on deep representation on the off-the-shelf cnns for artistic style transfer. to improve the efficiency, alternative approaches have been proposed by substituting the iterative optimization procedure with pre-trained feed-forward cnn. and perceptual loss has also been adopted for style transfer and other generation tasks. motivated by these works, both identity-aware adaptive perceptual loss and perceptual regularization are exploited in our diat model to meet the requirement of facial attribute transfer. another representative approach is generative adversarial network (gan), where a discriminator and a generator are alternatingly trained as an adversarial game. the generator aims to generate images to match the data distribution, while the discriminator attempts to distinguish between the generated images and the training data. laplacian pyramid of gans is further suggested to generate high quality image in a coarse-to-fine manner. radford et al. extend gan with the fully deep convolutional networks (i. e., dcgan) for image generation. to learn disentangled representations, information-theoretic extension of gan is proposed by maximizing the mutual information between a subset of noise variables and the generated results. in, wgan and wgan-gp minimize the wasserstein-1 distance between the generated distribution and the real distribution to improve the stability of learning generator. in this work, we adopt the wgan framework to learn our diat model, and further suggest adaptive perceptual loss for identity-aware transfer and perceptual regularization to suppress visual artifacts. subsection: cnn for face generation facial attribute transfer has received considerable recent attention. larsen et al. present to combine variational autoencode with gan (vae/ gan) for image generation. by modeling the attribute vector as the difference between the mean latent representations of the images with and without the reference attribute, vae/ gan can provide a flexible solution to arbitrary facial attribute transfer, but is limited in transfer performance. li et al. suggest an attribute driven and identity-preserving face generation model by solving an optimization problems with perceptual loss, which is computationally expensive and can not obtain high quality results. perarnau et al. adopt an encoder-decoder architecture, where attribute transfer can be conducted by editing the latent representation. shen et al. learn the residual image in the gan framework, and adopt dual learning to learn two reverse attribute transfer models simultaneously. zhou et al. propose a model to learn object transfiguration from two sets of unpaired images that have the opposite attribute. however, most existing methods can not achieve high quality transfer results, and visible artifacts and over-smoothing usually are inevitable. in comparison, our diat model can achieve much better transfer results than the competing methods. besides, cnns have also been developed for other face generation tasks. for painting style transfer of head portrait, selim et al. modify the perceptual loss to balance the contribution of the input photograph and the aligned exemplar painting. gucluturk et al. train a feed-forward cnn with perceptual loss for sketch inversion. yeh et al. apply dcgan to semantic face inpainting in an optimization manner. section: deep cnns for identity-aware attribute transfer in this section, we present our diat model for identity-aware transfer of facial attribute. as illustrated in fig. [reference], our model involves a mask network and an attribute transform network which collaborate to produce the transfer result. to train our model, we incorporate the adversarial attribute loss, adaptive perceptual loss, perceptual regularization and attribute ratio regularization. subsection: network architecture most facial attributes, e. g., expression and accessory, are local-based and only related to part of facial image. even for global attributes such as age and gender, some parts, e. g., the background, should also keep the same with the source image. for the sake of preserving attribute irrelevant feature, it is natural to only perform attribute transfer in image region related to specific attribute. however, it is not a trivial issue to find the attribute relevant region. one possible solution is to manually specify the relevant region for each attribute given a new transfer task, but it undoubtedly restricts the universality and adaptivity of the solution. in this work, we aim to provide a unified solution to attribute transfer, which indicates that we only require to prepare training data and retrain the model when a new transfer task comes. to this end, our whole attribute transfer network is comprised of two sub-networks, i. e., mask network and attribute transform network. both mask network and attribute transform network take the source image as input. the mask network is utilized to predict a mask to indicate the attribute relevant region, while the attribute transform network is used to produce the transformed image. given and, the final transfer result can be obtained by, where denotes the element-wise product operator. we also note that both attribute transform network and mask network can be learned from training data in an end-to-end manner. in the following, we describe the architecture of attribute transform network and mask network, respectively. attribute transform network. we adopt the unet for attribute transform due to its good tradeoff between efficiency and reconstruction ability. in general, the unet architecture involves an encoder subnetwork and a decoder subnetwork, then skip connection and pooling operation are further introduced to exploit multi-scale information. as for attribute transform, we design a 10-layer unet, which includes 5 convolution layers for encoding and another 5 convolution layers for decoding. in the encoder, we use convolution with stride 2 for downsampling. in the decoder, a depth to width (dtow) layer is deployed for upsampling, and the element-wise summation operation is adopted to fuse the feature maps from the encoder and decoder subnetworks. the detailed parameters of the attribute transform network are summarized in table [reference]. mask network. as for mask network, we first adopt a 5-layer fully convolutional network to generate a binary mask for indicating the attribute relevant region. a batch normalization layer is added after each convolution layer. then, upsampling is deployed by simply replicating each element in the binary mask times. in order to make the generated image smooth, we further utilize a gaussian filter with the standard deviation of to produce the final mask. to sum up, the details of the mask network are given in table [reference]. in our mask network, relu is adopted for nonlinearity for the first 4 convolution layers. as for the fifth convolution layer, we adopt the sigmoid nonlinearity, and the binarization operation is then used to obtain the binary mask, where denotes an element of the feature map. however, the gradient of the binarizer is zero almost everywhere except that it is infinite when, making any layer before the binarizer never be updated during training. as a remedy, we follow the straight-through estimator on gradient, and introduce a piecewise linear proxy function to approximate, during training, is still used in forward-propagation calculation, while is used in back-propagation, with its gradient computed by, subsection: model objective by enforcing proper constraints on transfer result, both the attribute transform network and mask network can be learned from training data. however, the ground truth of attribute transfer usually is unavailable and not unique. for example, it is generally impossible to obtain the ground truth of gender transfer in reality. instead, the training data used in this work includes a guided set of images with the desired reference attribute and a source set of input images not with the reference attribute. and we do not require the images from guided set to have the same identity with theose from source set. for the sake of identity-aware attribute transfer, we define two alternative losses: (i) adversarial attribute loss to make the transfer result exhibit the desired attribute, and (ii) adaptive perceptual loss and identity loss to make the generated image keep the same or similar identity to input image. to suppress the visual artifacts of the transfer result, we further include (iii) a perceptual regularization defined on a denoising network. finally, (iv) an attribute ratio regularization is deployed on to guide the learning of mask network. in the following, we provide more details on these losses and regularization terms. adversarial attribute loss. adversarial strategy is a common stratey that is widely used in security problems. for computer vision, an adversarial learning framework called generative adversarial networks (gans) also shows powerful ability on generating images that fulfil the distribution of a set of images without any ground truth targets. considering the infeasibility of obtaining the ground truth transfer result, we define the adversarial attribute loss based on the guided set and the source set. if the guided set is of large scale, it can provide a natural representation of the attribute distribution. therefore, the goal of adversarial attribute loss is to make that the distribution of generated images matches the real attribute distribution. to this end, we adopt the generative adversarial network framework, where the generator is the attribute transfer network, and the discriminator is used to define the adversarial attribute loss. the details of the discriminator are provided in table [reference], which contains 6 convolution layers followed by another two fully-connected layers. denote by an input image from, and an image from. let be the distribution of the input images, be the distribution of the images with the reference attribute. the discriminator is defined as to output the probability that the image comes from the set. to train the generator and the discriminator, we take use of the following adversarial attribute loss, in order to improve the training stability, the improved wasserstein gan is adopted by defining the loss as, for simplicity, we respectively define the adversarial attribute losses for the generator and discriminator as follows, adaptive perceptual loss. the adaptive perceptual loss is introduced to guarantee that the transfer result keeps the same or similar identity with the input image. due to identity is a high level semantic concept, it is not proper to define identity-aware loss by forcing two images to be exactly the same in pixel domain. instead, we define the squared-error loss on the feature representations of the discriminator, resulting in our adaptive perceptual loss. denote by the discriminator, and the feature map of the-th convolution layer., and represent the channel number, height, and width of the feature map, respectively. we then define the perceptual loss between and on the-th convolution layer as, and the identity-aware adaptive perceptual loss is further defined as, we note that the discriminator is learned from training data. thus, the network parameters of will be changed along with the updating of discriminator, and thus we name as adaptive perceptual loss. in contrast, conventional perceptual loss is defined on the off-the-shelf cnns (e. g., vgg-face). compared with conventional perceptual loss, our adaptive perceptual loss generally is more effective in improving the training efficiency and attribute transfer performance: the training efficiency of adaptive perceptual loss can be further explained from two aspects. (i) for conventional perceptual loss, the forward and backward calculations are required for both the off-the-shelf cnn and the discriminator during training. due to that the adaptive perceptual loss is defined on the discriminator, it is sufficient to only conduct forward and backward calculation on the discriminator, making our diat more efficient in training. (ii) for conventional gan, the generator usually is difficult to be trained. as for our diat, it can be trained by both the adversarial attribute loss and adaptive perceptual loss, greatly accelerating the training speed. actually, the adaptive perceptual loss is defined on the third and fourth convolution layers of the discriminator, which can serve as some kind of hidden-layer supervision and benefit the convergence of network training. for conventional perceptual loss, the off-the-shelf cnns generally are pre-trained using other training data and are not tailored to attribute transfer. one plausible choice is the vgg-face, which, however, is trained for face recognition and may not be suitable for identity-aware attribute transfer. in comparison, our adaptive perceptual loss is defined on the discriminator which is trained for modeling. such loss can thus provide natural balance between identity similarity and attribute transfer and benefit transfer performance. for example, in terms of gender transfer, the introduction of adaptive perceptual loss will allow the adaptive adjustment on the length of hair. identity loss. the proposed adaptive perceptual loss does help keep the content similarity between the generated face and the reference. however, it can not guarantee the identity by itself. to further enhance the identity keeping property, we add constrains on the feature representation extracted for face recognition or verification. in face verification task, two faces are from the same person when the distance between two features are smaller than ceratain threshold. here, we adopt vgg-face and model the distance between the features of the generated face and the reference as the identity loss, perceptual regularization. despite the use of adversarial attribute loss and adaptive perceptual loss, visual artifacts are still inevitable in the transfer result. image regularization is thus required to encourage the spatial smoothness while preserving small scale details of the generated face. one choice is the total variation (tv) regularizer which has been adopted in cnn feature visualization and artistic style transfer. however, the tv regularizer is limited in recovering small-scale texture details and suppressing complex artifacts. moreover, it is a generic model that does not consider the characteristics of facial images. in this work, we take the facial characteristics into account and train a denoising network for perceptual regularization. to train the denoising network, we generate the noisy image by adding gaussian noise with the standard deviation of to the clean facial image from celeba. inspired by residual learning, we train the denoising network through learning the residual between the noise image and the clean image. taking the noise image as input, the denoising network utilizes a fully convolutional network of 6 layers to predict the residual. the denoising result can then be obtained by. the architecture of is listed in table [reference]. denote by a training set, where denotes the-th noisy image and the corresponding clean image. the objective for learning is given as, given the denoising network and the transfer result, we define the perceptual regularization as, where denotes the frobenius norm. note that predicts the residual between the latent clean image and. minimizing makes be close to the clean image, and can be used to suppress the noise and artifacts in. furthermore, the threshold is introduced for better preserving of small scale details, and we empirically set be a value in the range of. note that the regularizer in eqn. ([reference]) is defined on the denoising network, and thus is named as perceptual regularization. attribute ratio regularization. the size of attribute relevant region varies for different attributes. for example, the region related to mouth open/ close mainly includes the mouth and should be small. for glasses removal, the attribute relevant region includes the two eyes and is relatively large. as for gender transfer, all the face region and the hair should be attribute relevant. therefore, we introduce an attribute ratio regularization term to constrain the size of attribute relevant region. specifically, such regularization is defined on the binary mask in eqn. ([reference]). denote by the image size, and the expected ratio of the region for a specific attribute. the attribute ratio regularization is then defined as, in our experiments, we set smaller value for local attribute and larger value for global attribute. objective function. we define the objective function for learning the transfer model and the discriminator by combining the adversarial attribute loss, adaptive perceptual loss, perceptual regularization, and attribute ratio regularization. the transfer model is learned by minimizing the following objective, where.,, and are the tradeoff parameters for the adaptive perceptual loss, attribute ratio regularization, and perceptual regularization, respectively. however, it is difficult to set the tradeoff parameter. instead, we empirically find that the transfer model can be stably learned by alternatingly minimizing and during training. finally, the discriminator is learned by minimizing the following objective, subsection: learning algorithm generally, both the generator and the discriminator are difficult to converge in gan. therefore, we adopt a two-stage strategy for learning the transfer model and the discriminator: (i) we first combine the source set and the guided set to pre-train for initialization, and (ii) alternate between updating and. the procedure for training the transfer model is summarized in algorithm [reference]. initialization. for the initialization of the, we only consider the attribute transform network, and leave the mask network be learned in the second stage. note that the transform network has the architecture of auto-encoder. thus, it can be pre-trained by minimizing the following reconstruction objective on and, as for the initialization of the discriminator, we use the images in as negative samples and the images in as positive samples. then the discriminator can be pre-trained by minimizing the following objective, where is for positive image and for negative image. by this way, the initialization can provide a good start point and benefit the convergence and stability of diat training. network training. after the initialization of and, network training is further performed by updating the whole (including both and) and alternatingly. moreover, is updated by first iterations for minimizing and then iterations for minimizing. we apply the rmsprop solver to train the transfer network and the discriminator with a learning rate of. [! tbp] learning the attribute transfer network [1] source set, guided set,,,. mini-batch size is. the attribute transfer network pre-train the transform model by minimizing the objective in eqn. ([reference]). pre-train the discriminator by minimizing the objective in eqn. ([reference]). not converged select a mini-bath from to generate the transfer results, which is further combined with another mini-batch from to form the set for training the discriminator. here we set. use the rmsprop solver to update the discriminator with eqn. ([reference]) using the mini-batch. clip the parameters of the discriminator. use the rmsprop solver to update by minimizing in eqn. ([reference]). use the rmsprop solver to update by minimizing in eqn. ([reference]). return the transfer network subsection: extension to face hallucination besides facial attribute transfer, our diat can also be extended to other face editing tasks. here we use the face hallucination as an example. face hallucination is undoubtedly a global transfer task, and thus we remove the mask network as well as the attribute ratio regularization, making. moreover, the input image in face hallucination is of low resolution (lr) while the output image is of high resolution (hr). to be consistent with attribute transfer, we super-resolve lr image to the size of hr image with the bicubic interpolator, which is taken as input to. furthermore, the ground truth hr images can be available to guide the network training for face hallucination. denote by the super-resolved image by bicubic interpolator, and the ground truth hr image. then, the pixel-wise reconstruction loss is defined as, we further modify the definition of by removing the attribute ratio regularization and adding reconstruction loss, where is the tradeoff parameter for pixel-wise reconstruction loss, and we set in our experiment. given the training data, the models can then be learned by updating and alternatingly. section: experimental results in this section, we first describe the experimental settings, including the training and testing data, competing methods, model and learning parameters. experiments are then performed for local and global attribute transfer. quantitative metrics and the results on real images are also reported. moreover, we analyze the effect of adaptive perceptual loss and perceptual regularization. finally, the results are reported to further assess the performance of our diat for face hallucination. the source code will be given after the publication of this work. subsection: experimental settings our diat models are trained using a subset of the aligned celeba dataset by removing the images with poor quality. the size of the aligned images is. due to the limitation of the gpu memory, we sample the central part of each image and resize it to. for each attribute transfer task, we use all the images with the reference attribute from training set to form the guided set, and randomly select 10, 000 training images not with the reference attribute as the source set. after training, 2, 000 images apart from the images for training are adopted to assess the attribute transfer performance. and we also test the models on other real images from the website istock. only a few methods have been proposed for facial attribute transfer. in our experiments, we compare our diat with the convolutional attribute-driven and identity-preserving model (cnia), icgan and vae/ gan due to that their codes are available. as for icgan and vae/ gan, the original image size is not the same with our diat, so we resize the result to the same size with diat for comparison. for the task of glasses removal, we can first manually detect the region of glasses, and then use some face inpainting methods (e. g., semantic inpainting) to recover the missing pixels. thus we also compare our diat with semantic inpainting for glasses removal. all the experiments are conducted on a computer with the gtx titanx gpu of 12 gb memory. we set the parameters and for diat. for the threshold in the perceptual regularization, we set it to be a value in the range of. as for in the attribute ration regularization, we set it to be (i) for small local attributes (e. g., mouth), (ii) for large local attributes (e. g., eyes), and (iii) for global attributes (e. g., gender and age). subsection: local attribute transfer we assess the local attribute transfer models on three tasks, i. e., mouth open, mouth close, and eyeglasses removal. fig. [reference] illustrates the transfer results by our diat. it can be seen that our diat performs favorably for transferring the input images to the desired attribute with satisfying visual quality. benefited from the mask network, the results by diat can preserve more identity-aware and attribute irrelevant details. moreover, when the training data are sufficient, it is feasible to separately train two diat models for reverse tasks, e. g., one for mouth open and another for mouth close. we further compare our diat with three competing methods, i. e., cnia, icgan and vae/ gan. as shown in fig. [reference], the results by our diat are visually more pleasing than those by cnia for all the three local attribute transfer tasks. in terms of run time, cnia takes about seconds () to deal with an image, while our diat only needs. fig. [reference] further provides the results by icgan, vae/ gan, and our diat. in comparison with the competing methods, our diat can well address the attribute transfer tasks while recovering more visual details in both attribute relevant and attribute irrelevant regions. finally, for glasses removal, we compare our diat with semantic inpainting, and the results in fig. [reference] clearly demonstrate the superiority of diat. subsection: global attribute transfer we consider two global attribute transfer tasks, i. e., gender transfer and age transfer. for gender transfer, we only evaluate the model for male-to-female. for age transfer, we only test the model for older-to-younger. fig. [reference] shows the transfer results, and our diat is also effective for global attribute transfer. even gender transfer certainly causes the change of the identity, as shown in fig. [reference] (a), our diat can still retain most identity-aware features, making the transfer result similar to the input image in appearance. figs. [reference] and [reference] show the results by our diat, cnai, icgan and vae/ gan. compared with the competing methods, the results by our diat well exhibit the desired attribute, and are of high visual quality with photo-realistic details. finally, we also note that for gender transfer our diat is able of adjusting the hair length due to the introduction of adaptive perceptual loss. subsection: quantitative evaluation given each attribute transfer task, we randomly select images without the reference attribute from the testing partition of celeba to form our testing set. then, three groups of experiments are conducted to evaluate the transfer performance quantitatively: attribute classification. for attribute transfer, it is natural to require the transfer result to exhibit the desired attribute. thus, we first train a cnn-based attribute classifier (including two convolution layers, three residual blocks and two fully-connected layers) using the training set of celeba. given an attribute transfer task, we test the classification accuracy of the desired attribute for the transfer results of 2, 000 testing images. tabel [reference] lists the classification accuracy for five attribute transfer tasks, i. e., mouth open, mouth close, glasses removal, gender transfer, and age transfer. it can be observed that our diat achieves satisfying accuracy (i. e.,) for all the tasks, indicating that the results by our diat generally are with the desired attribute. identity verification. as for local attribute transfer, we also require the transfer result to preserve the identity of input image. here we use the open source face recognition platform openface for matching the input image with the transfer result. by setting the threshold be 0.99, table [reference] lists the identity verification accuracy for mouth open, mouth close, and glasses removal. the results demonstrate that our diat can well preserve the identity-aware feature for local attribute transfer. image quality. image quality is another crucial metric to assess attribute transfer. however, the ground truth of transfer result is unavailable, making it difficult to perform quantitative evaluation. here we use a pair of reverse attribute transfer tasks (i. e., mouth close and mouth open) as an example, and adopt an indirect scheme to compute average psnr on the 2000 testing images. specifically, we first perform mouth open to the images with mouth close, and then perform reverse mouth close to the transfer results. finally, the input images are taken as ground truth and the images after two steps of transfer can be viewed as the generated images. by this way, we obtain the average psnr of 33.27db, indicating the effectiveness of our diat for local attribute transfer. subsection: results on other real facial images to assess the generalization ability, we use the diat models learned on celeba to other real facial images from the website istock. each test image is first aligned with the 5 facial landmarks, and then input to the diat models. taking mouth open and gender transfer as examples, fig. [reference] gives the transfer results on 15 images for each task, clearly demonstrating the generalization ability of our models to other real facial images. subsection: evaluation on adaptive perceptual loss and perceptual regularization we also implement a variant of diat (i. e., diat-1) by replacing the adaptive perceptual loss with the conventional perceptual loss defined on vgg-face. taking gender transfer as an example, fig. [reference] compares diat with diat-1. it can be observed that diat converges very fast and can generate satisfying results after 4 epochs of training. in comparison, diat-1 requires much more epochs in training, and the gender just begins to be modified after 18 epochs. moreover, the adoption of adaptive perceptual loss also benefits the transfer performance, and adaptive adjustment on the hair length can be observed on the transfer results by diat. furthermore, fig. [reference] shows the transfer results by diat with the perceptual regularization and the tv regularization. it can be clearly seen that the perceptual regularization is more effective on suppressing noise and artifacts while preserving sharp edges and fine details. subsection: results of the learnt mask fig. [reference] gives the masks generated by the mask network for different task. for the local attribute transformation tasks such as glasses removal and closing mouth, the generated masks accurately cover the local facial part which is related to the attribute. for global transformation like gender transformation, the mask covers most of the face and keep the background out. subsection: experiments on face hallucination finally, we evaluate the performance of diat for face hallucination. table [reference] lists the average psnr and ssim values on the 2, 000 testing images by diat, bicubic interpolator, and unet, while fig. [reference] shows the super-resolved images. even our diat achieves lower psnr/ ssim than the baseline unet, it is much better in terms of visual quality, and can generate hallucinated image with rich textures and sharp edges. section: conclusion a deep identity-aware transfer (i. e., diat) model is presented for facial attribute transfer. considering that some attributes may be only related with parts of facial image, the whole transfer model consists of two subnetworks, i. e., mask network and attribute transform network, which work collaboratively to produce the transfer result. in order to train the model, we further incorporate adversarial attribute loss, adaptive perceptual loss with perceptual regularization and attribute ratio regularization. experiments show that our model can obtain satisfying results for both local and global attribute transfer. even for some identity-related attributes (e.g., gender transfer), our diat can obtain visually impressive results with minor modification on identity-related features. our diat can also be extended to face hallucination and performs favorably in recovering facial details. in future work, we will further improve the visual quality and diversity of the transfer results, and extend our model to arbitrary attribute transfer. bibliography: references",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "deep convolutional network model",
                        82
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "classification accuracy",
                        38939
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "deep identity-aware transfer of facial attributes",
                        10
                    ],
                    [
                        "identity-aware transfer",
                        119
                    ],
                    [
                        "diat",
                        145
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "2c1d608e3cd43b3104d76296a14f1aef0e6b2c64-46",
    "doctext": "document: dynamic evaluation of neural sequence models we present methodology for using dynamic evaluation to improve neural sequence models. models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. dynamic evaluation outperforms existing adaptation approaches in our comparisons. dynamic evaluation improves the state-of-the-art word-level perplexities on the penn treebank and wikitext-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and hutter prize datasets to 1.19 bits/ char and 1.08 bits/ char respectively. section: introduction sequence generation and prediction tasks span many modes of data, ranging from audio and language modelling, to more general timeseries prediction tasks. applications of such models include speech recognition, machine translation, dialogue generation, speech synthesis, forecasting, and music generation, among others. neural networks can be applied to these tasks by predicting sequence elements one-by-one, conditioning on the history of sequence elements, forming an autoregressive model. convolutional neural networks (cnns) and recurrent neural networks (rnns), including long-short term memory (lstm) networks in particular, have achieved many successes at these tasks. however, in their basic form, these models have a limited ability to adapt to recently observed parts of a sequence. many sequences contain repetition; a pattern that occurs once is more likely to occur again. for instance, a word that occurs once in a document is much more likely to occur again. a sequence of handwriting will generally stay in the same handwriting style. a sequence of speech will generally stay in the same voice. although rnns have a hidden state that can summarize the recent past, they are often unable to exploit new patterns that occur repeatedly in a test sequence. this paper concerns dynamic evaluation, which we investigate as a candidate solution to this problem. our approach adapts models to recent sequences using gradient descent based mechanisms. we show several ways to improve on past dynamic evaluation approaches in section [reference], and use our improved methodology to achieve state-of-the-art results in section [reference] and section [reference]. in section [reference] we design a method to dramatically to reduce the number of adaptation parameters in dynamic evaluation, making it practical in a wider range of situations. in section [reference] we analyse dynamic evaluation's performance over varying time-scales and distribution shifts, and demonstrate that dynamically evaluated models can generate conditional samples that repeat many patterns from the conditioning data. section: motivation generative models can assign probabilities to sequences by modelling each term in the factorization given by the product rule. the probability of a sequence factorizes as methods that apply this factorization either use a fixed context when predicting, for instance as in n-grams or cnns, or use a recurrent hidden state to summarize the context, as in an rnn. however, for longer sequences, the history often contains re-occurring patterns that are difficult to capture using models with fixed parameters (static models). in many domains, in a dataset of sequences, each sequence is generated from a slightly different distribution. at any point in time, the history of a sequence contains useful information about the generating distribution for that specific sequence. therefore adapting the model parameters learned during training is justified. we aim to infer a set of model parameters from that will better approximate within sequence. many sequence modelling tasks are characterised by sequences generated from slightly different distributions as in the scenario described above. the generating distribution may also change continuously across a single sequence; for instance, a text excerpt may change topic. furthermore, many machine learning benchmarks do not distinguish between sequence boundaries, and concatenate all sequences into one continuous sequence. thus, many sequence modelling tasks could be seen as having a local distribution as well as a global distribution. during training time, the goal is to find the best fixed model possible for. however, during evaluation time, a model that can infer the current from the recent history has an advantage. section: dynamic evaluation dynamic evaluation methods continuously adapt the model parameters, learned at training time, to parts of a sequence during evaluation. the goal is to learn adapted parameters that provide a better model of the local sequence distribution,. when dynamic evaluation is applied in the present work, a long test sequence is divided up into shorter sequences of length. we define to be a sequence of shorter sequence segments the initial adapted parameters are set to, and used to compute the probability of the first segment,. this probability gives a cross entropy loss, with gradient, which is computed using truncated back-propagation through time. the gradient is used to update the model, resulting in adapted parameters, before evaluating. the same procedure is then repeated for, and for each in the sequence as shown in figure [reference]. gradients for each loss are only backpropagated to the beginning of, so computation is linear in the sequence length. each update applies one maximum likelihood training step to approximate the current local distribution. the computational cost of dynamic evaluation is one forward pass and one gradient computation through the data, with some slight overhead to apply the update rule for every sequence segment. as in all autoregressive models, dynamic evaluation only conditions on sequence elements that it has already predicted, and so evaluates a valid log-probability for each sequence. dynamic evaluation can also be used while generating sequences. in this case, the model generates each sequence segment using fixed weights, and performs a gradient descent based update step on. applying dynamic evaluation for sequence generation could result in generated sequences with more consistent regularities, meaning that patterns that occur in the generated sequence are more likely to occur again. section: background subsection: related approaches adaptive language modelling was first considered for n-grams, adapting to recent history via caching, and other methods. more recently, the neural cache approach and the closely related pointer sentinel-lstm have been used to for adaptive neural language modelling. neural caching has recently been used to improve the state-of-the-art at word-level language modelling. the neural cache model learns a type of non-parametric output layer on the fly at test time, which allows the network to adapt to recent observations. each past hidden state is paired with the next input, and is stored as a tuple. when a new hidden state is observed, the output probabilities are adjusted to give a higher weight to output words that coincided with past hidden states with a large inner product. where is a one hot encoding of, and is a scaling parameter. the cache probabilities are interpolated with the base network probabilities to adapt the base network at test time. the neural cache closely relates to dynamic evaluation, as both methods can be added on top of a base model for adaptation at test time. the main difference is the mechanism used to fit to recent history: the neural cache approach uses a non-parametric, nearest neighbours-like method, whereas dynamic evaluation uses a gradient descent based method to change model parameters dynamically. both methods rely on an autoregressive factorisation, as they depend on observing sequence elements after they are predicted in order to perform adaptation. dynamic evaluation and neural caching methods are therefore both applicable to sequence prediction and generation tasks, but not directly to more general supervised learning tasks. one drawback of the neural cache method is that it can not adjust the recurrent hidden state dynamics. as a result, the neural cache's ability to capture information that occurs jointly between successive sequence elements is limited. this capability is critical for adapting to sequences where each element has very little independent meaning, e.g. character level language modelling. subsection: dynamic evaluation in neural networks dynamic evaluation of neural language models was proposed by. their approach simply used stochastic gradient descent (sgd) updates at every time step, computing the gradient with fully truncated backpropagation through time, which is equivalent to setting in equation ([reference]). dynamic evaluation has since been applied to character and word-level language models. previous work using dynamic evaluation considered it as an aside, and did not explore it in depth. section: update rule methodology for dynamic evaluation we propose several changes to's dynamic evaluation method with sgd and fully truncated backpropagation, which we refer to as traditional dynamic evaluation. the first modification reduces the update frequency, so that gradients are backpropagated over more timesteps. this change provides more accurate gradient information, and also improves the computational efficiency of dynamic evaluation, since the update rule is applied much less often. we use sequence segments of length 5 for word-level tasks and 20 for character-level tasks. next, we add a global decay prior to bias the model towards the parameters learned during training. our motivation for dynamic evaluation assumes that the local generating distribution is constantly changing, so it is potentially desirable to weight recent sequence history higher in adaptation. adding a global decay prior accomplishes this by causing previous adaptation updates to decay exponentially over time. for sgd with a global prior, learning rate and decay rate; we form the update rule we then consider using an rmsprop derived update rule for the learning rule in place of sgd. rmsprop uses a moving average of recent squared gradients to scale learning rates for each weight. in dynamic evaluation, near the start of a test sequence, rmsprop has had very few gradients to average, and therefore may not be able to leverage its updates as effectively. for this reason, we collect mean squared gradients,, on the training data rather than on recent test data (which is what rmsprop would do). is given by where is the number of training batches and is the gradient on the th training batch. the mini-batch size for this computation becomes a hyper-parameter, as larger mini-batches will result in smaller mean squared gradients. the update rule, which we call rms with a global prior in our experiments, is then where is a stabilization parameter. for the decay step of our update rule, we also consider scaling the decay rate for each parameter proportionally to. parameters with a high rms gradient affect the dynamics of the network more, so it makes sense to decay them faster. is divided by its mean, resulting in a normalized version of with a mean of 1: we clip the values of to be no greater than to be sure that the decay rate does not exceed for any parameter. combining the learning component and the regularization component results in the final update equation, which we refer to as rms with an rms global prior section: sparse dynamic evaluation mini-batching over sequences is desirable for some test-time sequence modelling applications because it allows faster processing of multiple sequences in parallel. dynamic evaluation has a high memory cost for mini-batching because it is necessary to store a different set of parameters for each sequence in the mini-batch. therefore, we consider a sparse dynamic evaluation variant that updates a smaller number of parameters. we introduce a new adaptation matrix which is initialized to zeros. multiplies hidden state vector of an rnn at every time-step to get a new hidden state, via then replaces and is propagated throughout the network via both recurrent and feed-forward connections. applying dynamic evaluation to avoids the need to apply dynamic evaluation to the original parameters of the network, reduces the number of adaptation parameters, and makes mini-batching less memory intensive. we reduce the number of adaptation parameters further by only using to transform an arbitrary subset of hidden units. this results in being an matrix with adaptation parameters. if is chosen to be much less than the number of hidden units, this reduces the number of adaptation parameters dramatically. in section [reference] we experiment with sparse dynamic evaluation for character-level language models. section: experiments we applied dynamic evaluation to word-level and character-level language modelling. in all tasks, we evaluate dynamic evaluation on top of a base model. after training the base model, we tune hyper-parameters for dynamic evaluation on the validation set, and evaluate both the static and dynamic versions of the model on the test set. we also consider follow up experiments that analyse the sequence lengths for which dynamic evaluation is useful. code for our dynamic evaluation methodology is available. subsection: word-level language modelling we train base models on the penn treebank and wikitext-2 datasets, and compare the performance of static and dynamic evaluation. these experiments compare dynamic evaluation against past approaches such as the neural cache and measure dynamic evaluation's general performance across different models and datasets. ptb is derived from articles of the wall street journal. it contains 929k training tokens and a vocab size limited to 10k words. it is one of the most commonly used benchmarks in language modelling. we consider two baseline models on ptb, a standard lstm implementation with recurrent dropout, and the recent state-of-the-art awd-lstm. our standard lstm was taken from the chainer tutorial on language modelling, and used two lstm layers with 650 units each, trained with sgd and regularized with recurrent dropout. on our standard lstm, we experiment with traditional dynamic evaluation as applied by, as well as each modification we make building up to our final update rule as described in section [reference]. as our final update rule (rms+ rms global prior) worked best, we use this for all other experiments and use ''dynamic eval'' by default to refer to this update rule in tables. we applied dynamic evaluation on a more powerful model, the asgd weight-dropped lstm. the awd-lstm is a vanilla lstm that combines the use of drop-connect on recurrent weights for regularization, and a variant of averaged stochastic gradient descent for optimisation. our model, which used 3 layers and tied input and output embeddings, was intended to be a direct replication of awd-lstm, using code from their implementation. results are given in table [reference]. dynamic evaluation gives significant overall improvements to both models on this dataset. dynamic evaluation also achieves better final results than the neural cache on both a standard lstm and the awd-lstm reimplementation, and improves the state-of-the-art on ptb. wikitext-2 is roughly twice the size of ptb, with 2 million training tokens and a vocab size of 33k. it features articles in a non-shuffled order, with dependencies across articles that adaptive methods should be able to exploit. for this dataset, we use the same baseline lstm implementation and awd-lstm re-implementation as on ptb. results are given in table [reference]. dynamic evaluation improves the state-of-the-art perplexity on wikitext-2, and provides a significantly greater improvement than neural caching to both base models. this suggests that dynamic evaluation is effective at exploiting regularities that co-occur across non-shuffled documents. subsection: character-level language modelling we consider dynamic evaluation on the text8, and hutter prize datasets. the hutter prize dataset is comprised of wikipedia text, and includes xml and characters from non-latin languages. it is 100 million utf-8 bytes long and contains 205 unique bytes. similarly to other reported results, we use a 90-5-5 split for training, validation, and testing. the text8 dataset is also derived from wikipedia text, but has all xml removed, and is lower cased to only have 26 characters of english text plus spaces. as with hutter prize, we use the standard 90-5-5 split for training, validation, and testing for text8. we used a multiplicative lstm (mlstm) as our base model for both datasets. the mlstms for both tasks used 2800 hidden units, an embedding layer of 400 units, weight normalization, variational dropout, and adam for training. we also consider sparse dynamic evaluation, as described in section [reference], on the hutter prize dataset. for sparse dynamic evaluation, we adapted a subset of hidden units, resulting in a adaptation matrix and 250k adaptation parameters. all of our dynamic evaluation results in this section use the final update rule given in section [reference]. results for hutter prize are given in table [reference], and results for text8 are given in table [reference]. dynamic evaluation achieves large improvements to our base models and state-of-the-art results on both datasets. sparse dynamic evaluation also achieves significant improvements on hutter prize using only 0.5% of the adaptation parameters of regular dynamic evaluation. subsection: time-scales of dynamic evaluation we measure time-scales at which dynamic evaluation gains an advantage over static evaluation. starting from the model trained on hutter prize, we plot the performance of static and dynamic evaluation against the number of characters processed on sequences from the hutter prize test set, and sequences in spanish from the european parliament dataset. the hutter prize data experiments show the timescales at which dynamic evaluation gained the advantage observed in table [reference]. we divided the hutter prize test set into 500 sequences of length 10000, and applied static and dynamic evaluation to these sequences using the same model and methodology used to obtain results in table [reference]. losses were averaged across these 500 sequences to obtain average losses at each time step. plots of the average cross-entropy errors against the number of hutter characters sequenced are given in figure [reference]. the spanish experiments measure how dynamic evaluation handles large distribution shifts between training and test time, as hutter prize contains very little spanish. we used the first 5 million characters of the spanish european parliament data in place of the hutter prize test set. the spanish experiments used the same base model and dynamic evaluation settings as hutter prize. plots of the average cross-entropy errors against the number of spanish characters sequenced are given in figure [reference]..4.4 on both datasets, dynamic evaluation gave a very noticeable advantage after a few hundred characters. for spanish this advantage continued to grow as more of the sequence was processed, whereas for hutter, this advantage was maximized after viewing around 2-3k characters. the advantage of dynamic evaluation was also much greater on spanish sequences than hutter sequences. we also drew 300 character conditional samples from the static and dynamic versions of our model after viewing 10k characters of spanish. for the dynamic model, we continued to apply dynamic evaluation during sampling as well, by the process described in section [reference]. the conditional samples are given in the appendix. the static samples quickly switched to english that resembled hutter prize data. the dynamic model generated data with some spanish words and a number of made up words with characteristics of spanish words for the entirety of the sample. this is an example of the kinds of features that dynamic evaluation was able to learn to model on the fly. section: conclusion this work explores and develops methodology for applying dynamic evaluation to sequence modelling tasks. experiments show that the proposed dynamic evaluation methodology gives large test time improvements across character and word level language modelling. our improvements to language modelling have applications to speech recognition and machine translation over longer contexts, including broadcast speech recognition and paragraph level machine translation. overall, dynamic evaluation is shown to be an effective method for exploiting pattern re-occurrence in sequences. plus 0.3ex bibliography: references appendix: appendix subsection: dynamic samples conditioned on spanish 300 character samples generated from the dynamic version of the model trained on hutter prize, conditioned on 10k of spanish characters. the final sentence fragment of the 10k conditioning characters is given to the reader, with the generated text given in bold: tiene importancia este compromiso en la medida en que la comisi\u00f3n es un organismo que tiene el montembre tas proced\u00edns la conscriptione se ha tesalo del p\u00f3mienda que et hanemos que pe la siemina. de la pedrera orden es se\u00f1ora presidente civil, orden de siemin presente relevante fr\u00f3nmida que esculdad pludiore e formidad president de la presidenta antidorne adamirmidad i ciemano de el 200'. fo subsection: static samples conditioned on spanish 300 character samples generated from the static version of the model trained on hutter prize, conditioned on 10k of spanish characters. the final sentence fragment of the 10k conditioning characters is given to the reader, with the generated text given in bold: tiene importancia este compromiso en la medida en que la comisi\u00f3n es un organismo que tiene el monde,& lt;br& gt;there is a secret act in the world except cape town, seen in now flat comalo and ball market and has seen the closure of the eagle as imprints in a dallas within the country.& quot; is a topic for an increasingly small contract saying allan roth acquired the government in [[1916]].===",
    "templates": [
        {
            "Material": [
                [
                    [
                        "hutter prize",
                        605
                    ],
                    [
                        "hutter prize datasets",
                        16159
                    ],
                    [
                        "hutter",
                        18581
                    ]
                ]
            ],
            "Method": [],
            "Metric": [],
            "Task": [
                [
                    [
                        "language modelling",
                        791
                    ],
                    [
                        "adaptive language modelling",
                        6425
                    ],
                    [
                        "character level language modelling",
                        8463
                    ],
                    [
                        "word-level language modelling",
                        13431
                    ],
                    [
                        "character-level language modelling",
                        16075
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "penn treebank",
                        462
                    ],
                    [
                        "ptb",
                        13775
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "word-level perplexities",
                        431
                    ],
                    [
                        "perplexity",
                        15824
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "language modelling",
                        791
                    ],
                    [
                        "adaptive language modelling",
                        6425
                    ],
                    [
                        "character level language modelling",
                        8463
                    ],
                    [
                        "word-level language modelling",
                        13431
                    ],
                    [
                        "character-level language modelling",
                        16075
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "text8",
                        595
                    ],
                    [
                        "english text",
                        16590
                    ]
                ]
            ],
            "Method": [],
            "Metric": [],
            "Task": [
                [
                    [
                        "language modelling",
                        791
                    ],
                    [
                        "adaptive language modelling",
                        6425
                    ],
                    [
                        "character level language modelling",
                        8463
                    ],
                    [
                        "word-level language modelling",
                        13431
                    ],
                    [
                        "character-level language modelling",
                        16075
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "wikitext-2 datasets",
                        480
                    ],
                    [
                        "wikitext-2",
                        15400
                    ],
                    [
                        "wikipedia text",
                        16223
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "word-level perplexities",
                        431
                    ],
                    [
                        "perplexity",
                        15824
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "language modelling",
                        791
                    ],
                    [
                        "adaptive language modelling",
                        6425
                    ],
                    [
                        "character level language modelling",
                        8463
                    ],
                    [
                        "word-level language modelling",
                        13431
                    ],
                    [
                        "character-level language modelling",
                        16075
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "2ead783089fc757052abb908287a2fb743a4ebef-47",
    "doctext": "document: squeezed very deep convolutional neural networks for text classification most of the research in convolutional neural networks has focused on increasing network depth to improve accuracy, resulting in a massive number of parameters which restricts the trained network to platforms with memory and processing constraints. we propose to modify the structure of the very deep convolutional neural networks (vdcnn) model to fit mobile platforms constraints and keep performance. in this paper, we evaluate the impact of temporal depthwise separable convolutions and global average pooling in the network parameters, storage size, and latency. the squeezed model (svdcnn) is between 10x and 20x smaller, depending on the network depth, maintaining a maximum size of 6 mb. regarding accuracy, the network experiences a loss between 0.4% and 1.3% and obtains lower latencies compared to the baseline model. section: introduction the general trend in deep learning approaches has been developing models with increasing layers. deeper neural networks have achieved high-quality results in different tasks such as image classification, detection, and segmentation. deep models can also learn hierarchical feature representations from images. in the natural language processing (nlp) field, the belief that compositional models can also be used to text-related tasks is more recent. the increasing availability of text data motivates research for models able to improve accuracy in different language tasks. following the image classification convolutional neural network (cnn) tendency, the research in text classification has placed effort into developing deeper networks. the first cnn based approach for text was a shallow network with one layer. following this work, deeper architectures were proposed. conneau et al. were the first to propose very deep convolutional neural networks (vdcnn) applied to text classification. vdcnn accuracy increases with depth. the approach with 29 layers is the state-of-the-art accuracy of cnns for text classification. however, regardless of making networks deeper to improve accuracy, little effort has been made to build text classification models to constrained resources. it is a very different scenario compared to image approaches, where size and speed constrained models have been proposed. in real-world applications, size and speed are constraints to an efficient mobile and embedded deployment of deep models. several relevant real-world applications depend on text classification tasks such as sentiment analysis, recommendation and opinion mining. the appeal for these applications combined with the boost in mobile devices usage motivates the need for research in restrained text classification models. concerning mobile development, there are numerous benefits to developing smaller models. some of the most relevant are requiring fewer data transferring while updating the client model and increasing usability by diminishing the inference time. such advantages would boost the usage of deep neural models in text-based applications for embedded platforms. in this paper, we investigate modifications on the network proposed by conneau et al. with the aim of reducing its number of parameters, storage size and latency with minimal performance degradation. to achieve these improvements we used temporal depthwise separable convolution and global average pooling techniques. therefore, our main contribution is to propose the squeezed very deep convolutional neural networks (svdcnn), a text classification model which requires significantly fewer parameters compared to the state-of-the-art cnns. section ii provides an overview of the state-of-the-art in cnns for text classification. section iii presents the vdcnn model. section iv explains the proposed model svdcnn and the subsequent impact in the total number of parameters of the network. section v details how we perform experiments. section vi analyses the results and lastly, section vii, presents conclusions and direction for future works. section: related work cnns were originally designed for computer vision with the aim of considering feature extraction and classification as one task. although cnns are very successful in image classification tasks, its use in text classification is relatively new and has some peculiarities. contrasting with traditional image bi-dimensional representations, texts are one-dimensionally represented. due to this property, the convolutions are designed as temporal convolutions. furthermore, it is necessary to generate a numerical representation from the text so the network can be trained using this representation. this representation, namely embeddings, is usually obtained through the application of a lookup table, generated from a given dictionary. an early approach for text classification tasks consisted of a shallow neural network working on the word level and using only one convolutional layer. the author reported results in smaller datasets. later, zhang et al. proposed the first cnn performing on a character level (char-cnn), which allowed them to train up to 6 convolutional layers, followed by three fully connected classification layers. char-cnn uses convolutional kernels of size 3 and 7, as well as simple max-pooling layers. conneau et al. (2016) proposed the very deep cnn (vdcnn) also on a character level, presenting improvements compared to char-cnn. conneau et al. (2016) have shown that text classification accuracy increases when the proposed model becomes deeper. vdcnn uses only small kernel convolutions and pooling operations. the proposed architecture relies on the vgg and resnet philosophy: the number of feature maps and the temporal resolution is modeled so that their product is constant. this approach makes it easier to control the memory footprint of the network. both zhang and conneau et al. cnns utilized standard convolutional blocks and fully connected layers to combine convolution information. this architecture choice increases the number of parameters and storage size of the models. however, size and speed was not the focus of those works. the idea of developing smaller and more efficient cnns without losing representative accuracy is a less explored research direction in nlp, but it has already been a trend for computer vision applications. most approaches consist in compressing pre-trained networks or training small networks directly. a recent tendency in deep models is replacing standard convolutional blocks with depthwise separable convolutions (dscs). the purpose is to reduce the number of parameters and consequently the model size. dscs were initially introduced in and since then have been successfully applied to image classification and machine translation to reduce the computation in convolutional blocks. another approach is the use of a global average pooling (gap) layer at the output of the network to replace fully connected layers. this approach has become a standard architectural decision for newer cnns. section: vdcnn model for text classification the vdcnn is a modular architecture for text classification tasks developed to offer different depth levels (9, 17, 29 and 49). fig. [reference] presents the architecture for depth 9. the network begins with a lookup table, which generates the embeddings for the input text and stores them in a 2d tensor of size. the number of input characters is fixed to 1, 024 while the embedding dimension is 16. the embedding dimension can be seen as the number of rgb channels of an image. the following layer (3, temp convolution, 64) applies 64 temporal convolutions of kernel size 3, so the output tensor has size. its primary function is to fit the lookup table output with the modular network segment input composed by convolutional blocks. each aforenamed block is a sequence of two temporal convolutional layers, each one accompanied by a temporal batch normalization layer and a relu activation. besides, the different network depths are obtained varying the number of convolutional blocks. as a convention, the depth of a network is given as its total number of convolutions. for instance, the architecture of depth 17 has two convolutional blocks of each level of feature maps, which results in 4 convolutional layers for each level (see table [reference]). considering the first convolutional layer of the network, we obtain the depth. the different depth architectures provided by vdcnn model are summarized in table [reference]. the following rule is employed to minimize the network's memory footprint: before each convolutional block doubling the number of feature maps, a pooling layer halves the temporal dimension. this strategy is inspired by the vgg and resnets philosophy and results in three levels of feature maps: 128, 256 and 512 (see fig. [reference]). additionally, the vdcnn network also contains shortcut connections for each convolutional blocks implemented through the usage of convolutions. lastly, for the classification task, the most valuable features are extracted using-max pooling, generating a one-dimensional vector which supplies three fully connected layers with relu hidden units and softmax outputs. the number of hidden units is 2, 048, and they do not use dropout but rather batch normalization after convolutional layers perform the network regularization. section: svdcnn model for text classification the primary objective is reducing the number of parameters so that the resulting network has a significative lower storage size. we first propose to modify the convolutional blocks of vdcnn model by the usage of temporal depthwise separable convolutions (tdscs). next, we reduce the number of fully connected layers using the global average pooling (gap) technique. the resulting proposed architecture is called squeezed very deep convolutional neural networks (svdcnn). paragraph: temporal depthwise separable convolutions (tdscs) the use of tdscs over standard convolutions allowed reducing the number of parameters without relevant accuracy loss. tdscs work decompounding the standard convolution into two parts: depthwise and pointwise. the first one is responsible for applying a convolutional filter to each channel of the input at a time. for an image input, one possibility of channels are the rgb components, whereas in a text input the dimensions of the embedding can be used instead. for both cases mentioned above, the result is one feature map by channel. the second convolution unifies the generated feature maps successively applying 1x1 convolutions so that the target amount of feature maps can be achieved. [] [] tdscs are dscs which work with one-dimensional convolutions. although dscs hold verified results in image classification networks, the usage of its temporal version for text related tasks is less explored. fig. 2a presents the architecture of a temporal standard convolution while fig. 2b presents the tdsc. for a more formal definition, let be the number of parameters of a temporal standard convolution, where in and out are the numbers of input and output channels respectively, and is the kernel size: alternatively, a tdsc achieves fewer parameters (): in the vdcnn model, one convolutional block is composed of two temporal standard convolutional layers. the first one doubles the number of feature maps while the second keeps the same value received as input. besides, each convolutional layer is followed by a batch normalization and a relu layers. in our model, we proposed changing the temporal standard convolutions by tdscs. [] [] fig. 3 presents the standard convolutional block on the left and the proposed convolutional block using tdsc on the right. the pattern used in the figure for the convolutional layers is the following:\" kernel size, conv type, output feature maps\"; as a brief example consider\" 3x1, temporal conv, 256\", which means a temporal convolution with kernel size 3 and 256 feature maps as output. from equation 1, we have the number of parameters of the original convolutional block () as follows: moreover, from equation 2, the number of parameters of the proposed convolutional block () that uses tdsc being: for illustration, following the same characteristics of fig. 3, consider that the number of input channels in is equal to 128 and the number of output channels out is equal to 256. our proposed approach accumulates a total of 99, 456 parameters. in contrast, there are 294, 912 parameters in the original convolutional block. the use of tdsc yields a reduction of 66.28% in the network size. lastly, since each standard temporal convolution turns into two (depthwise and pointwise), the number of convolutions per convolutional block has doubled. nevertheless, these two convolutions work as one because it is not possible to use them separately keeping the same propose. in this way, we count them as one layer in the network depth. this decision holds the provided depth architectures the same as the vdcnn model summarized in table [reference], contributing to a proper comparison between the models. paragraph: global average pooling (gap) the vdcnn model uses a-max pooling layer followed by three fully connected (fc) layers to perform the classification task (fig. 4a). although this approach is the traditional architecture choice for text classification cnns, it introduces a significant number of parameter in the network. the resulting number of the fc layers parameters () aforementioned is presented below, for a problem with four target classes: [] [] instead of maintaining these fully connected layers, we directly aggregate the output of the last convolutional block through the usage of an average pooling layer. this method, known as global average pooling, contributes substantially to the parameters reduction without degrading the network accuracy significantly. the number of resulting feature maps given by the average pooling layer was the same as the original-max pooling layer. fig. 4b presents this proposed modification. the number of parameters obtained by the usage of gap () is revealed as follows: our proposed approach accumulates a total of 16, 384 parameters. in contrast, there are 12, 591, 104 parameters in the original classification method. the use of gap yields a reduction of 99.86%. section: experiments the experiment goal is to investigate the impact of modifying the convolutional block of vdcnn to tdscs and using gap instead of the original fully connected layers. we evaluate char-cnn, vdcnn, and svdcnn according to the number of parameters, storage size, inference time and accuracy. the source code of the proposed model is available in the github repository svdcnn the original vdcnn paper reported the number of parameters of the convolutional layers, in which we reproduce in this article. for svdcnn and char-cnn, we calculated the abovementioned number from the network architecture implemented in pytorch. as for the fc layer's parameters, the number is obtained as the summation of the product of the input and output size of each fc layer for each cnn. considering the network parameters and assuming that one float number on cuda environment takes 4 bytes, we can calculate the network storage in megabytes, for all the models, as follows: regarding the inference time, its average and standard deviation were calculated as the time to predict one instance of the ag's news dataset throughout 1, 000 repetitions. the svdcnn experimental settings are similar to the original vdcnn paper, using the same dictionary and the same embedding size of 16. the training is also performed with sgd, utilizing size batch of 64, with a maximum of 100 epochs. we use an initial learning rate of 0.01, a momentum of 0.9 and a weight decay of 0.001. all the experiments were performed on an nvidia gtx 1060 gpu+ intel core i7 4770s cpu. the model's performance is evaluated on three large-scale public datasets also used by zhang et al. in the introduction of char-cnn and vdcnn models. table [reference] presents the details of the utilized datasets: ag's news, yelp polarity and yelp full. section: results table [reference] presents the number of parameters, storage size, and accuracy for the svdcnn, vdcnn, and char-cnn in all datasets. the use of tdscs promoted a significant reduction in convolutional parameters compared to vdcnn. for the most in-depth network evaluated, which contains 29 convolutional layers (depth 29), the number of parameters of these convolutional layers had a reduction of 66.08%, from 4.6 to 1.56 million parameters. this quantity is slightly larger than the one obtained from the char-cnn, 1.40 million parameters, but this network has only six convolutional layers (depth 6). the network reduction obtained by the gap is even more representative since both compared models use three fc layers for their classification tasks. considering a dataset with four target classes, and comparing svdcnn with vdcnn, the number of parameters of the fc layers has passed from 12.59 to 0.02 million parameters, representing a reduction of 99.84%. following with the same comparison, but to char-cnn, the proposed model is 99.82% smaller, 0.02 against 11.36 million of fc parameters. the reduction of the total parameters impacts directly on the storage size of the networks. while our most in-depth model (29) occupies only 6 mb, vdcnn with the same depth occupies 64.16 mb of storage. likewise, char-cnn (which has depth 6) occupies 43.25 mb. this reduction is a significant result because many embedded platforms have several memory constraints. for example, fpgas often have less than 10 mb of on-chip memory and no off-chip memory or storage. regarding accuracy results, usually, a model with such parameter reduction should present some loss of accuracy in comparison to the original model. nevertheless, the performance difference between vdcnn and svdcnn models varies between 0.4 and 1.3%, which is pretty modest considering the parameters and storage size reduction aforementioned. in table [reference], it is possible to see the accuracy scores obtained by the compared models. another two fundamental results obtained are a) the base property of vdcnn model is preserved on its squeezed model: the performance still increasing up with the depth and b) the performance evaluated for the most extensive dataset, i.e., yelp review (62.30%), still overcomes the accuracy of the char-cnn model (62.05%). deep learning processing architecture has the property of being high parallelizable; it is expected smaller latencies when performing inferences in hardware with high parallelization power. despite this property, the model ability to use all hardware parallel potential available also depends on the network architecture. the more parameters per layers, the more parallelizable a model tends to be, while the increase of the depth gets the opposite result. another natural comprehension fact is if a model has few parameters, there exists less content to be processed, and then we have a faster inference time. concerning mobile devices, the presence of dedicated hardware for deep learning is not entirely feasible. this hardware usually requires more energy and dissipates more heat, two undesirable features for a mobile platform. therefore, obtaining fewer inference times, even out of environments with high parallelization capabilities, is a pretty desirable characteristic for a model designed to work on mobile platforms. the latency ratio between cpu and gpu inference times indicates how undependable of dedicated hardware a model is, with higher values meaning more independence. the inference times obtained for the three models compared are available in table [reference]. as explained in section iv a), each convolutional layer of the convolutional blocks was substituted by two convolutions. this change could impact the inference time negatively, but the significant parameter reduction allows the svdcnn to obtain better results than the vdcnn model. the cpu inference time obtained by the proposed model was smaller than the base model for the depth 9 (25.88ms against 29, 13ms) and depth 17 (47.80ms against 48.05ms), while the ratio was higher for all depths (0.20 against 0.15 in average). these results, as explained above, are pretty significant for mobile platforms. looking to char-cnn, this model got notably inferior results compared to the proposed method, with 313.53ms of cpu inference time and ratio of 0.03. section: conclusion in this paper, we presented a squeezed version of the vdcnn model considering the number of parameters and size. the new model proprieties became it feasible for mobile platforms. to achieve this goal, we analyzed the impact of including temporal depthwise separable convolutions and a global average pooling layer in a very deep convolutional neural network for text classification. the svdcnn model reduces about 92.45% the number of parameters and storage size while presents an inference time ratio (cpu/ gpu), 31.94% higher. for future works, we plan to evaluate other techniques able to reduce storage size, such as model compression. moreover, the model accuracy over even more massive datasets will be evaluated as well as the efficiency of its depth 49 configuration. section: acknowledgment we would like to thank facepe and cnpq (brazilian research agencies) for financial support. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "ag's news",
                        16109
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "squeezed very deep convolutional neural networks",
                        10
                    ],
                    [
                        "svdcnn",
                        670
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "text classification",
                        63
                    ],
                    [
                        "image classification",
                        1114
                    ],
                    [
                        "text classification tasks",
                        2511
                    ],
                    [
                        "classification",
                        4181
                    ],
                    [
                        "image classification tasks",
                        4246
                    ],
                    [
                        "classification task",
                        9022
                    ],
                    [
                        "image classification networks",
                        10764
                    ],
                    [
                        "text classification cnns",
                        13353
                    ],
                    [
                        "classification tasks",
                        16895
                    ]
                ]
            ]
        },
        {
            "Material": [],
            "Method": [
                [
                    [
                        "squeezed very deep convolutional neural networks",
                        10
                    ],
                    [
                        "svdcnn",
                        670
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "sentiment analysis",
                        2545
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "3070a1bd503c3767def898bbd50c7eea2bbf29c9-48",
    "doctext": "document: wider or deeper: revisiting the resnet model for visual recognition the trend towards increasingly deep neural networks has been driven by a general observation that increasing depth increases the performance of a network. recently, however, evidence has been amassing that simply increasing depth may not be the best way to increase performance, particularly given other limitations. investigations into deep residual networks have also suggested that they may not in fact be operating as a single deep network, but rather as an ensemble of many relatively shallow networks. we examine these issues, and in doing so arrive at a new interpretation of the unravelled view of deep residual networks which explains some of the behaviours that have been observed experimentally. as a result, we are able to derive a new, shallower, architecture of residual networks which significantly outperforms much deeper models such as resnet-200 on the imagenet classification dataset. we also show that this performance is transferable to other problem domains by developing a semantic segmentation approach which outperforms the state-of-the-art by a remarkable margin on datasets including pascal voc, pascal context, and cityscapes. the architecture that we propose thus outperforms its comparators, including very deep resnets, and yet is more efficient in memory use and sometimes also in training time. the code and models are available at. section: introduction the convolutional networks used by the computer vision community have been growing deeper and deeper each year since krizhevsky et al. proposed alexnet in 2012. the deepest network in the literature is a residual network (resnet) with 1, 202 trainable layers, which was trained using the tiny images in the cifar-10 dataset. the image size here is important, because it means that the size of corresponding feature maps is relatively small, which is critical in training extremely deep models. most networks operating on more practically interesting image sizes tend to have the order of one, to two, hundred layers, the 200-layer resnet and 96-layer inception-resnet. the progression to deeper networks continues, however, with zhao et al. having trained a 269-layer network for semantic image segmentation. these networks were trained using the imagenet classification dataset, where the images are of much higher resolution. each additional layer requires not only additional memory, but also additional training. the marginal gains achieved by each additional layer diminish with depth, however, to the point where zhao et al. achieved only an improvement of 1.1% (from 42.2% to 43.3% by mean intersection-over-union scores) after almost doubling the number of layers (from 152 to 269). on the other hand, zagoruyko and komodakis showed that it is possible to train much shallower but wider networks on cifar-10, which outperform a resnet with its more than one thousand layers. the question thus naturally arises as to whether deep, or wide, is the right strategy. in order to examine the issue we first need to understand the mechanism behind resnets. veit et al. have claimed that they actually behave as exponential ensembles of relatively shallow networks. however, there is a gap between their proposed unravelled view of a resnet, and a real exponential ensemble of sub-networks, as illustrated in the top row of fig. [reference]. since the residual units are non-linear, we can not further split the bottom path into two sub-networks, i.e., and. it turns out that resnets are only assembling linearly growing numbers of sub-networks. besides, the key characteristic of our introduced view is that it depends on the effective depth of a network. this amounts to the number of residual units which backward gradients during training can go through. when, the two-unit resnet in fig. [reference] can be seen as an ensemble of three sub-networks, i.e.,,, and, as shown in the bottom left. when, nothing changes except that we replace the third sub-network with a shallower one, as shown in the bottom right example. the superscripts in and denote their actual depths. about the unravelled view, the effective depth of a resnet, and the actual depth of a sub-network, more details will be provided in the sequence. it is also worth noting that veit et al. empirically found that most gradients in a 110-layer resnet can only go through up to seventeen residual units, which supports our above hypothesis that the effective depth exists for a specific network. in this paper, our contributions include: we introduce a further developed intuitive view of resnets, which helps us understand their behaviours, and find possible directions to further improvements. we propose a group of relatively shallow convolutional networks based on our new understanding. some of them achieve the state-of-the-art results on the imagenet classification dataset. we evaluate the impact of using different networks on the performance of semantic image segmentation, and show these networks, as pre-trained features, can boost existing algorithms a lot. we achieve the best results on pascal voc, pascal context, and cityscapes. section: related work our work here is closely related to two topics, residual network (resnet) based image classification and semantic image segmentation using fully convolutional networks. as we have noted above, he et al. recently proposed the resnets to combat the vanishing gradient problem during training very deep convolutional networks. resnets have outperformed previous models at a variety of tasks, such as object detection and semantic image segmentation. they are gradually replacing vggnets in the computer vision community, as the standard feature extractors. nevertheless, the real mechanism underpinning the effectiveness of resnets is not yet clear. veit et al. claimed that they behave like exponential ensembles of relatively shallow networks, yet the 'exponential' nature of the ensembles has yet to be theoretically verified. residual units are usually non-linear, which prevents a resnet from exponentially expanding into separated sub-networks, as illustrated in fig. [reference]. it is also unclear as to whether a residual structure is required to train very deep networks. for example, szegedy et al. showed that it is 'not very difficult' to train competitively deep networks, even without residual shortcuts. currently, the most clear advantage of resnets is in their fast convergence. szegedy et al. observed similar empirically results to support that. on the other hand, zagoruyko and komodakis found that a wide sixteen-layer resnet outperformed the original thin thousand-layer resnet on datasets composed of tiny images such as cifar-10. the analysis we present here is motivated by their empirical testing, but aims at a more theoretical approach, and the observation that a grid search of configuration space is impractical on large scale datasets such as the imagenet classification dataset. semantic image segmentation amounts to predicting the categories for each pixel in an image. long et al. proposed the fully convolutional networks (fcn) to this end. fcns soon became the mainstream approach to dense prediction based tasks, especially due to its efficiency. besides, empirical results in the literature showed that stronger pre-trained features can yet further improve their performance. we thus here base our semantic image segmentation approach on fully convolutional networks, and will show the impact of different pre-trained features on final segmentation results. section: residual networks revisited we are concerned here with the full pre-activation version of residual networks (resnet). for shortcut connections, we consider identity mappings only. we omit the raw input and the top-most linear classifier for clarity. usually, there may be a stem block or several traditional convolution layers directly after the raw input. we omit these also, for the purpose of simplicity. for the residual unit, let be the input, and let be its trainable non-linear mappings, also named block. the output of unit is recursively defined as: where denotes the trainable parameters, and is often two or three stacked convolution stages. in the full pre-activation version, the components of a stage are in turn a batch normalization, a rectified linear unit (relu) non-linearity, and a convolution layer. subsection: residual networks unravelled online applying eqn. ([reference]) in one substitution step, we expand the forward pass into: which describes the unravelled view by veit et al., as shown in the top row of fig. [reference]. since is non-linear, we can not derive eqn. ([reference]) from eqn. ([reference]). so the whole network is not equivalent to an exponentially growing ensemble of sub-networks. it is rather, more accurately, described as a linearly growing ensemble of sub-networks. for the two-unit resnet as illustrated in fig. [reference], there are three, e.g.,,, and, sub-networks respectively corresponding to the three terms in eqn. ([reference]), i.e.,,, and. veit et al. in showed that the paths which gradients take through a resnet are typically far shorter than the total depth of that network. they thus introduced the idea of effective depth as a measure for the true length of these paths. by characterising the units of a resnet given its effective depth, we illuminate the impact of varying paths that gradients actually take, as in fig. [reference]. we here illustrate this impact in terms of small effective depths, because to do so for larger ones would require diagrams of enormous networks. the impact is the same, however. take the resnet in fig. [reference] for example again. in an sgd iteration, the backward gradients are: where denotes the derivative of to its input. when effective depth, both terms in eqn. ([reference]) are non-zeros, which corresponds to the bottom-left case in fig. [reference]. namely, block 1 receives gradients from both and. however, when effective depth, the gradient vanishes after passing through block 2. namely,. so, the second term in eqn. ([reference]) also goes to zeros, which is illustrated by the bottom-right case in fig. [reference]. the weights in block 1 indeed vary across different iterations, but they are updated only by. to, block 1 is no more than an additional input providing preprocessed representations, because block 1 is not end-to-end trained, from the point of view of. in this case, we name to have an actual depth of one. we say that the resnet is over-deepened, and that it can not be trained in a fully end-to-end manner, even with those shortcut connections. let be the total number of residual units. we can see a resnet as an ensemble of different sub-networks, i.e.,. the actual depth of is. we show an unravelled three-unit resnet with different effective depths in fig. [reference]. by way of example, note that in fig. [reference] contains only block 1, whereas contains both block 1 and block 2. among the cases illustrated, the bottom left example is more complicated, where and. from the point of view of, the gradient of block 1 is, where the first term is non-zero. will thus update block 1 at each iteration. considering the non-linearity in block 3, it is non-trivial to tell if this is as good as the fully end-to-end training case, as illustrated by in the top right example. an investigation of this issue remains future work. subsection: residual networks behaviours revisited very deep resnets. conventionally, it is not easy to train very deep networks due to the vanishing gradient problem. to understand how a very deep resnet is trained, the observation by veit et al. is important, i.e., gradients vanish exponentially as the length of paths increases. now refer to the top-right example in fig. [reference]. this is somewhat similar to the case of a shallow or reasonably deep resnet, when. at the beginning, the shallowest sub-network, i.e.,, converges fast, because it gives block 1 the largest gradients. from the point of view of, block 2 may also receive large gradients due to the path with a length of one. however, the input of block 2 partly depends on block 1. it would not be easy for block 2 to converge before the output of block 1 stabilises. similarly, block 3 will need to wait for blocks 1 and 2, and so forth. in this way, a resnet seems like an ensemble with a growing number of sub-networks. besides, each newly added sub-network will have a larger actual depth than all the previous ones. note that littwin and wolf, in a concurrent work, have theoretically showed that resnets are virtual ensembles whose depth grows as training progresses. their result to some extent coincides with the above described process. the story will however be different when the actual depth becomes as large as the effective depth. refer to the bottom-right example in fig. [reference]. this is somewhat similar to the case of an over-deepened resnet, when is much larger than. again, block 1 in gets trained and stabilises first. however, this time is not fully end-to-end trained any more. since gives no gradients to block 1, it becomes a one-block sub-network trained on top of some preprocessed representations, which are obtained by adding the output of block 1 up to the original input. in this way, the newly added sub-network still has an actual depth of one, which is no deeper than the previous one, i.e.,, and so forth for. resnets thus avoid the vanishing gradient problem by reshaping themselves into multiple shallower sub-networks. this is just another view of delivering gradients to bottom layers through shortcut connections. researchers have claimed that the residual shortcut connections are not necessary even in very deep networks. however, there are usually short paths in their proposed networks as well. for example, the 76-layer inception-v4 network has a much shorter twenty-layer route from its input to the output. there might be differences in the details between fusion by concatenation (inception-v4) and fusion by summation (resnets). however, the manner of avoiding the vanishing gradient problem is probably similar, i.e., using shortcuts, either with trainable weights or not. we are thus not yet in a position to be able to claim that the vanishing gradient problem has been solved. wide resnets. conventionally, wide layers are more prone to over-fitting, and sometimes require extra regularization such as dropout. however, zagoruyko and komodakis showed the possibility to effectively train times wider resnets, even without any extra regularization. to understand how a wide resnet is trained, refer to the top right example in fig. [reference]. this simulates the case of a rather shallow network, when is smaller than. we reuse the weights of block 1 for four times. among these, block 1 is located in three different kinds of circumstances. in the bottom-most path of the sub-network, it is supposed to learn some low-level features; in, it should learn both low-level and mid-level features; and in, it has to learn everything. this format of weight sharing may suppress over-fitting, especially for those units far from the top-most linear classifier. hence resnets inherently introduce regularization by weight sharing among multiple very different sub-networks. residual unit choices. for better performance, we hope that a resnet should expand into a sufficiently large number of sub-networks, some of which should have large model capacity. so, given our previous observations, the requirements for an ideal mapping function in a residual unit are, 1) being strong enough to converge even if it is reused in many sub-networks, and 2) being shallow enough to enable an large effective depth. since it is very hard to build a model with large capacity using a single trainable layer, the most natural choice would be a residual unit with two wide convolution stages. this coincides with empirical results reported by zagoruyko and komodakis. they found that, among the most trivial structure choices, the best one is to stack two convolution stages. subsection: wider or deeper? to summarize the previous subsections, shortcut connections enable us to train wider and deeper networks. as they growing to some point, we will face the dilemma between width and depth. from that point, going deep, we will actually get a wider network, with extra features which are not completely end-to-end trained; going wider, we will literally get a wider network, without changing its end-to-end characteristic. we have learned the strength of depth from the previous plain deep networks without any shortcuts, e.g., the alexnet and vggnets. however, it is not clear whether those extra features in very deep residual networks can perform as well as conventional fully end-to-end trained features. so in this paper, we only favour a deeper model, when it can be completely end-to-end trained. in practice, algorithms are often limited by their spatial costs. one way is to use more devices, which will however increase communication costs among them. with similar memory costs, a shallower but wider network can have times more number of trainable parameters. therefore, given the following observations in the literature, zagoruyko and komodakis found that the performance of a resnet was related to the number of trainable parameters. szegedy et al. came to a similar conclusion, according to the comparison between their proposed inception networks. veit et al. found that there is a relatively small effective depth for a very deep resnet, e.g., seventeen residual units for a 110-layer resnet. most of the current state-of-the-art models on the imagenet classification dataset seem over-deepened, e.g., the 200-layer resnet and 96-layer inception-resnet. the reason is that, to effectively utilize gpu memories, we should make a model shallow. according to our previous analysis, paths longer than the effective depth in resnets are not trained in a fully end-to-end manner. thus, we can remove most of these paths by directly reducing the number of residual units. for example, in our best performing network, there are exactly seventeen residual units. with empirical results, we will show that our fully end-to-end networks can perform much better than the previous much deeper resnets, especially as feature extractors. however, even if a rather shallow network (eight-unit, or twenty-layer) can outperform resnet-152 on the imagenet classification dataset, we will not go that shallow, because an appropriate depth is vital to train good features. section: approach to image classification we show the proposed networks in fig. [reference]. there are three architectures, with different input sizes. dashed blue rectangles to denote convolution stages, which are respectively composed of a batch normalization, an relu non-linearity and a convolution layer, following the second version of resnets. the closely stacked two or three convolution stages denote different kinds of residual units (b1-b7), with inner shortcut connections. each kind corresponds to a level, where all units share the same kernel sizes and numbers of channels, as given in the dashed black rectangles in the left-most column of fig. [reference]. as mentioned before, there are two 3 3 convolution layers in most residual units (b1-b5). however, in b6 and b7, we use bottleneck structures as in resnets, except that we adjust the numbers of channels to avoid drastic changes in width. each of our networks usually consists of one b6, one b7, and different numbers of b1-b5. for those with a 224 224 input, we do not use b1 due to limited gpu memories. each of the green triangles denotes a down-sampling operation with a rate of two, which is clear given the feature map sizes of different convolution stages (in dashed blue rectangles). to this end, we can let the first convolution layer at according levels have a stride of two. or, we can use an extra spatial pooling layer, whose kernel size is three and stride is two. in a network whose classification results are reported in this paper, we always use pooling layers for down-sampling. we average the top-most feature maps into 4, 096-dimensional final features, which matches the cases of alexnet and vggnets. we will show more details about network structures in subsection [reference]. implementation details. we run all experiments using the mxnet framework, with four devices (two k80 or four maxwell titan x cards) on a single node. we follow settings in the re-implementation of resnets by gross and wilber as possible. but, we use a linear learning rate schedule, which was reported as a better choice by mishkin et al.. take model a in table [reference] for example. we start from 0.1, and linearly reduce the learning rate to within 450k iterations. section: approach to semantic image segmentation our approach is similar to the fully convolutional networks (fcn) implemented in the first version of deeplab. however, without getting too many factors entangled, we in this paper do not introduce any multi-scale structures, deep supervision signals, or global context features. besides, we do not apply any multi-scale testing, model averaging or crf based post-processing, except for the test set of ade20 k. given a pre-trained network, there are three steps to reshape it into a network suitable for semantic image segmentation, as stated below. 1) resolution. to generate score maps at 1/ 8 resolution, we remove down-sampling operations and increase dilation rates accordingly in some convolution layers. for clarity, first suppose that we always down-sample features maps using a convolution layer with a stride of two. take networks with 224 224 inputs for example. we set stride of the first convolution layer in b5 to one, and increase the dilation rate from one to two for the following layers; we do the same thing to the first convolution layer in b6 too, and increase the dilation rate from two to four for the following layers. in the case of down-sampling using a pooling layer, everything is the same except that we set stride of that pooling layer to one. sometimes, we will have to apply a pooling layer with dilation. on the other hand, we do not make any change for networks with 56 56 inputs, since there are only three down-sampling operations in each of them. it is notable that all down-sampling operations are implemented using spatial pooling layers in our originally pre-trained networks. we find it harmful for fcns in our preliminary experiments, probably due to too strong spatial invariance. to this end, we replace several top-most down-sampling operations in a network, and then tune it for some additional iterations. take model a in table [reference] for example again. we remove the top-most three pooling layers (before b4, b5 and b6), increase the strides of according convolution layers up to two, and tune it for 45k iterations using the imagenet dataset, starting from a learning rate of 0.01. 2) classifier. we remove the top-most linear classifier and the global pooling layer, and then consider two cases. for one thing, we follow a basic large field-of-view setting in deeplab-v2, called '1 convolution'. namely, we just add back a single linear layer as the new classifier. for anther, we insert an additional non-linear convolution stage (without batch normalization) below the linear classifier. this case is called '2 convolutions'. both of the added layers have 3 3 kernels, with a dilation rate of twelve. the top-most two-layer classifier thus has a receptive field of 392 392 on the final feature maps. by default, we let the number of channels in the hidden layer be 512. 3) dropout. to alleviate over-fitting, we also apply the traditional dropout to very wide residual units. the dropout rate is 0.3 for those with 2, 048 channels, e.g., the last three units in resnets and the second last units (b6) in our networks; while 0.5 for those with 4, 096 channels, e.g., the top-most units (b7) in our networks. implementation details. we fix the moving means and variations in batch normalization layers during fine-tuning. we use four devices on a single node. the batch size is sixteen, so there are four examples per device. we first tune each network for a number of iterations, keeping the learning rate unchanged at 0.0016. and then, we reduce the learning rate gradually during another number of iterations, following a linear schedule. for datasets with available testing sets, we evaluate these numbers of iterations on validation sets. during training, we first resize an image by a ratio randomly sampled from, and then generate a sample by cropping one 500 500 sub-window at a randomly selected location. section: experimental results subsection: image classification results we evaluate our proposed networks on the ilsvrc 2012 classification dataset, with 1.28 million images for training, respectively belonging to 1, 000 categories. we report top-1 and top-5 error rates on the validation set. we compare various networks in table [reference], where we obtain all the results by testing on a single crop. however, we list the ten-crop result for vgg16 since it is not inherently a fully convolutional network. for networks trained with 224 224 inputs, the testing crop size is 320 320, following the setting used by he et al.. for those with 112 112 and 56 56 inputs, we use 160 160 and 80 80 crops respectively. for inception networks, the testing crop size is 299 299. the names of our proposed networks are composed of training crop sizes and the numbers of residual units on different levels. take 56-1-1-1-1-9-1-1 for example. its input size is 56, and there are only one unit on all levels except for level 5 (b5 in fig. [reference]). notable points about the results are as follows. 1) relatively shallow networks can outperform very deep ones, which is probably due to large model capacity, coinciding with the results reported by zagoruyko and komodakis. for example, the much shallower model b achieves similar error rates as resnet-152, and even runs slightly faster. and particularly, model a performs the best among all the networks. 2) we can trade performance for efficiency by using a small input size. for example, model d performs slightly worse than resnet-152, but is almost two times faster. this may be useful when efficiency is strictly required. mishkin et al. also reduced the input size for efficiency. however, they did not remove down-sampling operations accordingly to preserve the size of final feature maps, which resulted in much degraded performance. 3) models c, d and e perform comparably, even though model c has larger depth and more parameters. this comparison shows the importance of designing a network properly. in these models, we put too many layers on low resolution levels (7 7, b5 in fig. [reference]). subsection: semantic image segmentation results we evaluate our proposed networks on four widely used datasets. when available, we report, 1) the pixel accuracy, which is the percentage of correctly labelled pixels on a whole test set, 2) the mean pixel accuracy, which is the mean of class-wise pixel accuracies, and 3) the mean iou score, which is the mean of class-wise intersection-over-union scores. pascal voc 2012. this dataset consists of daily life photos. there are 1, 464 labelled images for training and another 1, 449 for validation. pixels either belong to the background or twenty object categories, including bus, car, cat, sofa, monitor, etc. following the common criteria in the literature, we augment the dataset with extra labelled images from the semantic boundaries dataset. so in total, there are 10, 582 images for training. we first compare different networks in table [reference]. notable points about the results are as follows. 1) we can not make statistically significant improvement by using resnet-152 instead of resnet-101. however, model a performs better than resnet-152 by 3.4%. using one hidden layer leads to a further improvement by 2.1%. 2) the very deep resnet-152 uses too many memories due to intentionally enlarged depth. with our settings, it even can not be tuned using many mainstream gpus with only 12 gb memories. 3) model b performs worse than resnet-101, even if it performs better on the classification task as shown in table [reference]. this shows that it is not reliable to tell a good feature extractor only depending on its classification performance. and it again shows why we should favour deeper models. 4) model a2 performs worse than model a on this dataset. we initialize it using weights from model a, and tune it with the places 365 data for 45k iterations. this is reasonable since there are only object categories in this dataset, while places 365 is for scene classification tasks. we then compare our method with previous ones on the test set in table [reference]. only using the augmented pascal voc data for training, we achieve a mean iou score of 82.5%, which is better than the previous best one by 3.4%. this is a significant margin, considering that the gap between resnet-based and vggnet-based methods is 3.8%. our method wins for seventeen out of the twenty object categories, which was the official criteria used in the pascal voc challenges. in some works, models were further pre-trained using the microsoft coco data, which consists of 120k labelled images. in this case, the current best mean iou is 79.7% reported by chen et al.. they also used multi-scale structure and crf-based post-processing in their submission, which we do not consider here. nevertheless, our method outperforms theirs by 2.8%, which further shows the effectiveness of our features pre-trained only using the imagenet classification data. aero. bicy. bird boat bott. bus car cat chai. cow dini. dog hors. moto. pers. pott. shee. sofa trai. tvmo. mean cityscapes. this dataset consists of street scene photos taken by car-carried cameras. there are 2975 labelled images for training and another 500 for validation. besides, there is also an extended set with 19, 998 coarsely labelled images. pixels belong to nineteen semantic classes, including road, car, pedestrian, bicycle, etc. these classes further belong to seven categories, i.e., flat, nature, object, sky, construction, human, and vehicle. we first compare different networks in table [reference]. on this dataset, resnet-152 again shows no advantage against resnet-101. however, model a1 outperforms resnet-101 by 4.2% in terms of mean iou scores, which again is a significant margin. because there are many scene classes, models pre-trained using places 365 are supposed to perform better, which coincides with our results. we then compare our method with previous ones on the test set in table [reference]. the official criteria on this dataset includes two levels, i.e., class and category. besides, there is also an instance-weighted iou score for each of the two, which assigns high scores to those pixels of small instances. namely, this score penalizes methods ignoring small instances, which may cause fatal problems in vehicle-centric scenarios. our method achieves a class-level iou score of 78.4%, and outperforms the previous best one by 6.6%. furthermore, in the case of instance-weighted iou score, our method also performs better than the previous best one by 6.4%. it is notable that these significant improvements show the strength of our pre-trained features, considering that deeplab-v2 uses resnet-101, and lrr uses much more data for training. ade20 k. this dataset consists of both indoor and outdoor images with large variations. there are 20, 210 labelled images for training and another 2k for validation. pixels belong to 150 semantic categories, including sky, house, bottle, food, toy, etc. we first compare different networks in table [reference]. on this dataset, resnet-152 performs slightly better than resnet-101. however, model a2 outperforms resnet-152 by 4.0% in terms of mean iou scores. being similar with cityscapes, this dataset has many scene categories. so, model a2 performs slightly better than model a. another notable point is that, model c takes the second place on this dataset, even if it performs worse than model a in the image classification task on the imagenet dataset. this shows that large model capacity may become more critical in complicated tasks, since there are more parameters in model c. we then compare our method with others on the test set in table [reference]. the official criteria on this dataset is the average of pixel accuracies and mean iou scores. for better performance, we apply multi-scale testing, model averaging and post-processing with crfs. our model a2 performs the best among all methods using only a single pre-trained model. however, in this submission, we only managed to include two kinds of pre-trained features, i.e., models a and c. nevertheless, our method only performs slightly worse than the winner by a margin of 0.47%. pascal context. this dataset consists of images from pascal voc 2010 with extra object and stuff labels. there are 4, 998 images for training and another 5, 105 for validation. pixels either belong to the background category or 59 semantic categories, including bag, food, sign, ceiling, ground and snow. all images in this dataset are no larger than 500 500. since the test set is not available, here we directly apply the hyper-parameters which are used on the pascal voc dataset. our method again performs the best with a clear margin by all the three kinds of scores, as shown in table [reference]. in particular, we improve the iou score by 2.4% compared to the previous best method. section: conclusion we have analysed the resnet architecture, in terms of the ensemble classifiers therein and the effective depths of the residual units. on the basis of that analysis we calculated a new, more spatially efficient, and better performing architecture which actually achieves fully end-to-end training for large networks. using this new architecture we designed a group of correspondingly shallow networks, and showed that they outperform the previous very deep residual networks not only on the imagenet classification dataset, but also when applied to semantic image segmentation. these results show that the proposed architecture delivers better feature extraction performance than the current state-of-the-art. appendix: appendix subsection: network structures the graph structures of model a for the imagenet (ilsvrc 2012) classification can be accessed at: model a2 for the pascal voc 2012 segmentation can be accessed at:. subsection: gradients in residual networks we show results of the experiment on gradients proposed by veit et al., with various residual networks. namely, for a trained network with units, we sample individual paths of a certain length, and measure the norm of gradients that arrive at the input. each time, we first feed a batch forward through the whole network; then during the backward pass, we randomly sample units. for them, we only propagate gradients through their trainable mapping functions, but without their shortcut connections. for the remaining units, we do the opposite, namely, only propagating gradients through their shortcut connections. we record the norm of those gradients that reach the input for varying path length, and show the results in fig. [reference]. note the varying magnitude and maximum path length in individual figures. these are compared to the middle part of fig. 6 in. however, differently we further divide the computed norm of a batch by its number of examples. according to the results in fig. [reference], resnet-110 trained on cifar-10, as well as resnet-101 and resnet-152 trained on ilsvrc 2012, generate much smaller gradients from their long paths than from their short paths. in contrast, our model a trained on ilsvrc 2012, generates more comparable gradients from its paths with different lengths. subsection: qualitative results we show qualitative results of semantic image segmentation on pascal voc, cityscapes, ade20 k, and pascal context, respectively in figs. [reference], [reference], [reference], [reference] and [reference], and show some failure cases in figs [reference] and [reference]. in a difference map, grey and black respectively denotes correctly and wrongly labelled pixels, while white denotes the officially ignored pixels during evaluation. note that we do not apply post-processing with crfs, which can smooth the output but is too slow in practice, especially for large images. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cityscapes",
                        1221
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "mean iou score",
                        27288
                    ],
                    [
                        "mean iou",
                        29534
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semantic image segmentation",
                        2246
                    ],
                    [
                        "segmentation",
                        7578
                    ],
                    [
                        "pascal voc 2012 segmentation",
                        34683
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "pascal voc",
                        1189
                    ],
                    [
                        "pascal context",
                        1201
                    ],
                    [
                        "pascal voc 2012",
                        27368
                    ],
                    [
                        "augmented pascal voc data",
                        29011
                    ],
                    [
                        "pascal voc challenges",
                        29362
                    ],
                    [
                        "pascal voc 2010",
                        33152
                    ],
                    [
                        "pascal voc dataset",
                        33562
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "mean iou score",
                        27288
                    ],
                    [
                        "mean iou",
                        29534
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semantic image segmentation",
                        2246
                    ],
                    [
                        "segmentation",
                        7578
                    ],
                    [
                        "pascal voc 2012 segmentation",
                        34683
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "310fed695da4f36139b70b23065c34fe9fb29ca6-49",
    "doctext": "document: shorten spatial-spectral rnn with parallel-gru for hyperspectral image classification convolutional neural networks (cnns) attained a good performance in hyperspectral sensing image (hsi) classification, but cnns consider spectra as orderless vectors. therefore, considering the spectra as sequences, recurrent neural networks (rnns) have been applied in hsi classification, for rnns is skilled at dealing with sequential data. however, for a long-sequence task, rnns is difficult for training and not as effective as we expected. besides, spatial contextual features are not considered in rnns. in this study, we propose a shorten spatial-spectral rnn with parallel-gru (st-ss-pgru) for hsi classification. a shorten rnn is more efficient and easier for training than band-by-band rnn. by combining converlusion layer, the st-sspgru model considers not only spectral but also spatial feature, which results in a better performance. an architecture named parallel-gru is also proposed and applied in st-ss-pgru. with this architecture, the model gets a better performance and is more robust. deep learning, gated recurrent unit (gru), long short-term memory (lstm), recurrent neural networks (rnn), hyperspectral image classification section: introduction hyperspectral image (hsi) has attracted considerable attention in the remote sensing community and been widely used in various areas. with the rich spectral information in hsi, different land cover categories can potentially be differentiated precisely. in recent years, deep learning has been widely used in various fields, including hsi classification. convolutional neural networks (cnns) and residual networks (resnets) have obtained a successful result for hsi classification. recurrent neural networks (rnns) are also applied in hsi classification. because of the ability to extract the spatial contextual information, cnns and resnets can achieve a high accuracy in the classification task. however, cnns and resnets consider spectra as orderless vectors in-dimensional feature space where represents the number of bands. however, spectra can be seen as orderly and continuing sequences in the spectral space. in other words, cnns and resnets ignore the continuity of spectra. rnns have proved effective in solving many challenging problems involving sequential data, such as natural language processing (nlp) and prediction of time series. considering the spectrum as a sequential sequence, the application of rnns is reasonable as it can take full advantage of the high spectral resolution characteristics of hsi. however, for a long-sequence task, rnns is not as effective as we expected. long distance dependence, gradient vanish and overfitting are prone to occur. even if the long short-term memory network (lstm) is used to solve the long-distance dependence problem, rnns is still hard for training and easily overfitting in a long-sequence task. in previous work, 3d-cnn is applied in hsi classification and obtained a good behavior. for rnns, convolutional-lstm (clstm) also achieved a good performance in hsi classification. 3d-cnns and clstm consider both spatial contextual information and spectral continuity, which result in a high accuracy. nevertheless, it takes a long time to train these two models. in, lstm and its variant, gru, are applied in his classification, and it is proved that gru has a better performance in his classification. to solve the problem that rnns are easily over-fitting and difficult for training, proposed band-group lstm, which can effectively make training easier by reducing the number of timestep in lstm. in this study, a shorten spatial-spectral rnn with parallel-gru (st-ss-pgru) is proposed. this study contributes to the literature in 2 major respects: a shorten rnn with gru is applied in his classification. the model is more efficient and easier for training than band-by-band rnn. by combining converlusion layer, an advanced model shorten spatial-spectral rnn with gru is proposed. the model considers not only spectral but also spatial feature, which leads to a better performance. an architecture named parallel-gru is proposed and the model with this architecture has a better performance and is more robust. the remainder of this paper is organized as follows. in the methodology section, firstly the structure of traditional rnn, lstm and gru are introduced and then the architecture of the proposed models are described. in the experimental section, the network setup, the experimental results, and the comparison of different models are provided. finally, the conclusion section concludes the paper. section: methodology subsection: recurrent neural networks (rnn) different from artificial neural network (ann), rnn, a neural network with recurrent unit, has a better performance in solving many challenging problems involving sequential data analysis. the state of each time step of the recurrent unit is not only related to the input of the current step, but also related to the state of the previous step. thus, the state of the preceding step can effectively influence the next step. given a sequence sample, in which is the data at th timestep. for the th recurrent unit, its hidden state can be described as: where is the initial state of the recurrent unit, is a nonlinear function. normally, is set as a zero vector. optionally, in th timestep, the recurrent unit may have an output. for some task, the rnn model will finally have an output vector, while for classification tasks, only one output is needed. generally, the last output is adopted: the recurrent unit in a traditional rnn is shown in fig. [reference]. in the traditional rnn model, the update rule of the recurrent hidden state and output in eq. ([reference]) and ([reference]) is usually implemented as follows: where, and are the weight matrices. and are the bias vectors, and is an activation function, such as the sigmoid function or the hyperbolic tangent function. subsection: long short-term memory (lstm) the traditional rnn has the problem of long-distance dependence. the rnn has the capability to connect different timesteps related information. however, when the sequence is too long, the rnn becomes unable to connect related information as the distance increases, because the information losses when propagating through multi-time-step recurrent units. by using long short-term memory (lstm), the problems have been solved. as fig. [reference] shows, lstm contains a forget gate, an input gate and an output gate.' gate' structure is actually a logistic regression model so that part of the information is filtered selectively, while the rest is reserved and passes through the gate. lstm can simulate the process of forgetting and memory and calculate the probability of forgetting and memory, so information flow could be preserved in long-distance propagation. the structure of lstm can be described as: where eq. ([reference]), ([reference]) and ([reference]) represent forget gate, input gate and output gate.,,,,,, and are the weight matrices.,, and are the bias vectors. refers to sigmoid function and tanh refers to the hyperbolic tangent function: subsection: gated recurrent unit (gru) over the years, there have been many variants of lstm, but there is no evidence to show that there is not a superior variant. any variant may have advantages in a particular problem. gru is a variant of lstm. with fewer parameters, it is much easier for training than lstm, and usually achieves the same performance as lstm in some tasks. it is considered that using gru in a hsi classification task is more appropriate than using lstm. the main difference between lstm and gru is that an update gate and a reset gate are adopted in gru, instead of using a forget gate, an input gate and an output gate. the structure of the gru is shown in fig. [reference], which can be defined as follows: where eq. ([reference]) and ([reference]) represent update gate and reset gate.,,,, and are the weight matrices., and are the bias vectors. subsection: the proposed model subsubsection: shorten spatial-spectral rnn with gru (st-ss-gru) a shorten spatial-spectral rnn with gru (st-ss-gru) model for hsi classification is shown in fig. [reference]. for each pixel, a square subgraph composed of 5 5 pixels centered on it is used as a training sample. the first part of st-ss-gru is actually a 3d-convolutional layer but both the depth and stride of the kernels are 1. three different convolution kernels (1\u00e3\u00971, 3\u00e3\u00973, 5\u00e3\u00975) were used to convolve different bands. the output of this part is a sequence with the same length as the original input. the output sequence is a' spectra' with the spatial contextual feature. every timestep of the sequence is a feature vector. the second part is a shorten rnn with gru (st-gru). the structure of st-gru is shown in fig. [reference]. the 1d converlusion layer before gru is used to reduce the number of timesteps so that the network is easier for training. subsubsection: parallel-gru architecture in order to make the model more robust, a parallel-gru (pgru) architecture is proposed. the architecture of shorten parallel-gru (st-pgru) is shown in fig. [reference]. the architecture is actually a combination of several gru units. the output of the architecture is the summation of every unit. the shorten spatial-spectral rnn with parallel-gru (st-ss-pgru) is similar to st-ss-gru, except that st-gru is replaced by st-ss-pgru. section: experiment subsection: data in the experiment, two hsi datasets, including the pavia university and indian pines, are used to evaluate the performance of the proposed model. the pavia university dataset was acquired by the reflective optics system imaging spectrometer (rosis) sensor over pavia, northern italy in 2001. the corrected data, with a spatial resolution of 1.3 m per pixel, contains 103 spectral bands ranging from 0.43 to 0.86. the image, with 610 340 pixels, is differentiated into 9 ground truth classes. table [reference] provides information about all classes of the dataset with their corresponding training and test sample. the indian pines dataset was acquired by the tairborne visible/ infrared imaging spectrometer (aviris) sensor over the indian pines test site in north-western indiana in 1992. the corrected data with a moderate spatial resolution of 20 m contains 200 spectral bands ranging from 0.4 to 2.5. the image consists of 145 145 pixels, which are differentiated into 16 ground truth classes. table [reference] provides information about all classes of the dataset with their corresponding training and test sample. subsection: result table [reference] and [reference] list the results obtained by the experiment, and fig. [reference] and [reference] show the classification maps on the pavia university dataset and the indian pines dataset. note that the accuracies list in table [reference] and [reference] are overall accuracies (oa) along with the standard deviation, from 10 independent runs on each dataset. the experiment is implemented with an intel i7-7700 k 4.20ghz processor with 16 gb of ram and an nvidia gtx1050ti graphic card under python3.6 with tensorflow1.8.0. first of all, for all the datasets, gru outperforms lstm. in addition, it is observed that lstm is difficult to converge in the experiment, while gru is not. thus, it is reasonable to indicate that gru is a better choice for a hsi classification task. furthermore, it is apparent that st-gru increases the accuracy significantly by 5.33% and 3.52% in the pavia university dataset and the indian pines correspondingly. with converlusion layers, st-ss-gru has a better than st-gru. the accuracy of st-ss-gru is 4.55% and 6.63% higher than that in st-gru. after parallel-gru is adopted, the model gains the best performance in this experiment. the accuracy of st-ss-pgru is 1.64% and 3.19% higher than st-ss-gru. what is more, the standard deviation of st-ss-pgru is smaller than other models, which indicate that st-ss-pgru is more robust. comparing the processing time of different methods, st-gru is significantly faster in training than band-by-band gru. st-ss-gru and st-ss-pgru are as slow as lstm and gru in training, but they have higher accuracies than lstm and gru. section: conclusion in the study, a st-ss-pgru model is proposed for hsi classification. what is more, an architecture named parallel-gru is proposed to promote the performance and robustness. then an experiment is conducted to compare the performance of different models. from the experiment, it is confirmed that gru performs better than lstm in hsi classification task. moreover, it is apparent that the proposed models are a lot more accurate, more robust and faster than the traditional gru network. specifically, st-gru effectively reduced the training time and promoted the accuracy. st-ss-gru needs more time for training but gains a better performance than st-gru. the proposed architecture parallel-gru also provided a satisfactory result in the experiment. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "indian pines",
                        9592
                    ],
                    [
                        "indian pines dataset",
                        10139
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "shorten spatial-spectral rnn with parallel-gru",
                        634
                    ],
                    [
                        "st-ss-pgru",
                        683
                    ],
                    [
                        "st-sspgru model",
                        834
                    ],
                    [
                        "shorten spatial-spectral rnn with gru",
                        8101
                    ],
                    [
                        "st-ss-gru",
                        8141
                    ],
                    [
                        "st-ss-pgru model",
                        12314
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        1927
                    ],
                    [
                        "accuracies",
                        10882
                    ],
                    [
                        "oa",
                        10960
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "hyperspectral image classification",
                        61
                    ],
                    [
                        "hyperspectral sensing image",
                        164
                    ],
                    [
                        "hsi",
                        194
                    ],
                    [
                        "classification",
                        198
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "pavia university",
                        9571
                    ],
                    [
                        "pavia university dataset",
                        9670
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "shorten spatial-spectral rnn with parallel-gru",
                        634
                    ],
                    [
                        "st-ss-pgru",
                        683
                    ],
                    [
                        "st-sspgru model",
                        834
                    ],
                    [
                        "shorten spatial-spectral rnn with gru",
                        8101
                    ],
                    [
                        "st-ss-gru",
                        8141
                    ],
                    [
                        "st-ss-pgru model",
                        12314
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        1927
                    ],
                    [
                        "accuracies",
                        10882
                    ],
                    [
                        "oa",
                        10960
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "hyperspectral image classification",
                        61
                    ],
                    [
                        "hyperspectral sensing image",
                        164
                    ],
                    [
                        "hsi",
                        194
                    ],
                    [
                        "classification",
                        198
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "3208b5abafa1146e7df1073090523c822909275e-50",
    "doctext": "document: text classification improved by integrating bidirectional lstm with two-dimensional max pooling recurrent neural network (rnn) is one of the most popular architectures used in natural language processsing (nlp) tasks because its recurrent structure is very suitable to process variable-length text. rnn can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. and this matrix includes two dimensions: the time-step dimension and the feature vector dimension. then most existing models usually utilize one-dimensional (1d) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. however, the features on the feature vector dimension are not mutually independent, and simply applying 1d pooling operation over the time-step dimension independently may destroy the structure of the feature representation. on the other hand, applying two-dimensional (2d) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. to integrate the features on both dimensions of the matrix, this paper explores applying 2d max pooling operation to obtain a fixed-length representation of the text. this paper also utilizes 2d convolution to sample more meaningful information of the matrix. experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. specifically, one of the proposed models achieves highest accuracy on stanford sentiment treebank binary classification and fine-grained classification tasks. section: introduction this work is licenced under a creative commons attribution 4.0 international licence. licence details: text classification is an essential component in many nlp applications, such as sentiment analysis, relation extraction and spam detection. therefore, it has attracted considerable attention from many researchers, and various types of models have been proposed. as a traditional method, the bag-of-words (bow) model treats texts as unordered sets of words. in this way, however, it fails to encode word order and syntactic feature. recently, order-sensitive models based on neural networks have achieved tremendous success for text classification, and shown more significant progress compared with bow models. the challenge for textual modeling is how to capture features for different text units, such as phrases, sentences and documents. benefiting from its recurrent structure, rnn, as an alternative type of neural networks, is very suitable to process the variable-length text. rnn can capitalize on distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. this matrix includes two dimensions: the time-step dimension and the feature vector dimension, and it will be updated in the process of learning feature representation. then rnn utilizes 1d max pooling operation or attention-based operation, which extracts maximum values or generates a weighted representation over the time-step dimension of the matrix, to obtain a fixed-length vector. both of the two operators ignore features on the feature vector dimension, which maybe important for sentence representation, therefore the use of 1d max pooling and attention-based operators may pose a serious limitation. convolutional neural networks (cnn) utilizes 1d convolution to perform the feature mapping, and then applies 1d max pooling operation over the time-step dimension to obtain a fixed-length output. however the elements in the matrix learned by rnn are not independent, as rnn reads a sentence word by word, one can effectively treat the matrix as an' image'. unlike in nlp, cnn in image processing tasks applies 2d convolution and 2d pooling operation to get a representation of the input. it is a good choice to utilize 2d convolution and 2d pooling to sample more meaningful features on both the time-step dimension and the feature vector dimension for text classification. above all, this paper proposes bidirectional long short-term memory networks with two-dimensional max pooling (blstm-2dpooling) to capture features on both the time-step dimension and the feature vector dimension. it first utilizes bidirectional long short-term memory networks (blstm) to transform the text into vectors. and then 2d max pooling operation is utilized to obtain a fixed-length vector. this paper also applies 2d convolution (blstm-2dcnn) to capture more meaningful features to represent the input text. the contributions of this paper can be summarized as follows: this paper proposes a combined framework, which utilizes blstm to capture long-term sentence dependencies, and extracts features by 2d convolution and 2d max pooling operation for sequence modeling tasks. to the best of our knowledge, this work is the first example of using 2d convolution and 2d max pooling operation in nlp tasks. this work introduces two combined models blstm-2dpooling and blstm-2dcnn, and verifies them on six text classification tasks, including sentiment analysis, question classification, subjectivity classification, and newsgroups classification. compared with the state-of-the-art models, blstm-2dcnn achieves excellent performance on out of tasks. specifically, it achieves highest accuracy on stanford sentiment treebank binary classification and fine-grained classification tasks. to better understand the effect of 2d convolution and 2d max pooling operation, this paper conducts experiments on stanford sentiment treebank fine-grained task. it first depicts the performance of the proposed models on different length of sentences, and then conducts a sensitivity analysis of 2d filter and max pooling size. the remainder of the paper is organized as follows. in section 2, the related work about text classification is reviewed. section 3 presents the blstm-2dcnn architectures for text classification in detail. section 4 describes details about the setup of the experiments. section 5 presents the experimental results. the conclusion is drawn in the section 6. section: related work deep learning based neural network models have achieved great improvement on text classification tasks. these models generally consist of a projection layer that maps words of text to vectors. and then combine the vectors with different neural networks to make a fixed-length representation. according to the structure, they may divide into four categories: recursive neural networks (recnn), rnn, cnn and other neural networks. recursive neural networks: recnn is defined over recursive tree structures. in the type of recursive models, information from the leaf nodes of a tree and its internal nodes are combined in a bottom-up manner. socher2013recursive introduced recursive neural tensor network to build representations of phrases and sentences by combining neighbour constituents based on the parsing tree. irsoy2014deep proposed deep recursive neural network, which is constructed by stacking multiple recursive layers on top of each other, to modeling sentence. recurrent neural networks: rnn has obtained much attention because of their superior ability to preserve sequence information over time. tang2015target developed target dependent long short-term memory networks (lstm), where target information is automatically taken into account. tai2015improved generalized lstm to tree-lstm where each lstm unit gains information from its children units. zhou2016attention introduced blstm with attention mechanism to automatically select features that have a decisive effect on classification. yang2016hierarchical introduced a hierarchical network with two levels of attention mechanisms, which are word attention and sentence attention, for document classification. this paper also implements an attention-based model blstm-att like the model in zhou2016attention. convolution neural networks: cnn is a feedforward neural network with 2d convolution layers and 2d pooling layers, originally developed for image processing. then cnn is applied to nlp tasks, such as sentence classification, and relation classification. the difference is that the common cnn in nlp tasks is made up of 1d convolution layers and 1d pooling layers. kim2014convolutional defined a cnn architecture with two channels. proposed a dynamic-max pooling mechanism for sentence modeling. conducted a sensitivity analysis of one-layer cnn to explore the effect of architecture components on model performance. yin2016multichannel introduced multichannel embeddings and unsupervised pretraining to improve classification accuracy. conducted a sensitivity analysis of one-layer cnn to explore the effect of architecture components on model performance. usually there is a misunderstanding that 1d convolutional filter in nlp tasks has one dimension. actually it has two dimensions, where,. as is equal to the word embeddings size, the window slides only on the time-step dimension, so the convolution is usually called 1d convolution. while in this paper varies from 2 to, to avoid confusion with common cnn, the convolution in this work is named as 2d convolution. the details will be described in section [reference]. other neural networks: in addition to the models described above, lots of other neural networks have been proposed for text classification. iyyer2015deep introduced a deep averaging network, which fed an unweighted average of word embeddings through multiple hidden layers before classification. zhou2015c used cnn to extract a sequence of higher-level phrase representations, then the representations were fed into a lstm to obtain the sentence representation. the proposed model blstm-2dcnn is most relevant to dscnn and rcnn. the difference is that the former two utilize lstm, bidirectional rnn respectively, while this work applies blstm, to capture long-term sentence dependencies. after that the former two both apply 1d convolution and 1d max pooling operation, while this paper uses 2d convolution and 2d max pooling operation, to obtain the whole sentence representation. section: model as shown in figure 1, the overall model consists of four parts: blstm layer, two-dimensional convolution layer, two dimensional max pooling layer, and output layer. the details of different components are described in the following sections. subsection: blstm layer lstm was firstly proposed by hochreiter1997long to overcome the gradient vanishing problem of rnn. the main idea is to introduce an adaptive gating mechanism, which decides the degree to keep the previous state and memorize the extracted features of the current data input. given a sequence, where is the length of input text, lstm processes it word by word. at time-step, the memory and the hidden state are updated with the following equations: where is the input at the current time-step,, and is the input gate activation, forget gate activation and output gate activation respectively, is the current cell state, denotes the logistic sigmoid function and denotes element-wise multiplication. for the sequence modeling tasks, it is beneficial to have access to the past context as well as the future context. schuster1997bidirectional proposed blstm to extend the unidirectional lstm by introducing a second hidden layer, where the hidden to hidden connections flow in opposite temporal order. therefore, the model is able to exploit information from both the past and the future. in this paper, blstm is utilized to capture the past and the future information. as shown in figure 1, the network contains two sub-networks for the forward and backward sequence context respectively. the output of the word is shown in the following equation: here, element-wise sum is used to combine the forward and backward pass outputs. subsection: convolutional neural networks since blstm has access to the future context as well as the past context, is related to all the other words in the text. one can effectively treat the matrix, which consists of feature vectors, as an' image', so 2d convolution and 2d max pooling operation can be utilized to capture more meaningful information. subsubsection: two-dimensional convolution layer a matrix,, is obtained from blstm layer, where is the size of word embeddings. then narrow convolution is utilized to extract local features over. a convolution operation involves a 2d filter, which is applied to a window of k words and d feature vectors. for example, a feature is generated from a window of vectors by where ranges from 1 to, ranges from 1 to, represents dot product, is a bias and an is a non-linear function such as the hyperbolic tangent. this filter is applied to each possible window of the matrix to produce a feature map: with. it has described the process of one convolution filter. the convolution layer may have multiple filters for the same size filter to learn complementary features, or multiple kinds of filter with different size. subsubsection: two-dimensional max pooling layer then 2d max pooling operation is utilized to obtain a fixed length vector. for a 2d max pooling, it is applied to each possible window of matrix o to extract the maximum value: where represents the 2d max pooling function,, and. then the pooling results are combined as follows: where, and the length of is. subsection: output layer for text classification, the output of 2d max pooling layer is the whole representation of the input text. and then it is passed to a softmax classifier layer to predict the semantic relation label from a discrete set of classes. the classifier takes the hidden state as input: a reasonable training objective to be minimized is the categorical cross-entropy loss. the loss is calculated as a regularized sum: where is the one-hot represented ground truth, is the estimated probability for each class by softmax, is the number of target classes, and is an l2 regularization hyper-parameter. training is done through stochastic gradient descent over shuffled mini-batches with the adadelta update rule. training details are further introduced in section [reference]. section: experimental setup subsection: datasets the proposed models are tested on six datasets. summary statistics of the datasets are in table 1. mrhttps:// www.cs.cornell.edu/ people/ pabo/ movie-review-data/: sentence polarity dataset from pang2005seeing. the task is to detect positive/ negative reviews. sst-1http:// nlp.stanford.edu/ sentiment/: stanford sentiment treebank is an extension of mr from socher2013recursive. the aim is to classify a review as fine-grained labels (very negative, negative, neutral, positive, very positive). sst-2: same as sst-1 but with neutral reviews removed and binary labels (negative, positive). for both experiments, phrases and sentences are used to train the model, but only sentences are scored at test time. thus the training set is an order of magnitude larger than listed in table 1. subjhttp:// www.cs.cornell.edu/ people/ pabo/ movie-review-data/: subjectivity dataset. the task is to classify a sentence as being subjective or objective. trechttp:// cogcomp.cs.illinois.edu/ data/ qa/ qc/: question classification dataset. the task involves classifying a question into 6 question types (abbreviation, description, entity, human, location, numeric value). 20newsgroupshttp:// web.ist.utl.pt/ acardoso/ datasets/: the 20ng dataset contains messages from twenty newsgroups. we use the bydate version preprocessed by cachopo2007improving. we select four major categories (comp, politics, rec and religion) followed by hingmire2013document. subsection: word embeddings the word embeddings are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data. in particular, our experiments utilize the glove embeddings trained by pennington2014glove on 6 billion tokens of wikipedia 2014 and gigaword 5. words not present in the set of pre-trained words are initialized by randomly sampling from uniform distribution in. the word embeddings are fine-tuned during training to improve the performance of classification. subsection: hyper-parameter settings for datasets without a standard development set we randomly select of the training data as the development set. the evaluation metric of the 20ng is the macro-f1 measure followed by the state-of-the-art work and the other five datasets use accuracy as the metric. the final hyper-parameters are as follows. the dimension of word embeddings is 300, the hidden units of lstm is 300. we use 100 convolutional filters each for window sizes of (3, 3), 2d pooling size of (2, 2). we set the mini-batch size as 10 and the learning rate of adadelta as the default value 1.0. for regularization, we employ dropout operation with dropout rate of 0.5 for the word embeddings, 0.2 for the blstm layer and 0.4 for the penultimate layer, we also use l2 penalty with coefficient over the parameters. these values are chosen via a grid search on the sst-1 development set. we only tune these hyper-parameters, and more finer tuning, such as using different numbers of hidden units of lstm layer, or using wide convolution, may further improve the performance. section: results subsection: overall performance this work implements four models, blstm, blstm-att, blstm-2dpooling, and blstm-2dcnn. table 2 presents the performance of the four models and other state-of-the-art models on six classification tasks. the blstm-2dcnn model achieves excellent performance on 4 out of 6 tasks. especially, it achieves and test accuracies on sst-1 and sst-2 respectively. blstm-2dpooling performs worse than the state-of-the-art models. while we expect performance gains through the use of 2d convolution, we are surprised at the magnitude of the gains. blstm-cnn beats all baselines on sst-1, sst-2, and trec datasets. as for subj and mr datasets, blstm-2dcnn gets a second higher accuracies. some of the previous techniques only work on sentences, but not paragraphs/ documents with several sentences. our question becomes whether it is possible to use our models for datasets that have a substantial number of words, such as 20ng and where the content consists of many different topics. for that purpose, this paper tests the four models on document-level dataset 20ng, by treating the document as a long sentence. compared with rcnn, blstm-2dcnn achieves a comparable result. besides, this paper also compares with renn, rnn, cnn and other neural networks: compared with renn, the proposed two models do not depend on external language-specific features such as dependency parse trees. cnn extracts features from word embeddings of the input text, while blstm-2dpooling and blstm-2dcnn captures features from the output of blstm layer, which has already extracted features from the original input text. blstm-2dcnn is an extension of blstm-2dpooling, and the results show that blstm-2dcnn can capture more dependencies in text. adasent utilizes a more complicated model to form a hierarchy of representations, and it outperforms blstm-2dcnn on subj and mr datasets. compared with dscnn, blstm-2dcnn outperforms it on five datasets. compared with these results, 2d convolution and 2d max pooling operation are more effective for modeling sentence, even document. to better understand the effect of 2d operations, this work conducts a sensitivity analysis on sst-1 dataset. subsection: effect of sentence length figure 2 depicts the performance of the four models on different length of sentences. in the figure, the x-axis represents sentence lengths and y-axis is accuracy. the sentences collected in test set are no longer than 45 words. the accuracy here is the average value of the sentences with length in the window. each data point is a mean score over 5 runs, and error bars have been omitted for clarity. it is found that both blstm-2dpooling and blstm-2dcnn outperform the other two models. this suggests that both 2d convolution and 2d max pooling operation are able to encode semantically-useful structural information. at the same time, it shows that the accuracies decline with the length of sentences increasing. in future work, we would like to investigate neural mechanisms to preserve long-term dependencies of text. subsection: effect of 2d convolutional filter and 2d max pooling size we are interested in what is the best 2d filter and max pooling size to get better performance. we conduct experiments on sst-1 dataset with blstm-2dcnn and set the number of feature maps to 100. to make it simple, we set these two dimensions to the same values, thus both the filter and the pooling are square matrices. for the horizontal axis, c means 2d convolutional filter size, and the five different color bar charts on each c represent different 2d max pooling size from 2 to 6. figure 3 shows that different size of filter and pooling can get different accuracies. the best accuracy is 52.6 with 2d filter size (5, 5) and 2d max pooling size (5, 5), this shows that finer tuning can further improve the performance reported here. and if a larger filter is used, the convolution can detector more features, and the performance may be improved, too. however, the networks will take up more storage space, and consume more time. section: conclusion this paper introduces two combination models, one is blstm-2dpooling, the other is blstm-2dcnn, which can be seen as an extension of blstm-2dpooling. both models can hold not only the time-step dimension but also the feature vector dimension information. the experiments are conducted on six text classificaion tasks. the experiments results demonstrate that blstm-2dcnn not only outperforms recnn, rnn and cnn models, but also works better than the blstm-2dpooling and dscnn. especially, blstm-2dcnn achieves highest accuracy on sst-1 and sst-2 datasets. to better understand the effective of the proposed two models, this work also conducts a sensitivity analysis on sst-1 dataset. it is found that large filter can detector more features, and this may lead to performance improvement. section: acknowledgements we thank anonymous reviewers for their constructive comments. this research was funded by the national high technology research and development program of china (no.2015aa015402), and the national natural science foundation of china (no. 61602479), and the strategic priority research program of the chinese academy of sciences (grant no. xdb02070005). bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "sst-2",
                        14902
                    ],
                    [
                        "sst-2 datasets",
                        22089
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "blstm-2dpooling",
                        4365
                    ],
                    [
                        "blstm-2dcnn",
                        4695
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        1713
                    ],
                    [
                        "classification accuracy",
                        8840
                    ],
                    [
                        "accuracies",
                        17814
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "sentiment analysis",
                        1438
                    ],
                    [
                        "sensitivity analysis",
                        22194
                    ]
                ]
            ]
        },
        {
            "Material": [],
            "Method": [],
            "Metric": [],
            "Task": [
                [
                    [
                        "text classification",
                        10
                    ],
                    [
                        "text classification tasks",
                        1401
                    ],
                    [
                        "classification",
                        7840
                    ],
                    [
                        "document classification",
                        8005
                    ],
                    [
                        "sentence classification",
                        8329
                    ],
                    [
                        "classification tasks",
                        17685
                    ],
                    [
                        "text classificaion tasks",
                        21841
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "33bcc97b605f00145098d095be2841a1fa6b9a95-51",
    "doctext": "document: order-embeddings of images and language hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. in this paper we advocate for explicitly modeling the partial order structure of this hierarchy. towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. we show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval. section: introduction computer vision and natural language processing are becoming increasingly intertwined. recent work in vision has moved beyond discriminating between a fixed set of object classes, to automatically generating open-ended lingual descriptions of images vinyals2015show. recent methods for natural language processing such as flickr30k learn the semantics of language by grounding it in the visual world. looking to the future, autonomous artificial agents will need to jointly model vision and language in order to parse the visual world and communicate with people. but what, precisely, is the relationship between images and the words or captions we use to describe them? it is akin to the hypernym relation between words, and textual entailment among phrases: captions are simply abstractions of images. in fact, all three relations can be seen as special cases of a partial order over images and language, illustrated in figure [reference], which we refer to as the visual-semantic hierarchy. as a partial order, this relation is transitive:\" woman walking her dog\",\" woman walking\",\" person walking\",\" person\", and\" entity\" are all valid abstractions of the rightmost image. our goal in this work is to learn representations that respect this partial order structure. most recent approaches to modeling the hypernym, entailment, and image-caption relations involve learning distributed representations or embeddings. this is a very powerful and general approach which maps the objects of interest\u2014 words, phrases, images\u2014 to points in a high-dimensional vector space. one line of work, exemplified by chopra2005learning and first applied to the caption-image relationship by socher2014grounded, requires the mapping to be distance-preserving: semantically similar objects are mapped to points that are nearby in the embedding space. a symmetric distance measure such as euclidean or cosine distance is typically used. since the visual-semantic hierarchy is an anti symmetric relation, we expect this approach to introduce systematic model error. other approaches do not have such explicit constraints, learning a more-or-less general binary relation between the objects of interest, e.g. bordes2011learning, socher2013reasoning, ma2015multimodal. notably, no existing approach directly imposes the transitivity and antisymmetry of the partial order, leaving the model to induce these properties from data. in contrast, we propose to exploit the partial order structure of the visual-semantic hierarchy by learning a mapping which is not distance-preserving but order-preserving between the visual-semantic hierarchy and a partial order over the embedding space. we call embeddings learned in this way order-embeddings. this idea can be integrated into existing relational learning methods simply by replacing their comparison operation with ours. by modifying existing methods in this way, we find that order-embeddings provide a marked improvement over the state-of-art for hypernymy prediction and caption-image retrieval, and near state-of-the-art performance for natural language inference. this paper is structured as follows. we begin, in section [reference], by giving a unified mathematical treatment of our tasks, and describing the general approach of learning order-embeddings. in the next three sections we describe in detail the tasks we tackle, how we apply the order-embeddings idea to each of them, and the results we obtain. the tasks are hypernym prediction (section [reference]), caption-image retrieval (section [reference]), and textual entailment (section [reference]). in the supplementary material, we visualize novel vector regularities that emerge in our learned embeddings of images and language. section: learning order-embeddings to unify our treatment of various tasks, we introduce the problem of partial order completion. in partial order completion, we are given a set of positive examples of ordered pairs drawn from a partially ordered set, and a set of negative examples which we know to be unordered. our goal is to predict whether an unseen pair is ordered. note that hypernym prediction, caption-image retrieval, and textual entailment are all special cases of this task, since they all involve classifying pairs of concepts in the (partially ordered) visual-semantic hierarchy. we tackle this problem by learning a mapping from into a partially ordered embedding space. the idea is to predict the ordering of an unseen pair in based on its ordering in the embedding space. this is possible only if the mapping satisfies the following crucial property: theorem:. a function: f\u2192 (x,\u2aafx)(y,\u2aafy) is an order-embedding if for all\u2208u, vx, this definition implies that each combination of embedding space, order, and order-embedding determines a unique completion of our data as a partial order. in the following, we first consider the choice of and, and then discuss how to find an appropriate. subsection: the reversed product order on the choice of and is somewhat application-dependent. for the purpose of modeling the semantic hierarchy, our choices are narrowed by the following considerations. much of the expressive power of human language comes from abstraction and composition. for any two concepts, say\" dog\" and\" cat\", we can name a concept that is an abstraction of the two, such as\" mammal\", as well as a concept that composes the two, such as\" dog chasing cat\". so, in order to represent the visual-semantic hierarchy, we need to choose an order that is rich enough to embed these two relations. we also restrict ourselves to orders with a top element, which is above every other element in the order. in the visual-semantic hierarchy, this element represents the most general possible concept; practically, it provides an anchor for the embedding. finally, we choose the embedding space to be continuous in order to allow optimization with gradient-based methods. a natural choice that satisfies all three properties is the reversed product order on, defined by the conjunction of total orders on each coordinate: for all vectors with nonnegative coordinates. note the reversal of direction: smaller coordinates imply higher position in the partial order. the origin is then the top element of the order, representing the most general concept. instead of viewing our embeddings as single points, we can also view them as sets. the meaning of a word is then the union of all concepts of which it is a hypernym, and the meaning of a sentence is the union of all sentences that entail it. the visual-semantic hierarchy can then be seen as a special case of the subset relation, a connection also used by flickr30k. subsection: penalizing order violations having fixed the embedding space and order, we now consider the problem of finding an order-embedding into this space. in practice, the order embedding condition (definition [reference]) is too restrictive to impose as a hard constraint. instead, we aim to find an approximate order-embedding: a mapping which violates the order-embedding condition, imposed as a soft constraint, as little as possible. more precisely, we define a penalty that measures the degree to which a pair of points violates the product order. in particular, we define the penalty for an ordered pair of points in as crucially, according to the reversed product order; if the order is not satisfied, is positive. this effectively imposes a strong prior on the space of relations, encouraging our learned relation to satisfy the partial order properties of transitivity and antisymmetry. this penalty is key to our method. throughout the remainder of the paper, we will use it where previous work has used symmetric distances or learned comparison operators. recall that and are our positive and negative examples, respectively. then, to learn an approximate order-embedding, we could use a max-margin loss which encourages positive examples to have zero penalty, and negative examples to have penalty greater than a margin: in practice we are often not given negative examples, in which case this loss admits the trivial solution of mapping all objects to the same point. the best way of dealing with this problem depends on the application, so we will describe task-specific variations of the loss in the next several sections. section: hypernym prediction to test the ability of our model to learn partial orders from incomplete data, our first task is to predict withheld hypernym pairs in wordnet miller1995wordnet. a hypernym pair is a pair of concepts where the first concept is a specialization or an instance of the second, e.g., (woman, person) or (new york, city). our setup differs significantly from previous work in that we use only the wordnet hierarchy as training data. the most similar evaluation has been that of baroni2012entailment, who use external linguistic data in the form of distributional semantic vectors. bordes2011learning and socher2013reasoning also evaluate on the wordnet hierarchy, but they use other relations in wordnet as training data (and external linguistic data, in socher's case). additionally, the latter two consider only direct hypernyms, rather than the full, transitive hypernymy relation. but predicting the transitive hypernym relation is a better-defined problem because individual hypernym edges in wordnet vary dramatically in the degree of abstraction they require. for instance, (person, organism) is a direct hypernym pair, but it takes eight hypernym edges to get from cat to organism. subsection: loss function to apply order-embeddings to hypernymy, we follow the setup of socher2013reasoning in learning an n-dimensional vector for each concept in wordnet, but we replace their neural tensor network with our order-violation penalty defined in eq. ([reference]). just like them, we corrupt each hypernym pair by replacing one of the two concepts with a randomly chosen concept, and use these corrupted pairs as negative examples for both training and evaluation. we use their max-margin loss, which encourages the order-violation penalty to be zero for positive examples, and greater than a margin for negative examples: where is our order-violation penalty, and is a corrupted version of. since we learn an independent embedding for each concept, the mapping is simply a lookup table. subsection: dataset the transitive closure of the wordnet hierarchy gives us edges between concepts in wordnet. like bordes2011learning, we randomly select edges for the test split, and another for the development set. note that the majority of test set edges can be inferred simply by applying transitivity, giving us a strong baseline. subsection: details of training we learn a 50-dimensional nonnegative vector for each concept in wordnet using the max-margin objective ([reference]) with margin, sampling 500 true and 500 false hypernym pairs in each batch. we train for 30-50 epochs using the adam optimizer adam with learning rate and early stopping on the validation set. during evaluation, we find the optimal classification threshold on the validation set, then apply it to the test set. subsection: results since our setup is novel, there are no published numbers to compare to. we therefore compare three variants of our model to two baselines, with results shown in table [reference]. the transitive closure baseline involves no learning; it simply classifies hypernyms pairs as positive if they are in the transitive closure of the union of edges in the training and validation sets. the word2gauss baseline evaluates the approach of word2gauss to represent words as gaussian densities rather than points in the embedding space. this allows a natural representation of hierarchies using the kl divergence. we used 50-dimensional diagonal gaussian embeddings, trained for 200 epochs on a max-margin objective with margin, chosen by grid search. order-embeddings (symmetric) is our full model, but using symmetric cosine distance instead of our asymmetric penalty. order-embeddings (bilinear) replaces our penalty with the bilinear model used by socher2013reasoning. order-embeddings is our full model. only our full model can do better than the transitive baseline, showing the value of exploiting partial order structure in contrast to using symmetric similarity or learning a general binary relation as most previous work and our bilinear baseline do. the resulting 50-dimensional embeddings are difficult to visualize. to give some intuition for the structure being learned, figure [reference] shows the results of a toy 2d experiment. section: caption-image retrieval the caption-image retrieval task has become a standard evaluation of joint models of vision and language hodosh2013framing, lincvpr14. the task involves ranking a large dataset of images by relevance for a query caption (image retrieval), and ranking captions by relevance for a query image (caption retrieval). given a set of aligned image-caption pairs as training data, the goal is then to learn a caption-image compatibility score to be used at test time. many modern approaches model the caption-image relationship symmetrically, either by embedding into a common\" visual-semantic\" space with inner-product similarity socher2014grounded, kiros2014, or by using canonical correlations analysis between distributed representations of images and captions klein2015fisher. while karpathydeep and plummer2015flickr30k model a finer-grained alignment between regions in the image and segments of the caption, the similarity they use is still symmetric. an alternative is to learn an unconstrained binary relation, either with a neural language model conditioned on the image vinyals2015show, mao2015 or using a multimodal cnn ma2015multimodal. in contrast to these lines of work, we propose to treat the caption-image pairs as a two-level partial order with captions above the images they describe, and let with our order-violation penalty defined in eq ([reference]), and are embedding functions from captions and images into. subsection: loss function to facilitate comparison, we use the same pairwise ranking loss that socher2014grounded, kiros2014 and karpathydeep have used on this task\u2014 simply replacing their symmetric similarity measure with our asymmetric order-violation penalty. this loss function encourages for ground truth caption-image pairs to be greater than that for all other pairs, by a margin: where is a ground truth caption-image pair, goes over captions that no describe, and goes over image not described by. subsection: image and caption embeddings to learn and, we use the approach of kiros2014 except, since we are embedding into, we constrain the embedding vectors to have nonnegative entries by taking their absolute value. thus, to embed images, we use where is a learned matrix, being the dimensionality of the embedding space. is the same image feature used by klein2015fisher: we rescale images to have smallest side pixels, we take crops from the corners, center, and their horizontal reflections, run the 10 crops through the 19-layer vgg network of vgg (weights pre-trained on imagenet and fixed during training), and average their fc7 features. to embed the captions, we use a recurrent neural net encoder with gru activations gru, so, the absolute value of hidden state after processing the last word. subsection: dataset we evaluate on the microsoft coco dataset coco, which has over 120, 000 images, each with at least five human-annotated captions per image. this is by far the largest dataset commonly used for caption-image retrieval. we use the data splits of karpathydeep for training (113, 287 images), validation (5000 images), and test (5000 images). subsection: details of training to train the model, we use the standard pairwise ranking objective from eq. ([reference]). we sample minibatches of 128 random image-caption pairs, and draw all contrastive terms from the minibatch, giving us 127 contrastive images for each caption and captions for each image. we train for 15-30 epochs using the adam optimizer with learning rate, and early stopping on the validation set. we set the dimension of the embedding space and the gru hidden state to, the dimension of the learned word embeddings to, and the margin to. all these hyperparameters, as well as the learning rate and batchsize, were selected using the validation set. for consistency with kiros2014 and to mitigate overfitting, we constrain the caption and image embeddings to have unit l2 norm. this constraint implies that no two points can be exactly ordered with zero order-violation penalty, but since we use a ranking loss, only the relative size of the penalties matters. subsection: results l\u2014 cccc\u2014 cccc align caption retrieval align image retrieval model align r@1 align r@10 align med r align mean ralign r@1 align r@10 align med r align mean ralign align 1k test images align align align align align align align mnlm align 43.4 align 85.8 align 2 align* align 31.0 align 79.9 align 3 align* m-rnn align41.0 align 83.5 align 2 align* align 29.0 align 77.0 align 3 align* dvsa align 38.4 align 80.5 align 1 align* align 27.4 align 74.8 align 3 align* stv align 33.8 align 82.1 align 3 align* align 25.9 align 74.6 align 4 align* fv align 39.4align 80.9 align 2 align 10.4 align 25.1 align 76.6 align 4 align 11.1 m-cnn align 38.3 align 81.0 align 2 align* align 27.4 align 79.5 align 3 align* m-cnn\u2062ens align 42.8 align 84.1 align 2 align* align 32.6 align 82.8 align 3 align* align align align align align align align order-embeddings (reversed) align 11.2 align 44.0 align 14.2 align 86.6 align 12.3 align 53.5 align 9.0 align 30.1order-embeddings (1-crop) align 41.4 align 84.2 align 2.0 align 8.7 align 33.5 align 82.2 align 2.6align 10.0 order-embeddings (symm.) align 45.4 align 88.7 align 2.0 align 5.8 align 36.3 align 85.8 align 2.0 align 9.0 order-embeddings align 46.7 align 88.9 align 2.0 align 5.7 align 37.9 align 85.9 align 2.0 align 8.1 5k test images align align align align align align align dvsa align 11.8 align 45.4 align 12.2 align* align 8.9 align 36.3 align 19.5 align* fv align 17.3 align 50.2 align 10.0 align 46.4 align 10.8 align 40.1 align 17.0 align 49.3 align align align align align align align order-embeddings (symm.) align 21.5 align 62.9 align 6.0 align 24.4 align 16.8 align 56.3 align 8.0 align 40.4 order-embeddings align 23.3 align 65.0 align 5.0 align 24.4 align 18.0 align 57.6 align 7.0 align 35.9 given a query caption or image, we sort all the images or captions of the test set in order of increasing penalty. we use standard ranking metrics for evaluation. we measure recall@k, the percent of queries for which the gt term is one of the first k retrieved; and median and mean rank, which are statistics over the position of the gt term in the retrieval order. table shows a comparison between all state-of-the-art and some older methodsnote that the numbers for mnlm come not from the published paper but from the recently released code at http:// github.com/ ryankiros/ visual-semantic-embedding. along with our own; see for a more complete listing. the best results overall are in bold, and the best results using 1-crop vgg image features are underlined. note that the comparison is additionally complicated by the following:-cnn is an ensemble of four different models, whereas the other entries are all single models. stv and fv use external text corpora to learn their language features, whereas the other methods learn them from scratch. to facilitate the comparison and to evaluate the contributions of various components of our model, we evaluate four variations of order-embeddings: order-embeddings is our full model as described above. order-embeddings (reversed) reverses the order of captions and image embeddings in our order-violation penalty\u2014 placing images above captions in the partial order learned by our model. this seemingly slight variation performs atrociously, confirming our prior that captions are much more abstract than images, and should be placed higher in the semantic hierarchy. order-embeddings (1-crop) computes the image feature using just the center crop, instead of averaging over 10 crops. order-embeddings (symm.) replaces our asymmetric penalty with the symmetric cosine distance, and allows embedding coordinates to be negative\u2014 essentially replicating mnlm, but with better image features. here we find that a different margin (=\u03b10.2) works best. between these four models, the only previous work whose results are incommensurable with ours is dvsa, since it uses the less discriminative cnn of but 20 region features instead of a single whole-image feature. aside from this limitation, and if only single models are considered, order-embeddings significantly outperform the state-of-art approaches for image retrieval even when we control for image features. subsection: exploration why would order-embeddings do well on such a shallow partial order? why are they much more helpful for image retrieval than for caption retrieval? intuitively, symmetric similarity should fail when an image has captions with very different levels of detail, because the captions are so dissimilar that it is impossible to map both their embeddings close to the same image embedding. order-embeddings do n't have this problem: the less detailed caption can be embedded very far away from the image while remaining above it in the partial order. to evaluate this intuition, we use caption length as a proxy for level of detail and select, among pairs of co-referring captions in our validation set, the 100 pairs with the biggest length difference. for image retrieval with 1000 target images, the mean rank over captions in this set is for order-embeddings and for cosine similarity, a much bigger difference than over the entire dataset. some particularly dramatic examples of this are shown in figure [reference]. moreover, if we use the shorter caption as a query, and retrieve captions in order of increasing error, the mean rank of the longer caption is for order-embeddings and for cosine similarity, showing that order-embeddings are able to capture the relatedness of co-referring captions with very different lengths. this also explains why order-embeddings provide a much smaller improvement for caption retrieval than for image retrieval: all the caption retrieval metrics are based on the position of the first ground truth caption in the retrieval order, so the embeddings need only learn to retrieve one of each image's five captions well, which symmetric similarity is well suited for. section: textual entailment/ natural language inference natural language inference can be seen as a generalization of hypernymy from words to sentences. for example, from\" woman walking her dog in a park\" we can infer both\" woman walking her dog\" and\" dog in a park\", but not\" old woman\" or\" black dog\". given a pair of sentences, our task is to predict whether we can infer the second sentence (the hypothesis) from the first (the premise). subsection: loss function to apply order-embeddings to this task, we again view it as partial order completion\u2014 we can infer a hypothesis from a premise exactly when the hypothesis is above the premise in the visual-semantic hierarchy. unlike our other tasks, for which we had to generate contrastive negatives, datasets for natural language inference include labeled negative examples. so, we can simply use a max-margin loss: where are positive and negative pairs of premise and hypothesis. to embed sentences, we use the same gru encoder as in the caption-image retrieval task. subsection: dataset to evaluate order-embeddings on the natural language inference task, we use the recently proposed snli corpus snli, which contains 570, 000 pairs of sentences, each labeled with\" entailment\" if the inference is valid,\" contradiction\" if the two sentences contradict, or\" neutral\" if the inference is invalid but there is no contradiction. our method only allows us to discriminate between entailment and non-entailment, so we merge the\" contradiction\" and\" neutral\" classes together to serve as our negative examples. subsection: implementation details just as for caption-image ranking, we set the dimensions of the embedding space and gru hidden state to be, the dimension of the word embeddings to be, and constrain the embeddings to have unit l2 norm. we train for 10 epochs with batches of sentence pairs. we use the adam optimizer with learning rate and early stopping on the validation set. during evaluation, we find the optimal classification threshold on validation, then use the threshold to classify the test set. subsection: results the state-of-the-art method for 3-class classification on snli is that of rocktaschel2015reasoning. unfortunately, they do not compute 2-class accuracy, so we can not compare to them directly. as a bridge to facilitate comparison, we use a challenging baseline which can be evaluated on both the 2-class and 3-class problems. the baseline, referred to as skip-thoughts, involves a feedforward neural network on top of skip-thought vectors kiros2015skip, a state-of-the-art semantic representation of sentences. given pairs of sentence vectors and, the input to the network is the concatenation of, and the absolute difference. we tuned the number of layers, layer dimensionality and dropout rates to optimize performance on the development set, using the adam optimizer. batch normalization ioffe2015batch and prelu units he2015delving were used. our best network used 2 hidden layers of 1000 units each, with dropout rate of 0.5 across both the input and hidden layers. we did not backpropagate through the skip-thought encoder. we also evaluate against eop classifier, a 2-class baseline introduced by snli, and against a version of our model where our order-violation penalty is replaced with the symmetric cosine distance, order-embeddings (symmetric). the results for all models are shown in table [reference]. we see that order-embeddings outperform the skip-thought baseline despite not using external text corpora. while our method is almost certainly worse than the state-of-the-art method of rocktaschel2015reasoning, which uses a word-by-word attention mechanism, it is also much simpler. section: conclusion and future work we introduced a simple method to encode order into learned distributed representations, which allows us to explicitly model the partial order structure of the visual-semantic hierarchy. our method can be easily integrated into existing relational learning methods, as we demonstrated on three challenging tasks involving computer vision and natural language processing. on two of these tasks, hypernym prediction and caption-image retrieval, our methods outperform all previous work. a promising direction of future work is to learn better classifiers on imagenet imagenet, which has over 21k image classes arranged by the wordnet hierarchy. previous approaches, including frome2013devise and norouzi2013zero have embedded words and images into a shared semantic space with symmetric similarity\u2014 which our experiments suggest to be a poor fit with the partial order structure of wordnet. we expect significant progress on imagenet classification, and the related problems of one-shot and zero-shot learning, to be possible using order-embeddings. going further, order-embeddings may enable learning the entire semantic hierarchy in a single model which jointly reasons about hypernymy, entailment, and the relationship between perception and language, unifying what have been until now almost independent lines of work. subsubsection: acknowledgments we thank kaustav kundu for many fruitful discussions throughout the development of this paper. the work was supported in part by an nserc graduate scholarship. bibliography: references section: supplementary material mikolov2013linguistic showed that word representations learned using word2vec exhibit semantic regularities, such as. kiros2014 showed that similar regularities hold for joint image-language models. we find that order-embeddings exhibit a novel form of regularity, shown in figure [reference]. the elementwise and operations in the embedding space roughly correspond to composition and abstraction, respectively.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "snli",
                        24372
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "gru encoder",
                        24202
                    ],
                    [
                        "skip-thought encoder",
                        26328
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "2-class accuracy",
                        25455
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "natural language processing",
                        654
                    ],
                    [
                        "natural language inference",
                        3703
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "34f92431bb8cb9fa6a1210c5d6e4642079ffac7a-52",
    "doctext": "document: sliding line point regression for shape robust scene text detection traditional text detection methods mostly focus on quadrangle text. in this study we propose a novel method named sliding line point regression (slpr) in order to detect arbitrary-shape text in natural scene. slpr regresses multiple points on the edge of text line and then utilizes these points to sketch the outlines of the text. the proposed slpr can be adapted to many object detection architectures such as faster r-cnn and r-fcn. specifically, we first generate the smallest rectangular box including the text with region proposal network (rpn), then isometrically regress the points on the edge of text by using the vertically and horizontally sliding lines. to make full use of information and reduce redundancy, we calculate x-coordinate or y-coordinate of target point by the rectangular box position, and just regress the remaining y-coordinate or x-coordinate. accordingly we can not only reduce the parameters of system, but also restrain the points which will generate more regular polygon. our approach achieved competitive results on traditional icdar2015 incidental scene text benchmark and curve text detection dataset ctw1500. section: introduction text detection is important in our daily life as it can be applied in many areas, such as digitization of text, text translation, etc. in this study, we focus on scene text detection. some of the previous methods have obtained good results on many horizontal scene texts dataset based on faster r-cnn or ssd. some methods also tried to solve arbitrary-oriented text detection problem. and regressed first a horizontal rectangle and then a quadrilateral. aimed to generate an irregular polygon after regressing a rectangle. the methods mentioned above mostly treated a text line as a quadrilateral which can be completely represented by four points. however, besides the quadrilateral shape, there are many other various shapes of text line in natural scene. therefore, recent research have begun to explore curve text line detection. in this paper we explore both arbitrary-oriented and curve text detection. our method named sliding line point regression (slpr) is based on 2-step object detection methods using faster r-cnn or r-fcn. firstly we propose some interesting rectangular regions with region proposal network (rpn), then regress the points on the edge of text. we generate some rules to determine which points should be regressed so that there will be relevance between points. different from which directly regressed both x-coordinate and y-coordinate of fixed annotated points and employed rnn to learn their relevance, we introduce some rules to vertically and horizontally slide lines along text and then regress the intersection points of sliding lines and text lines as illustrated in fig. [reference]. in this way, we can only regress x-coordinate or y-coordinate of these points, then calculate other coordinates with the position of rectangle, yielding reduction of unnecessary computation and improvement of performance. the contributions of this paper are as follows: 1. we explore regressing multiple points on the border of text line, try to handle arbitrary-oriented and curve text detection based on faster r-cnn and r-fcn. 2. we introduce a sliding line method to determine the ground truth points for the regression, and we make full use of the relevance of these points to generate more regular polygon. section: related work in recent years, scene text detection and recognition has drawn more and more attention. but scene text detection remains a difficult problem due to its complicated orientation and background. all the methods can be divided into three categories: character based methods, word based methods and segmentation based methods. character based methods often need synthetic datasets because labeling characters in text lines requires additional efforts. however, the generated data is greatly deviated from the real data, which can not make the trained model to achieve the state-of-the-art results on real dataset such as the popular icdar2015 incidental scene text benchmark. in order to solve this problem, used semi-supervised method to finetune model on real data and obtained good results. the segmentation based methods have also been used in text detection recently. trained a fully convolutional network (fcn) to predict the salient map of text regions, then traced the text line by combining the salient map and character components. added the border class to separate text from their neighbor. and generated text maps and regress the size and angle of the corresponding quadrilateral, or coordinates of four vertexes at the same time. compared with the traditional segmentation methods, they made a huge breakthrough on icdar2015 incidental scene text benchmark. many methods of object detection can be applied to text detection, e.g., faster r-cnn, ssd, r-fcn and yolo. used irregular convolutional filters instead of the standard convolutional filters to make the network more suitable for long text detection. used the attention map to remove background noise. recently more and more researchers proposed 2-step methods based on faster r-cnn or r-fcn. firstly generated axis-aligned bounding boxes and then regressed the text quadrangle. they used multi-scale pool operations on the roipool layer. tried to segment and detect text simultaneously. considering the particularity of text line, appended different angle anchors which are suitable for arbitrary-oriented text line. more recently, considered the polygon case and labeled a new dataset of curve text. also constructed a curve text dataset named ctw1500, and they proposed a new structure named curve text detector (ctd) to solve curve text detection problem. section: method our model can be applied to any 2-step object detection framework such as faster r-cnn and r-fcn. our system simultaneously regresses the minimum rectangle including text line and the coordinates of some specific points on the boundary of text line. more specifically, take faster r-cnn as an example, we first get some interesting regions using the rpn, then we not only regress the position of the rectangle, but also regress the coordinates of the points on the edge of the text line, finally we can get arbitrary shape text area. subsection: which points should be regressed? obviously, how to determine the point set for restoring the polygon is quite important. we believe the simpler the rules, the easier the neural net learns. we do not regress the fixed points such as vertexes on the polygon because there are a large variety of shapes and angles in natural scene and it is difficult to define the order of fixed feature points for all shapes. although for quadrilateral, we can perfectly restore it by regressing the corresponding four vertices, the determination of the order of four vertices requires a complicated rule which is difficult for the neural net to learn. alternatively, as shown in fig. [reference], we introduce some rules to vertically and horizontally slide the lines (we use equidistant sliding in our experiment) on text line and then regress the intersection of sliding lines and text line border. on the other hand, the correlation exists among the coordinates of different intersection points due to the constraints of the sliding lines. it is not necessary to regress both x-coordinate and y-coordinate of all points simultaneously. if it is horizontal sliding, the x-coordinate of the point on the text boundary can be calculated by the coordinates of the rectangle, so we only need to regress the y-coordinate of these points. similarly, if it is vertical sliding, we only need to regress the x-coordinate of these points. this method not only reduces the computational complexity of the network, but also adds restraints to the regressed points as the prior knowledge which can prevent generating polygons with weird shapes and further improve the accuracy. as for the number of sliding lines, we observe that this parameter is not sensitive to quadrangle text line. but in order to restore other shape text line well, after balancing the performance and network complexity, seven sliding lines are used for we decided to for vertical and horizontal directions, respectively. accordingly a total of 14 lines with 28 intersection points are generated. subsection: the multi-task learning to optimize the neural network parameters, as illustrated in fig. [reference], we adopt the multi-task learning to define the loss function as: where is the region proposal loss, is region proposal classification loss, is box regression loss. is the loss for the second step after rpn. similarly, the first two items and are respectively classification loss and box regression loss., and are the related weighting factors, which are all set to 1 in this study. is the proposed new loss item for slpr: is the smooth l1 loss for the box regression task: in eq. ([reference]), represents the number of sliding lines in one direction and we set in our experiments. in general, each line has two intersection points with the text line border. if there are more than two intersection points, we take the smallest and the largest coordinates. is x-coordinate of the intersection point of vertically sliding lines and text line border while is y-coordinate of the intersection point of horizontally sliding lines and text line border. and are the corresponding estimated points from neural net outputs. for horizontally sliding lines, we only regress the y-coordinate of its intersection point. for vertically sliding lines, we only regress the x-coordinate of its intersection point. the other coordinates can be restored through the coordinates of the rectangle: and represent the minimum x-coordinate and y-coordinate of the rectangular border while and represent the maximum x-coordinate and y-coordinate of the rectangular border. is the floor function. in a word, in order to regress the coordinates of polygon, 32 parameters should be considered including 4 parameters for the rectangle and 28 parameters to represent x and y coordinates of intersection points on text line border. subsection: restoration of polygon through the above slpr method, we can obtain multiple points from the output of neural nets. to restore the final quadrilateral or polygon, the following two approaches are adopted and compared: subsubsection: only using points in long side (pls) the text line always extends to the long side, and the lines that slide along the long side can better reflect the shape of the text. in fact we can restore the polygon by only scanning the long side, as shown in fig. [reference]. specifically, we firstly judge whether the text line is horizontal or vertical through the regressed rectangle, and then restore the polygon through points in the corresponding direction. taking the vertical direction as an example in fig. [reference], since we do not regress the intersection point on the rectangular border, we firstly extend the four lines near the border to find four intersection points with the rectangle, then we connect four new points and other intersection points to generate polygon. subsubsection: using both of horizontal and vertical points (bhvp) in fact, if we use both horizontal and vertical points to restore polygon, we can calculate a polygon or quadrangle that passes through these points roughly as shown in fig. [reference] by using the method in. in this way we can obtain dense enough points in both horizontal and vertical direction and we do not need to calculate the intersection with the rectangle as in pls method. however, we observe bhvp is not as effective as pls for the polygon case. so we use this method only on the quadrilateral dataset (icdar2015 incidental scene text). subsection: polygonal non-maximum suppression non-maximum suppression (nms) is a basic method commonly used in the object detection, and its purpose is to remove duplicate boxes. the traditional nms method is based on rectangular boxes, which is not the best choice for other shapes. in recent years, other nms approaches were investigated, e.g., locality-aware nms, inclined nms, mask-nms and polygonal nms (pnms). as we consider the polygon in this study, both nms and pnms are compared in our experiment. section: experiments subsection: datasets subsubsection: icdar2015 incidental scene text. icdar2015 incidental scene text dataset is commonly used benchmark for detecting arbitrary-angle quadrangular text lines. it contains 1000 images for training, 500 images for testing. some words which are too short, or unclear is annotated as do n't cared samples. subsubsection: ctw1500 curve text dataset (ctw1500) is constructed by yuliang et al.. different from traditional text datasets, a text line is labelled by a polygon with 14 points. subsection: implementation details since our proposed slpr can be applied to any 2-step object detection framework. we adopted faster r-cnn in icdar2015 incidental scene text. and because also proposed a 2-step framework based on r-fcn while presenting the ctw1500 dataset, to perform a fair comparison, we directly used the network in from. all experiments were implemented in caffe by using the nvidia gtx 1080ti gpu. subsubsection: icdar2015 incidental scene text. for faster r-cnn structure, we used an additional anchor and replaced roipool with roialign because the text line is smaller than other objects. we set anchor scales as [] and set ratios as [0.5, 1, 2]. the base network is vgg16, which is initialized by the pre-trained model on imagenet database. we used stochastic gradient descent (sgd) with back-propagation and the maximum iteration was. learning rates started from, decays to one-tenth every iterations. we set weight decay as, and momentum as. we used 1000 training incidental images in icdar2015 incidental scene text and the 229 training images from icdar 2013 to train our network. in order to prevent over-fitting we employed data augmentation. specifically, we randomly resized the images to where the numbers represent the length of the short side, and randomly rotated the images among. subsubsection: ctw1500 we used the curve text detector (ctd) network which is based r-fcn from. also added lstm units named transverse and longitudinal offset connection (tloc) to learn the correlation of points. but we removed it. as we only used pls to restore polygon, eq. ([reference]) was modified as: equals to when is true, otherwise. and are the height and width of the rectangle. because most of the texts in this dataset are horizontal text line, to solve the imbalance between horizontal and vertical samples, we added to balance the losses between them. and when is close to, the text line may be judged as horizontal or vertical, so we set as. the base network is resnet-50, which is initialized by the pre-trained model on imagenet database. the max iteration was. the learning rate in this experiment was always. we set weight decay as, and momentum as. to conduct a fair comparison, we only used the training set in ctw1500 to train our network and did not use data augmentation. subsection: results subsubsection: icdar2015 incidental scene text table [reference] shows the results of slpr system with different settings. first, for the restoration of the quadrangle for the text region, bhvp using all the points can achieve better results than pls using only the long-side points. second, even we aim to detect the quadrangle in this dataset, pnms still outperforms nms. finally, the use of multi-scale is one way to improve detection performance on different target sizes. we also test the multi-scale results of our system at (850, 1000), which yields about 1% absolute improvement of hmean measure. fig. [reference] lists several challenging examples of detection results on icdar2015 incidental scene text dataset. table [reference] gives the comparison of slpr with state-of-the-art results on icdar2015 incidental scene text. we can observe that our method achieved the competitive results on this dataset. subsubsection: ctw1500 table [reference] shows the results of our method with different nms settings. different from the observation in icdar2015 incidental scene text, our method achieved the best result on nms0.3, namely the traditional nms method with the threshold 0.3 for calculating the iou (intersection-over-union). table [reference] lists the results of our method compared with ctd and ctd+ tloc. we removed tloc from as our base network which is the same as ctd. clearly, the hmean performance of our slpr method could be increased by over the ctd method, demonstrating the effectiveness of our simple rules to set the regression points. even compared with the ctd+ tloc method with an additional lstm network, slpr still achieved improvement of hmean performance. fig. [reference] gives several examples of the detection results of ctd, ctd+ tloc and our slpr. we can observe that our method generated smoother regions and better detection results compared with ctd, which implied that the proposed slpr can better handle the arbitrary-oriented case due to the novel design of the horizontally and vertically symmetrical scanning using sliding lines. section: conclusion in this study, we propose a novel slpr method for the text detection in arbitrary-shape case. compared with the curve text detection method ctd+ tloc, slpr is more concise without using lstm and obtains better performance. in the traditional quadrangle dataset (icdar2015 incidental scene text), slpr also achieves the state-of-the-art performance. section: acknowledgment this work was supported in part by the national key r& d program of china under contract no. 2017yfb1002202, in part by the national natural science foundation of china under grants 61671422 and u1613211, in part by the moe-microsoft key laboratory of ustc. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "icdar2015 incidental scene text benchmark",
                        1140
                    ],
                    [
                        "icdar2015",
                        12491
                    ],
                    [
                        "incidental scene text dataset",
                        12501
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "sliding line point regression",
                        10
                    ],
                    [
                        "slpr",
                        287
                    ],
                    [
                        "multi-scale pool operations",
                        5358
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "hmean measure",
                        15862
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "shape robust scene text detection",
                        44
                    ],
                    [
                        "scene text detection",
                        1408
                    ],
                    [
                        "scene text detection and recognition",
                        3519
                    ],
                    [
                        "long text detection",
                        5099
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "curve text detection dataset ctw1500",
                        1186
                    ],
                    [
                        "ctw1500",
                        5715
                    ],
                    [
                        "curve text dataset",
                        12779
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "sliding line point regression",
                        10
                    ],
                    [
                        "slpr",
                        287
                    ],
                    [
                        "multi-scale pool operations",
                        5358
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "hmean measure",
                        15862
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "text detection",
                        1246
                    ],
                    [
                        "curve text line detection",
                        2053
                    ],
                    [
                        "curve text detection",
                        2133
                    ],
                    [
                        "detection",
                        15700
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "35756f711a97166df11202ebe46820a36704ae77-53",
    "doctext": "document: unsupervised representation learning with deep convolutional generative adversarial networks in recent years, supervised learning with convolutional networks (cnns) has seen huge adoption in computer vision applications. comparatively, unsupervised learning with cnns has received less attention. in this work we hope to help bridge the gap between the success of cnns for supervised learning and unsupervised learning. we introduce a class of cnns called deep convolutional generative adversarial networks (dcgans), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. additionally, we use the learned features for novel tasks-demonstrating their applicability as general image representations../ section: introduction learning reusable feature representations from large unlabeled datasets has been an area of active research. in the context of computer vision, one can leverage the practically unlimited amount of unlabeled images and videos to learn good intermediate representations, which can then be used on a variety of supervised learning tasks such as image classification. we propose that one way to build good image representations is by training generative adversarial networks (gans) goodfellow2014, and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks. gans provide an attractive alternative to maximum likelihood techniques. one can additionally argue that their learning process and the lack of a heuristic cost function (such as pixel-wise independent mean-square error) are attractive to representation learning. gans have been known to be unstable to train, often resulting in generators that produce nonsensical outputs. there has been very limited published research in trying to understand and visualize what gans learn, and the intermediate representations of multi-layer gans. in this paper, we make the following contributions we propose and evaluate a set of constraints on the architectural topology of convolutional gans that make them stable to train in most settings. we name this class of architectures deep convolutional gans (dcgan) we use the trained discriminators for image classification tasks, showing competitive performance with other unsupervised algorithms. we visualize the filters learnt by gans and empirically show that specific filters have learned to draw specific objects. we show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples. section: related work subsection: representation learning from unlabeled data unsupervised representation learning is a fairly well studied problem in general computer vision research, as well as in the context of images. a classic approach to unsupervised representation learning is to do clustering on the data (for example using k-means), and leverage the clusters for improved classification scores. in the context of images, one can do hierarchical clustering of image patches coates2012learning to learn powerful image representations. another popular method is to train auto-encoders (convolutionally, stacked vincent2010stacked, separating the what and where components of the code zhao2015stacked, ladder structures rasmus2015semi) that encode an image into a compact code, and decode the code to reconstruct the image as accurately as possible. these methods have also been shown to learn good feature representations from image pixels. deep belief networks lee2009convolutional have also been shown to work well in learning hierarchical representations. subsection: generating natural images generative image models are well studied and fall into two categories: parametric and non-parametric. the non-parametric models often do matching from a database of existing images, often matching patches of images, and have been used in texture synthesis efros1999texture, super-resolution freeman2002example and in-painting hays2007scene. parametric models for generating images has been explored extensively (for example on mnist digits or for texture synthesis portilla2000parametric). however, generating natural images of the real world have had not much success until recently. a variational sampling approach to generating images kingma2013auto has had some success, but the samples often suffer from being blurry. another approach generates images using an iterative forward diffusion process sohl2015deep. generative adversarial networks goodfellow2014 generated images suffering from being noisy and incomprehensible. a laplacian pyramid extension to this approach denton2015deep showed higher quality images, but they still suffered from the objects looking wobbly because of noise introduced in chaining multiple models. a recurrent network approach gregor2015draw and a deconvolution network approach dosovitskiy2014learning have also recently had some success with generating natural images. however, they have not leveraged the generators for supervised tasks. subsection: visualizing the internals of cnns one constant criticism of using neural networks has been that they are black-box methods, with little understanding of what the networks do in the form of a simple human-consumable algorithm. in the context of cnns, zeiler et. al. zeiler2014visualizing showed that by using deconvolutions and filtering the maximal activations, one can find the approximate purpose of each convolution filter in the network. similarly, using a gradient descent on the inputs lets us inspect the ideal image that activates certain subsets of filters inceptionism2015. section: approach and model architecture historical attempts to scale up gans using cnns to model images have been unsuccessful. this motivated the authors of lapgan denton2015deep to develop an alternative approach to iteratively upscale low resolution generated images which can be modeled more reliably. we also encountered difficulties attempting to scale gans using cnn architectures commonly used in the supervised literature. however, after extensive model exploration we identified a family of architectures that resulted in stable training across a range of datasets and allowed for training higher resolution and deeper generative models. core to our approach is adopting and modifying three recently demonstrated changes to cnn architectures. the first is the all convolutional net springenberg2014striving which replaces deterministic spatial pooling functions (such as maxpooling) with strided convolutions, allowing the network to learn its own spatial downsampling. we use this approach in our generator, allowing it to learn its own spatial upsampling, and discriminator. second is the trend towards eliminating fully connected layers on top of convolutional features. the strongest example of this is global average pooling which has been utilized in state of the art image classification models inceptionism2015. we found global average pooling increased model stability but hurt convergence speed. a middle ground of directly connecting the highest convolutional features to the input and output respectively of the generator and discriminator worked well. the first layer of the gan, which takes a uniform noise distribution as input, could be called fully connected as it is just a matrix multiplication, but the result is reshaped into a 4-dimensional tensor and used as the start of the convolution stack. for the discriminator, the last convolution layer is flattened and then fed into a single sigmoid output. see fig. [reference] for a visualization of an example model architecture. third is batch normalization ioffe2015batch which stabilizes learning by normalizing the input to each unit to have zero mean and unit variance. this helps deal with training problems that arise due to poor initialization and helps gradient flow in deeper models. this proved critical to get deep generators to begin learning, preventing the generator from collapsing all samples to a single point which is a common failure mode observed in gans. directly applying batchnorm to all layers however, resulted in sample oscillation and model instability. this was avoided by not applying batchnorm to the generator output layer and the discriminator input layer. the relu activation nair2010rectified is used in the generator with the exception of the output layer which uses the tanh function. we observed that using a bounded activation allowed the model to learn more quickly to saturate and cover the color space of the training distribution. within the discriminator we found the leaky rectified activation maas2013rectifier xu2015empirical to work well, especially for higher resolution modeling. this is in contrast to the original gan paper, which used the maxout activation goodfellow2013maxout. architecture guidelines for stable deep convolutional gans replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). use batchnorm in both the generator and the discriminator. remove fully connected hidden layers for deeper architectures. use relu activation in generator for all layers except for the output, which uses tanh. use leakyrelu activation in the discriminator for all layers. section: details of adversarial training we trained dcgans on three datasets, large-scale scene understanding (lsun) yu2015construction, imagenet-1k and a newly assembled faces dataset. details on the usage of each of these datasets are given below. no pre-processing was applied to training images besides scaling to the range of the tanh activation function [- 1, 1]. all models were trained with mini-batch stochastic gradient descent (sgd) with a mini-batch size of 128. all weights were initialized from a zero-centered normal distribution with standard deviation 0.02. in the leakyrelu, the slope of the leak was set to 0.2 in all models. while previous gan work has used momentum to accelerate training, we used the adam optimizer kingma2014adam with tuned hyperparameters. we found the suggested learning rate of 0.001, to be too high, using 0.0002 instead. additionally, we found leaving the momentum term at the suggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped stabilize training. subsection: lsun as visual quality of samples from generative image models has improved, concerns of over-fitting and memorization of training samples have risen. to demonstrate how our model scales with more data and higher resolution generation, we train a model on the lsun bedrooms dataset containing a little over 3 million training examples. recent analysis has shown that there is a direct link between how fast models learn and their generalization performance hardt2015train. we show samples from one epoch of training (fig. [reference]), mimicking online learning, in addition to samples after convergence (fig. [reference]), as an opportunity to demonstrate that our model is not producing high quality samples via simply overfitting/ memorizing training examples. no data augmentation was applied to the images. subsubsection: deduplication to further decrease the likelihood of the generator memorizing input examples (fig. [reference]) we perform a simple image de-duplication process. we fit a 3072-128-3072 de-noising dropout regularized relu autoencoder on 32x32 downsampled center-crops of training examples. the resulting code layer activations are then binarized via thresholding the relu activation which has been shown to be an effective information preserving technique and provides a convenient form of semantic-hashing, allowing for linear time de-duplication. visual inspection of hash collisions showed high precision with an estimated false positive rate of less than 1 in 100. additionally, the technique detected and removed approximately 275, 000 near duplicates, suggesting a high recall. subsection: faces we scraped images containing human faces from random web image queries of peoples names. the people names were acquired from dbpedia, with a criterion that they were born in the modern era. this dataset has 3 m images from 10 k people. we run an opencv face detector on these images, keeping the detections that are sufficiently high resolution, which gives us approximately 350, 000 face boxes. we use these face boxes for training. no data augmentation was applied to the images. subsection: imagenet-1k we use imagenet-1k deng2009imagenet as a source of natural images for unsupervised training. we train on min-resized center crops. no data augmentation was applied to the images. section: empirical validation of dcgans capabilities subsection: classifying cifar-10 using gans as a feature extractor one common technique for evaluating the quality of unsupervised representation learning algorithms is to apply them as a feature extractor on supervised datasets and evaluate the performance of linear models fitted on top of these features. on the cifar-10 dataset, a very strong baseline performance has been demonstrated from a well tuned single layer feature extraction pipeline utilizing k-means as a feature learning algorithm. when using a very large amount of feature maps (4800) this technique achieves 80.6% accuracy. an unsupervised multi-layered extension of the base algorithm reaches 82.0% accuracy coates2011selecting. to evaluate the quality of the representations learned by dcgans for supervised tasks, we train on imagenet-1k and then use the discriminator's convolutional features from all layers, maxpooling each layers representation to produce a spatial grid. these features are then flattened and concatenated to form a 28672 dimensional vector and a regularized linear l2-svm classifier is trained on top of them. this achieves 82.8% accuracy, out performing all k-means based approaches. notably, the discriminator has many less feature maps (512 in the highest layer) compared to k-means based techniques, but does result in a larger total feature vector size due to the many layers of spatial locations. the performance of dcgans is still less than that of exemplar cnns, a technique which trains normal discriminative cnns in an unsupervised fashion to differentiate between specifically chosen, aggressively augmented, exemplar samples from the source dataset. further improvements could be made by finetuning the discriminator's representations, but we leave this for future work. additionally, since our dcgan was never trained on cifar-10 this experiment also demonstrates the domain robustness of the learned features. subsection: classifying svhn digits using gans as a feature extractor on the streetview house numbers dataset (svhn) netzer2011reading, we use the features of the discriminator of a dcgan for supervised purposes when labeled data is scarce. following similar dataset preparation rules as in the cifar-10 experiments, we split off a validation set of 10, 000 examples from the non-extra set and use it for all hyperparameter and model selection. 1000 uniformly class distributed training examples are randomly selected and used to train a regularized linear l2-svm classifier on top of the same feature extraction pipeline used for cifar-10. this achieves state of the art (for classification using 1000 labels) at 22.48% test error, improving upon another modifcation of cnns designed to leverage unlabled data zhao2015stacked. additionally, we validate that the cnn architecture used in dcgan is not the key contributing factor of the model's performance by training a purely supervised cnn with the same architecture on the same data and optimizing this model via random search over 64 hyperparameter trials bergstra2012hpopt. it achieves a signficantly higher 28.87% validation error. section: investigating and visualizing the internals of the networks we investigate the trained generators and discriminators in a variety of ways. we do not do any kind of nearest neighbor search on the training set. nearest neighbors in pixel or feature space are trivially fooled theis2015d by small image transforms. we also do not use log-likelihood metrics to quantitatively assess the model, as it is a poor theis2015d metric. subsection: walking in the latent space the first experiment we did was to understand the landscape of the latent space. walking on the manifold that is learnt can usually tell us about signs of memorization (if there are sharp transitions) and about the way in which the space is hierarchically collapsed. if walking in this latent space results in semantic changes to the image generations (such as objects being added and removed), we can reason that the model has learned relevant and interesting representations. the results are shown in fig. [reference]. subsection: visualizing the discriminator features previous work has demonstrated that supervised training of cnns on large image datasets results in very powerful learned features zeiler2014visualizing. additionally, supervised cnns trained on scene classification learn object detectors oquab14. we demonstrate that an unsupervised dcgan trained on a large image dataset can also learn a hierarchy of features that are interesting. using guided backpropagation as proposed by springenberg2014striving, we show in fig. [reference] that the features learnt by the discriminator activate on typical parts of a bedroom, like beds and windows. for comparison, in the same figure, we give a baseline for randomly initialized features that are not activated on anything that is semantically relevant or interesting. subsection: manipulating the generator representation subsubsection: forgetting to draw certain objects in addition to the representations learnt by a discriminator, there is the question of what representations the generator learns. the quality of samples suggest that the generator learns specific object representations for major scene components such as beds, windows, lamps, doors, and miscellaneous furniture. in order to explore the form that these representations take, we conducted an experiment to attempt to remove windows from the generator completely. on 150 samples, 52 window bounding boxes were drawn manually. on the second highest convolution layer features, logistic regression was fit to predict whether a feature activation was on a window (or not), by using the criterion that activations inside the drawn bounding boxes are positives and random samples from the same images are negatives. using this simple model, all feature maps with weights greater than zero (200 in total) were dropped from all spatial locations. then, random new samples were generated with and without the feature map removal. the generated images with and without the window dropout are shown in fig. [reference], and interestingly, the network mostly forgets to draw windows in the bedrooms, replacing them with other objects. subsubsection: vector arithmetic on face samples in the context of evaluating learned representations of words mikolov2013distributed demonstrated that simple arithmetic operations revealed rich linear structure in representation space. one canonical example demonstrated that the vector (\"king\")-vector (\"man\")+ vector (\"woman\") resulted in a vector whose nearest neighbor was the vector for queen. we investigated whether similar structure emerges in the representation of our generators. we performed similar arithmetic on the vectors of sets of exemplar samples for visual concepts. experiments working on only single samples per concept were unstable, but averaging the vector for three examplars showed consistent and stable generations that semantically obeyed the arithmetic. in addition to the object manipulation shown in (fig. [reference]), we demonstrate that face pose is also modeled linearly in space (fig. [reference]). these demonstrations suggest interesting applications can be developed using representations learned by our models. it has been previously demonstrated that conditional generative models can learn to convincingly model object attributes like scale, rotation, and position dosovitskiy2014learning. this is to our knowledge the first demonstration of this occurring in purely unsupervised models. further exploring and developing the above mentioned vector arithmetic could dramatically reduce the amount of data needed for conditional generative modeling of complex image distributions. section: conclusion and future work we propose a more stable set of architectures for training generative adversarial networks and we give evidence that adversarial networks learn good representations of images for supervised learning and generative modeling. there are still some forms of model instability remaining-we noticed as models are trained longer they sometimes collapse a subset of filters to a single oscillating mode. further work is needed to tackle this from of instability. we think that extending this framework to other domains such as video (for frame prediction) and audio (pre-trained features for speech synthesis) should be very interesting. further investigations into the properties of the learnt latent space would be interesting as well. subsubsection: acknowledgments we are fortunate and thankful for all the advice and guidance we have received during this work, especially that of ian goodfellow, tobias springenberg, arthur szlam and durk kingma. additionally we'd like to thank all of the folks at indico for providing support, resources, and conversations, especially the two other members of the indico research team, dan kuster and nathan lintz. finally, we'd like to thank nvidia for donating a titan-x gpu used in this work. bibliography: references section: supplementary material subsection: evaluating dcgans capability to capture data distributions we propose to apply standard classification metrics to a conditional version of our model, evaluating the conditional distributions learned. we trained a dcgan on mnist (splitting off a 10 k validation set) as well as a permutation invariant gan baseline and evaluated the models using a nearest neighbor classifier comparing real data to a set of generated conditional samples. we found that removing the scale and bias parameters from batchnorm produced better results for both models. we speculate that the noise introduced by batchnorm helps the generative models to better explore and generate from the underlying data distribution. the results are shown in table [reference] which compares our models with other techniques. the dcgan model achieves the same test error as a nearest neighbor classifier fitted on the training dataset-suggesting the dcgan model has done a superb job at modeling the conditional distributions of this dataset. at one million samples per class, the dcgan model outperforms infimnist loosli-canu-bottou-2006, a hand developed data augmentation pipeline which uses translations and elastic deformations of training examples. the dcgan is competitive with a probabilistic generative data augmentation technique utilizing learned per class transformations hauberg2015 while being more general as it directly models the data instead of transformations of the data.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar-10",
                        13013
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "deep convolutional generative adversarial networks",
                        52
                    ],
                    [
                        "dcgans",
                        519
                    ],
                    [
                        "deep convolutional gans",
                        2387
                    ],
                    [
                        "dcgan",
                        2413
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "generating images",
                        4295
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "3858e5175799b97805b2b70ff54e8a7e0718870f-54",
    "doctext": "document: deep learning for smile recognition inspired by recent successes of deep learning in computer vision, we propose a novel application of deep convolutional neural networks to facial expression recognition, in particular smile recognition. a smile recognition test accuracy of 99.45% is achieved for the denver intensity of spontaneous facial action (disfa) database, significantly outperforming existing approaches based on hand-crafted features with accuracies ranging from 65.55% to 79.67%. the novelty of this approach includes a comprehensive model selection of the architecture parameters, allowing to find an appropriate architecture for each expression such as smile. this is feasible because all experiments were run on a tesla k40c gpu, allowing a speedup of factor 10 over traditional computations on a cpu. section: introduction neural networks are celebrating a comeback under the term\" deep learning\" for the last ten years by training many hidden layers allowing to self-learn complex feature hierarchies. this makes them of particular interest for computer vision, in which feature description is a long-standing issue. many advances have been reported in this period, including new training methods and a paradigm shift of training from cpus to gpus. as a result, those advances allow to train more reliable models much faster. this has for example resulted in breakthroughs in signal processing. nonetheless, deep neural networks are not a magic bullet and successful training is still heavily based on experimentation. the facial action coding system (facs) is a system to taxonomize any facial expression of a human being by their appearance on the face. action units describe muscles or muscle groups in the face, are set or unset and the activation may be on different intensity levels. state-of-the art approaches in this field mostly rely on hand-crafted features leaving a lot of potential for higher accuracies. in contrast to other fields such as face or gesture recognition, only very few works on deep learning applied to facial expression recognition have been reported so far in which the architecture parameters are fixed. we are not aware of publications in which the architecture of a deep neural network for facial expression recognition is subject to extensive model selection. this allows to learn appropriate architectures per action unit. section: deep neural networks training neural networks is difficult, as their cost functions have many local minima. the more hidden layers, the more difficult the training of a neural network. hence, training tends to converge to a local minimum, resulting in poor generalization of the network. in order to overcome these issues, a variety of new concepts have been proposed in the literature, of which only a few can be named in this chapter. unsupervised pre-training methods, such as autoencoders allow to initialize the weights well in order for backpropagation to quickly optimize them. the rectified linear unit (relu) and dropout are new regularization methods. the new training methods and other new concepts can also lead to significant improvements of shallow neural networks with just a few hidden layers. convolutional neural networks (cnns) were initially proposed by lecun for the recognition of hand-written digits. a cnn consists of two layers: a convolutional layer, followed by a subsampling layer. inspired by biological processes and exploiting the fact that nearby pixels are strongly correlated, cnns are relatively insensitive to small translations or rotations of the image input. training deep neural networks is slow due to the number of parameters in the model. as the training can be described in a vectorized form, it is possible to massively parallelize it. modern gpus have thousands of cores and are therefore an ideal candidate for the execution of the training of neural networks. significant speedups of factor 10 or higher have been reported. a difficulty is to write gpu code. in the last few years, more abstract libraries have been released. section: disfa database the denver intensity of spontaneous facial action (disfa) database consists of 27 videos of 4844 frames each, with 130, 788 images in total. action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. disfa was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. in detail, 30, 792 have this action unit set, 82, 176 images have some action unit (s) set and 48, 612 images have no action unit (s) set at all. fig. [reference] contains a sample image of disfa. [width=0.2] mouth [width=0.2] face in the original paper on disfa multi-class svms were trained for the different levels 0-5 of action unit intensity. test accuracies for the individual levels and for the binary action unit recognition problem are reported for three different hand-crafted feature description techniques. in those three cases, accuracies of 65.55%, 72.94% and 79.67% for smile recognition are reported. section: smile recognition in the following experiments, an aligned version of disfa is used. in this aligned version, the faces have been cropped and annotated with facial landmark points. facial landmark points allow to compute a bounding box to fit the mouth in all images. in the experiments, two inputs are used: the mouth and face, downscaled to and pixels, respectively. both inputs are used to assess if the mouth alone is as expressive as or even more expressive than the entire face for smile recognition. subsection: model the architecture of the network is as follows: the input images are fed into a convolution comprising a convolutional and a subsampling layer. that convolution may be followed by more convolutions to become gradually more invariant to distortions in the input. in the second stage, a regular neural network follows the convolutions in order to discriminate the features learned by the convolutions. the output layer consists of two units for smile or no smile. the novelty of this approach is that the exact number of convolutions, number of hidden layers and size of hidden layers are not fixed but subject to extensive model selection in sec. [reference]. subsection: experiment setting due to training time constraints, some parameters have been fixed to reasonable and empirical values, such as the size of convolutions (pixels, 32 feature maps) and the size of subsamplings (pixels using max pooling). all layers use relu units, except of softmax being used in the output layer. the learning rate is fixed to and not subject to model selection as it would significantly prolong the model selection. the same considerations apply to the momentum, which is fixed to. the entire database has been randomly split into a 60%/ 20%/ 20% training/ validation/ test ratio. training neural networks comes with uncertainties, mostly due to the random initialization of the weights, but also due to that random split of the data. evaluations have shown that for 10 similar experiments carried out, the standard deviation of the test accuracy is 0.041725%. because of this low standard deviation, performing each experiment exactly once has only a very low bias and is therefore relatively safe to do for reasons of faster training time. throughout the experiments, the classification rate is used as the accuracy measure. the model is implemented using lasagne and the generated cuda code is executed on a tesla k40c as training on a gpu allows to perform a comprehensive model selection in a feasible amount of time. stochastic gradient descent with a batch size of 500 is used. subsection: parameter optimization table [reference] contains the four parameters to be optimized: the number of convolutions, the number of hidden layers, the number of units per hidden layer and the dropout factor. each parameter was optimized independently due to training time constraints. this may not lead to an optimal model, but has proven to work empirically well. each model was trained for 50 epochs in the model selection. parameters and possible values used in model selection. for both inputs, table [reference] contains the final models selected. for the mouth input, there is a preference to more convolutions and more hidden layers. this is the case because slight translations or rotations in the mouth input have stronger consequences on the classification result. in the entire face, that sort of distortions may be less of a problem because other parts of the face such as the cheeks contribute to smile recognition, too. selected parameter values for mouth and face input. subsection: results and discussion both final models were trained for 1000 epochs. the test accuracies of both models started to converge after about 300 epochs. for the mouth and face inputs, the best accuracies were achieved after 700 and 1000 epochs with 99.45% and 99.34%, respectively. both models significantly outperform the state-of-the-art svm baselines reported in sec. [reference] ranging from 65.55% to 79.67%. overall, there is no strong preference for either the mouth or face input. further experiments with a reduced dataset containing only 70% of the images that have no action unit (s) set at all support this hypothesis. concretely, the test accuracies for the mouth and face input reduced to 99.24% and 99.26%, respectively. thus, the difference between the two models has been further reduced and this time giving a very low preference for the face input. nonetheless, this difference is not representative as it is within the experiment error standard deviation reported in sec. [reference]. training time per epoch are 82 seconds and 41 seconds for the mouth and face input models, respectively. experiments have shown that the training time mostly depends on the number of convolutions. using the tesla k40c gpu has allowed to speed up the training time by factor ten over the use of a cpu to execute the cpu code generated by the library. this clearly demonstrates the importance of training on a gpu to do a comprehensive model selection in a feasible amount of time. section: conclusions and future work deep learning is an umbrella term for training neural networks with potentially many hidden layers using new training methods allowing to learn complex feature hierarchies from data. applied to action unit recognition and smile recognition in particular, a deep convolutional neural network model with an overall accuracy of 99.45% significantly outperforms existing approaches. the underlying extensive model selection allows to find for each action unit an appropriate architecture in order to maximize test accuracies. in the future, we will extend the model to images from multiple databases and to make predictions in image sequences. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "denver intensity of spontaneous facial action",
                        312
                    ],
                    [
                        "disfa",
                        360
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "deep convolutional neural networks",
                        146
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "smile recognition test accuracy",
                        250
                    ],
                    [
                        "accuracies",
                        460
                    ],
                    [
                        "test accuracy",
                        7221
                    ],
                    [
                        "accuracy measure",
                        7497
                    ],
                    [
                        "accuracy",
                        10612
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "smile recognition",
                        28
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "398c296d0cc7f9d180f84969f8937e6d3a413796-55",
    "doctext": "document: multi-column deep neural networks for image classification traditional methods of computer vision and machine learning can not match human performance on tasks such as the recognition of handwritten digits or traffic signs. our biologically plausible deep artificial neural network architectures can. small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. only winner neurons are trained. several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. graphics cards allow for fast training. on the very competitive mnist handwriting benchmark, our method is the first to achieve near-human performance. on a traffic sign recognition benchmark it outperforms humans by a factor of two. we also improve the state-of-the-art on a plethora of common image classification benchmarks. 04-12 section: introduction recent publications suggest that unsupervised pre-training of deep, hierarchical neural networks improves supervised pattern classification. here we train such nets by simple online back-propagation, setting new, greatly improved records on mnist, latin letters, chinese characters, traffic signs, norb (jittered, cluttered) and cifar10 benchmarks. we focus on deep convolutional neural networks (dnn), introduced by, improved by, refined and simplified by. lately, dnn proved their mettle on data sets ranging from handwritten digits (mnist), handwritten characters to 3d toys (norb) and faces. dnns fully unfold their potential when they are big and deep. but training them requires weeks, months, even years on cpus. high data transfer latency prevents multi-threading and multi-cpu code from saving the situation. in recent years, however, fast parallel neural net code for graphics cards (gpus) has overcome this problem. carefully designed gpu code for image classification can be up to two orders of magnitude faster than its cpu counterpart. hence, to train huge dnn in hours or days, we implement them on gpu, building upon the work of. the training algorithm is fully online, i.e. weight updates occur after each error back-propagation step. we will show that properly trained big and deep dnns can outperform all previous methods, and demonstrate that unsupervised initialization/ pretraining is not necessary (although we do n't deny that it might help sometimes, especially for small datasets). we also show how combining several dnn columns into a multi-column dnn (mcdnn) further decreases the error rate by 30-40%. section: architecture the initially random weights of the dnn are iteratively trained to minimize the classification error on a set of labeled training images; generalization performance is then tested on a separate set of test images. our architecture does this by combining several techniques in a novel way: (1) unlike the shallow nn used in many 1990s applications, ours are deep, inspired by the neocognitron, with many (6-10) layers of non-linear neurons stacked on top of each other, comparable to the number of layers found between retina and visual cortex of macaque monkeys. (2) it was shown that such multi-layered dnn are hard to train by standard gradient descent, the method of choice from a mathematical/ algorithmic point of view. today's computers, however, are fast enough for this, more than 60000 times faster than those of the early 90s. carefully designed code for massively parallel graphics processing units (gpus normally used for video games) allows for gaining an additional speedup factor of 50-100 over serial code for standard computers. given enough labeled data, our networks do not need additional heuristics such as unsupervised pre-training or carefully prewired synapses. (3) the dnn of this paper (fig. [reference] a) have 2-dimensional layers of winner-take-all neurons with overlapping receptive fields whose weights are shared. given some input pattern, a simple max pooling technique determines winning neurons by partitioning layers into quadratic regions of local inhibition, selecting the most active neuron of each region. the winners of some layer represent a smaller, down-sampled layer with lower resolution, feeding the next layer in the hierarchy. the approach is inspired by hubel and wiesel's seminal work on the cat's primary visual cortex, which identified orientation-selective simple cells with overlapping local receptive fields and complex cells performing down-sampling-like operations. (4) note that at some point down-sampling automatically leads to the first 1-dimensional layer. from then on, only trivial 1-dimensional winner-take-all regions are possible, that is, the top part of the hierarchy becomes a standard multi-layer perceptron (mlp). receptive fields and winner-take-all regions of our dnn often are (near -) minimal, e.g., only 2x2 or 3x3 neurons. this results in (near -) maximal depth of layers with non-trivial (2-dimensional) winner-take-all regions. in fact, insisting on minimal 2x2 fields automatically defines the entire deep architecture, apart from the number of different convolutional kernels per layer and the depth of the plain mlp on top. (5) only winner neurons are trained, that is, other neurons can not forget what they learnt so far, although they may be affected by weight changes in more peripheral layers. the resulting decrease of synaptic changes per time interval corresponds to biologically plausible reduction of energy consumption. our training algorithm is fully online, i.e. weight updates occur after each gradient computation step. (6) inspired by microcolumns of neurons in the cerebral cortex, we combine several dnn columns to form a multi-column dnn (mcdnn). given some input pattern, the predictions of all columns are democratically averaged. before training, the weights (synapses) of all columns are randomly initialized. various columns can be trained on the same inputs, or on inputs preprocessed in different ways. the latter helps to reduce both error rate and number of columns required to reach a given accuracy. the mcdnn architecture and its training and testing procedures are illustrated in figure [reference]. section: experiments in the following we give a detailed description of all the experiments we performed. we evaluate our architecture on various commonly used object recognition benchmarks and improve the state-of-the-art on all of them. the description of the dnn architecture used for the various experiments is given in the following way: 2x48x48-100c5-mp2-100c5-mp2-100c4-mp2-300n-100n-6n represents a net with 2 input images of size 48x48, a convolutional layer with 100 maps and 5x5 filters, a max-pooling layer over non overlapping regions of size 2x2, a convolutional layer with 100 maps and 4x4 filters, a max-pooling layer over non overlapping regions of size 2x2, a fully connected layer with 300 hidden units, a fully connected layer with 100 hidden units and a fully connected output layer with 6 neurons (one per class). we use a scaled hyperbolic tangent activation function for convolutional and fully connected layers, a linear activation function for max-pooling layers and a softmax activation function for the output layer. all dnn are trained using on-line gradient descent with an annealed learning rate. during training, images are continually translated, scaled and rotated (even elastically distorted in case of characters), whereas only the original images are used for validation. training ends once the validation error is zero or when the learning rate reaches its predetermined minimum. initial weights are drawn from a uniform random distribution in the range. subsection: mnist the original mnist digits are normalized such that the width or height of the bounding box equals 20 pixels. aspect ratios for various digits vary strongly and we therefore create six additional datasets by normalizing digit width to 10, 12, 14, 16, 18, 20 pixels. this is like seeing the data from different angles. we train five dnn columns per normalization, resulting in a total of 35 columns for the entire mcdnn. all 1x29x29-20c4-mp2-40c5-mp3-150n-10n dnn are trained for around 800 epochs with an annealed learning rate (i.e. initialized with 0.001 multiplied by a factor of 0.993/ epoch until it reaches 0.00003). training a dnn takes almost 14 hours and after 500 training epochs little additional improvement is observed. during training the digits are randomly distorted before each epoch (see fig. [reference] a for representative characters and their distorted versions). the internal state of a single dnn is depicted in figure [reference] c, where a particular digit is forward propagated through a trained network and all activations together with the network weights are plotted. results of all individual nets and various mcdnn are summarized in table [reference]. mcdnn of 5 nets trained with the same preprocessor achieve better results than their constituent dnns, except for original images (tab. [reference]). the mcdnn has a very low 0.23% error rate, improving state of the art by at least 34% (tab. [reference]). this is the first time an artificial method comes close to the 0.2% error rate of humans on this task. many of the wrongly classified digits either contain broken or strange strokes, or have wrong labels. the 23 errors (fig. [reference] b) are associated with 20 correct second guesses. we also trained a single dnn on all 7 datasets simultaneously which yielded worse result (0.52%) than both mcdnn and their individual dnn. this shows that the improvements come from the mcdnn and not from using more preprocessed data. how are the mcdnn errors affected by the number of preprocessors? we train 5 dnns on all 7 datasets. a mcdnn' out-of-7' (from 1 to 7) averages nets trained on datasets. table [reference] shows that more preprocessing results in lower mcdnn error. we also train 5 dnn for each odd normalization, i.e. w11, w13, w15, w17 and w19. the 60-net mcdnn performs (0.24%) similarly to the 35-net mcdnn, indicating that additional preprocessing does not further improve recognition. we conclude that mcdnn outperform dnn trained on the same data, and that different preprocessors further decrease the error rate. subsection: nist sd 19 the 35-columns mcdnn architecture and preprocessing used for mnist are also applied to latin characters from nist sd 19. for all tasks our mcdnn achieves recognition rates 1.5-5 times better than any published result (tab. [reference]). in total there are 82000 characters in the test set, but there are many more easy to classify digits (58000) than hard to classify letters (24000). this explains the lower overall error rate of the 62-class problem compared to the 52-class letters problem. from all errors of the 62-class problem 3% of the 58000 digits are misclassified and 33% of the 24000 letters are misclassified. letters are in general more difficult to classify, but there is also a higher amount of confusion between similar lower-and upper-case letters such as i, i and o, o for example. indeed, error rates for the case insensitive task drop from 21% to 7.37%. if the confused upper-and lower-case classes are merged, resulting in 37 different classes, the error rate is only slightly bigger (7.99%). upper-case letters are far easier to classify (1.83% error rate) than lowercase letters (7.47%) due to the smaller writer dependent in-class variability. for a detailed analysis of all the errors and confusions between different classes, the confusion matrix is most informative (supplementary material fig. s1). subsection: chinese characters compared to latin character recognition, isolated chinese character recognition is a much harder problem, mainly because of the much larger category set, but also because of wide variability of writing styles, and the confusion between similar characters. we use a dataset from the institute of automation of chinese academy of sciences (casia), which contains 300 samples for each of 3755 characters (in gb1 set). this resulted in a data set with more than 1 million characters (3 gb of data) which posed a major computational challenge even to our system. without our fast gpu implementation the nets on this task would train for more than one year. only the forward propagation of the training set takes 27h on a normal cpu, and training a single epoch would consequently have lasted several days. on our fast gpu implementation on the other hand, training a single epoch takes 3.4h, which makes it feasible to train a net within a few days instead of many months. we train following dnn, 1x48x48-100c3-mp2-200c2-mp2-300c2-mp2-400c2-mp2-500n-3755n, on offline as well as on online characters. for the offline character recognition task, we resize all characters to 40x40 pixels and place them in the center of a 48x48 image. the contrast of each image is normalized independently. as suggested by the organizers, the first 240 writers from the database casia-hwdb1.1 are used for training and the remaining 60 writers are used for testing. the total numbers of training and test characters are 938679 and 234228, respectively. for the online dataset, we draw each character from its list of coordinates, resize the resulting images to 40x40 pixels and place them in the center of a 48x48 image. additionally, we smooth-out the resulting images with a gaussian blur filter over a 3x3 pixel neighborhood and uniform standard deviation of 0.75. as suggested by the organizers, the characters of 240 writers from database casia-olhwdb1.1 are used for training the classifier and the characters of the remaining 60 writers are used for testing. the resulting numbers of training and test characters are 939564 and 234800, respectively. all methods previously applied to this dataset perform some feature extraction followed by a dimensionality reduction, whereas our method directly works on raw pixel intensities and learns the feature extraction and dimensionality reduction in a supervised way. on the offline task we obtain an error rate of 6.5% compared to 10.01% of the best method. even though much information is lost when drawing a character from it's coordinate sequence, we obtain a recognition rate of 5.61% on the online task compared to 7.61% of the best method. we conclude that on this very hard classification problem, with many classes (3755) and relatively few samples per class (240), our fully supervised dnn beats the current state-of-the-art methods by a large margin. subsection: traffic signs recognizing traffic signs is essential for the automotive industry's efforts in the field of driver's assistance, and for many other traffic-related applications. we use the gtsrb traffic sign dataset. the original color images contain one traffic sign each, with a border of 10% around the sign. they vary in size from to pixels and are not necessarily square. the actual traffic sign is not always centered within the image; its bounding box is part of the annotations. the training set consists of 26640 images; the test set of 12569 images. we crop all images and process only within the bounding box. our dnn implementation requires all training images to be of equal size. after visual inspection of the image size distribution we resize all images to pixels. as a consequence, scaling factors along both axes are different for traffic signs with rectangular bounding boxes. resizing forces them to have square bounding boxes. our mcdnn is the only artificial method to outperform humans, who produced twice as many errors. since traffic signs greatly vary in illumination and contrast, standard image preprocessing methods are used to enhance/ normalize them (fig. [reference] a and supplementary material). for each dataset five dnn are trained (architecture: 3x48x48-100c7-mp2-150c4-150mp2-250c4-250mp2-300n-43n), resulting in a mcdnn with 25 columns, achieving an error rate of 0.54% on the test set. figure [reference] b depicts all errors, plus ground truth and first and second predictions. over 80% of the 68 errors are associated with correct second predictions. erroneously predicted class probabilities tend to be very low\u2014 here the mcdnn is quite unsure about its classifications. in general, however, it is very confident\u2014 most of its predicted class probabilities are close to one or zero. rejecting only 1% percent of all images (confidence below 0.51) results in an even lower error rate of 0.24%. to reach an error rate of 0.01% (a single misclassification), only 6.67% of the images have to be rejected (confidence below 0.94). our method outperforms the second best algorithm by a factor of 3. it takes 37 hours to train the mcdnn with 25 columns on four gpus. the trained mcdnn can check 87 images per second on one gpu (and 2175 images/ s/ dnn). subsection: cifar 10 cifar10 is a set of natural color images of 32x32 pixels. it contains 10 classes, each with 5000 training samples and 1000 test samples. images vary greatly within each class. they are not necessarily centered, may contain only parts of the object, and show different backgrounds. subjects may vary in size by an order of magnitude (i.e., some images show only the head of a bird, others an entire bird from a distance). colors and textures of objects/ animals also vary greatly. our dnn input layers have three maps, one for each color channel (rgb). we use a 10-layer architecture with very small kernels: 3x32x32-300c3-mp2-300c2-mp2-300c3-mp2-300c2-mp2-300n-100n-10n. just like for mnist, the initial learning rate 0.001 decays by a factor of 0.993 after every epoch. transforming cifar color images to gray scale reduces input layer complexity but increases error rates. hence we stick to the original color images. as for mnist, augmenting the training set with randomly (by at most 5%) translated images greatly decreases the error from 28% to 20% (the nn-inherent local translation invariance by itself is not sufficient). by additional scaling (up to 15%), rotation (up to), and up to 15% translation, the individual net errors decrease by another 3% (tab. [reference]). the above small maximal bounds prevent loss of too much information leaked beyond the pixels rectangle. we repeat the experiment with different random initializations and compute mean and standard deviation of the error, which is rather small for original images, showing that our dnn are robust. our mcdnn obtains a very low error rate of 11.21%, greatly rising the bar for this benchmark. the confusion matrix (figure [reference]) shows that the mcdnn almost perfectly separates animals from artifacts, except for planes and birds, which seems natural, although humans easily distinguish almost all the incorrectly classified images, even if many are cluttered or contain only parts of the objects/ animals (see false positive and false negative images in figure [reference]). there are many confusions between different animals; the frog class collects most false positives from other animal classes, with very few false negatives. as expected, cats are hard to tell from dogs, collectively causing 15.25% of the errors. the mcdnn with 8 columns (four trained on original data and one trained for each preprocessing used also for traffic signs) reaches a low 11.21% error rate, far better than any other algorithm. subsection: norb we test a mcdnn with four columns on norb (jittered-cluttered), a collection of stereo images of 3d models (figure [reference]). the objects are centrally placed on randomly chosen backgrounds, and there is also cluttering from a peripherally placed second object. this database is designed for experimenting with 3d object recognition from shape. it contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. the objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees). the training set has 10 folds of 29160 images each for a total of 291600 images; the testing set consists of two folds totalizing 58320 images. no preprocessing is used for this dataset. we scale down images from the original 108x108 to 48x48 pixels. this size is big enough to preserve the details present in images and small enough to allow fast training. we perform two rounds of experiments, using only the first two folds (to compare with previous results that do not use the entire training data) and using all training data. we tested several distortion parameters with small nets and found that maximum rotation of, maximum translation of 15% and maximum scaling of 15% are good choices, hence we use them for all norb experiments. to compare to previous results, we first train only on the first 2-folds of the data. the net architecture is deep, but has few maps per layer: 2x48x48-50c5-mp2-50c5-mp2-50c4-mp2-300n-100n-6n. the learning rate setup is: eta start 0.001; eta factor 0.95; eta stop 0.000003. due to small net size, training is fast at 156s/ epoch for 114 epochs. testing one sample requires 0.5ms. even when we use less data to train, the mcdnn greatly improves the state of the art from 5% to 3.57% (table [reference]). our method is fast enough to process the entire training set though. we use the same architecture but double the number of maps when training with all 10 folds: 2x48x48-100c5-mp2-100c5-mp2-100c4-mp2-300n-100n-6n. the learning rate setup remains the same. training time increases to 34min/ epoch because the net is bigger, and we use five times more data. testing one sample takes 1.3ms. all of this pays off, resulting in a very low 2.70% error rate, further improving the state of the art. although norb has only six classes, training and test instances sometimes differ greatly, making classification hard. more than 50% of the errors are due to confusions between cars and trucks. considering second predictions, too, the error rate drops from 2.70% to 0.42%, showing that 84% of the errors are associated with a correct second prediction. section: conclusion this is the first time human-competitive results are reported on widely used computer vision benchmarks. on many other image classification datasets our mcdnn improves the state-of-the-art by 30-80% (tab. [reference]). we drastically improve recognition rates on mnist, nist sd 19, chinese characters, traffic signs, cifar10 and norb. our method is fully supervised and does not use any additional unlabeled data source. single dnn already are sufficient to obtain new state-of-the-art results; combining them into mcdnns yields further dramatic performance boosts. section: acknowledgment this work was partially supported by a fp7-ict-2009-6 eu grant under project code 270247: a neuro-dynamic framework for cognitive robotics: scene representations, behavioral sequences, and learning. bibliography: references section: supplementary material subsection: experiment details subsubsection: nist sd 19 the confusion matrix of the 62 characters task (fig. [reference]) shows that most of the errors are due to confusions between digits and letters and between lower-and upper-case letters. not very surprisingly, the confusion matrix for the digit task (fig. [reference]) shows that confusions between fours and nines are the most common error source. for the 52 letter task (case sensitive) the confusion matrix (fig. [reference]) shows that the mcdnn has mainly problems with upper-and lower-case confusions of the same letter. other hard-to-distinguish classes are:' q' and' g',' l' and' i'. for the upper-case letter task the confusion matrix (figure [reference]) shows that the mcdnn has problems with letters of similar shape, i.e.' d', and' o',' v' and' u' etc. the total error of 1.82% is very low though. for the lower-case letter task the confusion matrix (fig. [reference]) shows that like with upper-case letters, the mcdnn has problems with letters of similar shapes, i.e.' g', and' q',' v' and' u' etc. but the total error is much higher (7.47%) than for the upper-case letters task. for the merged-case letter task (37 classes) the confusion matrix (figure [reference]) shows that the mcdnn has mostly problems with letters of similar shapes, i.e.' l', and' i'. all upper-lower-case confusions of identical letters from the 52 class task vanish, the error shrinks by a factor of almost three down to 7.99%. the experiments on different subsets of the 62 character task clearly show that it is very hard to distinguish between small and capital letters. also, digits 0 and 1 are hard to separate from letters o and i. many of these problems could be alleviated by incorporating context where possible. subsubsection: traffic signs high contrast variation among the images calls for normalization. we test the following standard contrast normalizations: image adjustment (imadjust) increases image contrast by mapping pixel intensities to new values such that 1% of the data is saturated at low and high intensities. histogram equalization (histeq) enhances contrast by transforming pixel intensities such that the output image histogram is roughly uniform. adaptive histogram equalization (adapthisteq) operates (unlike histeq) on tiles rather than the entire image, we tiled the image in 8 nonoverlapping regions of 6x6 pixels. each tile's contrast is enhanced such that its histogram becomes roughly uniform. contrast normalization (conorm) enhances edges, filtering the input image by a difference of gaussians, using a filter size of 5x5 pixels. note that the above normalizations, except conorm, are performed in a color space with image intensity as one of its components. for this purpose we transform the image from rgb-to lab-space, then perform normalization, then transform the result back to rgb-space. the effect of the four different normalizations is summarized in figure [reference], where histograms of pixel intensities together with original and normalized images are shown. the dnn have three maps for the input layer, one for each color channel (rgb). the rest of the net architecture is detailed in table [reference]. we use a 10-layer architecture with very small max-pooling kernels. subsubsection: cifar10 subsubsection: norb",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar10 benchmarks",
                        1369
                    ],
                    [
                        "cifar 10",
                        16966
                    ],
                    [
                        "cifar10",
                        16975
                    ],
                    [
                        "cifar color images",
                        17759
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "multi-column deep neural networks",
                        10
                    ],
                    [
                        "multi-column dnn",
                        2602
                    ],
                    [
                        "mcdnn",
                        2621
                    ],
                    [
                        "mcdnn architecture",
                        6211
                    ],
                    [
                        "60-net mcdnn",
                        10113
                    ],
                    [
                        "35-columns mcdnn architecture",
                        10410
                    ],
                    [
                        "nn",
                        18034
                    ],
                    [
                        "mcdnns",
                        22750
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "image classification",
                        48
                    ],
                    [
                        "image classification benchmarks",
                        979
                    ],
                    [
                        "classification problem",
                        14475
                    ],
                    [
                        "classification",
                        21960
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "gtsrb traffic sign dataset",
                        14855
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "multi-column deep neural networks",
                        10
                    ],
                    [
                        "multi-column dnn",
                        2602
                    ],
                    [
                        "mcdnn",
                        2621
                    ],
                    [
                        "mcdnn architecture",
                        6211
                    ],
                    [
                        "60-net mcdnn",
                        10113
                    ],
                    [
                        "35-columns mcdnn architecture",
                        10410
                    ],
                    [
                        "nn",
                        18034
                    ],
                    [
                        "mcdnns",
                        22750
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "accuracy",
                        6197
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "traffic signs",
                        219
                    ],
                    [
                        "traffic sign recognition",
                        841
                    ],
                    [
                        "recognition",
                        10240
                    ],
                    [
                        "3d object recognition",
                        19803
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "gtsrb traffic sign dataset",
                        14855
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "multi-column deep neural networks",
                        10
                    ],
                    [
                        "multi-column dnn",
                        2602
                    ],
                    [
                        "mcdnn",
                        2621
                    ],
                    [
                        "mcdnn architecture",
                        6211
                    ],
                    [
                        "60-net mcdnn",
                        10113
                    ],
                    [
                        "35-columns mcdnn architecture",
                        10410
                    ],
                    [
                        "nn",
                        18034
                    ],
                    [
                        "mcdnns",
                        22750
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "error rate",
                        2649
                    ],
                    [
                        "classification error",
                        2773
                    ],
                    [
                        "validation error",
                        7641
                    ],
                    [
                        "learning rate",
                        7678
                    ],
                    [
                        "mcdnn errors",
                        9793
                    ],
                    [
                        "mcdnn error",
                        10015
                    ],
                    [
                        "error rates",
                        11215
                    ],
                    [
                        "recognition rate",
                        14357
                    ],
                    [
                        "net errors",
                        18200
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "traffic signs",
                        219
                    ],
                    [
                        "traffic sign recognition",
                        841
                    ],
                    [
                        "recognition",
                        10240
                    ],
                    [
                        "3d object recognition",
                        19803
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "mnist",
                        1281
                    ],
                    [
                        "nist sd 19",
                        10515
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "multi-column deep neural networks",
                        10
                    ],
                    [
                        "multi-column dnn",
                        2602
                    ],
                    [
                        "mcdnn",
                        2621
                    ],
                    [
                        "mcdnn architecture",
                        6211
                    ],
                    [
                        "60-net mcdnn",
                        10113
                    ],
                    [
                        "35-columns mcdnn architecture",
                        10410
                    ],
                    [
                        "nn",
                        18034
                    ],
                    [
                        "mcdnns",
                        22750
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "error",
                        18007
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "image classification",
                        48
                    ],
                    [
                        "image classification benchmarks",
                        979
                    ],
                    [
                        "classification problem",
                        14475
                    ],
                    [
                        "classification",
                        21960
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "3bf52841ade9f6adc55e102a4460af922e52bb7d-56",
    "doctext": "we introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. the system represents the state-of-the-art in latent variable models for both the imagenet and omniglot datasets. we show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'. conceptualcompression section: introduction images contain a large amount of information that is a priori stored independently in the pixels. in the semi-supervised learning regime where a large number of images is available but only a small number of labels, one would like to leverage this information to create representations that allow for better (and especially faster) generalization. intuitively one expects such representations to explicitly extract global conceptual aspects of an image. in this paper we propose a method that is able to transform an image into a progression of increasingly detailed representations, ranging from global conceptual aspects to low level details (see figures [reference]& [reference]). at the same time, our model greatly improves latent variable image modeling compared to earlier implementations of deep variational auto-encoders. furthermore, it has the advantage of being a simple homogeneous architecture not requiring complex design choices, which is similar to the recurrent structure of draw (). last, it provides an important insight into building good variational auto-encoder models of images: the use of multiple layers of stochastic variables that are all 'close' to the pixels significantly improves performance. the system's ability to stratify information enables it to perform high quality lossy compression, by storing only a subset of latent variables, starting with the high level ones, and generating the remainder during decompression (see figure [reference]). currently the ultimate arbiter of lossy compression remains human evaluation. other simple measures such as the l2 distance between compressed and original images are inappropriate- for example if a particular generated grass texture is sharp, but different from the one in the original image, it will yield a large l2 distance yet should, at the same time, be considered conceptually close to the original. achieving good lossy compression while storing only high level latent variables would imply that representations learned at a high level contain information similar to that used by humans to judge images. as humans outperform the best machines at learning abstract representations, human evaluation of lossy compression obtained by these generative models constitutes a reasonable test of the quality of representations learned by these models. in the following we discuss variational auto-encoders and compression in more detail, present the algorithm and demonstrate the results on generation quality and compression. subsection: variational auto-encoders numerous techniques exist for unsupervised learning in deep networks, e.g. sparse auto-encoders and sparse coding, denoising auto-encoders, deconvolutional networks, restricted boltzmann machines, deep boltzmann machines, generative adversarial networks and variational auto-encoders. in this paper we focus on the class of models in the variational auto-encoding framework. since we are also interested in compression, we present them from an information-theoretic perspective. variational auto-encoders typically consist of two neural networks: one that generates samples from latent variables ('imagination'), and one that infers latent variables from observations ('recognition'). the two networks share the latent variables. intuitively speaking one might think of these variables as specifying, for a given image, at different levels of abstraction, whether a particular object such as a cat or a dog is present in the input, or perhaps what the exact position and intensity of an edge at a given location might be. during the recognition phase the network acquires information about the input and stores it in the latent variables, reducing their uncertainty. for example, at first not knowing whether a cat or a dog is present in the image, the network observes the input and becomes nearly certain that it is a cat. the reduction in uncertainty is quantitatively equal to the amount of information the network acquired about the input. during generation the network starts with uncertain latent variables and selects their values from a prior distribution that specifies this uncertainty (e.g. it chooses a dog). different choices will produce different samples. variational auto-encoders provide a natural framework for unsupervised learning- we can build networks with layers of stochastic variables and expect that, after learning, the representations become increasingly more abstract for higher levels of the hierarchy. the questions then are: can such a framework indeed discover such representations both in principle and in practice, are such networks powerful enough for modeling real data, and what techniques one needs to make it work well. subsection: conceptual compression variational auto-encoders can not only be used for representation learning but also for compression. the training objective of variational auto-encoders is to compress the total amount of information needed to encode the input. they achieve this by using information-carrying latent variables that express what, before compression, was encoded using a larger amount of information in the input. the information in the layers and the remaining information in the input can be encoded in practice as explained later in this paper. the amount of lossless compression one is able to achieve is bounded by the underlying entropy of the image distribution. additionally, most image information as measured in bits is contained in the fine details of the image. thus we might reasonably expect that lossless compression will never improve by more than a factor of two in comparison to current performance. lossy compression, on the other hand, holds much more potential for improvement. in this case we want to compress an image by a certain amount, allowing some information loss, while maximizing both quality and similarity to the original image. as an example, at a low level of compression (close to lossless compression), we could start by reducing pixel precision, e.g. from 8 bits to 7 bits. then, as in jpeg, we could express a local 8x8 neighborhood in a discrete cosine transform basis and store only the most significant components. this way, instead of introducing quantization artifacts in the image that would appear if we kept decreasing pixel precision, we preserve higher level structures but to a lower level of precision. however, what can we do beyond that as we keep pushing the compression? we would like to preserve the most important aspects of the image. what determines what is important? let us imagine that we are compressing images of cats and dogs and would like to compress an image down to one bit. what would that bit be? one would imagine that it should represent whether the image contains either a cat or a dog. how would we then get an image out of this single bit? if we have a good generative model, we can simply generate the entire image from this one latent variable, an image of a cat if the bit corresponds to 'cat', and an image of a dog otherwise. now let us imagine that instead of compressing to one bit we wanted to compress down to ten bits. now we can store the most important properties of the animal as well- e.g. its type, color, and basic pose. the rest would be 'filled in' by the generative model that is conditioned on this information. if we increase the number of bits further we can preserve more and more about the image, while generating the fine details such as hair, or the exact pattern of the floor, etc. most bits are in fact about such low level details. we call this kind of compression- compressing by giving priority to higher levels of representation and generating the remainder- 'conceptual compression'. we suggest that this should be the ultimate objective of lossy compression. importantly, if we solve deep representation learning with latent variable generative models that generate high quality samples, we achieve the objective of lossy compression mentioned above. we can see this as follows. assume that the network has learned a hierarchy of progressively more abstract representations. then, to get different levels of compression, we can store only the corresponding number of topmost layers and generate the rest. by solving unsupervised deep learning, the network would order information according to its importance and store it with that priority. while the ultimate goal of unsupervised learning remains elusive, we make a step in this direction, and show that our network learns to order information from a rather global level to precise details in images, without being hand-engineered to do this explicitly, as illustrated in figures [reference] and [reference]. this information separation already allows us to achieve better compression quality than jpeg and jpeg2000 as shown in figure [reference]. while we are not bound by the same constraints as these algorithms, such as speed and memory, these results demonstrate the potential of this method, which will get better as latent variable generative models improve. subsection: the importance of recurrent feedback what are the challenges involved in turning latent variable models into state-of-the-art generative models of images? many successful vision architectures (e.g.) have highly over-complete representations that contain many more neurons in hidden layers than pixels. these representations need to be combined to get a very sharp distribution at the pixel level if the pixels are modeled independently. this distribution corresponds to salt and pepper noise which is not present in natural images to a perceptible level. this poses a major challenge. after experimenting with deep variational auto-encoders we concluded that it was exceedingly difficult to obtain satisfactory results with a single computational pass through the network. instead we propose that the network needs the ability to correct itself over a number of time steps. thus, sharp reconstructions should not be a property of high-precision values in the network, but should rather be the result of an iterative feedback mechanism that is robust to network parameter change. such a mechanism is provided by the draw algorithm, which is a recurrent type of variational auto-encoder. at each time step, draw maintains a provisionary reconstruction, takes in information about a given image, stores it in latent variables and updates the reconstruction. keeping track of the reconstruction aids the iterative feedback mechanism which is learned by back-propagation. computation is both deep- in iterations- and close to the pixels. we introduce convolutional draw. it features convolutions, latent prior modeling, a gaussian input distribution (for natural images) and, in some experiments, a multi-layer architecture. however, it does not use an explicit attentional mechanism. we note that even the single-layer version is already a deep generative model which can decide to process higher level information first before focusing on details, as we demonstrate to some degree. we also experiment with making convolutional draw hierarchical in a similar way that we would build conventional deep variational auto-encoders- stacking more layers of latent and deterministic variables. we believe that the recurrence is important not just for accurate pixel reconstructions, but also at higher levels. for example, when the network decides to generate edges at different locations, it needs to make sure that they are aligned. it is hard to imagine this happening in a single computational pass through the network. similarly at higher levels, when it decides to generate objects, they need to be generated with the right relationship to one another. and finally at the scene level, one probably does not generate entire scenes at once, but rather one step at a time. subsection: comparison to non-variational models let us relate this discussion to two other families of generative models, specifically generative adversarial networks (gans;) and auto-regressive pixel models. gans have been demonstrated to be able to generate realistic looking images, with properly aligned edges, using a simple feedforward generative network. gans also contain two networks- a generative network that is the same as in variational auto-encoders, and a classification network. the classification network is presented with both real and model-generated images and tries to classify them according to their true nature- real or model-generated. the generative network gets gradients from the classification network, changing its weights in an attempt to make the generated images be judged as real ones by the classification network. this makes the generation network produce realistic images that 'fool' the classification network. it needs to produce a wide diversity of images, not just one realistic image, because if it produced only one (or a small number of them), the classification network would classify that image as generated and others as realistic, and be almost always correct. this actually happens in practice, and one has to apply a variety of techniques, e.g. as in, to obtain sufficient image diversity. however the extent of gans' sampling diversity is unknown and currently there is no satisfactory measure for it. so while a given network does n't produce just one image, it is possible that it produces only a tiny subset of possible realistic images, as it simply competes with the power of the classifier. for example if it generates a sharp image, it is unclear whether the system is also capable of generating its translated version, or simply a slightly distorted version. finally there is another way to get low uncertainty at the pixel level: instead of predicting pixels independently given the latents, we can decide not to use latents and iterate sequentially over the pixels, predicting a given pixel from the previous ones (from top left to bottom right) in an autoregressive fashion. this is as 'close' to the pixels as one can possibly be, and furthermore the procedure is purely deterministic. the disadvantage is conceptual- the information and decisions are not done at a conceptual level but at the pixel level. for example when generating cats vs dogs the decisions at the first set of pixels (top left of the image) will contain no information about a hypothetical cat or dog. but as we get to the region where these objects are, we start choosing pixels that will slowly tip the probability of generating a cat vs a dog one way or the other. as we start generating an ear, it will more likely be a cat's or a dog's and so on. however this pixel level approach and our approach are orthogonal and can be easily combined, for example by feeding the output of convolutional draw into the conditional computation of a pixel level model. in this paper we study the latent variable approach and make the pixels independent given the latents. section: convolutional draw in this section we describe the details of a single-layer version of the algorithm. convolutional draw contains the following variables: input, reconstruction, reconstruction error, the state of the encoder recurrent net, the state of the decoder recurrent net and latent variable. the variables, and are recurrent (passed between different time steps) and are initialized with learned biases. then at each time step, convolutional draw performs the following updates: where denotes a convolution and rnn denotes a recurrent network. we use lstm with convolutional operators instead of the usual linear ones. the final value of contains the parameters of the input distribution. for binary images we use the bernoulli distribution. for natural images we use the gaussian distribution with mean and log variance given by splitting the vector to obtain the input cost and the total cost: where the handling of real valued-ness of the inputs ([reference], [reference]) is explained below, and being the standard setting. the algorithm is schematically illustrated in the first layer of figure [reference]. the network is trained by calculating the gradient of this loss and using a stochastic gradient descent algorithm. stochastic back-propagation through a sampling function is done as in variational auto-encoders. both the approximate posterior and the prior are gaussian, with mean and log variance being linear functions of or, respectively. let us discuss how we handle the input distribution for natural images. each pixel (per color) is one of 256 values. we could use a soft-max distribution to model it. this would result in a rather large output vector at every time step and also does not take advantage of the underlying real valued-ness of intensities and therefore we opted for a gaussian distribution. however this still needs to be converted to a discrete distribution over 256 values to calculate the negative likelihood loss. instead of this, we add uniform noise to the input with width corresponding to the spacing between discrete values and calculate where with if inputs are scaled to the interval. subsection: multi-layer architectures next we explain how we can stack convolutional draw with a two layer example. the first layer is the same as the one just introduced. the second layer has the same structure: recurrent encoder, recurrent decoder and a stochastic layer. the input to the second layer is the mean of the approximate posterior of the first layer. the output of the second layer biases the prior of the latent variable of the first layer and is also passed as input into the first layer decoder recurrent net. this is illustrated in figure [reference]. we do n't use any reconstruction or error in the second layer. here we describe a given computational step in detail. indices and denote the variables of layers and, respectively. in addition, let be the mean of. then, the update at a given time step is given by systems with more layers can be built analogously. section: compression here we show how one can turn variational auto-encoders including convolutional draw into compression algorithms. we have not built the actual compressor, however, as we explain, we have strong reasons to believe it would perform as well as calculated here. two basic approaches exist. the first one is less convenient because it needs to add extra data to the bitstream when compressing an image but has essentially a guaranteed compression rate. the other one may require some experimentation but is expected to yield a similar compression rate and can be used on a given image without needing extra data. the underlying compression mechanism for all cases is arithmetic coding. arithmetic coding takes as input a sequence of discrete variables and a set of probabilities that predict the variable at time from previous variables. it then compresses this sequence to bits plus a constant of order one. compressing inputs using variational auto-encoders proceeds as follows: discretize each latent variable in each layer using the width of (eq. [reference]), treat the resulting variables as a sequence with predictions (eq. [reference]) and compress using arithmetic coding. for this to work as explained, several things are needed. first, the discretization should be independent of the input. this can be achieved by training the network with the variance of being a learned constant that does not depend on the input. we found that this does not have much effect on the likelihood. second, one should evaluate the log likelihood using this discretization. one has to decide on the exact manner in which should be computed for each discrete value. significant tuning might be required here, for the obtained likelihoods to be as good as the ones obtained with sampling. however this is likely to be fruitful since there exists a second, less convenient way to compress that is guaranteed to achieve this rate. this second approach uses bits-back coding. we explain only the basic idea here. we discretize the latents down to a very high level of precision and use to transmit the information. because the discretization precision is high, the probabilities for discrete values are easily assigned. that will preserve the information but it will cost many bits, namely where is a probability under that discretization. now, instead of sampling from the approximate posterior when encoding an input, we encode bits of other information into the choice of, that we also want to transmit. when is recovered at the receiving end, both the information about the current input and the other information is recovered and thus the information needed to encode the current input is. the expectation of this quantity is the kl-divergence in ([reference]), which therefore measures the amount of information stored in a given latent layer. the disadvantage of this approach is that we can not encode a given input without also having some other information we want to transmit. however, this coding scheme works even if the variance of the approximate posterior is dependent on the input. section: results for natural images, all models except otherwise specified were single-layer, with, a kernel size of, and stride 2 convolutions between input layers and hidden layers with latent feature maps. we trained the models on cifar-10 and imagenet with and lstm feature maps respectively. we use the version of imagenet presented in that will soon be released as a standard dataset. we train the network with the adam algorithm with learning rate. occasionally, we find that the cost suddenly increases dramatically. this is probably due to the gaussian nature of the distribution, when a given variable is produced too far from the mean relative to sigma. we observed this happening approximately once per run. to be able to keep training we store older parameters, detect such jumps and revert to the old parameters when they occur. the network then just keeps training as if nothing had happened. subsection: modeling quality subsubsection: omniglot the recently introduced omniglot dataset is comprised of character classes drawn from multiple alphabets with just samples per class. referred to by many as the 'inverse of mnist', it was designed to study conceptual representations and generative models in a low-data regime. table [reference] shows likelihoods of different models compared to ours. for our model, we only calculate the upper bound (variational bound) and therefore the true likelihood is actually better. samples generated by the model are shown in figure [reference]. subsubsection: cifar-10 table [reference] shows likelihoods of different models on cifar-10. we see that our method outperforms previous methods except for the just released pixel rnn model of. as mentioned, the advantage of our model compared to such auto-regressive models is that it is a latent variable model that can be used for representation learning and lossy compression. at the same time, the two approaches are orthogonal and can be combined, for example by feeding the output of convolutional draw into the recurrent network of pixel rnn. we also report the likelihood for a (non-recurrent) variational auto-encoder and standard draw. for the variational auto-encoder we tested architectures with multiple layers, both deterministic and stochastic but with standard functional forms, and this was the best result that we obtained. convolutional draw performs significantly better. subsubsection: imagenet additionally, we trained on the version of imagenet prepared in which was created with the aim of making a standardized dataset to test generative models. the results are in table [reference]. note that being a new dataset, no other methods have been reported on it. in figure [reference] and figure [reference] we show generations from the model. we trained networks with varying input cost scales as explained in the next section. the generations are sharp and contain many details, unlike previous versions of variational auto-encoder that tend to generate blurry images. subsection: input cost scaling each pixel (and color channel) of the data consists of 256 values, and as such, likelihood and lossless compression are well defined. when compressing the image there is much to be gained in capturing precise correlations between nearby pixels. there are a lot more bits in these low level details than in the higher level structure that we are actually interested in when learning higher level representations. the network might focus on these details, ignoring higher level structure. one way to make it focus less on the details is to scale down the cost of the input relative to the latents, that is, setting in ([reference]). generations for different cost scalings are shown in figure [reference], with the original objective being scale. we see that lower scales indeed have a 'cleaner' high level structure. scale 1 contains a lot of information at the precise pixel values and the network tries to capture that, while not being good enough to properly align details and produce realistic patterns. this might be simply a matter of scaling, making layers larger, networks deeper, using more iterations, or using better functional forms. subsection: the dependence on computational depth convolutional draw uses many iterations and might be considered expensive. however we found that networks with a larger number of time steps train faster per data example as shown in figure [reference] (left). to study how they train with respect to real time, we multiply the time scale of each input by the number of iterations as seen in figure [reference] (right). we see that despite having to do several iterations, up to about, convolutional draw does not take more wall clock time to train than the same architecture with smaller. for larger, the training slows down, but it does eventually reach better performance than at lower. subsection: information distribution we look at how much information different levels and time steps contain. this information is simply the kl divergence in ([reference]) and ([reference]). for a two layer system with one convolutional and one fully connected layer, this is shown in figure [reference]. we see that the higher level contains information mainly at the beginning of the computation, whereas the lower layer starts with low information which then gradually increases. this is desirable from a conceptual point of view. it suggests that the network first finds out about an overall structure of the image and then explains the details contained within that structure. understanding the overall structure rapidly is also convenient if the algorithm needs to respond to observations in a timely manner. subsection: lossy compression we can compress an image with loss of information by storing only a subset of the latent variables, typically the high levels of the hierarchy. we can do this in multilayer convolutional draw, storing only higher levels. however we can also store only a subset of time steps, specifically a given number of time steps at the beginning, and let the network generate the rest. the units not stored should be generated from the prior distribution (equation [reference]). however we can also generate a more likely image by lowering the variance of the prior gaussian. we show generations with full variance in row 3 of each block of figure [reference] and with variance zero in row 4 of that figure. we see that using the original variance, the network generates sharp details. because the generative model is not perfect, the resulting images are less realistic looking as we lower the number of stored time steps. for zero variance we see that the network starts with rough details making a smooth image and then refines it with more time steps. all these generations are produced with a single-layer convolutional draw, and thus, despite being single-layer, it achieves some level of 'conceptual compression' by first capturing the global structure of the image and then focusing on details. there is another dimension we can vary for lossy compression- the input scale introduced in subsection [reference]. even if we store all the latent variables (but not the input bits), the reconstructed images will get less detailed as we scale down the input cost. to build a really good compressor, at each compression rate, we need to find which of the networks, input scales and number of time steps would produce visually good images. for several compression levels, we have looked at images produced by different methods and selected qualitatively which network gave the best looking images. we have not done this per image, just per compression level. we then display compressed images that we have not seen with this selection. we compare our results to jpeg and jpeg2000 compression which we obtained using imagemagick. we found however that these compressors are unable to produce reasonable results for small images () at high compression rates. instead, we concatenated 100 images into one image, compressed that and extracted back the compressed small images. the number of bits per image reported is then the number of bits of this image divided by 100. this is actually unfair to our algorithm since any correlations between nearby images can be exploited. nevertheless we show the comparison in figure [reference]. our algorithm shows better quality than jpeg and jpeg 2000 at all levels where a corruption is easily detectable. note that even when our algorithm is trained on one specific image size, it can be used on arbitrarily sized images for those networks that contain only convolutional operators. section: conclusion in this paper, we introduced convolutional draw, a state-of-the-art generative model which demonstrates the potential of sequential computation and recurrent neural networks in scaling up latent variable models. during inference, the algorithm sequentially arrives at a natural stratification of information, ranging from global aspects to low-level details. an interesting feature of the method is that, when we restrict ourselves to storing just the high level latent variables, we arrive at a 'conceptual compression' algorithm that rivals the quality of jpeg2000. as a generative model, it outperforms earlier latent variable models on both the omniglot and imagenet datasets. section: acknowledgements we thank aaron van den oord, diederik kingma and koray kavukcuoglu for fruitful discussions. bibliography: references section: appendix figures [reference], [reference] show image generations for input scaling and of ([reference]), while figures [reference], [reference] show generations, also for and.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar-10",
                        21910
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "convolutional draw",
                        11223
                    ],
                    [
                        "convolutional draw hierarchical",
                        11687
                    ]
                ]
            ],
            "Metric": [],
            "Task": []
        }
    ]
}
{
    "docid": "3cdb1364c3e66443e1c2182474d44b2fb01cd584-57",
    "doctext": "document: segnet: a deep convolutional encoder-decoder architecture for image segmentation we present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed segnet. this core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. the architecture of the encoder network is topologically identical to the 13 convolutional layers in the vgg16 network. the role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. the novelty of segnet lies is in the manner in which the decoder upsamples its lower resolution input feature map (s). specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. this eliminates the need for learning to upsample. the upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. we compare our proposed architecture with the widely adopted fcn and also with the well known deeplab-largefov, deconvnet architectures. this comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. segnet was primarily motivated by scene understanding applications. hence, it is designed to be efficient both in terms of memory and computational time during inference. it is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. we also performed a controlled benchmark of segnet and other architectures on both road scenes and sun rgb-d indoor scene segmentation tasks. these quantitative assessments show that segnet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. we also provide a caffe implementation of segnet and a web demo at. deep convolutional neural networks, semantic pixel-wise segmentation, indoor scenes, road scenes, encoder, decoder, pooling, upsampling. section: introduction semantic segmentation has a wide array of applications ranging from scene understanding, inferring support-relationships among objects to autonomous driving. early methods that relied on low-level vision cues have fast been superseded by popular machine learning algorithms. in particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images. now there is an active interest for semantic pixel-wise labelling,,,,,,,,,,,. however, some of these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. the results, although very encouraging, appear coarse. this is primarily because max pooling and sub-sampling reduce feature map resolution. our motivation to design segnet arises from this need to map low resolution features to input resolution for pixel-wise classification. this mapping must produce features which are useful for accurate boundary localization. our architecture, segnet, is designed to be an efficient architecture for pixel-wise semantic segmentation. it is primarily motivated by road scene understanding applications which require the ability to model appearance (road, building), shape (cars, pedestrians) and understand the spatial-relationship (context) between different classes such as road and side-walk. in typical road scenes, the majority of the pixels belong to large classes such as road, building and hence the network must produce smooth segmentations. the engine must also have the ability to delineate objects based on their shape despite their small size. hence it is important to retain boundary information in the extracted image representation. from a computational perspective, it is necessary for the network to be efficient in terms of both memory and computation time during inference. the ability to train end-to-end in order to jointly optimise all the weights in the network using an efficient weight update technique such as stochastic gradient descent (sgd) is an additional benefit since it is more easily repeatable. the design of segnet arose from a need to match these criteria. the encoder network in segnet is topologically identical to the convolutional layers in vgg16. we remove the fully connected layers of vgg16 which makes the segnet encoder network significantly smaller and easier to train than many other recent architectures. the key component of segnet is the decoder network which consists of a hierarchy of decoders one corresponding to each encoder. of these, the appropriate decoders use the max-pooling indices received from the corresponding encoder to perform non-linear upsampling of their input feature maps. this idea was inspired from an architecture designed for unsupervised feature learning. reusing max-pooling indices in the decoding process has several practical advantages; (i) it improves boundary delineation, (ii) it reduces the number of parameters enabling end-to-end training, and (iii) this form of upsampling can be incorporated into any encoder-decoder architecture such as with only a little modification. one of the main contributions of this paper is our analysis of the segnet decoding technique and the widely used fully convolutional network (fcn). this is in order to convey the practical trade-offs involved in designing segmentation architectures. most recent deep architectures for segmentation have identical encoder networks, i.e vgg16, but differ in the form of the decoder network, training and inference. another common feature is they have trainable parameters in the order of hundreds of millions and thus encounter difficulties in performing end-to-end training. the difficulty of training these networks has led to multi-stage training, appending networks to a pre-trained architecture such as fcn, use of supporting aids such as region proposals for inference, disjoint training of classification and segmentation networks and use of additional training data for pre-training or for full training. in addition, performance boosting post-processing techniques have also been popular. although all these factors improve performance on challenging benchmarks, it is unfortunately difficult from their quantitative results to disentangle the key design factors necessary to achieve good performance. we therefore analysed the decoding process used in some of these approaches and reveal their pros and cons. we evaluate the performance of segnet on two scene segmentation tasks, camvid road scene segmentation and sun rgb-d indoor scene segmentation. pascal voc12 has been the benchmark challenge for segmentation over the years. however, the majority of this task has one or two foreground classes surrounded by a highly varied background. this implicitly favours techniques used for detection as shown by the recent work on a decoupled classification-segmentation network where the classification network can be trained with a large set of weakly labelled data and the independent segmentation network performance is improved. the method of also use the feature maps of the classification network with an independent crf post-processing technique to perform segmentation. the performance can also be boosted by the use additional inference aids such as region proposals,. therefore, it is different from scene understanding where the idea is to exploit co-occurrences of objects and other spatial-context to perform robust segmentation. to demonstrate the efficacy of segnet, we present a real-time online demo of road scene segmentation into 11 classes of interest for autonomous driving (see link in fig. [reference]). some example test results produced on randomly sampled road scene images from google and indoor test scenes from the sun rgb-d dataset are shown in fig. [reference]. the remainder of the paper is organized as follows. in sec. [reference] we review related recent literature. we describe the segnet architecture and its analysis in sec. [reference]. in sec. [reference] we evaluate the performance of segnet on outdoor and indoor scene datasets. this is followed by a general discussion regarding our approach with pointers to future work in sec. [reference]. we conclude in sec. [reference]. section: literature review semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets. before the arrival of deep networks, the best performing methods mostly relied on hand engineered features classifying pixels independently. typically, a patch is fed into a classifier e.g. random forest or boosting to predict the class probabilities of the center pixel. features based on appearance or sfm and appearance have been explored for the camvid road scene understanding test. these per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order crf to improve the accuracy. more recent approaches have aimed to produce high quality unaries by trying to predict the labels for all the pixels in a patch as opposed to only the center pixel. this improves the results of random forest based unaries but thin structured classes are classified poorly. dense depth maps computed from the camvid video have also been used as input for classification using random forests. another approach argues for the use of a combination of popular hand designed features and spatio-temporal super-pixelization to obtain higher accuracy. the best performing technique on the camvid test addresses the imbalance among label frequencies by combining object detection outputs with classifier predictions in a crf framework. the result of all these techniques indicate the need for improved features for classification. indoor rgbd pixel-wise semantic segmentation has also gained popularity since the release of the nyu dataset. this dataset showed the usefulness of the depth channel to improve segmentation. their approach used features such as rgb-sift, depth-sift and pixel location as input to a neural network classifier to predict pixel unaries. the noisy unaries are then smoothed using a crf. improvements were made using a richer feature set including lbp and region segmentation to obtain higher accuracy followed by a crf. in more recent work, both class segmentation and support relationships are inferred together using a combination of rgb and depth based cues. another approach focuses on real-time joint reconstruction and semantic segmentation, where random forests are used as the classifier. gupta et al. use boundary detection and hierarchical grouping before performing category segmentation. the common attribute in all these approaches is the use of hand engineered features for classification of either rgb or rgbd images. the success of deep convolutional neural networks for object classification has more recently led researchers to exploit their feature learning capabilities for structured prediction problems such as segmentation. there have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions. however, the resulting classification is blocky. another approach using recurrent neural networks merges several low resolution predictions to create input image resolution predictions. these techniques are already an improvement over hand engineered features but their ability to delineate boundaries is poor. newer deep architectures particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions. the encoder network which produces these low resolution representations in all of these architectures is the vgg16 classification network which has convolutional layers and fully connected layers. this encoder network weights are typically pre-trained on the large imagenet object classification dataset. the decoder network varies between these architectures and is the part which is responsible for producing multi-dimensional features for each pixel for classification. each decoder in the fully convolutional network (fcn) architecture learns to upsample its input feature map (s) and combines them with the corresponding encoder feature map to produce the input to the next decoder. it is an architecture which has a large number of trainable parameters in the encoder network (134 m) but a very small decoder network (0.5 m). the overall large size of this network makes it hard to train end-to-end on a relevant task. therefore, the authors use a stage-wise training process. here each decoder in the decoder network is progressively added to an existing trained network. the network is grown until no further increase in performance is observed. this growth is stopped after three decoders thus ignoring high resolution feature maps can certainly lead to loss of edge information. apart from training related issues, the need to reuse the encoder feature maps in the decoder makes it memory intensive in test time. we study this network in more detail as it the core of other recent architectures. the predictive performance of fcn has been improved further by appending the fcn with a recurrent neural network (rnn) and fine-tuning them on large datasets,. the rnn layers mimic the sharp boundary delineation capabilities of crfs while exploiting the feature representation power of fcn's. they show a significant improvement over fcn-8 but also show that this difference is reduced when more training data is used to train fcn-8. the main advantage of the crf-rnn is revealed when it is jointly trained with an architecture such as the fcn-8. the fact that joint training helps is also shown in other recent results,. interestingly, the deconvolutional network performs significantly better than fcn although at the cost of a more complex training and inference. this however raises the question as to whether the perceived advantage of the crf-rnn would be reduced as the core feed-forward segmentation engine is made better. in any case, the crf-rnn network can be appended to any deep segmentation architecture including segnet. multi-scale deep architectures are also being pursued. they come in two flavours, (i) those which use input images at a few scales and corresponding deep feature extraction networks, and (ii) those which combine feature maps from different layers of a single deep architecture. the common idea is to use features extracted at multiple scales to provide both local and global context and the using feature maps of the early encoding layers retain more high frequency detail leading to sharper class boundaries. some of these architectures are difficult to train due to their parameter size. thus a multi-stage training process is employed along with data augmentation. the inference is also expensive with multiple convolutional pathways for feature extraction. others append a crf to their multi-scale network and jointly train them. however, these are not feed-forward at test time and require optimization to determine the map labels. several of the recently proposed deep architectures for segmentation are not feed-forward in inference time,,. they require either map inference over a crf, or aids such as region proposals for inference. we believe the perceived performance increase obtained by using a crf is due to the lack of good decoding techniques in their core feed-forward segmentation engine. segnet on the other hand uses decoders to obtain features for accurate pixel-wise classification. the recently proposed deconvolutional network and its semi-supervised variant the decoupled network use the max locations of the encoder feature maps (pooling indices) to perform non-linear upsampling in the decoder network. the authors of these architectures, independently of segnet (first submitted to cvpr 2015), proposed this idea of decoding in the decoder network. however, their encoder network consists of the fully connected layers from the vgg-16 network which consists of about of the parameters of their entire network. this makes training of their network very difficult and thus require additional aids such as the use of region proposals to enable training. moreover, during inference these proposals are used and this increases inference time significantly. from a benchmarking point of view, this also makes it difficult to evaluate the performance of their architecture (encoder-decoder network) without other aids. in this work we discard the fully connected layers of the vgg16 encoder network which enables us to train the network using the relevant training set using sgd optimization. another recent method shows the benefit of reducing the number of parameters significantly without sacrificing performance, reducing memory consumption and improving inference time. our work was inspired by the unsupervised feature learning architecture proposed by ranzato et al.. the key learning module is an encoder-decoder network. an encoder consists of convolution with a filter bank, element-wise tanh non-linearity, max-pooling and sub-sampling to obtain the feature maps. for each sample, the indices of the max locations computed during pooling are stored and passed to the decoder. the decoder upsamples the feature maps by using the stored pooled indices. it convolves this upsampled map using a trainable decoder filter bank to reconstruct the input image. this architecture was used for unsupervised pre-training for classification. a somewhat similar decoding technique is used for visualizing trained convolutional networks for classification. the architecture of ranzato et al. mainly focused on layer-wise feature learning using small input patches. this was extended by kavukcuoglu et. al. to accept full image sizes as input to learn hierarchical encoders. both these approaches however did not attempt to use deep encoder-decoder networks for unsupervised feature training as they discarded the decoders after each encoder training. here, segnet differs from these architectures as the deep encoder-decoder network is trained jointly for a supervised learning task and hence the decoders are an integral part of the network in test time. other applications where pixel wise predictions are made using deep networks are image super-resolution and depth map prediction from a single image. the authors in discuss the need for learning to upsample from low resolution feature maps which is the central topic of this paper. section: architecture segnet has an encoder network and a corresponding decoder network, followed by a final pixelwise classification layer. this architecture is illustrated in fig. [reference]. the encoder network consists of convolutional layers which correspond to the first convolutional layers in the vgg16 network designed for object classification. we can therefore initialize the training process from weights trained for classification on large datasets. we can also discard the fully connected layers in favour of retaining higher resolution feature maps at the deepest encoder output. this also reduces the number of parameters in the segnet encoder network significantly (from 134 m to 14.7 m) as compared to other recent architectures, (see. table [reference]). each encoder layer has a corresponding decoder layer and hence the decoder network has layers. the final decoder output is fed to a multi-class soft-max classifier to produce class probabilities for each pixel independently. each encoder in the encoder network performs convolution with a filter bank to produce a set of feature maps. these are then batch normalized,). then an element-wise rectified-linear non-linearity (relu) is applied. following that, max-pooling with a window and stride (non-overlapping window) is performed and the resulting output is sub-sampled by a factor of. max-pooling is used to achieve translation invariance over small spatial shifts in the input image. sub-sampling results in a large input image context (spatial window) for each pixel in the feature map. while several layers of max-pooling and sub-sampling can achieve more translation invariance for robust classification correspondingly there is a loss of spatial resolution of the feature maps. the increasingly lossy (boundary detail) image representation is not beneficial for segmentation where boundary delineation is vital. therefore, it is necessary to capture and store boundary information in the encoder feature maps before sub-sampling is performed. if memory during inference is not constrained, then all the encoder feature maps (after sub-sampling) can be stored. this is usually not the case in practical applications and hence we propose a more efficient way to store this information. it involves storing only the max-pooling indices, i.e, the locations of the maximum feature value in each pooling window is memorized for each encoder feature map. in principle, this can be done using 2 bits for each pooling window and is thus much more efficient to store as compared to memorizing feature map (s) in float precision. as we show later in this work, this lower memory storage results in a slight loss of accuracy but is still suitable for practical applications. the appropriate decoder in the decoder network upsamples its input feature map (s) using the memorized max-pooling indices from the corresponding encoder feature map (s). this step produces sparse feature map (s). this segnet decoding technique is illustrated in fig. [reference]. these feature maps are then convolved with a trainable decoder filter bank to produce dense feature maps. a batch normalization step is then applied to each of these maps. note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map, although its encoder input has 3 channels (rgb). this is unlike the other decoders in the network which produce feature maps with the same number of size and channels as their encoder inputs. the high dimensional feature representation at the output of the final decoder is fed to a trainable soft-max classifier. this soft-max classifies each pixel independently. the output of the soft-max classifier is a k channel image of probabilities where k is the number of classes. the predicted segmentation corresponds to the class with maximum probability at each pixel. we add here that two other architectures, deconvnet and u-net share a similar architecture to segnet but with some differences. deconvnet has a much larger parameterization, needs more computational resources and is harder to train end-to-end (table [reference]), primarily due to the use of fully connected layers (albeit in a convolutional manner) we report several comparisons with deconvnet later in the paper sec. [reference]. as compared to segnet, u-net (proposed for the medical imaging community) does not reuse pooling indices but instead transfers the entire feature map (at the cost of more memory) to the corresponding decoders and concatenates them to upsampled (via deconvolution) decoder feature maps. there is no conv5 and max-pool 5 block in u-net as in the vgg net architecture. segnet, on the other hand, uses all of the pre-trained convolutional layer weights from vgg net as pre-trained weights. subsection: decoder variants many segmentation architectures share the same encoder network and they only vary in the form of their decoder network. of these we choose to compare the segnet decoding technique with the widely used fully convolutional network (fcn) decoding technique. in order to analyse segnet and compare its performance with fcn (decoder variants) we use a smaller version of segnet, termed segnet-basic, which has 4 encoders and 4 decoders. all the encoders in segnet-basic perform max-pooling and sub-sampling and the corresponding decoders upsample its input using the received max-pooling indices. batch normalization is used after each convolutional layer in both the encoder and decoder network. no biases are used after convolutions and no relu non-linearity is present in the decoder network. further, a constant kernel size of over all the encoder and decoder layers is chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map (layer) can be traced back to a context window in the input image of pixels. this small size of segnet-basic allows us to explore many different variants (decoders) and train them in reasonable time. similarly we create fcn-basic, a comparable version of fcn for our analysis which shares the same encoder network as segnet-basic but with the fcn decoding technique (see fig. [reference]) used in all its decoders. on the left in fig. [reference] is the decoding technique used by segnet (also segnet-basic), where there is no learning involved in the upsampling step. however, the upsampled maps are convolved with trainable multi-channel decoder filters to densify its sparse inputs. each decoder filter has the same number of channels as the number of upsampled feature maps. a smaller variant is one where the decoder filters are single channel, i.e they only convolve their corresponding upsampled feature map. this variant (segnet-basic-singlechanneldecoder) reduces the number of trainable parameters and inference time significantly. on the right in fig. [reference] is the fcn (also fcn-basic) decoding technique. the important design element of the fcn model is dimensionality reduction step of the encoder feature maps. this compresses the encoder feature maps which are then used in the corresponding decoders. dimensionality reduction of the encoder feature maps, say of 64 channels, is performed by convolving them with trainable filters, where is the number of classes. the compressed channel final encoder layer feature maps are the input to the decoder network. in a decoder of this network, upsampling is performed by inverse convolution using a fixed or trainable multi-channel upsampling kernel. we set the kernel size to. this manner of upsampling is also termed as deconvolution. note that, in comparison, segnet the multi-channel convolution using trainable decoder filters is performed after upsampling to densifying feature maps. the upsampled feature map in fcn has channels. it is then added element-wise to the corresponding resolution encoder feature map to produce the output decoder feature map. the upsampling kernels are initialized using bilinear interpolation weights. the fcn decoder model requires storing encoder feature maps during inference. this can be memory intensive for embedded applications; for e.g. storing 64 feature maps of the first layer of fcn-basic at resolution in 32 bit floating point precision takes 11 mb. this can be made smaller using dimensionality reduction to the 11 feature maps which requires 1.9 mb storage. segnet on the other hand requires almost negligible storage cost for the pooling indices (mb if stored using 2 bits per pooling window). we can also create a variant of the fcn-basic model which discards the encoder feature map addition step and only learns the upsampling kernels (fcn-basic-noaddition). in addition to the above variants, we study upsampling using fixed bilinear interpolation weights which therefore requires no learning for upsampling (bilinear-interpolation). at the other extreme, we can add 64 encoder feature maps at each layer to the corresponding output feature maps from the segnet decoder to create a more memory intensive variant of segnet (segnet-basic-encoderaddition). here both the pooling indices for upsampling are used, followed by a convolution step to densify its sparse input. this is then added element-wise to the corresponding encoder feature maps to produce a decoders output. another and more memory intensive fcn-basic variant (fcn-basic-nodimreduction) is where there is no dimensionality reduction performed for the encoder feature maps. this implies that unlike fcn-basic the final encoder feature map is not compressed to channels before passing it to the decoder network. therefore, the number of channels at the end of each decoder is the same as the corresponding encoder (i.e). we also tried other generic variants where feature maps are simply upsampled by replication, or by using a fixed (and sparse) array of indices for upsampling. these performed quite poorly in comparison to the above variants. a variant without max-pooling and sub-sampling in the encoder network (decoders are redundant) consumes more memory, takes longer to converge and performs poorly. finally, please note that to encourage reproduction of our results we release the caffe implementation of all the variants. subsection: training we use the camvid road scenes dataset to benchmark the performance of the decoder variants. this dataset is small, consisting of 367 training and 233 testing rgb images (day and dusk scenes) at resolution. the challenge is to segment classes such as road, building, cars, pedestrians, signs, poles, side-walk etc. we perform local contrast normalization to the rgb input. the encoder and decoder weights were all initialized using the technique described in he et al.. to train all the variants we use stochastic gradient descent (sgd) with a fixed learning rate of 0.1 and momentum of 0.9 using our caffe implementation of segnet-basic. we train the variants until the training loss converges. before each epoch, the training set is shuffled and each mini-batch (12 images) is then picked in order thus ensuring that each image is used only once in an epoch. we select the model which performs highest on a validation dataset. we use the cross-entropy loss as the objective function for training the network. the loss is summed up over all the pixels in a mini-batch. when there is large variation in the number of pixels in each class in the training set (e.g road, sky and building pixels dominate the camvid dataset) then there is a need to weight the loss differently based on the true class. this is termed class balancing. we use median frequency balancing where the weight assigned to a class in the loss function is the ratio of the median of class frequencies computed on the entire training set divided by the class frequency. this implies that larger classes in the training set have a weight smaller than and the weights of the smallest classes are the highest. we also experimented with training the different variants without class balancing or equivalently using natural frequency balancing. subsection: analysis to compare the quantitative performance of the different decoder variants, we use three commonly used performance measures: global accuracy (g) which measures the percentage of pixels correctly classified in the dataset, class average accuracy (c) is the mean of the predictive accuracy over all classes and mean intersection over union (miou) over all classes as used in the pascal voc12 challenge. the miou metric is a more stringent metric than class average accuracy since it penalizes false positive predictions. however, miou metric is not optimized for directly through the class balanced cross-entropy loss. the miou metric otherwise known as the jacard index is most commonly used in benchmarking. however, csurka et al. note that this metric does not always correspond to human qualitative judgements (ranks) of good quality segmentation. they show with examples that miou favours region smoothness and does not evaluate boundary accuracy, a point also alluded to recently by the authors of fcn. hence they propose to complement the miou metric with a boundary measure based on the berkeley contour matching score commonly used to evaluate unsupervised image segmentation quality. csurka et al. simply extend this to semantic segmentation and show that the measure of semantic contour accuracy used in conjunction with the miou metric agrees more with human ranking of segmentation outputs. the key idea in computing a semantic contour score is to evaluate the f1-measure which involves computing the precision and recall values between the predicted and ground truth class boundary given a pixel tolerance distance. we used a value of of the image diagonal as the tolerance distance. the f1-measure for each class that is present in the ground truth test image is averaged to produce an image f1-measure. then we compute the whole test set average, denoted the boundary f1-measure (bf) by average the image f1 measures. we test each architectural variant after each iterations of optimization on the camvid validation set until the training loss converges. with a training mini-batch size of 12 this corresponds to testing approximately every 33 epochs (passes) through the training set. we select the iteration wherein the global accuracy is highest amongst the evaluations on the validation set. we report all the three measures of performance at this point on the held-out camvid test set. although we use class balancing while training the variants, it is still important to achieve high global accuracy to result in an overall smooth segmentation. another reason is that the contribution of segmentation towards autonomous driving is mainly for delineating classes such as roads, buildings, side-walk, sky. these classes dominate the majority of the pixels in an image and a high global accuracy corresponds to good segmentation of these important classes. we also observed that reporting the numerical performance when class average is highest can often correspond to low global accuracy indicating a perceptually noisy segmentation output. in table [reference] we report the numerical results of our analysis. we also show the size of the trainable parameters and the highest resolution feature map or pooling indices storage memory, i.e, of the first layer feature maps after max-pooling and sub-sampling. we show the average time for one forward pass with our caffe implementation, averaged over measurements using a input on an nvidia titan gpu with cudnn v3 acceleration. we note that the upsampling layers in the segnet variants are not optimised using cudnn acceleration. we show the results for both testing and training for all the variants at the selected iteration. the results are also tabulated without class balancing (natural frequency) for training and testing accuracies. below we analyse the results with class balancing. from the table [reference], we see that bilinear interpolation based upsampling without any learning performs the worst based on all the measures of accuracy. all the other methods which either use learning for upsampling (fcn-basic and variants) or learning decoder filters after upsampling (segnet-basic and its variants) perform significantly better. this emphasizes the need to learn decoders for segmentation. this is also supported by experimental evidence gathered by other authors when comparing fcn with segnet-type decoding techniques. when we compare segnet-basic and fcn-basic we see that both perform equally well on this test over all the measures of accuracy. the difference is that segnet uses less memory during inference since it only stores max-pooling indices. on the other hand fcn-basic stores encoder feature maps in full which consumes much more memory (11 times more). segnet-basic has a decoder with 64 feature maps in each decoder layer. in comparison fcn-basic, which uses dimensionality reduction, has fewer (11) feature maps in each decoder layer. this reduces the number of convolutions in the decoder network and hence fcn-basic is faster during inference (forward pass). from another perspective, the decoder network in segnet-basic makes it overall a larger network than fcn-basic. this endows it with more flexibility and hence achieves higher training accuracy than fcn-basic for the same number of iterations. overall we see that segnet-basic has an advantage over fcn-basic when inference time memory is constrained but where inference time can be compromised to some extent. segnet-basic is most similar to fcn-basic-noaddition in terms of their decoders, although the decoder of segnet is larger. both learn to produce dense feature maps, either directly by learning to perform deconvolution as in fcn-basic-noaddition or by first upsampling and then convolving with trained decoder filters. the performance of segnet-basic is superior, in part due to its larger decoder size. the accuracy of fcn-basic-noaddition is also lower as compared to fcn-basic. this shows that it is vital to capture the information present in the encoder feature maps for better performance. in particular, note the large drop in the bf measure between these two variants. this can also explain the part of the reason why segnet-basic outperforms fcn-basic-noaddition. the size of the fcn-basic-noaddition-nodimreduction model is slightly larger than segnet-basic since the final encoder feature maps are not compressed to match the number of classes. this makes it a fair comparison in terms of the size of the model. the performance of this fcn variant is poorer than segnet-basic in test but also its training accuracy is lower for the same number of training epochs. this shows that using a larger decoder is not enough but it is also important to capture encoder feature map information to learn better, particular the fine grained contour information (notice the drop in the bf measure). here it is also interesting to see that segnet-basic has a competitive training accuracy when compared to larger models such fcn-basic-nodimreduction. another interesting comparison between fcn-basic-noaddition and segnet-basic-singlechanneldecoder shows that using max-pooling indices for upsampling and an overall larger decoder leads to better performance. this also lends evidence to segnet being a good architecture for segmentation, particularly when there is a need to find a compromise between storage cost, accuracy versus inference time. in the best case, when both memory and inference time is not constrained, larger models such as fcn-basic-nodimreduction and segnet-encoderaddition are both more accurate than the other variants. particularly, discarding dimensionality reduction in the fcn-basic model leads to the best performance amongst the fcn-basic variants with a high bf score. this once again emphasizes the trade-off involved between memory and accuracy in segmentation architectures. the last two columns of table [reference] show the result when no class balancing is used (natural frequency). here, we can observe that without weighting the results are poorer for all the variants, particularly for class average accuracy and miou metric. the global accuracy is the highest without weighting since the majority of the scene is dominated by sky, road and building pixels. apart from this all the inference from the comparative analysis of variants holds true for natural frequency balancing too, including the trends for the bf measure. segnet-basic performs as well as fcn-basic and is better than the larger fcn-basic-noaddition-nodimreduction. the bigger but less efficient models fcn-basic-nodimreduction and segnet-encoderaddition perform better than the other variants. we can now summarize the above analysis with the following general points. the best performance is achieved when encoder feature maps are stored in full. this is reflected in the semantic contour delineation metric (bf) most clearly. when memory during inference is constrained, then compressed forms of encoder feature maps (dimensionality reduction, max-pooling indices) can be stored and used with an appropriate decoder (e.g. segnet type) to improve performance. larger decoders increase performance for a given encoder network. section: benchmarking we quantify the performance of segnet on two scene segmentation benchmarks using our caffe implementation. the first task is road scene segmentation which is of current practical interest for various autonomous driving related problems. the second task is indoor scene segmentation which is of immediate interest to several augmented reality (ar) applications. the input rgb images for both tasks were. we benchmarked segnet against several other well adopted deep architectures for segmentation such as fcn, deeplab-largfov and deconvnet. our objective was to understand the performance of these architectures when trained end-to-end on the same datasets. to enable end-to-end training we added batch normalization layers after each convolutional layer. for deeplab-largefov, we changed the max pooling 3 stride to 1 to achieve a final predictive resolution of. we restricted the feature size in the fully connnected layers of deconvnet to so as to enable training with the same batch size as other models. here note that the authors of deeplab-largefov have also reported little loss in performance by reducing the size of the fully connected layers. in order to perform a controlled benchmark we used the same sgd solver with a fixed learning rate of and momentum of. the optimization was performed for more than 100 epochs through the dataset until no further performance increase was observed. dropout of was added to the end of deeper convolutional layers in all models to prevent overfitting (see for example caffe prototxt). for the road scenes which have classes we used a mini-batch size of and for indoor scenes with classes we used a mini-batch size of. subsection: road scene segmentation a number of road scene datasets are available for semantic parsing. of these we choose to benchmark segnet using the camvid dataset as it contains video sequences. this enables us to compare our proposed architecture with those which use motion and structure and video segments. we also combine to form an ensemble of 3433 images to train segnet for an additional benchmark. for a web demo (see footnote [reference]) of road scene segmentation, we include the camvid test set to this larger dataset. here, we would like to note that another recent and independent segmentation benchmark on road scenes has been performed for segnet and the other competing architectures used in this paper. however, the benchmark was not controlled, meaning that each architecture was trained with a separate recipe with varying input resolutions and sometimes with a validation set included. therefore, we believe our more controlled benchmark can be used to complement their efforts. building tree sky car sign-symbol road pedestrian fence column-pole side-walk bicyclist class avg. global avg. miou bf the qualitative comparisons of segnet predictions with other deep architectures can be seen in fig. [reference]. the qualitative results show the ability of the proposed architecture to segment smaller classes in road scenes while producing a smooth segmentation of the overall scene. indeed, under the controlled benchmark setting, segnet shows superior performance as compared to some of the larger models. deeplab-largefov is the most efficient model and with crf post-processing can produce competitive results although smaller classes are lost. fcn with learnt deconvolution is clearly better than with fixed bilinear upsampling. deconvnet is the largest model and the most inefficient to train. its predictions do not retain small classes. we also use this benchmark to first compare segnet with several non deep-learning methods including random forests, boosting in combination with crf based methods. this was done to give the user a perspective of the improvements in accuracy that has been achieved using deep networks compared to classical feature engineering based techniques. the results in table [reference] show segnet-basic, segnet obtain competitive results when compared with methods which use crfs. this shows the ability of the deep architecture to extract meaningful features from the input image and map it to accurate and smooth class segment labels. the most interesting result here is the large performance improvement in class average and miou metrics that is obtained when a large training dataset, obtained by combining, is used to train segnet. correspondingly, the qualitative results of segnet (see fig. [reference]) are clearly superior to the rest of the methods. it is able to segment both small and large classes well. we remark here that we used median frequency class balancing in training segnet-basic and segnet. in addition, there is an overall smooth quality of segmentation much like what is typically obtained with crf post-processing. although the fact that results improve with larger training sets is not surprising, the percentage improvement obtained using pre-trained encoder network and this training set indicates that this architecture can potentially be deployed for practical applications. our random testing on urban and highway images from the internet (see fig. [reference]) demonstrates that segnet can absorb a large training set and generalize well to unseen images. it also indicates the contribution of the prior (crf) can be lessened when sufficient amount of training data is made available. in table [reference] we compare segnet's performance with now widely adopted fully convolutional architectures for segmentation. as compared to the experiment in table [reference], we did not use any class blancing for training any of the deep architectures including segnet. this is because we found it difficult to train larger models such as deconvnet with median frequency balancing. we benchmark performance at 40 k, 80 k and 80 k iterations which given the mini-batch size and training set size approximately corresponds to and 100 epochs. for the last test point we also report the maximum number of iterations (here atleast 150 epochs) beyond which we observed no accuracy improvements or when over-fitting set in. we report the metrics at three stages in the training phase to reveal how the metrics varied with training time, particularly for larger networks. this is important to understand if additional training time is justified when set against accuracy increases. note also that for each evaluation we performed a complete run through the dataset to obtain batch norm statistics and then evaluated the test model with this statistic (see for code.). these evaluations are expensive to perform on large training sets and hence we only report metrics at three time points in the training phase. from table [reference] we immediately see that segnet, deconvnet achieve the highest scores in all the metrics as compared to other models. deconvnet has a higher boundary delineation accuracy but segnet is much more efficient as compared to deconvnet. this can be seen from the compute statistics in table [reference]. fcn, deconvnet which have fully connected layers (turned into convolutional layers) train much more slowly and have comparable or higher forward-backward pass time with reference to segnet. here we note also that over-fitting was not an issue in training these larger models, since at comparable iterations to segnet their metrics showed an increasing trend. for the fcn model learning the deconvolutional layers as opposed to fixing them with bi-linear interpolation weights improves performance particularly the bf score. it also achieves higher metrics in a far lesser time. this fact agrees with our earlier analysis in sec. [reference]. surprisingly, deeplab-largefov which is trained to predict labels at a resolution of produces competitive performance given that it is the smallest model in terms of parameterization and also has the fastest training time as per table [reference]. however, the boundary accuracy is poorer and this is shared by the other architectures. deconvnet's bf score is higher than the other networks when trained for a very long time. given our analysis in sec. [reference] and the fact that it shares a segnet type architecture. the impact of dense crf post-processing can be seen in the last time point for deeplab-largefov-densecrf. both global and miou improve but class average diminshes. however a large improvement is obtained for the bf score. note here that the dense crf hyperparameters were obtained by an expensive grid-search process on a subset of the training set since no validation set was available. subsection: sun rgb-d indoor scenes sun rgb-d is a very challenging and large dataset of indoor scenes with training and testing images. the images are captured by different sensors and hence come in various resolutions. the task is to segment indoor scene classes including wall, floor, ceiling, table, chair, sofa etc. this task is made hard by the fact that object classes come in various shapes, sizes and in different poses. there are frequent partial occlusions since there are typically many different classes present in each of the test images. these factors make this one of the hardest segmentation challenges. we only use the rgb modality for our training and testing. using the depth modality would necessitate architectural modifications/ redesign. also the quality of depth images from current cameras require careful post-processing to fill-in missing measurements. they may also require using fusion of many frames to robustly extract features for segmentation. therefore we believe using depth for segmentation merits a separate body of work which is not in the scope of this paper. we also note that an earlier benchmark dataset nyuv2 is included as part of this dataset. road scene images have limited variation, both in terms of the classes of interest and their spatial arrangements. when captured from a moving vehicle where the camera position is nearly always parallel to the road surface limiting variability in view points. this makes it easier for deep networks to learn to segment them robustly. in comparison, images of indoor scenes are more complex since the view points can vary a lot and there is less regularity in both the number of classes present in a scene and their spatial arrangement. another difficulty is caused by the widely varying sizes of the object classes in the scene. some test samples from the recent sun rgb-d dataset are shown in fig. [reference]. we observe some scenes with few large classes and some others with dense clutter (bottom row and right). the appearance (texture and shape) can also widely vary in indoor scenes. therefore, we believe this is the hardest challenge for segmentation architectures and methods in computer vision. other challenges, such as pascal voc12 salient object segmentation have occupied researchers more, but we believe indoor scene segmentation is more challenging and has more current practical applications such as in ar and robotics. to encourage more research in this direction we compared well known deep architectures on the large sun rgb-d dataset. the qualitative results of segnet on samples of indoor scenes of different types such as bedroom, living room, laboratory, meeting room, bathroom are shown in fig. [reference]. we see that segnet obtains reasonable predictions when the size of the classes are large under different view points. this is particularly interesting since the input modality is only rgb. rgb images are also useful to segment thinner structures such as the legs of chairs and tables, lamps which is difficult to achieve using depth images from currently available sensors. this can be seen from the results of segnet, deconvnet in fig. [reference]. it is also useful to segment decorative objects such as paintings on the wall for ar tasks. however as compared to outdoor scenes the segmentation quality is clearly more noisy. the quality drops significantly when clutter is increased (see the result sample in the middle column). the quantitative results in table [reference] show that all the deep architectures share low miou and boundary metrics. the global and class averages (correlates well with miou) are also small. segnet outperforms all other methods in terms of g, c, bf metrics and has a slightly lower miou than deeplab-largefov. as a stand alone experiment we trained segnet with median frequency class balancing and the metrics were higher (see table [reference]) and this agrees with our analysis in sec. [reference]. interestingly, using the grid search based optimal hyperparameters for the dense-crf worsened all except the bf score metric for deeplab-largefov-densecrf. more optimal settings could perhaps be found but the grid search process was too expensive given the large inference time for dense-crfs. one reason for the overall poor performance is the large number of classes in this segmentation task, many of which occupy a small part of the image and appear infrequently. the accuracies reported in table [reference] clearly show that larger classes have reasonable accuracy and smaller classes have lower accuracies. this can be improved with larger sized datasets and class distribution aware training techniques. another reason for poor performance could lie in the inability of these deep architectures (all are based on the vgg architecture) to large variability in indoor scenes. this conjecture on our part is based on the fact that the smallest model deeplab-largefov produces the best accuracy in terms of miou and in comparison, larger parameterizations in deconvnet, fcn did not improve perfomance even with much longer training (deconvnet). this suggests there could lie a common reason for poor performance across all architectures. more controlled datasets are needed to verify this hypothesis. section: discussion and future work deep learning models have often achieved increasing success due to the availability of massive datasets and expanding model depth and parameterisation. however, in practice factors like memory and computational time during training and testing are important factors to consider when choosing a model from a large bank of models. training time becomes an important consideration particularly when the performance gain is not commensurate with increased training time as shown in our experiments. test time memory and computational load are important to deploy models on specialised embedded devices, for example, in ar applications. from an overall efficiency viewpoint, we feel less attention has been paid to smaller and more memory, time efficient models for real-time applications such as road scene understanding and ar. this was the primary motivation behind the proposal of segnet, which is significantly smaller and faster than other competing architectures, but which we have shown to be efficient for tasks such as road scene understanding. segmentation challenges such as pascal and ms-coco are object segmentation challenges wherein a few classes are present in any test image. scene segmentation is more challenging due to the high variability of indoor scenes and a need to segment a larger number of classes simultaneously. the task of outdoor and indoor scene segmentation are also more practically oriented with current applications such as autonomous driving, robotics and ar. the metrics we chose to benchmark various deep segmentation architectures like the boundary f1-measure (bf) was done to complement the existing metrics which are more biased towards region accuracies. it is clear from our experiments and other independent benchmarks that outdoor scene images captured from a moving car are easier to segment and deep architectures perform robustly. we hope our experiments will encourage researchers to engage their attention towards the more challenging indoor scene segmentation task. an important choice we had to make when benchmarking different deep architectures of varying parameterization was the manner in which to train them. many of these architectures have used a host of supporting techniques and multi-stage training recipes to arrive at high accuracies on datasets but this makes it difficult to gather evidence about their true performance under time and memory constraints. instead we chose to perform a controlled benchmarking where we used batch normalization to enable end-to-end training with the same solver (sgd). however, we note that this approach can not entirely disentangle the effects of model versus solver (optimization) in achieving a particular result. this is mainly due to the fact that training these networks involves gradient back-propagation which is imperfect and the optimization is a non-convex problem in extremely large dimensions. acknowledging these shortcomings, our hope is that this controlled analysis complements other benchmarks and reveals the practical trade-offs involved in different well known architectures. for the future, we would like to exploit our understanding of segmentation architectures gathered from our analysis to design more efficient architectures for real-time applications. we are also interested in estimating the model uncertainty for predictions from deep segmentation architectures,. section: conclusion we presented segnet, a deep convolutional network architecture for semantic segmentation. the main motivation behind segnet was the need to design an efficient architecture for road and indoor scene understanding which is efficient both in terms of memory and computational time. we analysed segnet and compared it with other important variants to reveal the practical trade-offs involved in designing architectures for segmentation, particularly training time, memory versus accuracy. those architectures which store the encoder network feature maps in full perform best but consume more memory during inference time. segnet on the other hand is more efficient since it only stores the max-pooling indices of the feature maps and uses them in its decoder network to achieve good performance. on large and well known datasets segnet performs competitively, achieving high scores for road scene understanding. end-to-end learning of deep segmentation architectures is a harder challenge and we hope to see more attention paid to this important problem. bibliography: references [] vijay badrinarayanan obtained his ph. d from inria rennes, france in 2009. he was a senior post-doctoral research associate at the machine intelligence laboratory, department of engineering, university of cambridge, u.k. he currently works as a principal engineer, deep learning at magic leap, inc. in mountain view, ca. his research interests are in probabilistic graphical models, deep learning applied to image and video based perception problems. [] alex kendall graduated with a bachelor of engineering with first class honours in 2013 from the university of auckland, new zealand. in 2014 he was awarded a woolf fisher scholarship to study towards a ph. d at the university of cambridge, u.k. he is a member of the machine intelligence laboratory and is interested in applications of deep learning for mobile robotics. [] roberto cipolla obtained a b.a. (engineering) degree from the university of cambridge in 1984, an m.s.e. (electrical engineering) from the university of pennsylvania in 1985 and a d.phil. (computer vision) from the university of oxford in 1991. from 1991-92 was a toshiba fellow and engineer at the toshiba corporation research and development centre in kawasaki, japan. he joined the department of engineering, university of cambridge in 1992 as a lecturer and a fellow of jesus college. he became a reader in information engineering in 1997 and a professor in 2000. he became a fellow of the royal academy of engineering (freng) in 2010. his research interests are in computer vision and robotics. he has authored 3 books, edited 9 volumes and co-authored more than 300 papers.",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "segnet",
                        10
                    ],
                    [
                        "segnet encoder network",
                        4566
                    ],
                    [
                        "segnet decoding technique",
                        5445
                    ],
                    [
                        "segmentation architectures",
                        5600
                    ],
                    [
                        "classification and segmentation networks",
                        6173
                    ],
                    [
                        "decoupled classification-segmentation network",
                        7115
                    ],
                    [
                        "independent segmentation network",
                        7258
                    ],
                    [
                        "segnet architecture",
                        8201
                    ],
                    [
                        "deep segmentation architecture",
                        14434
                    ],
                    [
                        "segnet-basic",
                        24057
                    ],
                    [
                        "segnet-basic-singlechanneldecoder",
                        25576
                    ],
                    [
                        "segnet decoder",
                        27822
                    ],
                    [
                        "segnet-basic-encoderaddition",
                        27891
                    ],
                    [
                        "caffe implementation of segnet-basic",
                        29684
                    ],
                    [
                        "segnet variants",
                        34449
                    ],
                    [
                        "segnet-type decoding techniques",
                        35283
                    ],
                    [
                        "fcn-basic-noaddition and segnet-basic-singlechanneldecoder",
                        37971
                    ],
                    [
                        "fcn-basic-nodimreduction and segnet-encoderaddition",
                        38425
                    ],
                    [
                        "segnet type architecture",
                        48251
                    ],
                    [
                        "deep segmentation architectures",
                        55502
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "computational time",
                        1456
                    ],
                    [
                        "memory and computation time",
                        4061
                    ],
                    [
                        "smooth quality of segmentation",
                        44814
                    ],
                    [
                        "computational load",
                        54482
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semantic pixel-wise segmentation",
                        181
                    ],
                    [
                        "semantic segmentation",
                        2216
                    ],
                    [
                        "pixel-wise semantic segmentation",
                        3314
                    ],
                    [
                        "indoor rgbd pixel-wise semantic segmentation",
                        10003
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "camvid video",
                        9489
                    ],
                    [
                        "camvid test",
                        9762
                    ],
                    [
                        "camvid road scenes dataset",
                        29095
                    ],
                    [
                        "camvid dataset",
                        30289
                    ],
                    [
                        "camvid validation set",
                        32924
                    ],
                    [
                        "camvid test set",
                        33300
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "segnet",
                        10
                    ],
                    [
                        "segnet encoder network",
                        4566
                    ],
                    [
                        "segnet decoding technique",
                        5445
                    ],
                    [
                        "segmentation architectures",
                        5600
                    ],
                    [
                        "classification and segmentation networks",
                        6173
                    ],
                    [
                        "decoupled classification-segmentation network",
                        7115
                    ],
                    [
                        "independent segmentation network",
                        7258
                    ],
                    [
                        "segnet architecture",
                        8201
                    ],
                    [
                        "deep segmentation architecture",
                        14434
                    ],
                    [
                        "segnet-basic",
                        24057
                    ],
                    [
                        "segnet-basic-singlechanneldecoder",
                        25576
                    ],
                    [
                        "segnet decoder",
                        27822
                    ],
                    [
                        "segnet-basic-encoderaddition",
                        27891
                    ],
                    [
                        "caffe implementation of segnet-basic",
                        29684
                    ],
                    [
                        "segnet variants",
                        34449
                    ],
                    [
                        "segnet-type decoding techniques",
                        35283
                    ],
                    [
                        "fcn-basic-noaddition and segnet-basic-singlechanneldecoder",
                        37971
                    ],
                    [
                        "fcn-basic-nodimreduction and segnet-encoderaddition",
                        38425
                    ],
                    [
                        "segnet type architecture",
                        48251
                    ],
                    [
                        "deep segmentation architectures",
                        55502
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "mean intersection over union",
                        31221
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "semantic pixel-wise segmentation",
                        181
                    ],
                    [
                        "semantic segmentation",
                        2216
                    ],
                    [
                        "pixel-wise semantic segmentation",
                        3314
                    ],
                    [
                        "indoor rgbd pixel-wise semantic segmentation",
                        10003
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "camvid video",
                        9489
                    ],
                    [
                        "camvid test",
                        9762
                    ],
                    [
                        "camvid road scenes dataset",
                        29095
                    ],
                    [
                        "camvid dataset",
                        30289
                    ],
                    [
                        "camvid validation set",
                        32924
                    ],
                    [
                        "camvid test set",
                        33300
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "segnet",
                        10
                    ],
                    [
                        "segnet encoder network",
                        4566
                    ],
                    [
                        "segnet decoding technique",
                        5445
                    ],
                    [
                        "segmentation architectures",
                        5600
                    ],
                    [
                        "classification and segmentation networks",
                        6173
                    ],
                    [
                        "decoupled classification-segmentation network",
                        7115
                    ],
                    [
                        "independent segmentation network",
                        7258
                    ],
                    [
                        "segnet architecture",
                        8201
                    ],
                    [
                        "deep segmentation architecture",
                        14434
                    ],
                    [
                        "segnet-basic",
                        24057
                    ],
                    [
                        "segnet-basic-singlechanneldecoder",
                        25576
                    ],
                    [
                        "segnet decoder",
                        27822
                    ],
                    [
                        "segnet-basic-encoderaddition",
                        27891
                    ],
                    [
                        "caffe implementation of segnet-basic",
                        29684
                    ],
                    [
                        "segnet variants",
                        34449
                    ],
                    [
                        "segnet-type decoding techniques",
                        35283
                    ],
                    [
                        "fcn-basic-noaddition and segnet-basic-singlechanneldecoder",
                        37971
                    ],
                    [
                        "fcn-basic-nodimreduction and segnet-encoderaddition",
                        38425
                    ],
                    [
                        "segnet type architecture",
                        48251
                    ],
                    [
                        "deep segmentation architectures",
                        55502
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "inference time",
                        17163
                    ],
                    [
                        "average time",
                        34250
                    ],
                    [
                        "training time",
                        46401
                    ],
                    [
                        "test time memory",
                        54461
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "real-time joint reconstruction and semantic segmentation",
                        10689
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "camvid video",
                        9489
                    ],
                    [
                        "camvid test",
                        9762
                    ],
                    [
                        "camvid road scenes dataset",
                        29095
                    ],
                    [
                        "camvid dataset",
                        30289
                    ],
                    [
                        "camvid validation set",
                        32924
                    ],
                    [
                        "camvid test set",
                        33300
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "segnet",
                        10
                    ],
                    [
                        "segnet encoder network",
                        4566
                    ],
                    [
                        "segnet decoding technique",
                        5445
                    ],
                    [
                        "segmentation architectures",
                        5600
                    ],
                    [
                        "classification and segmentation networks",
                        6173
                    ],
                    [
                        "decoupled classification-segmentation network",
                        7115
                    ],
                    [
                        "independent segmentation network",
                        7258
                    ],
                    [
                        "segnet architecture",
                        8201
                    ],
                    [
                        "deep segmentation architecture",
                        14434
                    ],
                    [
                        "segnet-basic",
                        24057
                    ],
                    [
                        "segnet-basic-singlechanneldecoder",
                        25576
                    ],
                    [
                        "segnet decoder",
                        27822
                    ],
                    [
                        "segnet-basic-encoderaddition",
                        27891
                    ],
                    [
                        "caffe implementation of segnet-basic",
                        29684
                    ],
                    [
                        "segnet variants",
                        34449
                    ],
                    [
                        "segnet-type decoding techniques",
                        35283
                    ],
                    [
                        "fcn-basic-noaddition and segnet-basic-singlechanneldecoder",
                        37971
                    ],
                    [
                        "fcn-basic-nodimreduction and segnet-encoderaddition",
                        38425
                    ],
                    [
                        "segnet type architecture",
                        48251
                    ],
                    [
                        "deep segmentation architectures",
                        55502
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "memory",
                        1445
                    ],
                    [
                        "miou metric",
                        31317
                    ],
                    [
                        "semantic contour accuracy",
                        32191
                    ],
                    [
                        "human ranking of segmentation outputs",
                        32275
                    ],
                    [
                        "semantic contour score",
                        32342
                    ],
                    [
                        "semantic contour delineation metric",
                        39762
                    ],
                    [
                        "miou",
                        42920
                    ],
                    [
                        "class average and miou metrics",
                        44376
                    ],
                    [
                        "segmentation quality",
                        51973
                    ],
                    [
                        "low miou and boundary metrics",
                        52210
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "real-time joint reconstruction and semantic segmentation",
                        10689
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "sun rgb-d dataset",
                        8027
                    ],
                    [
                        "rgb or rgbd images",
                        11012
                    ],
                    [
                        "rgb images",
                        29242
                    ],
                    [
                        "rgb input",
                        29445
                    ],
                    [
                        "sun rgb-d indoor scenes",
                        48677
                    ],
                    [
                        "sun rgb-d",
                        48701
                    ],
                    [
                        "recent sun rgb-d dataset",
                        50511
                    ],
                    [
                        "large sun rgb-d dataset",
                        51187
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "segnet",
                        10
                    ],
                    [
                        "segnet encoder network",
                        4566
                    ],
                    [
                        "segnet decoding technique",
                        5445
                    ],
                    [
                        "segmentation architectures",
                        5600
                    ],
                    [
                        "classification and segmentation networks",
                        6173
                    ],
                    [
                        "decoupled classification-segmentation network",
                        7115
                    ],
                    [
                        "independent segmentation network",
                        7258
                    ],
                    [
                        "segnet architecture",
                        8201
                    ],
                    [
                        "deep segmentation architecture",
                        14434
                    ],
                    [
                        "segnet-basic",
                        24057
                    ],
                    [
                        "segnet-basic-singlechanneldecoder",
                        25576
                    ],
                    [
                        "segnet decoder",
                        27822
                    ],
                    [
                        "segnet-basic-encoderaddition",
                        27891
                    ],
                    [
                        "caffe implementation of segnet-basic",
                        29684
                    ],
                    [
                        "segnet variants",
                        34449
                    ],
                    [
                        "segnet-type decoding techniques",
                        35283
                    ],
                    [
                        "fcn-basic-noaddition and segnet-basic-singlechanneldecoder",
                        37971
                    ],
                    [
                        "fcn-basic-nodimreduction and segnet-encoderaddition",
                        38425
                    ],
                    [
                        "segnet type architecture",
                        48251
                    ],
                    [
                        "deep segmentation architectures",
                        55502
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "mean intersection over union",
                        31221
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "image segmentation",
                        72
                    ],
                    [
                        "segmentation",
                        1296
                    ],
                    [
                        "sun rgb-d indoor scene segmentation tasks",
                        1763
                    ],
                    [
                        "smooth segmentations",
                        3742
                    ],
                    [
                        "scene segmentation tasks",
                        6740
                    ],
                    [
                        "camvid road scene segmentation",
                        6766
                    ],
                    [
                        "sun rgb-d indoor scene segmentation",
                        6801
                    ],
                    [
                        "robust segmentation",
                        7705
                    ],
                    [
                        "real-time online demo of road scene segmentation",
                        7778
                    ],
                    [
                        "class segmentation",
                        10545
                    ],
                    [
                        "category segmentation",
                        10876
                    ],
                    [
                        "data augmentation",
                        15132
                    ],
                    [
                        "smooth segmentation",
                        33456
                    ],
                    [
                        "scene segmentation benchmarks",
                        40183
                    ],
                    [
                        "road scene segmentation",
                        40263
                    ],
                    [
                        "indoor scene segmentation",
                        40394
                    ],
                    [
                        "segmentation benchmark",
                        42404
                    ],
                    [
                        "segmentation challenges",
                        49261
                    ],
                    [
                        "pascal voc12 salient object segmentation",
                        50887
                    ],
                    [
                        "segmentation task",
                        53002
                    ],
                    [
                        "object segmentation challenges",
                        55071
                    ],
                    [
                        "scene segmentation",
                        55155
                    ],
                    [
                        "outdoor and indoor scene segmentation",
                        55316
                    ],
                    [
                        "indoor scene segmentation task",
                        55949
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "3d597e16ef56119bb6e6d78e3f379623862f0752-58",
    "doctext": "on gradient regularizers for mmd gans section: abstract we propose a principled method for gradient-based regularization of the critic of gan-like models trained by adversarially optimizing the kernel of a maximum mean discrepancy (mmd). we show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. the new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on 160\u00d7 160 celeba and 64\u00d7 64 unconditional imagenet. section: however, when the mmd kernel is not based directly on image pixels, but on learned features of images. wasserstein-inspired gradient regularization approaches can be used on the mmd critic when learning these features: [reference] uses weight clipping [reference], and [reference][reference] use a gradient penalty [reference]. the recent sobolev gan [reference] uses a similar constraint on the expected gradient norm, but phrases it as estimating a sobolev ipm rather than loosely approximating wasserstein. this expectation can be taken over the same distribution as [reference], but other measures are also proposed, such as (p+ q\u03b8)/ 2. a second recent approach, the spectrally normalized gan [reference], controls the lipschitz constant of the critic by enforcing the spectral norms of the weight matrices to be 1. gradient penalties also benefit gans based on f-divergences [reference]: for instance, the spectral normalization technique of [reference] can be applied to the critic network of an f-gan. alternatively, a gradient penalty can be defined to approximate the effect of blurring p and q\u03b8 with noise [reference], which addresses the problem of non-overlapping support [reference]. this approach has recently been shown to yield locally convergent optimization in some cases with non-continuous distributions, where the original gan does not [reference]. in this paper, we introduce a novel regularization for the mmd gan critic of [reference][reference][reference], which directly targets generator performance, rather than adopting regularization methods intended to approximate wasserstein distances [reference][reference]. the new mmd regularizer derives from an approach widely used in semi-supervised learning [reference][reference], where the aim is to define a classification function f which is positive on p (the positive class) and negative on q\u03b8 (negative class), in the absence of labels on many of the samples. the decision boundary between the classes is assumed to be in a region of low density for both p and q\u03b8: f should therefore be flat where p and q\u03b8 have support (areas with constant label), and have a larger slope in regions of low density. bousquet et al. [reference] propose as their regularizer on f a sum of the variance and a density-weighted gradient norm. we adopt a related penalty on the mmd critic, with the difference that we only apply the penalty on p: thus, the critic is flatter where p has high mass, but does not vanish on the generator samples from q\u03b8 (which we optimize). in excluding q\u03b8 from the critic function constraint, we also avoid the concern raised by [reference] that a critic depending on q\u03b8 will change with the current minibatch-potentially leading to less stable learning. the resulting discrepancy is no longer an integral probability metric: it is asymmetric, and the critic function class depends on the target p being approximated. we first discuss in section 2 how mmd-based losses can be used to learn implicit generative models, and how a naive approach could fail. this motivates our new discrepancies, introduced in section 3. section 4 demonstrates that these losses outperform state-of-the-art models for image generation. section: learning implicit generative models with mmd-based losses an igm is a model q\u03b8 which aims to approximate a target distribution p over a space x\u2286 r d. we will define q\u03b8 by a generator function g\u03b8: z\u2192 x, implemented as a deep network with parameters\u03b8, where z is a space of latent codes, say r 128. we assume a fixed distribution on z, say z\u223c uniform [[reference] 128, and call q\u03b8 the distribution of g\u03b8 (z). we will consider learning by minimizing a discrepancy d between distributions, with d (p, q\u03b8)\u2265 0 and d (p, p)= 0, which we call our loss. we aim to minimize d (p, q\u03b8) with stochastic gradient descent on an estimator of d. in the present work, we will build losses d based on the maximum mean discrepancy, an integral probability metric where the critic class is the unit ball within h k, the reproducing kernel hilbert space with a kernel k. the optimization in (1) admits a simple closed-form optimal critic, f there is also an unbiased, closed-form estimator of mmd 2 k with appealing statistical properties [reference]-in particular, its sample complexity is independent of the dimension of x, compared to the exponential dependence [reference] of the wasserstein distance w (p, q)= sup the mmd is continuous in the weak topology for any bounded kernel with lipschitz embeddings [reference][reference]], meaning that if p n converges in distribution to p, p n d\u2212\u2192 p, then mmd (p n, p)\u2192 0. (w is continuous in the slightly stronger wasserstein topology [reference][reference]; p n w\u2212\u2192 p implies p n d\u2212\u2192 p, and the two notions coincide if x is bounded.) continuity means the loss can provide better signal to the generator as q\u03b8 approaches p, as opposed to e.g. jensen-shannon where the loss could be constant until suddenly jumping to 0 [e.g. 3, example 1]. the mmd is also strict, meaning it is zero iff p= q\u03b8, for characteristic kernels [reference]. the gaussian kernel yields an mmd both continuous in the weak topology and strict. thus in principle, one need not conduct any alternating optimization in an igm at all, but merely choose generator parameters\u03b8 to minimize mmd k. despite these appealing properties, using simple pixel-level kernels leads to poor generator samples [reference][reference][reference][reference]. more recent mmd gans [reference][reference][reference] achieve better results by using a parameterized family of kernels, {k\u03c8}\u03c8\u2208\u03c8, in the optimized mmd loss previously studied by [reference][reference]: we primarily consider kernels defined by some fixed kernel k on top of a learned low-dimensional representation in practice, k is a simple characteristic kernel, e.g. gaussian, and\u03c6\u03c8 is usually a deep network with output dimension say s= 16 [reference] or even s= 1 (in our experiments). if\u03c6\u03c8 is powerful enough, this choice is sufficient; we need not try to ensure each k\u03c8 is characteristic, as did [reference]. proposition 1. suppose k= k\u2022\u03c6\u03c8, with k characteristic and {\u03c6\u03c8} rich enough that for any p= q, there is a\u03c8\u2208\u03c8 for which\u03c6\u03c8# p=\u03c6\u03c8# q. proof. let\u03c8\u2208\u03c8 be such that\u03c6\u03c8 (p)=\u03c6\u03c8 (q). then, since k is characteristic, to estimate d\u03c8 mmd, one can conduct alternating optimization to estimate a\u03c8 and then update the generator according to mmd k\u03c8, similar to the scheme used in gans and wgans. (this form of estimator is justified by an envelope theorem [reference], although it is invariably biased [reference].) unlike d gan or w, fixing a\u03c8 and optimizing the generator still yields a sensible distance mmd k\u03c8. early attempts at minimizing d\u03c8 mmd in an igm, though, were unsuccessful [reference]. this could be because for some kernel classes, d\u03c8 mmd is stronger than wasserstein or mmd. example 1 (diracgan [reference]). we wish to model a point mass at the origin of r, p=\u03b4 0, with any possible point mass, q\u03b8=\u03b4\u03b8 for\u03b8\u2208 r. we use a gaussian kernel of any bandwidth, which can be written as shows that the optimized mmd distance is not continuous in the weak or wasserstein topologies. this also causes optimization issues. figure 1 (a) shows gradient vector fields in parameter space,. some sequences following v (e.g. a) converge to an optimal solution (0,\u03c8), but some (b) move in the wrong direction, and others (c) are stuck because there is essentially no gradient. figure 1 (c, red) shows that the optimal d\u03c8 mmd critic is very sharp near p and q; this is less true for cases where the algorithm converged. we can avoid these issues if we ensure a bounded lipschitz critic: are uniformly bounded and have a common lipschitz constant: sup x\u2208x,\u03c8\u2208\u03c8|f\u03c8 (x)|<\u221e and sup\u03c8\u2208\u03c8 f\u03c8 lip<\u221e. in particular, this holds when k\u03c8= k\u2022\u03c6\u03c8 and 2 f# p denotes the pushforward of a distribution: if x\u223c p, then f (x)\u223c f# p. [reference] [27, theorem 4] makes a similar claim to proposition 2, but its proof was incorrect: it tries to uniformly bound mmd k\u03c8\u2264 w 2, but the bound used is for a wasserstein in terms of proof. the main result is [reference][reference]. to show the claim for indeed, if we put a box constraint on\u03c8 [reference] or regularize the gradient of the critic function [reference], the resulting mmd gan generally matches or outperforms wgan-based models. unfortunately, though, an additive gradient penalty does n't substantially change the vector field of figure 1 (a), as shown in figure 5 (appendix b). we will propose distances with much better convergence behavior. section: new discrepancies for learning implicit generative models our aim here is to introduce a discrepancy that can provide useful gradient information when used as an igm loss. proofs of results in this section are deferred to appendix a. section: lipschitz maximum mean discrepancy proposition 2 shows that an mmd-like discrepancy can be continuous under the weak topology even when optimizing over kernels, if we directly restrict the critic functions to be lipschitz. we can easily define such a distance, which we call the lipschitz mmd: for some\u03bb> 0, lipmmd k,\u03bb (p, q):= sup for a universal kernel k, we conjecture that lim\u03bb\u21920 lipmmd k,\u03bb (p, q)\u2192 w (p, q). but for any k and\u03bb, lipmmd is upper-bounded by w, as (4) optimizes over a smaller set of functions than [reference]. lipmmd (p, q):= sup\u03c8\u2208\u03c8 lipmmd k\u03c8,\u03bb (p, q) is also upper-bounded by w, and hence is continuous in the wasserstein topology. it also shows excellent empirical behavior on example 1 (figure 1 constraining the mean gradient rather than the maximum, as we will do next, is far more tractable. section: gradient-constrained maximum mean discrepancy we define the gradient-constrained mmd for\u03bb> 0 and using some measure\u00b5 as where rather than directly constraining the lipschitz constant, the second term\u2207f 2 l 2 (\u00b5) encourages the function f to be flat where\u00b5 has mass. in experiments we use\u00b5= p, flattening the critic near the target sample. we add the first term following [reference]: in one dimension and with\u00b5 uniform,\u00b7 s (\u00b5),\u00b7, 0 is then an rkhs norm with the kernel\u03ba (x, y)= exp (\u2212 x\u2212 y), which is also a sobolev space. the correspondence to a sobolev norm is lost in higher dimensions [reference][reference]], but we also found the first term to be beneficial in practice. we can exploit some properties of h k to compute (5) analytically. call the difference in kernel mean m with mth entry\u03b7 (x m), and\u2207\u03b7 (x)\u2208 where k is the kernel matrix, and h that of derivatives of both arguments as long as p and q have integrable first moments, and\u00b5 has second moments, assumptions (a) to (d) are satisfied e.g. by a gaussian or linear kernel on top of a differentiable\u03c6\u03c8. we can thus estimate the gcmmd based on samples from p, q, and\u00b5 by using the empirical mean\u03b7 for\u03b7. this discrepancy indeed works well in practice: appendix f.2 shows that optimizing our estimate of d\u00b5,\u03c8,\u03bb gcmmd= sup\u03c8\u2208\u03c8 gcmmd\u00b5, k\u03c8,\u03bb yields a good generative model on mnist. but the linear system of size m+ m d is impractical: even on 28\u00d7 28 images and using a low-rank approximation, the model took days to converge. we therefore design a less expensive discrepancy in the next section. the gcmmd is related to some discrepancies previously used in igm training. the fisher gan [reference] uses only the variance constraint f along with a vanishing boundary condition on f to ensure a well-defined solution (although this was not used in the implementation, and can cause very unintuitive critic behavior; see appendix c). the authors considered several choices of\u00b5, including the wgan-gp measure [reference] and mixtures (p+ q\u03b8)/ 2. rather than enforcing the constraints in closed form as we do, though, these models used additive regularization. we will compare to the sobolev gan in experiments. section: scaled maximum mean discrepancy we will now derive a lower bound on the gradient-constrained mmd which retains many of its attractive qualities but can be estimated in time linear in the dimension d. we then define the scaled maximum mean discrepancy based on this bound of proposition 4: 4 we use (m, i) to denote (m\u2212 1) d+ i; thus\u2207\u03b7 (x) stacks\u2207\u03b7 (x1),...,\u2207\u03b7 (xm) into one vector. [reference] we use\u2202ik (x, y) to denote the partial derivative with respect to xi, and\u2202 i+ d k (x, y) that for yi. because the constraint in the optimization of (7) is more restrictive than in that of (5), we have that smmd\u00b5, k,\u03bb (p, q)\u2264 gcmmd\u00b5, k,\u03bb (p, q). the sobolev norm f s (\u00b5),\u03bb, and a fortiori the gradient norm under\u00b5, is thus also controlled for the smmd critic. we also show in appendix f.1 that smmd\u00b5, k,\u03bb behaves similarly to gcmmd\u00b5, k,\u03bb on gaussians. f. estimating these terms based on samples from\u00b5 is straightforward, giving a natural estimator for the smmd. of course, if\u00b5 and k are fixed, the smmd is simply a constant times the mmd, and so behaves in essentially the same way as the mmd. but optimizing the smmd over a kernel family\u03c8, d\u00b5,\u03c8,\u03bb figure 1 (b) shows the vector field for the optimized smmd loss in example 1, using the wgan-gp measure\u00b5= uniform (0,\u03b8). the optimization surface is far more amenable: in particular the location c, which formerly had an extremely small gradient that made learning effectively impossible, now converges very quickly by first reducing the critic gradient until some signal is available. figure 1 s a fully-connected l-layer network with leaky-relu\u03b1 activations whose layers do not increase in width, and k satisfying mild smoothness conditions q k<\u221e (assumptions (ii) to (v) in appendix a.2). let\u03c8\u03ba be the set of parameters where each layer's weight matrices have condition number cond (. smmd (p n, p)\u2192 0, even if\u00b5 is chosen to depend on p and q. section: uniform bounds vs bounds in expectation controlling not necessarily imply a bound on f lip\u2265 sup x\u2208x\u2207f\u03c8 (x), and so does not in general give continuity via proposition 2. theorem 1 implies that when the network's weights are well-conditioned, it is sufficient to only control\u2207f\u03c8 2 l 2 (\u00b5), which is far easier in practice than controlling f lip. if we instead tried to directly controlled f lip with e.g. spectral normalization (sn) [reference], we could significantly reduce the expressiveness of the parametric family. in example 1, constraining\u03c6\u03c8 lip= 1 limits us to only\u03c8= {1}. thus d section: {1} mmd is simply the mmd with an rbf kernel of bandwidth 1, which has poor gradients when\u03b8 is far from 0 (figure 1 (c), blue). the cauchyschwartz bound of proposition 4 allows jointly adjusting the smoothness of k\u03c8 and the critic f, while sn must control the two independently. relatedly, limiting\u03c6 lip by limiting the lipschitz norm of each layer could substantially reduce capacity, while\u2207f\u03c8 l 2 (\u00b5) need not be decomposed by layer. another advantage is that\u00b5 provides a data-dependent measure of complexity as in [reference]: we do not needlessly prevent ourselves from using critics that behave poorly only far from the data. spectral parametrization when the generator is near a local optimum, the critic might identify only one direction on which q\u03b8 and p differ. if the generator parameterization is such that there is no local way for the generator to correct it, the critic may begin to single-mindedly focus on this difference, choosing redundant convolutional filters and causing the condition number of the weights to diverge. if this occurs, the generator will be motivated to fix this single direction while ignoring all other aspects of the distributions, after which it may become stuck. we can help avoid this collapse by using a critic parameterization that encourages diverse filters with higher-rank weight matrices. miyato et al. [reference] propose to parameterize the weight matrices as w=\u03b3w/ w op, where w op is the spectral norm ofw. this parametrization works particularly well with d section: experiments we evaluated unsupervised image generation on three datasets: cifar-10 [reference] (60 000 images, 32\u00d7 32), celeba [reference] (202 599 face images, resized and cropped to 160\u00d7 160 as in [reference]), and the more challenging ilsvrc2012 (imagenet) dataset [reference] (1 281 167 images, resized to 64\u00d7 64). code for all of these experiments is available at github.com/ michaelarbel/ scaled-mmd-gan. losses all models are based on a scalar-output critic network\u03c6\u03c8: x\u2192 r, except mmdgan-gp where\u03c6\u03c8: x\u2192 r 16 as in [reference]. the wgan and sobolev gan use a critic f=\u03c6\u03c8, while the gan uses a discriminator d\u03c8 (x)= 1/(1+ exp (\u2212\u03c6\u03c8 (x))). the mmd-based methods use a kernel 2/ 2), except for mmdgan-gp which uses a mixture of rq kernels as in [reference]. increasing the output dimension of the critic or using a different kernel did n't substantially change the performance of our proposed method. we also consider smmd with a linear top-level kernel, k (x, y)=\u03c6\u03c8 (x)\u03c6\u03c8 (y); because this becomes essentially identical to a wgan (appendix e), we refer to this method as swgan. smmd and swgan use\u00b5= p; sobolev gan uses\u00b5= (p+ q)/ 2 as in [reference]. we choose\u03bb and an overall scaling to obtain the losses:, swgan:. architecture for cifar-10, we used the cnn architecture proposed by [reference] with a 7-layer critic and a 4-layer generator. for celeba, we used a 5-layer dcgan discriminator and a 10-layer resnet generator as in [reference]. for imagenet, we used a 10-layer resnet for both the generator and discriminator. in all experiments we used 64 filters for the smallest convolutional layer, and double it at each layer (celeba/ imagenet) or every other layer (cifar-10). the input codes for the generator are drawn from uniform [\u22121, 1] 128. we consider two parameterizations for each critic: a standard one where the parameters can take any real value, and a spectral parametrization (denoted sn -) as above [reference]. models without explicit gradient control (sn-gan, sn-mmdgan, sn-mmgan-l2, sn-wgan) fix\u03b3= 1, for spectral normalization; others learn\u03b3, using a spectral parameterization. training all models were trained for 150 000 generator updates on a single gpu, except for imagenet where the model was trained on 3 gpus simultaneously. to limit communication overhead we averaged the mmd estimate on each gpu, giving the block mmd estimator [reference]. we always used 64 samples per gpu from each of p and q, and 5 critic updates per generator step. we used initial learning rates of 0.0001 for cifar-10 and celeba, 0.0002 for imagenet, and decayed these rates using the kid adaptive scheme of [reference]: every 2 000 steps, generator samples are compared to those from 20 000 steps ago, and if the relative kid test [reference] fails to show an improvement three consecutive times, the learning rate is decayed by 0.8. we used the adam optimizer [reference] with\u03b2 1= 0.5,\u03b2 2= 0.9. evaluation to compare the sample quality of different models, we considered three different scores based on the inception network [reference] trained for imagenet classification, all using default parameters in the implementation of [reference]. the inception score (is) [reference] is based on the entropy of predicted labels; higher values are better. though standard, this metric has many issues, particularly on datasets other than imagenet [reference][reference][reference]. the fid [reference] instead measures the similarity of samples from the generator and the target as the wasserstein-2 distance between gaussians fit to their intermediate representations. it is more sensible than the is and becoming standard, but its estimator is strongly biased [reference]. the kid [reference] is similar to fid, but by using a polynomial-kernel mmd its estimates enjoy better statistical properties and are easier to compare. (a similar score was recommended by [reference].) results table 1a presents the scores for models trained on both cifar-10 and celeba datasets. on cifar-10, sn-swgan and sn-smmdgan performed comparably to sn-gan. but on celeba, sn-swgan and sn-smmdgan dramatically outperformed the other methods with the same architecture in all three metrics. it also trained faster, and consistently outperformed other methods over multiple initializations (figure 2 (a)). it is worth noting that sn-swgan far outperformed wgan-gp on both datasets. table 1b presents the scores for smmdgan and sn-smmdgan trained on imagenet, and the scores of pre-trained models using bgan [reference] and sn-gan [reference]. [reference] the proposed methods substantially outperformed both methods in fid and kid scores. figure 3 shows samples on imagenet and celeba; appendix f.4 has more. spectrally normalized wgans/ mmdgans to control for the contribution of the spectral parametrization to the performance, we evaluated variants of mmdgans, wgans and sobolev-gan using spectral normalization (in table 2, appendix f.3). wgan and sobolev-gan led to unstable training and did n't converge at all (figure 11) despite many attempts. mmdgan converged on cifar-10 (figure 11) but was unstable on celeba (figure 10). the gradient control due to sn is thus probably too loose for these methods. this is reinforced by figure 2 (c), which shows that the expected gradient of the critic network is much better-controlled by smmd, even when sn is used. we also considered variants of these models with a learned\u03b3 while also adding a gradient penalty and an l 2 penalty on critic activations [reference]. these generally behaved similarly to mmdgan, and did n't lead to substantial improvements. we ran the same experiments on celeba, but aborted the runs early when it became clear that training was not successful. rank collapse we occasionally observed the failure mode for smmd where the critic becomes low-rank, discussed in section 3.3, especially on celeba; this failure was obvious even in the training objective. figure 2 (b) is one of these examples. spectral parametrization seemed to prevent this behavior. we also found one could avoid collapse by reverting to an earlier checkpoint and increasing the rkhs regularization parameter\u03bb, but did not do this for any of the experiments here. section: conclusion we studied gradient regularization for mmd-based critics in implicit generative models, clarifying how previous techniques relate to the d\u03c8 mmd loss. based on these insights, we proposed the gradientconstrained mmd and its approximation the scaled mmd, a new loss function for igms that controls gradient behavior in a principled way and obtains excellent performance in practice. one interesting area of future study for these distances is their behavior when used to diffuse particles distributed as q towards particles distributed as p. mroueh et al. [reference][reference] began such a study for the sobolev gan loss; [reference] proved convergence and studied discrete-time approximations. another area to explore is the geometry of these losses, as studied by bottou et al. [reference], who showed potential advantages of the wasserstein geometry over the mmd. their results, though, do not address any distances based on optimized kernels; the new distances introduced here might have interesting geometry of their own. section: 9 section: a proofs we first review some basic properties of reproducing kernel hilbert spaces. we consider here a separable rkhs h with basis (e i) i\u2208i, where i is either finite if h is finite-dimensional, or i= n otherwise. we also assume that the reproducing kernel k is continuously twice differentiable. we use a slightly nonstandard notation for derivatives:\u2202 i f (x) denotes the ith partial derivative of f evaluated at x, and\u2202 i\u2202 j+ d k (x, y) denotes we say that an operator a: h\u2192 h is hilbert-schmidt if a 2 hs= i\u2208i ae i 2 h is finite. a hs is called the hilbert-schmidt norm of a. the space of hilbert-schmidt operators itself a hilbert space with the inner product a, b hs= i\u2208i ae i, be i h. moreover, we say that an operator a is trace-class if its trace norm is finite, i.e. a 1= i\u2208i e i, (a* a) given two vectors f and g in h and a hilbert-schmidt operator a we have the following properties: (i) the outer product f\u2297 g is a hilbert-schmidt operator with hilbert-schmidt norm given by: ii) the inner product between two rank-one operators f\u2297 g and u the following identity holds: f, ag h= f\u2297 g, a hs. define the following covariance-type operators: [reference] these are useful in that, using [reference] and [reference] section: a.1 definitions and estimators of the new distances we will need the following assumptions about the distributions p and q, the measure\u00b5, and the kernel k: (a) p and q have integrable first moments. (b) k (x, x) grows at most linearly in x: for all x in x, k (x, x)\u2264 c (x+ 1) for some constant c. is automatically satisfied by a k such as the gaussian; when k is linear, it is true for a quite general class of networks\u03c6\u03c8 [7, lemma 1]. we will first give a form for the gradient-constrained mmd (5) in terms of the operator (10): proposition 5. under assumptions (a) to (d), the gradient-constrained mmd is given by proof of proposition 5. let f be a function in h. we will first express the squared\u03bb-regularized sobolev norm of f (6) as a quadratic form in h. recalling the reproducing properties of (8) and (9), we have: using property (ii) and the operator (10), one further gets under assumption (d), and using lemma 6, one can take the integral inside the inner product, which leads to f h. finally, using property (iii) it follows that under assumptions (a) and (b), lemma 6 applies, and it follows that k (x,\u00b7) is also bochner integrable under p and q. thus where\u03b7 is defined as this difference in mean embeddings. since d\u00b5,\u03bb is symmetric positive definite, its square-root d\u00b5,\u03bb is well-defined and is also invertible.\u00b5,\u03bb g. thus we can re-express the maximization problem in (5) in terms of g: proposition 5, though, involves inverting the infinite-dimensional operator d\u00b5,\u03bb and thus does n't directly give us a computable estimator. proposition 3 solves this problem in the case where\u00b5 is a discrete measure:\u03b4 xm be an empirical measure of m points. let\u03b7 (x)\u2208 r m have mth entry\u03b7 (x m), and\u2207\u03b7 (x)\u2208 r m d have (m, i) th entry 7\u2202 i\u03b7 (x m). then under assumptions (a) to (d), the gradient-constrained mmd is where k is the kernel matrix, and h that of derivatives of both arguments before proving proposition 3, we note the following interesting alternate form. let\u0113 i be the ith standard basis vector for r m+ m d, and define t: h\u2192 r m+ m d as the linear operator thus we can write 7 we use (m, i) to denote (m\u2212 1) d+ i; thus\u2207\u03b7 (x) stacks\u2207\u03b7 (x1),...,\u2207\u03b7 (xm) into one vector. proof of proposition 3. let g\u2208 h be the solution to the regression problem d\u00b5,\u03bb g=\u03b7: taking the inner product of both sides of (12) with k (x m,\u00b7) for each 1\u2264 m\u2264 m yields the following m equations: doing the same with\u2202 j k (x m,\u00b7) gives m d equations:. (14) from (12), it is clear that g is a linear combination of the form: where the coefficients\u03b1: satisfy the system of equations (13) and (14). we can rewrite this system as where i m, i m d are the identity matrices of dimension m, m d. since k and h must be positive semidefinite, an inverse exists. we conclude by noticing that the following result was key to our definition of the smmd in section 3.3. proposition 4. under assumptions (a) to (d), we have for all f\u2208 h that proof of proposition 4. the key idea here is to use the cauchy-schwarz inequality for the hilbertschmidt inner product. (a) follows from the reproducing properties (8) and (9) and property (ii). (b) is obtained using property (iii), while (c) follows from the cauchy-schwarz inequality and property (i). under assumptions (a) and (b), k (x,\u00b7) is bochner integrable with respect to any probability distribution p with finite first moment and the following relation holds: proof. the operator d x is positive self-adjoint. it is also trace-class, as by the triangle inequality by assumption (d), we have that the bochner integrability of k (x,\u00b7) under a distribution p with finite moment follows directly from assumptions (a) and (b), since section: a.2 continuity of the optimized scaled mmd in the wasserstein topology to prove theorem 1, we we will first need some new notation. we assume the kernel is k= k\u2022\u03c6\u03c8, i.e. k\u03c8 (x, y)= k (\u03c6\u03c8 (x),\u03c6\u03c8 (y)), where the representation function\u03c6\u03c8 is a network\u03c6\u03c8 (x): the intermediate representations h the elementwise activation function\u03c3 is given by\u03c3 0 (x)= x, and for l> 0 the activation\u03c3 l is a leaky relu with leak coefficient 0<\u03b1< 1: the parameter\u03c8 is the concatenation of all the layer parameters: we denote by\u03c8 the set of all such possible parameters, i.. define the following restrictions of\u03c8:\u03c8\u03ba is the set of those parameters such that w l have a small condition number, cond (w)=\u03c3 max (w)/\u03c3 min (w).\u03c8\u03ba 1 is the set of per-layer normalized parameters with a condition number bounded by\u03ba. recall the definition of scaled mmd, (7), where\u03bb> 0 and\u00b5 is a probability measure: the optimized smmd over the restricted set\u03c8\u03ba is given by: the constraint to\u03c8\u2208\u03c8\u03ba is critical to the proof. in practice, using a spectral parametrization helps enforce this assumption, as shown in figures 2 and 9. other regularization methods, like orthogonal normalization [reference], are also possible. we will use the following assumptions: (i)\u00b5 is a probability distribution absolutely continuous with respect to the lebesgue measure. (ii) the dimensions of the weights are decreasing per layer: (iii) the non-linearity used is leaky-relu, [reference], with leak coefficient\u03b1\u2208 (0, 1). (iv) the top-level kernel k is globally lipschitz in the rkhs norm: there exists a positive constant (v) there is some\u03b3 k> 0 for which k satisfies assumption (i) ensures that the points where\u03c6\u03c8 (x) is not differentiable are reached with probability 0 under\u00b5. this assumption can be easily satisfied e.g. if we define\u00b5 by adding gaussian noise to p. assumption (ii) helps ensure that the span of w l is never contained in the null space of w l+ 1. using leaky-relu as a non-linearity, assumption (iii), further ensures that the network\u03c6\u03c8 is locally full-rank almost everywhere; this might not be true with relu activations, where it could be always 0. assumptions (ii) and (iii) can be easily satisfied by design of the network. assumptions (iv) and (v) only depend on the top-level kernel k and are easy to satisfy in practice. in particular, they always hold for a smooth translation-invariant kernel, such as the gaussian, as well as the linear kernel. we are now ready to prove theorem 1. theorem 1. under assumptions (i) to (v), proof. define the pseudo-distance corresponding to the kernel k\u03c8 denote by w d\u03c8 (p, q) the optimal transport metric between p and q using the cost d\u03c8, given by where\u03c0 is the set of couplings with marginals p and q. by lemma 7, recall that\u03c6\u03c8 is lipschitz,\u03c6\u03c8 lip<\u221e, so along with assumption (iv) we have that where w is the standard wasserstein distance (2), and so we have that 2, and hence using lemma 8, we can write where we used\u03c6\u03c8 lip\u2264 l l=1 w l= 1. but by lemma 9, for lebesgue-almost all x, using assumption (i), this implies that thus for any\u03c8\u2208\u03c8\u03ba, the desired bound on d\u00b5,\u03c8\u03ba,\u03bb smmd follows immediately. lemma 7. let (x, y)\u2192 k (x, y) be the continuous kernel of an rkhs h defined on a polish space x, and define the corresponding pseudo-distance d k (x, y):= k (x,\u00b7)\u2212 k (y,\u00b7) h. then the following inequality holds for any distributions p and q on x, including when the quantities are infinite: proof. let p and q be two probability distributions, and let\u03c0 (p, q) be the set of couplings between them. let\u03c0*\u2208 argmin (x, y)\u223c\u03c0 [c k (x, y)] be an optimal coupling, which is guaranteed to exist take a sample (x, y)\u223c\u03c0 and a function f\u2208 h with f h\u2264 1. by the cauchy-schwarz inequality, taking the expectation with respect to\u03c0, we obtain the right-hand side is just the definition of w d k (p, q). by jensen's inequality, the left-hand side is lower-bounded by since\u03c0 has marginals p and q. we have shown so far that for any f\u2208 h with f h\u2264 1, the result follows by taking the supremum over f. proof. note that the condition number is unchanged, cond (w l)= cond (w l)\u2264\u03ba, and w l= 1, so\u03c8\u2208\u03c6\u03ba 1. it is also easy to see from [reference] that lemma 9. make assumptions (ii) and (iii), and let\u03c8\u2208\u03c8\u03ba 1. then the set of inputs for which any intermediate activation is exactly zero, has zero lebesgue measure. moreover, for any x/\u2208 n\u03c8,\u2207 x\u03c6\u03c8 (x) exists and proof. first, note that the network representation at layer l is piecewise affine. specifically, define m l x\u2208 r d l by, using assumption (iii), we will now show that the activation patterns are piecewise constant, so that thus, take some x/\u2208 n\u03c8, and find the smallest absolute value of its activations,=; clearly> 0. for any x with x\u2212 x<, we know that for all l and k, sign h proof. a more general version of this result can be found in [reference][reference]; we provide a proof here for completeness. if b has a nontrivial null space,\u03c3 min (b)= 0 and the inequality holds. otherwise, let r n* denote r n\\ {0}. recall that for c\u2208 r m\u00d7n with m\u2265 n, here a 1, a 2, a 3 and a 4 are defined by [reference] and are represented in figure 4: figure 4: decomposition of r 2 into 4 regions a 1, a 2, a 3 and a 4 as defined in [reference]. as\u03b1 approaches 0, the area of sets a 3 and a 4 becomes negligible. it is easy to see that whenever\u00b5 has a density, the probability of the sets a 3 and a 4 goes to 0 are\u03b1\u2192 0. hence one can deduce that on the other hand, the squared lipschitz constant of\u03c6 is given by (1\u2212\u03b3) 2+ (1+\u03b1\u2212\u03b3) 2 which converges to 2 (1\u2212\u03b3) 2. this shows that controlling the expectation of the gradient does n't allow to effectively control the lipschitz constant of\u03c6. section: monotonicity of the dimensions: we would like to consider a second example where assumption (ii) does n't hold. consider the following two layer network defined by: for\u03b2> 0. note that w\u03b2 is a full rank matrix, but assumption (ii) does n't hold. depending on the sign of the components of w\u03b2 x one has the following expression for\u2207\u03c6\u03b1 (x) 2: where (b i) 1\u2264i\u22646 are defined by (26) section: 20 the squared lipschitz constant is given by\u03c6 2 l (1\u2212\u03b3) 2+\u03b2 2 while the expected squared norm of the gradient of\u03c6 is given by: again the set b 4\u222a b 5 becomes negligible as\u03b2 approaches 0 which implies that 2. note that unlike in the first example in (21), the matrix w\u03b2 has a bounded condition number. in this example, the columns of w 0 are all in the null space of [\u22121 0 1], which implies\u2207\u03c6 0 (x)= 0 for all x\u2208 r 2, even though all matrices have full rank. b diracgan vector fields for more losses figure 5: vector fields for different losses with respect to the generator parameter\u03b8 and the feature representation parameter\u03c8; the losses use a gaussian kernel, and are shown in [reference]. following [reference], p=\u03b4 0, q=\u03b4\u03b8 and\u03c6\u03c8 (x)=\u03c8x. the curves show the result of taking simultaneous gradient steps in (\u03b8,\u03c8) beginning from three initial parameter values. figure 5 shows parameter vector fields, like those in figure 6, for example 1 for a variety of different losses: section: 21 the squared mmd between\u03b4 0 and\u03b4\u03b8 under a gaussian kernel of bandwidth 1/\u03c8 and is given by). mmd-gp-unif uses a gradient penalty as in [reference] where each samples from\u00b5* is obtained by first sampling x and y from p and q and then sampling uniformly between x and y. mmd-gp uses the same gradient penalty, but the expectation is taken under p rather than\u00b5*. sn-mmd refers to mmd with spectral normalization; here this means that\u03c8= 1. sobolev-mmd refers to the loss used in [reference] with the quadratic penalty only. gcmmd\u00b5, k,\u03bb is defined by (5), with\u00b5= n (0, 10 2). c vector fields of gradient-constrained mmd and sobolev gan critics mroueh et al. [reference] argue that the gradient of the critic (...) defines a transportation plan for moving the distribution mass (from generated to reference distribution) and present the solution of sobolev pde for 2-dimensional gaussians. we observed that in this simple example the gradient of the sobolev critic can be very high outside of the areas of high density, which is not the case with the gradient-constrained mmd. figure 6 presents critic gradients in both cases, using\u00b5= (p+ q)/ 2 for both. (a) gradient-constrained mmd critic gradient. this unintuitive behavior is most likely related to the vanishing boundary condition, assummed by sobolev gan. solving the actual sobolev pde, we found that the sobolev critic has very high gradients close to the boundary in order to match the condition; moreover, these gradients point in opposite directions to the target distribution. and the corresponding critic function is thus if we assume e x\u223cp\u03c6 (x)> e y\u223cq\u03c6 (y), as that is the goal of our critic training, we see that the mmd becomes identical to the wgan loss, and the gradient penalty is applied to the same function. (mmd gans, however, would typically train on the unbiased estimator of mmd 2, giving a very slightly different loss function. [reference] also applied the gradient penalty to\u03b7 rather than the true critic\u03b7/\u03b7.) the smmd with a linear kernel is thus analogous to applying the scaling operator to a wgan; hence the name swgan. f additional experiments f.1 comparison of gradient-constrained mmd to scaled mmd figure 7 shows the behavior of the mmd, the gradient-constrained smmd, and the scaled mmd when comparing gaussian distributions. we can see that mmd\u221d smmd and the gradientconstrained mmd behave similarly in this case, and that optimizing the smmd and the gradientconstrained mmd is also similar. optimizing the mmd would yield an essentially constant distance. section: f.2 igms with optimized gradient-constrained mmd loss we implemented the estimator of proposition 3 using the empirical mean estimator of\u03b7, and sharing samples for\u00b5= p. to handle the large but approximately low-rank matrix system, we used an incomplete cholesky decomposition [reference][reference] then the woodbury matrix identity allows an efficient evaluation: even though only a small is required for a good approximation, and the full matrices k, g, and h need never be constructed, backpropagation through this procedure is slow and not especially gpu-friendly; training on cpu was faster. thus we were only able to run the estimator on mnist, and even that took days to conduct the optimization on powerful workstations. the learned models, however, were reasonable. using a dcgan architecture, batches of size 64, and a procedure that otherwise agreed with the setup of section 4, samples with and without spectral normalization are shown in figures 8a and 8b. after the points in training shown, however, the same rank collapse as discussed in section 4 occurred. here it seems that spectral normalization may have delayed the collapse, but not prevented it. figure 8c shows generator loss estimates through training, including the obvious peak at collapse; figure 8d shows kid scores based on the mnist-trained convnet representation [reference], including comparable smmd models for context. the fact that smmd models converged somewhat faster than gradient-constrained mmd models here may be more related to properties of the estimator of proposition 3 rather than the distances; more work would be needed to fully compare the behavior of the two distances. figure 9 shows the distribution of critic weight singular values, like figure 2, at more layers. figure 11 and table 2 show results for the spectral normalization variants considered in the experiments. mmdgan, with neither spectral normalization nor a gradient penalty, did surprisingly well in this case, though it fails badly in other situations. figure 9 compares the decay of singular values for layer of the critic's network at both early and later stages of training in two cases: with or without the spectral parametrization. the model was trained on celeba using smmd. figure 11 shows the evolution per iteration of inception score, section: f.3 spectral normalization and scaled mmd section: section: it is undefined when any h because\u03c8\u2208\u03c8\u03ba 1, we have\u2264 1, and using assumption (ii) with lemma 10 gives x) denote the full activation patterns up to level l, we can thus write there are only finitely many possible values for h l x; we denote the set of such values as h l. then we have that because each w h l k is of rank d l, each set in the union is either empty or an affine subspace of dimension d\u2212 d l. as each d l> 0, each set in the finite union has zero lebesgue measure, and n\u03c8 also has zero lebesgue measure. thus, as bx= 0 for x= 0, section: a.2.1 when some of the assumptions do n't hold here we analyze through simple examples what happens when the condition number can be unbounded, and when assumption (ii), about decreasing widths of the network, is violated. condition number: we start by a first example where the condition number can be arbitrarily high. we consider a two-layer network on r 2, defined by where\u03b1> 0. as\u03b1 approaches 0 the matrix w\u03b1 becomes singular which means that its condition number blows up. we are interested in analyzing the behavior of the lipschitz constant of\u03c6 and the expected squared norm of its gradient under\u00b5 as\u03b1 approaches 0. one can easily compute the squared norm of the gradient of\u03c6 which is given by section: d an estimator for lipschitz mmd we now describe briefly how to estimate the lipschitz mmd in low dimensions. recall that for f\u2208 h k, it is the case that thus we can approximate the constraint f 2 lip+\u03bb f 2 h k\u2264 1 by enforcing the constraint on a set of m points {z i} reasonably densely covering the region around the supports of p and q, rather 22 than enforcing it at every point in x. an estimator of the lipschitz mmd based on x\u223c p n x and by the generalized representer theorem, the optimal f for (29) will be of the form writing\u03b4= (\u03b1,\u03b2,\u03b3), the objective function is linear in\u03b4, the constraints are quadratic, built from the following matrices, where the x and y samples are concatenated together, as are the derivatives with each dimension of the z samples: given these matrices, and letting where e (i, j) is the (i, j) th standard basis vector in r md, we have that thus the optimization problem (29) is a linear problem with convex quadratic constraints, which can be solved by standard convex optimization software. the approximation is reasonable only if we can effectively cover the region of interest with densely spaced {z i}; it requires a nontrivial amount of computation even for the very simple 1-dimensional toy problem of example 1. one advantage of this estimator, though, is that finding its derivative with respect to the input points or the kernel parameterization is almost free once we have computed the estimate, as long as our solver has computed the dual variables\u00b5 corresponding to the constraints in [reference]. we just need to exploit the envelope theorem and then differentiate the kkt conditions, as done for instance in [reference]. the differential of (29) ends up being, assuming the optimum of (29) is at\u03b4\u2208 r n x+ n y+ md and\u00b5\u2208 r m, d lipmmd k,\u03bb (x, y, z)=\u03b4 t dk db section:",
    "templates": [
        {
            "Material": [
                [
                    [
                        "cifar-10",
                        16615
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "sn-smmdgan",
                        20544
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "fid",
                        19932
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "image generation",
                        3938
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "cifar-10",
                        16615
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "sn-smmdgan",
                        20544
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "inception score",
                        19698
                    ],
                    [
                        "is",
                        19716
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "image generation",
                        3938
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "3f55d26dd638c849745b95e912c28d88445ba5e1-59",
    "doctext": "document: supervised learning of universal sentence representations from natural language inference data many modern nlp systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. in this paper, we show how universal sentence representations trained using the supervised data of the stanford natural language inference datasets can consistently outperform unsupervised methods like skipthought vectors on a wide range of transfer tasks. much like how computer vision uses imagenet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other nlp tasks. our encoder is publicly available. section: introduction distributed representations of words (or word embeddings) have shown to provide useful features for various tasks in natural language processing and computer vision. while there seems to be a consensus concerning the usefulness of word embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence. that is, how to capture the relationships among multiple words and phrases in a single vector remains an question to be solved. in this paper, we study the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks. two questions need to be solved in order to build such an encoder, namely: what is the preferable neural network architecture; and how and on what task should such a network be trained. following existing work on learning word embeddings, most current approaches consider learning sentence encoders in an unsupervised manner like skipthought or fastsent. here, we investigate whether supervised learning can be leveraged instead, taking inspiration from previous results in computer vision, where many models are pretrained on the imagenet before being transferred. we compare sentence embeddings trained on various supervised tasks, and show that sentence embeddings generated from models trained on a natural language inference (nli) task reach the best results in terms of transfer accuracy. we hypothesize that the suitability of nli as a training task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences. unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks. hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes. our experiments show that an encoder based on a bi-directional lstm architecture with max pooling, trained on the stanford natural language inference (snli) dataset, yields state-of-the-art sentence embeddings compared to all existing alternative unsupervised approaches like skipthought or fastsent, while being much faster to train. we establish this finding on a broad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information. section: related work transfer learning using supervised features has been successful in several computer vision applications. striking examples include face recognition and visual question answering, where image features trained on imagenet and word embeddings trained on large unsupervised corpora are combined. in contrast, most approaches for sentence representation learning are unsupervised, arguably because the nlp community has not yet found the best supervised task for embedding the semantics of a whole sentence. another reason is that neural networks are very good at capturing the biases of the task on which they are trained, but can easily forget the overall information or semantics of the input data by specializing too much on these biases. learning models on large unsupervised task makes it harder for the model to specialize. littwin2016multiverse showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder. they propose a loss that incorporates multiple orthogonal classifiers to counteract this effect. recent work on generating sentence embeddings range from models that compose word embeddings to more complex neural network architectures. skipthought vectors propose an objective function that adapts the skip-gram model for words to the sentence level. by encoding a sentence to predict the sentences around it, and using the features in a linear model, they were able to demonstrate good performance on 8 transfer tasks. they further obtained better results using layer-norm regularization of their model in. hill2016learning showed that the task on which sentence embeddings are trained significantly impacts their quality. in addition to unsupervised methods, they included supervised training in their comparison\u2014 namely, on machine translation data (using the wmt'14 english/ french and english/ german pairs), dictionary definitions and image captioning data (see also kiela2017learning) from the coco dataset. these models obtained significantly lower results compared to the unsupervised skip-thought approach. recent work has explored training sentence encoders on the snli corpus and applying them on the sick corpus, either using multi-task learning or pretraining. the results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead. to our knowledge, this work is the first attempt to fully exploit the snli corpus for building generic sentence encoders. as we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but human-annotated) data. section: approach this work combines two research directions, which we describe in what follows. first, we explain how the nli task can be used to train universal sentence encoding models using the snli task. we subsequently describe the architectures that we investigated for the sentence encoder, which, in our opinion, covers a suitable range of sentence encoders currently in use. specifically, we examine standard recurrent models such as lstms and grus, for which we investigate mean and max-pooling over the hidden representations; a self-attentive network that incorporates different views of the sentence; and a hierarchical convolutional network that can be seen as a tree-based method that blends different levels of abstraction. subsection: the natural language inference task the snli dataset consists of 570k human-generated english sentence pairs, manually labeled with one of three categories: entailment, contradiction and neutral. it captures natural language inference, also known in previous incarnations as recognizing textual entailment (rte), and constitutes one of the largest high-quality labeled resources explicitly constructed in order to require understanding sentence semantics. we hypothesize that the semantic nature of nli makes it a good candidate for learning universal sentence embeddings in a supervised way. that is, we aim to demonstrate that sentence encoders trained on natural language inference are able to learn sentence representations that capture universally useful features. models can be trained on snli in two different ways: (i) sentence encoding-based models that explicitly separate the encoding of the individual sentences and (ii) joint methods that allow to use encoding of both sentences (to use cross-features or attention from one sentence to the other). since our goal is to train a generic sentence encoder, we adopt the first setting. as illustrated in figure [reference], a typical architecture of this kind uses a shared sentence encoder that outputs a representation for the premise and the hypothesis. once the sentence vectors are generated, 3 matching methods are applied to extract relations between and: (i) concatenation of the two representations; (ii) element-wise product; and (iii) absolute element-wise difference. the resulting vector, which captures information from both the premise and the hypothesis, is fed into a 3-class classifier consisting of multiple fully-connected layers culminating in a softmax layer. subsection: sentence encoder architectures a wide variety of neural networks for encoding sentences into fixed-size representations exists, and it is not yet clear which one best captures generically useful information. we compare 7 different architectures: standard recurrent encoders with either long short-term memory (lstm) or gated recurrent units (gru), concatenation of last hidden states of forward and backward gru, bi-directional lstms (bilstm) with either mean or max pooling, self-attentive network and hierarchical convolutional networks. subsubsection: lstm and gru our first, and simplest, encoders apply recurrent neural networks using either lstm or gru modules, as in sequence to sequence encoders. for a sequence of words, the network computes a set of hidden representations, with (or using gru units instead). a sentence is represented by the last hidden vector,. we also consider a model bigru-last that concatenates the last hidden state of a forward gru, and the last hidden state of a backward gru to have the same architecture as for skipthought vectors. subsubsection: bilstm with mean/ max pooling for a sequence of t words, a bidirectional lstm computes a set of t vectors. for,, is the concatenation of a forward lstm and a backward lstm that read the sentences in two opposite directions: we experiment with two ways of combining the varying number of to form a fixed-size vector, either by selecting the maximum value over each dimension of the hidden units (max pooling) or by considering the average of the representations (mean pooling). subsubsection: self-attentive network the self-attentive sentence encoder uses an attention mechanism over the hidden states of a bilstm to generate a representation of an input sentence. the attention mechanism is defined as: where are the output hidden vectors of a bilstm. these are fed to an affine transformation (,) which outputs a set of keys. the represent the score of similarity between the keys and a learned context query vector. these weights are used to produce the final representation, which is a weighted linear combination of the hidden vectors. following lin2017structured we use a self-attentive network with multiple views of the input sentence, so that the model can learn which part of the sentence is important for the given task. concretely, we have 4 context vectors which generate 4 representations that are then concatenated to obtain the sentence representation. figure [reference] illustrates this architecture. subsubsection: hierarchical convnet one of the currently best performing models on classification tasks is a convolutional architecture termed adasent, which concatenates different representations of the sentences at different level of abstractions. inspired by this architecture, we introduce a faster version consisting of 4 convolutional layers. at every layer, a representation is computed by a max-pooling operation over the feature maps (see figure [reference]). the final representation concatenates representations at different levels of the input sentence. the model thus captures hierarchical abstractions of an input sentence in a fixed-size representation. subsection: training details for all our models trained on snli, we use sgd with a learning rate of 0.1 and a weight decay of 0.99. at each epoch, we divide the learning rate by 5 if the dev accuracy decreases. we use mini-batches of size 64 and training is stopped when the learning rate goes under the threshold of. for the classifier, we use a multi-layer perceptron with 1 hidden-layer of 512 hidden units. we use open-source glove vectors trained on common crawl 840b with 300 dimensions as fixed word embeddings. section: evaluation of sentence representations our aim is to obtain general-purpose sentence embeddings that capture generic information that is useful for a broad set of tasks. to evaluate the quality of these representations, we use them as features in 12 transfer tasks. we present our sentence-embedding evaluation procedure in this section. we constructed a sentence evaluation tool called senteval to automate evaluation on all the tasks mentioned in this paper. the tool uses adam to fit a logistic regression classifier, with batch size 64. paragraph: binary and multi-class classification we use a set of binary classification tasks (see table [reference]) that covers various types of sentence classification, including sentiment analysis (mr, sst), question-type (trec), product reviews (cr), subjectivity/ objectivity (subj) and opinion polarity (mpqa). we generate sentence vectors and train a logistic regression on top. a linear classifier requires fewer parameters than an mlp and is thus suitable for small datasets, where transfer learning is especially well-suited. we tune the l2 penalty of the logistic regression with grid-search on the validation set. paragraph: entailment and semantic relatedness we also evaluate on the sick dataset for both entailment (sick-e) and semantic relatedness (sick-r). we use the same matching methods as in snli and learn a logistic regression on top of the joint representation. for semantic relatedness evaluation, we follow the approach of and learn to predict the probability distribution of relatedness scores. we report pearson correlation. paragraph: sts14-semantic textual similarity while semantic relatedness is supervised in the case of sick-r, we also evaluate our embeddings on the 6 unsupervised semeval tasks of sts14. this dataset includes subsets of news articles, forum discussions, image descriptions and headlines from news articles containing pairs of sentences (lower-cased), labeled with a similarity score between 0 and 5. these tasks evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through pearson and spearman correlations. paragraph: paraphrase detection the microsoft research paraphrase corpus is composed of pairs of sentences which have been extracted from news sources on the web. sentence pairs have been human-annotated according to whether they capture a paraphrase/ semantic equivalence relationship. we use the same approach as with sick-e, except that our classifier has only 2 classes. paragraph: caption-image retrieval the caption-image retrieval task evaluates joint image and language feature models. the goal is either to rank a large collection of images by their relevance with respect to a given query caption (image retrieval), or ranking captions by their relevance for a given query image (caption retrieval). we use a pairwise ranking-loss: where consists of an image with one of its associated captions, and are negative examples of the ranking loss, is the margin and corresponds to the cosine similarity. and are learned linear transformations that project the caption and the image to the same embedding space. we use a margin and contrastive terms. we use the same splits as in, i.e., we use 113k images from the coco dataset (each containing 5 captions) for training, 5k images for validation and 5k images for test. for evaluation, we split the 5k images in 5 random sets of 1k images on which we compute recall@k, with k and median (med r) over the 5 splits. for fair comparison, we also report skipthought results in our setting, using 2048-dimensional pretrained resnet-101 with 113k training images. section: empirical results in this section, we refer to\" micro\" and\" macro\" averages of development set (dev) results on transfer tasks whose metrics is accuracy: we compute a\" macro\" aggregated score that corresponds to the classical average of dev accuracies, and the\" micro\" score that is a sum of the dev accuracies, weighted by the number of dev samples. subsection: architecture impact paragraph: model we observe in table [reference] that different models trained on the same nli corpus lead to different transfer tasks results. the bilstm-4096 with the max-pooling operation performs best on both snli and transfer tasks. looking at the micro and macro averages, we see that it performs significantly better than the other models lstm, gru, bigru-last, bilstm-mean, inner-attention and the hierarchical-convnet. table [reference] also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and bilstm-mean for instance. we hypothesize that some models are likely to over-specialize and adapt too well to the biases of a dataset without capturing general-purpose information of the input sentence. for example, the inner-attention model has the ability to focus only on certain parts of a sentence that are useful for the snli task, but not necessarily for the transfer tasks. on the other hand, bilstm-mean does not make sharp choices on which part of the sentence is more important than others. the difference between the results seems to come from the different abilities of the models to incorporate general information while not focusing too much on specific features useful for the task at hand. for a given model, the transfer quality is also sensitive to the optimization algorithm: when training with adam instead of sgd, we observed that the bilstm-max converged faster on snli (5 epochs instead of 10), but obtained worse results on the transfer tasks, most likely because of the model and classifier's increased capability to over-specialize on the training task. paragraph: embedding size figure [reference] compares the overall performance of different architectures, showing the evolution of micro averaged performance with regard to the embedding size. since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models. however, this is particularly true for some models (bilstm-max, hconvnet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows. we hypothesize that such networks are able to incorporate information that is not directly relevant to the objective task (results on snli are relatively stable with regard to embedding size) but that can nevertheless be useful as features for transfer tasks. subsection: task transfer we report in table [reference] transfer tasks results for different architectures trained in different ways. we group models by the nature of the data on which they were trained. the first group corresponds to models trained with unsupervised unordered sentences. this includes bag-of-words models such as word2vec-skipgram, the unigram-tfidf model, the paragraph vector model, the sequential denoising auto-encoder (sdae) and the sif model, all trained on the toronto book corpus. the second group consists of models trained with unsupervised ordered sentences such as fastsent and skipthought (also trained on the toronto book corpus). we also include the fastsent variant\" fastsent+ ae\" and the skipthought-ln version that uses layer normalization. we report results from models trained on supervised data in the third group, and also report some results of supervised methods trained directly on each task for comparison with transfer learning approaches. paragraph: comparison with skipthought the best performing sentence encoder to date is the skipthought-ln model, which was trained on a very large corpora of ordered sentences. with much less data (570k compared to 64 m sentences) but with high-quality supervision from the snli dataset, we are able to consistently outperform the results obtained by skipthought vectors. we train our model in less than a day on a single gpu compared to the best skipthought-ln network trained for a month. our bilstm-max trained on snli performs much better than released skipthought vectors on mr, cr, mpqa, sst, mrpc-accuracy, sick-r, sick-e and sts14 (see table [reference]). except for the subj dataset, it also performs better than skipthought-ln on mr, cr and mpqa. we also observe by looking at the sts14 results that the cosine metrics in our embedding space is much more semantically informative than in skipthought embedding space (pearson score of 0.68 compared to 0.29 and 0.44 for st and st-ln). we hypothesize that this is namely linked to the matching method of snli models which incorporates a notion of distance (element-wise product and absolute difference) during training. paragraph: nli as a supervised training set our findings indicate that our model trained on snli obtains much better overall results than models trained on other supervised tasks such as coco, dictionary definitions, nmt, ppdb and sst. for sst, we tried exactly the same models as for snli; it is worth noting that sst is smaller than nli. our representations constitute higher-quality features for both classification and similarity tasks. one explanation is that the natural language inference task constrains the model to encode the semantic information of the input sentence, and that the information required to perform nli is generally discriminative and informative. paragraph: domain adaptation on sick tasks our transfer learning approach obtains better results than previous state-of-the-art on the sick task-can be seen as an out-domain version of snli-for both entailment and relatedness. we obtain a pearson score of 0.885 on sick-r while obtained 0.868, and we obtain 86.3% test accuracy on sick-e while previous best hand-engineered models obtained 84.5%. we also significantly outperformed previous transfer learning approaches on sick-e that used the parameters of an lstm model trained on snli to fine-tune on sick (80.8% accuracy). we hypothesize that our embeddings already contain the information learned from the in-domain task, and that learning only the classifier limits the number of parameters learned on the small out-domain task. paragraph: image-caption retrieval results in table [reference], we report results for the coco image-caption retrieval task. we report the mean recalls of 5 random splits of 1 k test images. when trained with resnet features and 30k more training data, the skipthought vectors perform significantly better than the original setting, going from 33.8 to 37.9 for caption retrieval r@1, and from 25.9 to 30.6 on image retrieval r@1. our approach pushes the results even further, from 37.9 to 42.4 on caption retrieval, and 30.6 to 33.2 on image retrieval. these results are comparable to previous approach of that did not do transfer but directly learned the sentence encoding on the image-caption retrieval task. this supports the claim that pre-trained representations such as resnet image features and our sentence embeddings can achieve competitive results compared to features learned directly on the objective task. paragraph: multigenre nli the multinli corpus was recently released as a multi-genre version of snli. with 433 k sentence pairs, multinli improves upon snli in its coverage: it contains ten distinct genres of written and spoken english, covering most of the complexity of the language. we augment table 4 with our model trained on both snli and multinli (allnli). we observe a significant boost in performance overall compared to the model trained only on slni. our model even reaches adasent performance on cr, suggesting that having a larger coverage for the training task helps learn even better general representations. on semantic textual similarity sts14, we are also competitive with ppdb based paragram-phrase embeddings with a pearson score of 0.70. interestingly, on caption-related transfer tasks such as the coco image caption retrieval task, training our sentence encoder on other genres from multinli does not degrade the performance compared to the model trained only snli (which contains mostly captions), which confirms the generalization power of our embeddings. section: conclusion this paper studies the effects of training sentence embeddings with supervised data by testing on 12 different transfer tasks. we showed that models learned on nli can perform better than models trained in unsupervised conditions or on other supervised tasks. by exploring various architectures, we showed that a bilstm network with max pooling makes the best current universal sentence encoding methods, outperforming existing approaches like skipthought vectors. we believe that this work only scratches the surface of possible combinations of models and tasks for learning generic sentence embeddings. larger datasets that rely on natural language understanding for sentences could bring sentence embedding quality to the next level. bibliography: references section: appendix paragraph: max-pooling visualization for bilstm-max trained and untrained our representations were trained to focus on parts of a sentence such that a classifier can easily tell the difference between contradictory, neutral or entailed sentences. in table [reference] and table [reference], we investigate how the max-pooling operation selects the information from the hidden states of the bilstm, for our trained and untrained bilstm-max models (for both models, word embeddings are initialized with glove vectors). for each time step, we report the number of times the max-pooling operation selected the hidden state (which can be seen as a sentence representation centered around word). without any training, the max-pooling is rather even across hidden states, although it seems to focus consistently more on the first and last hidden states. when trained, the model learns to focus on specific words that carry most of the meaning of the sentence without any explicit attention mechanism. note that each hidden state also incorporates information from the sentence at different levels, explaining why the trained model also incorporates information from all hidden states.",
    "templates": [
        {
            "Material": [
                [
                    [
                        "stanford natural language inference datasets",
                        592
                    ],
                    [
                        "stanford natural language inference",
                        3164
                    ],
                    [
                        "snli",
                        3202
                    ],
                    [
                        "sts14",
                        14306
                    ],
                    [
                        "allnli",
                        24008
                    ],
                    [
                        "slni",
                        24108
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "max pooling",
                        3136
                    ],
                    [
                        "max-pooling",
                        6821
                    ],
                    [
                        "max-pooling operation",
                        11734
                    ],
                    [
                        "bilstm-4096",
                        16736
                    ],
                    [
                        "st-ln",
                        21081
                    ],
                    [
                        "bilstm network",
                        25066
                    ],
                    [
                        "max-pooling visualization",
                        25544
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "test accuracy",
                        22261
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "natural language inference",
                        902
                    ],
                    [
                        "nli",
                        2449
                    ],
                    [
                        "nli task",
                        6450
                    ],
                    [
                        "natural language inference task",
                        7084
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "stanford natural language inference datasets",
                        592
                    ],
                    [
                        "stanford natural language inference",
                        3164
                    ],
                    [
                        "snli",
                        3202
                    ],
                    [
                        "sts14",
                        14306
                    ],
                    [
                        "allnli",
                        24008
                    ],
                    [
                        "slni",
                        24108
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "max pooling",
                        3136
                    ],
                    [
                        "max-pooling",
                        6821
                    ],
                    [
                        "max-pooling operation",
                        11734
                    ],
                    [
                        "bilstm-4096",
                        16736
                    ],
                    [
                        "st-ln",
                        21081
                    ],
                    [
                        "bilstm network",
                        25066
                    ],
                    [
                        "max-pooling visualization",
                        25544
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "fewer parameters",
                        13488
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "natural language inference",
                        902
                    ],
                    [
                        "nli",
                        2449
                    ],
                    [
                        "nli task",
                        6450
                    ],
                    [
                        "natural language inference task",
                        7084
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "senteval",
                        12919
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "micro averaged performance",
                        18413
                    ],
                    [
                        "mrpc-accuracy",
                        20694
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "mr",
                        13275
                    ],
                    [
                        "sst",
                        13278
                    ],
                    [
                        "semantic textual similarity",
                        24279
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "senteval",
                        12919
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "sick-e",
                        13805
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "mr",
                        13275
                    ],
                    [
                        "sst",
                        13278
                    ],
                    [
                        "semantic textual similarity",
                        24279
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "senteval",
                        12919
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "semantic relatedness",
                        13725
                    ],
                    [
                        "sick-r",
                        13839
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "mr",
                        13275
                    ],
                    [
                        "sst",
                        13278
                    ],
                    [
                        "semantic textual similarity",
                        24279
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "senteval",
                        12919
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "sentence embeddings",
                        3240
                    ],
                    [
                        "similarity score",
                        14492
                    ],
                    [
                        "pearson and spearman correlations",
                        14649
                    ],
                    [
                        "cosine metrics",
                        20909
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "mr",
                        13275
                    ],
                    [
                        "sst",
                        13278
                    ],
                    [
                        "semantic textual similarity",
                        24279
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "french and english",
                        5472
                    ],
                    [
                        "human-generated english sentence pairs",
                        7150
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "transfer accuracy",
                        2493
                    ],
                    [
                        "dev accuracy",
                        12191
                    ],
                    [
                        "accuracy",
                        16349
                    ],
                    [
                        "classical average of dev accuracies",
                        16421
                    ],
                    [
                        "dev accuracies",
                        16501
                    ]
                ]
            ],
            "Task": []
        },
        {
            "Material": [
                [
                    [
                        "german pairs",
                        5492
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "transfer accuracy",
                        2493
                    ],
                    [
                        "dev accuracy",
                        12191
                    ],
                    [
                        "accuracy",
                        16349
                    ],
                    [
                        "classical average of dev accuracies",
                        16421
                    ],
                    [
                        "dev accuracies",
                        16501
                    ]
                ]
            ],
            "Task": []
        }
    ]
}
{
    "docid": "3fc5ed18c2294596af072df929c8ee12c71f96a2-60",
    "doctext": "document: classical structured prediction losses for sequence to sequence learning there has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. in this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. we also report new state of the art results on both iwslt'14 german-english translation as well as gigaword abstractive summarization. on the large wmt'14 english-french task, sequence-level training achieves 41.5 bleu which is on par with the state of the art. section: introduction sequence to sequence models are usually trained with a simple token-level likelihood loss sutskever2014sequence, bahdanau2014neural. however, at test time, these models do not produce a single token but a whole sequence. in order to resolve this inconsistency and to potentially improve generation, recent work has focused on training these models at the sequence-level, for instance using reinforce ranzato2015sequence, actor-critic bahdanau2016ac, or with beam search optimization wiseman2016acl. before the recent work on sequence level training for neural networks, there has been a large body of research on training linear models at the sequence level. for example, direct loss optimization has been popularized in machine translation with the minimum error rate training algorithm (mert; och 2003) and expected risk minimization has an extensive history in nlp. this paper revisits several objective functions that have been commonly used for structured prediction tasks in nlp gimpel+ smith2010acl and apply them to a neural sequence to sequence model gehring2017icml (\u00a7 [reference]). specifically, we consider likelihood training at the sequence-level, a margin loss as well as expected risk training. we also investigate several combinations of global losses with token-level likelihood. this is, to our knowledge, the most comprehensive comparison of structured losses in the context of neural sequence to sequence models (\u00a7 [reference]). we experiment on the iwslt'14 german-english translation task cettolo2014report as well as the gigaword abstractive summarization task rush2015abs. we achieve the best reported accuracy to date on both tasks. we find that the sequence level losses we survey perform similarly to one another and outperform beam search optimization wiseman2016acl on a comparable setup. on wmt'14 english-french, we also illustrate the effectiveness of risk minimization on a larger translation task. classical losses for structured prediction are still very competitive and effective for neural models (\u00a7 [reference],\u00a7 [reference]). section: sequence to sequence learning the general architecture of our sequence to sequence models follows the encoder-decoder approach with soft attention first introduced in. as a main difference, in most of our experiments we parameterize the encoder and the decoder as convolutional neural networks instead of recurrent networks gehring2016convolutional, gehring2017icml. our use of convolution is motivated by computational and accuracy considerations. however, the objective functions we present are model agnostic and equally applicable to recurrent and convolutional models. we demonstrate the applicability of our objective functions to recurrent models (lstm) in our comparison to wiseman2016acl in\u00a7 [reference]. notation. we denote the source sentence as, an output sentence of our model as, and the reference or target sentence as. for some objectives, we choose a pseudo reference instead, such as a model output with the highest bleu or rouge score among a set of candidate outputs,, generated by our model. concretely, the encoder processes a source sentence containing words and outputs a sequence of states. the decoder takes and generates the output sequence left to right, one element at a time. for each output, the decoder computes hidden state based on the previous state, an embedding of the previous target language word, as well as a conditional input derived from the encoder output. the attention context is computed as a weighted sum of at each time step. the weights of this sum are referred to as attention scores and allow the network to focus on the most relevant parts of the input at each generation step. attention scores are computed by comparing each encoder state to a combination of the previous decoder state and the last prediction; the result is normalized to be a distribution over input elements. at each generation step, the model scores for the possible next target words by transforming the decoder output via a linear layer with weights and bias:. this is turned into a distribution via a softmax:. our encoder and decoder use gated convolutional neural networks which enable fast and accurate generation. fast generation is essential to efficiently train on the model output as is done in this work as sequence-level losses require generating at training time. both encoder and decoder networks share a simple block structure that computes intermediate states based on a fixed number of input tokens and we stack several blocks on top of each other. each block contains a 1-d convolution that takes as input feature vectors and outputs another vector; subsequent layers operate over the output elements of the previous layer. the output of the convolution is then fed into a gated linear unit dauphin2017icml. in the decoder network, we rely on causal convolution which rely only on states from the previous time steps. the parameters of our model are all the weight matrices in the encoder and decoder networks. further details can be found in gehring2017icml. section: objective functions we compare several objective functions for training the model architecture described in\u00a7 [reference]. the corresponding loss functions are either computed over individual tokens (\u00a7 [reference]), over entire sequences (\u00a7 [reference]) or over a combination of tokens and sequences (\u00a7 [reference]). an overview of these loss functions is given in figure [reference]. subsection: token-level objectives most prior work on sequence to sequence learning has focused on optimizing token-level loss functions, i.e., functions for which the loss is computed additively over individual tokens. subsubsection: token negative log likelihood (toknll) token-level likelihood (toknll, equation [reference]) minimizes the negative log likelihood of individual reference tokens. it is the most common loss function optimized in related work and serves as a baseline for our comparison. subsubsection: token nll with label smoothing (tokls) likelihood training forces the model to make extreme zero or one predictions to distinguish between the ground truth and alternatives. this may result in a model that is too confident in its training predictions, which may hurt its generalization performance. label smoothing addresses this by acting as a regularizer that makes the model less confident in its predictions. specifically, we smooth the target distribution with a prior distribution that is independent of the current input szegedy2015inception, pereyra2017regularize, vaswani2017transformer. we use a uniform prior distribution over all words in the vocabulary,. one may also use a unigram distribution which has been shown to work better on some tasks pereyra2017regularize. label smoothing is equivalent to adding the kl divergence between and the model prediction to the negative log likelihood (tokls, equation [reference]). in practice, we implement label smoothing by modifying the ground truth distribution for word to be and for instead of and where is a smoothing parameter. subsection: sequence-level objectives we also consider a class of objective functions that are computed over entire sequences, i.e., sequence-level objectives. training with these objectives requires generating and scoring multiple candidate output sequences for each input sequence during training, which is computationally expensive but allows us to directly optimize task-specific metrics such as bleu or rouge. unfortunately, these objectives are also typically defined over the entire space of possible output sequences, which is intractable to enumerate or score with our models. instead, we compute our sequence losses over a subset of the output space,, generated by the model. we discuss approaches for generating this subset in\u00a7 [reference]. subsubsection: sequence negative log likelihood (seqnll) similar to toknll, we can minimize the negative log likelihood of an entire sequence rather than individual tokens (seqnll, equation [reference]). the log-likelihood of sequence is the sum of individual token log probabilities, normalized by the number of tokens to avoid bias towards shorter sequences: as target we choose a pseudo reference amongst the candidates which maximizes either bleu or rouge with respect to, the gold reference: as is common practice when computing bleu at the sentence-level, we smooth all initial counts to one (except for unigram counts) so that the geometric mean is not dominated by zero-valued-gram match counts lin2004orange. subsubsection: expected risk minimization (risk) this objective minimizes the expected value of a given cost function over the space of candidate sequences (risk, equation [reference]). in this work we use task-specific cost functions designed to maximize bleu or rouge lin2004rouge, e.g.,, for a given a candidate sequence and target. different to seqnll (\u00a7 [reference]), this loss may increase the score of several candidates that have low cost, instead of focusing on a single sequence which may only be marginally better than any alternatives. optimizing this loss is a particularly good strategy if the reference is not always reachable, although compared to classical phrase-based models, this is less of an issue with neural sequence to sequence models that predict individual words or even sub-word units. the risk objective is similar to the reinforce objective used in ranzato et al. ranzato2015sequence, since both objectives optimize an expected cost or reward williams1992reinforce. however, there are a few important differences: (1) whereas reinforce typically approximates the expectation with a single sampled sequence, the risk objective considers multiple sequences; (2) whereas reinforce relies on a baseline rewardranzato et al. estimate the baseline reward for reinforce with a separate linear regressor over the model's current hidden state. to determine the sign of the gradients for the current sequence, for the risk objective we instead estimate the expected cost over a set of candidate output sequences (see\u00a7 [reference]); and (3) while the baseline reward is different for every word in reinforce, the expected cost is the same for every word in risk minimization since it is computed on the sequence level based on the actual cost. subsubsection: max-margin maxmargin (equation [reference]) is a classical margin loss for structured prediction mmmn, structure_pred which enforces a margin between the model scores of the highest scoring candidate sequence and a reference sequence. we replace the human reference with a pseudo-reference since this setting performed slightly better in early experiments; is the candidate sequence with the highest bleu score. the size of the margin varies between samples and is given by the difference between the cost of and the cost of. in practice, we scale the margin by a hyper-parameter determined on the validation set:. for this loss we use the unnormalized scores computed by the model before the final softmax: subsubsection: multi-margin maxmargin only updates two elements in the candidate set. we therefore consider multimargin (equation [reference]) which enforces a margin between every candidate sequence and a reference sequence herbrich199icann, hence the name multi-margin. similar to maxmargin, we replace the reference with the pseudo-reference. subsubsection: softmax-margin finally, softmaxmargin (equation [reference]) is another classic loss that has been proposed by gimpel+ smith2010acl as another way to optimize task-specific costs. the loss augments the scores inside the of seqnll (equation [reference]) by a cost. the intuition is that we want to penalize high cost outputs proportional to their cost. subsection: combined objectives we also experiment with two variants of combining sequence-level objectives (\u00a7 [reference]) with token-level objectives (\u00a7 [reference]). first, we consider a weighted combination (weighted) of both a sequence-level and token-level objective wu2016google, e.g., for tokls and risk we have: where is a scaling constant that is tuned on a held-out validation set. second, we consider a constrained combination (constrained), where for any given input we use either the token-level or sequence-level loss, but not both. the motivation is to maintain good token-level accuracy while optimizing on the sequence-level. in particular, a sample is processed with the sequence loss if the token loss under the current model is at least as good as the token loss of a baseline model. otherwise, we update according to the token loss: in this work we use a fixed baseline model that was trained with a token-level loss to convergence. section: candidate generation strategies the sequence-level objectives we consider (\u00a7 [reference]) are defined over the entire space of possible output sequences, which is intractable to enumerate or score with our models. we therefore use a subset of candidate sequences, which we generate with our models. we consider two search strategies for generating the set of candidate sequences. the first is beam search, a greedy breadth-first search that maintains a\" beam\" of the top-scoring candidates at each generation step. beam search is the de facto decoding strategy for achieving state-of-the-art results in machine translation. the second strategy is sampling chatterjee, which produces independent output sequences by sampling from the model's conditional distribution. whereas beam search focuses on high probability candidates, sampling introduces more diverse candidates (see comparison in\u00a7 [reference]). we also consider both online and offline candidate generation settings in\u00a7 [reference]. in the online setting, we regenerate the candidate set every time we encounter an input sentence during training. in the offline setting, candidates are generated before training and are never regenerated. offline generation is also embarrassingly parallel because all samples use the same model. the disadvantage is that the candidates become stale. our model may perfectly be able to discriminate between them after only a single update, hindering the ability of the loss to correct eventual search errors. finally, while some past work has added the reference target to the candidate set, i.e.,, we find this can destabilize training since the model learns to assign low probabilities nearly everywhere, ruining the candidates generated by the model, while still assigning a slightly higher score to the reference (cf. shen2016mrt). accordingly we do not add the reference translation to our candidate sets. section: experimental setup subsection: translation we experiment on the iwslt'14 german to english cettolo2014report task using a similar setup as ranzato et al. ranzato2015sequence, which allows us to compare to other recent studies that also adopted this setup, e.g., wiseman2016acl. the training data consists of 160 k sentence pairs and the validation set comprises 7 k sentences randomly sampled and held-out from the train data. we test on the concatenation of all available test and dev sets of iwslt 2014, that is ted.tst2010, ted.tst2011, ted.tst2012 and ted.dev2010, tedx.dev2012 which is of similar size to the validation set. all data is lowercased and tokenized with a byte-pair encoding (bpe) of 14, 000 types sennrich2016bpe and we evaluate with case-insensitive bleu. we also experiment on the much larger wmt'14 english-french task. we remove sentences longer than 175 words as well as pairs with a source/ target length ratio exceeding 1.5 resulting in 35.5 m sentence-pairs for training. the source and target vocabulary is based on 40 k bpe types. results are reported on both newstest2014 and a validation set held-out from the training data comprising 26, 658 sentence pairs. we modify the fairseq-py toolkit to implement the objectives described in\u00a7 [reference]. our translation models have four convolutional encoder layers and three convolutional decoder layers with a kernel width of 3 and 256 dimensional hidden states and word embeddings. we optimize these models using nesterov's accelerated gradient method sutskever2013icml with a learning rate of 0.25 and momentum of 0.99. gradient vectors are renormalized to norm 0.1 pascanu2013difficulty. we train our baseline token-level models for 200 epochs and then anneal the learning by shrinking it by a factor of 10 after each subsequent epoch until the learning rate falls below. all sequence-level models are initialized with parameters of a token-level model before annealing. we then train sequence-level models for another 10 to 20 epochs depending on the objective. our batches contain 8 k tokens and we normalize gradients by the number of non-padding tokens per mini-batch. we use weight normalization for all layers except for lookup tables salimans2016weight. besides dropout on the embeddings and the decoder output, we also apply dropout to the input of the convolutional blocks at a rate of 0.3 srivastava2014dropout. we tuned the various parameters above and report accuracy on the test set by choosing the best configuration based on the validation set. we length normalize all scores and probabilities in the sequence-level losses by dividing by the number of tokens in the sequence so that scores are comparable between different lengths. additionally, when generating candidate output sequences during training we limit the output sequence length to be less than 200 tokens for efficiency. we generally use 16 candidate sequences per training example, except for the ablations where we use 5 for faster experimental turnaround. subsection: abstractive summarization for summarization we use the gigaword corpus as training data and pre-process it identically to rush2015abs resulting in 3.8 m training and 190 k validation examples. we evaluate on a gigaword test set of 2, 000 pairs identical to the one used by rush2015abs and report f1 rouge similar to prior work. our results are in terms of three variants of rouge, namely, rouge-1 (rg-1, unigrams), rouge-2 (rg-2, bigrams), and rouge-l (rg-l, longest-common substring). similar to ayana2016neural we use a source and target vocabulary of 30k words. our models for this task have 12 layers in the encoder and decoder each with 256 hidden units and kernel width 3. we train on batches of 8, 000 tokens with a learning rate of 0.25 for 20 epochs and then anneal as in\u00a7 [reference]. section: results subsection: comparison of sequence level losses first, we compare all objectives based on a weighted combination with token-level label smoothing (equation [reference]). we also show the likelihood baseline (mle) of wiseman2016acl, their beam search optimization method (bso), the actor critic result of bahdanau2016ac as well as the best reported result on this dataset to date by huang2017npbmt. we show a like-for-like comparison to wiseman2016acl with a similar baseline model below (\u00a7 [reference]). table [reference] shows that all sequence-level losses outperform token-level losses. our baseline token-level results are several points above other figures in the literature and we further improve these results by up to 0.61 bleu with risk training. subsection: combination with token-level loss next, we compare various strategies to combine sequence-level and token-level objectives (cf.\u00a7 [reference]). for these experiments we use 5 candidate sequences per training example for faster experimental turnaround. we consider risk as sequence-level loss and label smoothing as token-level loss. table [reference] shows that combined objectives perform better than pure risk. the weighted combination (equation [reference]) with performs best, outperforming constrained combination (equation [reference]). we also compare to randomly choosing between token-level and sequence-level updates and find it underperforms the more principled constrained strategy. in the remaining experiments we use the weighted strategy. subsection: effect of initialization so far we initialized sequence-level models with parameters from a token-level model trained with label smoothing. table [reference] shows that initializing weighted risk with token-level label smoothing achieves 0.7-0.8 better bleu compared to initializing with parameters from token-level likelihood. the improvement of initializing with toknll is only 0.3 bleu with respect to the toknll baseline, whereas, the improvement from initializing with tokls is 0.6-0.8 bleu. we believe that the regularization provided by label smoothing leads to models with less sharp distributions that are a better starting point for sequence-level training. subsection: online vs. offline candidate generation next, we consider the question if refreshing the candidate subset at every training step (online) results in better accuracy compared to generating candidates before training and keeping the set static throughout training (offline). table [reference] shows that offline generation gives lower accuracy. however the online setting is much slower, since regenerating the candidate set requires incremental (left to right) inference with our model which is very slow compared to efficient forward/ backward over large batches of pre-generated hypothesis. in our setting, offline generation has 26 times higher throughput than the online generation setting, despite the high inference speed of fairseq gehring2017icml. subsection: beam search vs. sampling and candidate set size so far we generated candidates with beam search, however, we may also sample to obtain a more diverse set of candidates shen2016mrt. figure [reference] compares beam search and sampling for various candidate set sizes on the validation set. beam search performs better for all candidate set sizes considered. in other experiments, we rely on a candidate set size of 16 which strikes a good balance between efficiency and accuracy. subsection: comparison to beam-search optimization next, we compare classical sequence-level training to the recently proposed beam search optimization wiseman2016acl. to enable a fair comparison, we re-implement their baseline, a single layer lstm encoder/ decoder model with 256-dimensional hidden layers and word embeddings as well as attention and input feeding luong2015effective. this baseline is trained with adagrad duchi2011adaptive using a learning rate of for five epochs, with batches of 64 sequences. for sequence-level training we initialize weights with the baseline parameters and train with adam kingma2014adam for another 10 epochs with learning rate and 16 candidate sequences per training example. we conduct experiments with risk since it performed best in trial experiments. different from other sequence-level experiments (\u00a7 [reference]), we rescale the bleu scores in each candidate set by the difference between the maximum and minimum scores of each sentence. this avoids short sentences dominating the sequence updates, since candidate sets for short sentences have a wider range of bleu scores compared to longer sentences; a similar rescaling was used by bahdanau2016ac. table [reference] shows the results from wiseman2016acl for their token-level likelihood baseline (mle), best beam search optimization results (bso), as well as our reimplemented baseline. risk significantly improves bleu compared to our baseline at+ 2.75 bleu, which is slightly better than the+ 2.33 bleu improvement reported for beam search optimization (cf. wiseman2016acl). this shows that classical objectives for structured prediction are still very competitive. subsection: wmt'14 english-french results next, we experiment on the much larger wmt'14 english-french task using the same model setup as gehring2017icml. we tokls for 15 epochs and then switch to sequence-level training for another epoch. table [reference] shows that sequence-level training can improve an already very strong model by another+ 0.37 bleu. next, we improve the baseline by adding self-attention paulus2017summary, vaswani2017transformer to the decoder network (tokls+ selfatt) which results in a smaller gain of+ 0.2 bleu by risk. if we train risk only on the news-commentary portion of the training data, then we achieve state of the art accuracy on this dataset of 41.5 bleu yingce2017deliberation. subsection: abstractive summarization our final experiment evaluates sequence-level training on gigaword headline summarization. there has been much prior art on this dataset originally introduced by rush2015abs who experiment with a feed-forward network (abs+). ayana2016neural report a likelihood baseline (rnn mle) and also experiment with risk training (rnn mrt). different to their setup we did not find a softmax temperature to be beneficial, and we use beam search instead of sampling to obtain the candidate set (cf.\u00a7 [reference]). suzuki2017cutting improve over an mle rnn baseline by limiting generation of repeated phrases. zhou2017seass also consider an mle rnn baseline and add an additional gating mechanism for the encoder. li2017drgd equip the decoder of a similar network with additional latent variables to accommodate the uncertainty of this task. table [reference] shows that our baseline (tokls) outperforms all prior approaches in terms of rouge-2 and rouge-l and it is on par to the best previous result for rouge-1. we optimize all three rouge metrics separately and find that risk can further improve our strong baseline. we also compared risk only training to weighted on this dataset (cf.\u00a7 [reference]) but accuracy was generally lower on the validation set: rg-1 (36.59 risk only vs. 36.67 weighted), rg-2 (17.34 vs. 18.05), and rg-l (33.66 vs. 33.98). section: conclusion we present a comprehensive comparison of classical losses for structured prediction and apply them to a strong neural sequence to sequence model. we found that combining sequence-level and token-level losses is necessary to perform best, and so is training on candidates decoded with the current model. we show that sequence-level training improves state-of-the-art baselines both for iwslt'14 german-english translation and gigaword abstractive sentence summarization. structured prediction losses are very competitive to recent work on reinforcement or beam optimization. classical expected risk can slightly outperform beam search optimization wiseman2016acl in a like-for-like setup. future work may investigate better use of already generated candidates since invoking generation for each batch slows down training by a large factor, e.g., mixing with fresh and older candidates inspired by mert. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "iwslt'14 german-english translation",
                        644
                    ],
                    [
                        "iwslt'14 german-english translation task",
                        2347
                    ],
                    [
                        "iwslt'14 german to english",
                        15565
                    ],
                    [
                        "iwslt 2014",
                        15995
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "bleu",
                        806
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "machine translation",
                        1597
                    ],
                    [
                        "translation task",
                        2791
                    ],
                    [
                        "translation",
                        15532
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "40a63746a710baf4a694fd5a4dd8b5a3d9fc2846-61",
    "doctext": "document: invertible conditional gans for image editing generative adversarial networks (gans) have recently demonstrated to successfully approximate complex data distributions. a relevant extension of this model is conditional gans (cgans), where the introduction of external information allows to determine specific representations of the generated images. in this work, we evaluate encoders to inverse the mapping of a cgan, i.e., mapping a real image into a latent space and a conditional representation. this allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. additionally, we evaluate the design of cgans. the combination of an encoder with a cgan, which we call invertible cgan (icgan), enables to re-generate real images with deterministic complex modifications. section: introduction image editing can be performed at different levels of complexity and abstraction. common operations consist in simply applying a filter to an image to, for example, augment the contrast or convert to grayscale. these, however, are low-complex operations that do not necessarily require to comprehend the scene or object that the image is representing. on the other hand, if one would want to modify the attributes of a face (e.g. add a smile, change the hair color or even the gender), this is a more complex and challenging modification to perform. in this case, in order to obtain realistic results, a skilled human with an image edition software would often be required. a solution to automatically perform these non-trivial operations relies on generative models. natural image generation has been a strong research topic for many years, but it has not been until 2015 that promising results have been achieved with deep learning techniques combined with generative modeling gregor2015, radford2015. generative adversarial networks (gans) is one of the state-of-the-art approaches for image generation. gans are especially interesting as they are directly optimized towards generating the most plausible and realistic data, as opposed to other models (e.g. variational autoencoders), which focus on an image reconstruction loss. additionally, gans are able to explicitly control generated images features with a conditional extension, conditional gans (cgans). however, the gan framework lacks an inference mechanism, i.e., finding the latent representation of an input image, which is a necessary step for being able to reconstruct and modify real images. in order to overcome this limitation, in this paper we introduce invertible conditional gans (icgans) for complex image editing as the union of an encoder used jointly with a cgan. this model allows to map real images into a high-feature space (encoder) and perform meaningful modifications on them (cgan). as a result, we can explicitly control the attributes of a real image (figure [reference]), which could be potentially useful in several applications, be it creative processes, data augmentation or face profiling. [width=0.85] images/ 1.intro/ pull_figure the summary of contributions of our work is the following: proposing icgans, composed of two crucial parts: an encoder and a cgan. we apply this model to mnist lecun1998 and celeba celeba datasets, which allows performing meaningful and realistic editing operations on them by arbitrarily changing the conditional information. introducing an encoder in the conditional gan framework to compress a real image into a latent representation and conditional vector. we consider several designs and training procedures to leverage the performance obtained from available conditional information. evaluating and refining cgans through conditional position and conditional sampling to enhance the quality of generated images. section: related work there are different approaches for generative models. among them, there are two promising ones that are recently pushing the state-of-the-art with highly plausible generated images. the first one is variational autoencoders (vae) gregor2015, kingma2013, rezende2014, kingma2014, which impose a prior representation space (e.g. normal distribution) in order to regularize and constrain the model to sample from it. however, vaes main limitation is the pixel-wise reconstruction error used as a loss function, which causes the output images to look blurry. the second approach is generative adversarial nets (gans). originally proposed by goodfellow et al. goodfellow2014, gans have been improved with a deeper architecture (dcgan) by radford et al. radford2015. the latest advances introduced several techniques that improve the overall performance for training gans salimans2016 and an unsupervised approach to disentangle feature representations chen2016. additionally, the most advanced and recent work on cgans trains a model to generate realistic images from text descriptions and landmarks. our work is considered in the content of the gan framework. the baseline will be the work of radford's et al. (dcgans) radford2015, which we will add a conditional extension. the difference of our approach to prior work is that we also propose an encoder (invertible cgan) with which we can, given an input image, to obtain its representation as a latent variable and a conditional vector. then, we can modify and to re-generate the original image with complex variations. dumoulin et al. dumoulin2016 and donahue et al. donahue2016 also proposed an encoder in gans, but in a non-conditional and jointly trained setting. additionally, makhzani et al. makhzani2015 and larsen et al. larsen2015 proposed a similar idea to this paper by combining a vae and a gan with promising results. reed et al. reed2016 implemented an encoder in a similar fashion to our approach. this paper builds alongside their work in a complementary manner. in our case, we analyze more deeply the encoder by including conditional information encoding and testing different architectures and training approaches. also, we evaluate unexplored design decisions for building a cgan. section: background: generative adversarial networks a gan is composed of two neural networks, a generator and a discriminator. both networks are iteratively trained competing against each other in a minimax game. the generator aims to approximate the underlying unknown data distribution to fool the discriminator, whilst the discriminator is focused on being able to tell which samples are real or generated. on convergence, we want, where is the generator distribution. more formally, considering the function, where and are the parameters of the generator and discriminator respectively, we can formulate gan training as optimizing where is a vector noise sampled from a known simple distribution (e.g. normal). gan framework can be extended with conditional gans (cgans) mirza2014. they are quite similar to vanilla (non-conditional) gans, the only difference is that, in this case, we have extra information (e.g. class labels, attribute information) for a given real sample. conditional information strictly depends on real samples, but we can model a density model in order to sample generated labels for generated data. then, equation [reference] can be reformulated for the cgan extension as once a cgan is trained, it allows us to generate samples using two level of variations: constrained and unconstrained. constrained variations are modeled with as it directly correlates with features of the data that are explicitly correlated with and the data itself. then, all the other variations of the data not modeled by (unconstrained variations) are encoded in. section: invertible conditional gans we introduce invertible conditional gans (icgans), which are composed of a cgan and an encoder. even though encoders have recently been introduced into the gan framework, we are the first ones to include and leverage the conditional information into the design of the encoding process. in section [reference] we explain how and why an encoder is included in the gan framework for a conditional setting. in section [reference], we introduce our approach to refine cgans on two aspects: conditional position and conditional sampling. the model architecture is described in section [reference]. subsection: encoder a generator from a gan framework does not have the capability to map a real image to its latent representation. to overcome this problem, we can train an encoder/ inference network that approximately inverses this mapping. this inversion would allow us to have a latent representation from a real image and, then, we would be able to explore the latent space by interpolating or adding variations on it, which would result in variations on the generated image. if combined with a cgan, once the latent representation has been obtained, explicitly controlled variations can be added to an input image via conditional information (e.g. generate a certain digit in mnist or specify face attributes on a face dataset). we call this combination invertible cgan, as now the mapping can be inverted: and, where is an input image and its reconstruction. see figure [reference] for an example on how a trained icgan is used. [width=0.92] images/ 4.icgans/ icgan_overview our approach consists of training an encoder once the cgan has been trained, as similarly considered by reed et al. in our case, however, the encoder is composed of two sub-encoders:, which encodes an image to, and, which encodes an image to. to train we use the generator to create a dataset of generated images and their latent vectors, and then minimize a squared reconstruction loss (eq. [reference]). for, we initially used generated images and their conditional information for training. however, we found that generated images tend to be noisier than real ones and, in this specific case, we could improve by directly training with real images and labels from the dataset (eq. [reference]). although and might seem completely independent, we can adopt different strategies to make them interact and leverage the conditional information (for an evaluation of them, see section [reference]): sng: one single encoder with shared layers and two outputs. that is, and are embedded in a single encoder. ind: two independent encoders. and are trained separately. ind-cond: two encoders, where is conditioned on the output of encoder. recently, dumoulin et al. dumoulin2016 and donahue et al. donahue2016 proposed different approaches on how to train an encoder in the gan framework. one of the most interesting approaches consists in jointly training the encoder with both the discriminator and the generator. although this approach is promising, our work has been completely independent of these articles and focuses on another direction, since we consider the encoder in a conditional setting. consequently, we implemented our aforementioned approach which performs nearly equally donahue2016 to their strategy. subsection: conditional gan we consider two main design decisions concerning cgans. the first one is to find the optimal conditional position on the generator and discriminator, which, to our knowledge, has not been previously addressed. secondly, we discuss the best approach to sample conditional information for the generator. conditional position in the cgan, the conditional information vector needs to be introduced in both the generator and the discriminator. in the generator, and (where are always concatenated in the filter dimension at the input level reed2016, mirza2014, gauthier2014. as for the discriminator, different authors insert in different parts of the model reed2016, mirza2014, gauthier2014. we expect that the earlier is positioned in the model the better since the model is allowed to have more learning interactions with. experiments regarding the optimal position will be detailed in section [reference]. conditional sampling there are two types of conditional information, and. the first one is trivially sampled from and is used for training the discriminator with a real image and its associated label. the second one is sampled from and serves as input to the generator along with a latent vector to generate an image, and it can be sampled using different approaches: kernel density estimation: also known as parzen window estimation, it consists in randomly sampling from a kernel (e.g. gaussian kernel with a cross-validated). direct interpolation: interpolate between label vectors from the training set reed2016. the reasoning behind this approach is that interpolations can belong to the label distribution. sampling from the training set,: use directly the real labels from the training set. as gauthier gauthier2014 pointed out, unlike the previous two approaches, this method could overfit the model by using the conditional information to reproduce the images of the training set. however, this is only likely to occur if the conditional information is, to some extent, unique for each image. in the case where the attributes of an image are binary, one attribute vector could describe a varied and large enough subset of images, preventing the model from overfitting given. kernel density estimation and direct interpolation are, at the end, two different ways to interpolate on. nevertheless, interpolation is mostly suitable when the attribute information is composed of real vectors, not binary ones. it is not the case of the binary conditional information of the datasets used in this paper (see section [reference] for dataset information). directly interpolating binary vectors would not create plausible conditional information, as an interpolated vector would not belong to nor. using a kernel density estimation would not make sense either, as all the binary labels would fall in the corners of a hypercube. therefore, we will directly sample from. subsection: model architecture conditional gan the work of this paper is based on the torch implementation of the dcgan. we use the recommended configuration for the dcgan, which trains with the adam optimizer adam2014 () with a learning rate of and a mini-batch size of (samples drawn independently at each update step) during epochs. the output image size used as a baseline is. also, we train the cgan with the matching-aware discriminator method from reed et al.. in figure [reference] we show an overview architecture of both generator and discriminator for the cgan. for a more detailed description of the model see table [reference]. [] [width=0.50] images/ 5.impl/ cgan_g_arch [] [width=0.475] images/ 5.impl/ cgan_d_arch width=1.0 encoder for simplicity, we show the architecture of the ind encoders (table [reference]), as they are the ones that give the best performance. batch normalization and non-linear activation functions are removed from the last layer to guarantee that the output distribution is similar to. additionally, after trying different configurations, we have replaced the last two convolutional layers with two fully connected layers at the end of the encoder, which yields a lower error. the training configuration (adam optimizer, batch size, etc) is the same as the one used for the cgan model. width=0.6 section: experiments subsection: datasets we use two image datasets of different complexity and variation, mnist lecun1998 and celebfaces attributes (celeba) celeba. mnist is a digit dataset of grayscale images composed of 60, 000 training images and 10, 000 test images. each sample is a centered image labeled with the class of the digit (0 to 9). celeba is a dataset composed of 202, 599 face colored images and 40 attribute binary vectors. we use the aligned and cropped version and scale the images down to. we also use the official train and test partitions, 182 k for training and 20 k for testing. of the original 40 attributes, we filter those that do not have a clear visual impact on the generated images, which leaves a total of 18 attributes. we will evaluate the quality of generated samples of both datasets. however, a quantitative evaluation will be performed on celeba only, as it is considerably more complex than mnist. subsection: evaluating the conditional gan the goals of this experiment are two. first, we evaluate the general performance of the cgan with an attribute predictor network (anet) on celeba dataset. second, we test the impact of adding in different layers of the cgan (section [reference], conditional position). we use an anet as a way to make a quantitative evaluation in a similar manner as salimans et al. inception model, as the output given by this anet (i.e., which attributes are detected on a generated sample) is a good indicator of the generator ability to model them. in other words, if the predicted anet attributes are closer to the original attributes used to generate an image, we expect that the generator has successfully learned the capability to generate new images considering the semantic meaning of the attributes. therefore, we use the generator to create images conditioned on attribute vectors (i.e.), and make the anet predict them. using the anet output, we build a confusion matrix for each attribute and compute the mean accuracy and f1-score to test the model and its inserted optimal position of in both generator and discriminator. width=0.80 in table [reference] we can see how cgans have successfully learned to generate the visual representations of the conditional attributes with an overall accuracy of%. the best accuracy is achieved by inserting in the first convolutional layer of the discriminator and at the input level for the generator. thus, we are going to use this configuration for the icgan. both accuracy and f1-score are similar as long as is not inserted in the last convolutional layers, in which case the performance considerably drops, especially in the generator. then, these results reinforce our initial intuition of being added at an early stage of the model to allow learning interactions with it. subsection: evaluating the encoder in this experiment, we prioritize the visual quality of reconstructed samples as an evaluation criterion. among the different encoder configurations of section [reference], ind and ind-cond yield a similar qualitative performance, being ind slightly superior. a comparison of these different configurations is shown in figure [reference] a and in figure [reference] b we focus on ind reconstructed samples. on another level, the fact that the generator is able, via an encoder, to reconstruct unseen images from the test set shows that the cgan is generalizing and suggests that it does not suffer from overfitting, i.e., it is not just memorizing and reproducing training samples. additionally, we compare the different encoder configurations in a quantitative manner by using the minimal squared reconstruction loss as a criterion. each encoder is trained minimizing with respect to latent representations () or conditional information (). then, we quantitatively evaluate different model architectures using as a metric on a test set of 150 k celeba generated images. we find that the encoder that yields the lowest is also ind (0.429), followed closely by ind-cnd (0.432), and being sng the worst case (0.500). furthermore, we can see an interesting property of minimizing a loss based on the latent space instead of a pixel-wise image reconstruction: reconstructed images tend to accurately keep high-level features of an input image (e.g. how a face generally looks) in detriment to more local details such as the exact position of the hair, eyes or face. consequently, a latent space based encoder is invariant to these local details, making it an interesting approach for encoding purposes. for example, notice how the reconstructions in the last row of celeba samples in figure [reference] b fill the occluded part of the face by a hand. another advantage with respect to element-wise encoders such as vae is that gan based reconstructions do not look blurry. [] [width=0.3255] images/ 6.exp/ encoders_comparison [] [width=0.525] images/ 6.exp/ subsection: evaluating the icgan in order to test that the model is able to correctly encode and re-generate a real image by preserving its main attributes, we take real samples from mnist and celeba test sets and reconstruct them with modifications on the conditional information. the result of this procedure is shown in figure [reference], where we show a subset of 9 of the 18 for celeba attributes for image clarity. we can see that, in mnist, we are able to get the hand-written style of real unseen digits and replicate these style on all the other digits. on the other hand, in celeba we can see how reconstructed faces generally match the specified attribute. additionally, we noticed that faces with uncommon conditions (e.g., looking away from the camera, face not centered) were the most likely to be noisy. furthermore, attributes such as mustache often fail to be generated especially on women samples, which might indicate that the generator is limited to some unusual attribute combinations. [] [width=0.95] images/ 6.exp/ icgan_mnist_final_small [] [width=0.95] images/ 6.exp/ manipulating the latent space the latent feature representation and conditional information learned by the generator can be further explored beyond encoding real images or randomly sampling. in order to do so, we linearly interpolate both and with pairs of reconstructed images from the celeba test set (figure [reference] a). all the interpolated faces are plausible and the transition between faces is smooth, demonstrating that the icgan learned manifold is also consistent between interpolations. then, this is also a good indicator that the model is generalizing the face representation properly, as it is not directly memorizing training samples. in addition, we perform in figure [reference] b an attribute transfer between pairs of faces. we infer the latent representation and attribute information of two real faces from the test set, swap between those faces and re-generate them. as we previously noticed, the results suggest that encodes pose, illumination and background information, while tends to represent unique features of the face. [] [width=0.555] images/ 6.exp/ icgan_interpolations [] [width=0.44] images/ 6.exp/ icgan_attributetransfer section: conclusions we introduce an encoder in a conditional setting within the gan framework, a model which we call invertible conditional gans (icgans). it solves the problem of gans lacking the ability to infer real samples to a latent representation, while also allowing to explicitly control complex attributes of generated samples with conditional information. we also refine the performance of cgans by testing the optimal position in which the conditional information is inserted in the model. we have found that for the generator, should be added at the input level, whereas the discriminator works best when is at the first layer. additionally, we evaluate several ways to training an encoder. training two independent encoders- one for encoding and another for encoding- has proven to be the best option in our experiments. the results obtained with a complex face dataset, celeba, are satisfactory and promising. acknowledgments this work is funded by the projects tin2013-41751-p of the spanish ministry of science and the chist era project pcin-2015-226. bibliography: references",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "invertible conditional gans",
                        10
                    ],
                    [
                        "invertible cgan",
                        724
                    ],
                    [
                        "icgan",
                        742
                    ],
                    [
                        "icgans",
                        2604
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "pixel-wise reconstruction error",
                        4263
                    ]
                ]
            ],
            "Task": []
        }
    ]
}
{
    "docid": "4265269bc894caa97efbfcfe5b83da7413f86a30-62",
    "doctext": "asymmetric tri-training for unsupervised domain adaptation section: abstract deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. it is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. in unsupervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. although many methods aim to match the distributions of source and target samples, simply matching the distribution can not ensure accuracy on the target domain. to learn discriminative representations for the target domain, we assume that artificially labeling target samples can result in a good representation. tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different domain. in this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. in our work, we use three networks asymmetrically. by asymmetric, we mean that two networks are used to label unlabeled target samples and one network is trained by the samples to obtain targetdiscriminative representations. we evaluate our method on digit recognition and sentiment analysis datasets. our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation. section: introduction with the development of deep neural networks including deep convolutional neural networks (cnn) [reference] the university of tokyo, tokyo, japan. correspondence to: kuniaki saito< k-saito@mi.t.u-tokyo.ac.jp>, yoshitaka ushiku< ushiku@mi.t.u-tokyo.ac.jp>, tatsuya harada< harada@mi.t.u-tokyo.ac.jp>. [reference], the recognition abilities of images and languages have improved dramatically. training deep-layered networks with a large number of labeled samples enables us to correctly categorize samples in diverse domains. in addition, the transfer learning of cnn is utilized in many studies. for object detection or segmentation, we can transfer the knowledge of a cnn trained with a large-scale dataset by fine-tuning it on a relatively small dataset [reference][reference]. moreover, features from a cnn trained on imagenet [reference]) are useful for multimodal learning tasks including image captioning [reference] and visual question answering [reference]. one of the problems of neural networks is that although they perform well on the samples generated from the same distribution as the training samples, they may find it difficult to correctly recognize samples from different distributions at the test time. one example is images collected from the internet, which may come in abundance and be fully labeled. they have a distribution different from the images taken from a camera. thus, a classifier that performs well on various domains is important for practical use. to realize this, it is necessary to learn domain-invariantly discriminative representations. however, acquiring such representations is not easy because it is often difficult to collect a large number of labeled samples and because samples from different domains have domain-specific characteristics. in unsupervised domain adaptation, we try to train a classifier that works well on a target domain on the condition that we are provided labeled source samples and unlabeled target samples during training. most of the previous deep domain adaptation methods have been proposed mainly under the assumption that the adaptation can be realized by matching the distribution of features from different domains. these methods aimed to obtain domain-invariant features by minimizing the divergence between domains as well as a category loss on the source domain [reference][reference]. however, as shown in [reference], theoretically, if a classifier that works well on both the source and the target domains does not exist, we can not expect a discriminative classifier for the target domain. that is, even if the distributions are matched on the nondiscriminative representations, the classifier may not work:$' 1.0;\"# -'\"'./ 7# (8'7/$#34\"'$* figure 1. outline of our model. we assign pseudo-labels to unlabeled target samples based on the predictions from two classifiers trained on source samples. well on the target domain. since directly learning discriminative representations for the target domain, in the absence of target labels, is considered very difficult, we propose to assign pseudo-labels to target samples and train targetspecific networks as if they were true labels. co-training and tri-training [reference] leverage multiple classifiers to artificially label unlabeled samples and retrain the classifiers. however, the methods do not assume labeling samples from different domains. since our goal is to classify unlabeled target samples that have different characteristics from labeled source samples, we propose asymmetric tri-training for unsupervised domain adaptation. by asymmetric, we mean that we assign different roles to three classifiers. in this paper, we propose a novel tri-training method for unsupervised domain adaptation, where we assign pseudolabels to unlabeled samples and train neural networks utilizing the samples. as described in fig. 1, two networks are used to label unlabeled target samples and the remaining network is trained by the pseudo-labeled target samples. our method does not need any special implementations. we evaluate our method on the digit classification task, traffic sign classification task and sentiment analysis task using the amazon review dataset, and demonstrate state-of-the-art performance in nearly all experiments. in particular, in the adaptation scenario, mnist\u2192svhn, our method outperformed other methods by more than 10%. section: related work as many methods have been proposed to tackle various tasks in domain adaptation, we present details of the research most closely related to our paper. a number of previous methods attempted to realize adaptation by utilizing the measurement of divergence between different domains [reference][reference][reference]. the methods are based on the theory proposed in [reference], which states that the expected loss for a target domain is bounded by three terms: (i) expected loss for the source domain; (ii) domain divergence between source and target; and (iii) the minimum value of a shared expected loss. the shared expected loss means the sum of the loss on the source and target domain. as the third term, which is usually considered to be very low, can not be evaluated when labeled target samples are absent, most methods try to minimize the first term and the second term. with regards to training deep architectures, the maximum mean discrepancy (mmd) or a loss of domain classifier network is utilized to measure the divergence corresponding to the second term [reference][reference][reference][reference]. however, the third term is very important in training cnn, which simultaneously extract representations and recognize them. the third term can easily be large when the representations are not discriminative for the target domain. therefore, we focus on how to learn target-discriminative representations considering the third term. in [reference]) the focus was on the point we have stated and a target-specific classifier was constructed using a residual network structure. different from their method, we constructed a target-specific network by providing artificially labeled target samples. several transductive methods use similarity of features to provide labels for unlabeled samples [reference][reference]. for unsupervised domain adaptation, in [reference], a method was proposed to learn labeling metrics by using the k-nearest neighbors between unlabeled target samples and labeled source samples. in contrast to this method, our method explicitly and simply backpropagates the category loss for target samples based on pseudo-labeled samples. our approach does not require any special modules. many methods proposed to give pseudo-labels to unlabeled samples by utilizing the predictions of a classifier and retraining it including the pseudo-labeled samples, which is called self-training. the underlying assumption of selftraining is that one's own high-confidence predictions are correct [reference]. as the predictions are mostly correct, utilizing samples with high confidence will further improve the performance of the classifier. co-training utilizes two classifiers, which have different views on one sample, to provide pseudo-labels [reference][reference]. then, the unlabeled samples are added to training set if at least one classifier is confident about the predictions. the generalization ability of co-training is theoretically ensured [reference][reference]) under some assumptions and applied to various tasks [reference][reference]. in [reference], the idea of co-training was incorporated into domain adaptation. tri-training can be regarded as the extension of co-training [reference]. similar to co-training, tritraining uses the output of three different classifiers to give pseudo-labels to unlabeled samples. tri-training does not require partitioning features into different views; instead, tri-training initializes each classifier differently. however, figure 2. the proposed method includes a shared feature extractor (f), classifiers for labeled samples (f1, f2), which learn from labeled source samples, and newly labeled target samples. in addition, a target-specific classifier (ft) learns from pseudo-labeled target samples. our method first trains networks from only labeled source samples, then labels the target samples based on the output of f1, f2. we train all architectures using them as if they are correctly labeled samples. tri-training does not assume that the unlabeled samples follow the different distributions from the ones which labeled samples are generated from. therefore, we develop a tritraining method suitable for domain adaptation by using three classifiers asymmetrically. in [reference], the effect of pseudo-labels in a neural network was investigated. they argued that the effect of training a classifier with pseudo-labels is equivalent to entropy regularization, thus leading to a low-density separation between classes. in addition, in our experiment, we observe that target samples are separated in hidden features. section: method in this section, we provide details of the proposed model for domain adaptation. we aim to construct a targetspecific network by utilizing pseudo-labeled target samples. simultaneously, we expect two labeling networks to acquire target-discriminative representations and gradually increase accuracy on the target domain. we show our proposed network structure in fig. 2. here f denotes the network which outputs shared features among three networks, f 1 and f 2 classify features generated from f. their predictions are utilized to give pseudo-labels. the classifier f t classifies features generated from f, which is a target-specific network. here f 1, f 2 learn from source and pseudo-labeled target samples and f t learns only from pseudo-labeled target samples. the shared network f learns from all gradients from f 1, f 2, f t. without such a shared network, another option for the network architecture we can think of is training three networks separately, but this is inefficient in terms of training and implementation. furthermore, by building a shared network f, f 1 and f 2 can also harness the target-discriminative representations learned by the feedback from f t. the set of source samples is defined as (x i, y i)\u223c t, and the pseudolabeled target set is section: loss for multiview features network in the existing works [reference] on co-training for domain adaptation, given features are divided into separate parts and considered to be different views. as we aim to label target samples with high accuracy, we expect f 1, f 2 to classify samples based on different viewpoints. therefore, we make a constraint for the weight of f 1, f 2 to make their inputs different to each other. we add the term|w 1 t w 2| to the cost function, where w 1, w 2 denote fully connected layers' weights of f 1 and f 2 which are first applied to the feature f (x i). each network will learn from different features with this constraint. the objective for learning f 1, f 2 is defined as where l y denotes the standard softmax cross-entropy loss function. we decided the trade-off parameter\u03bb based on validation split. section: learning procedure and labeling method pseudo-labeled target samples will provide targetdiscriminative information to the network. however, since they certainly contain false labels, we have to pick up reliable pseudo-labels. our labeling and learning method is aimed at realizing this. the entire procedure of training the network is shown in algorithm 1. first, we train the entire network with source training set s. here f 1, f 2 are optimized by eq. (1) and f t is trained on standard category loss. after training on s, to provide pseudo-labels, we use predictions of f 1 and f 2, namely y 1, y 2 obtained from x k. when c 1, c 2 denote the class which has the maximum predicted probability for y 1, y 2, we assign a pseudo-label to x k if the following two conditions are satisfied. first, we require c 1= c 2 to give pseudo-labels, which means two different classifiers agree with the prediction. the second requirement is that the maximizing probability of y 1 or y 2 exceeds the threshold parameter, which we set as 0.9 or 0.95 in the experiment. we suppose that unless one of two classifiers is confident of the prediction, the prediction is not reliable. if the two requirements are satisfied, x k,\u0177 k= c 1= c 2 is added to t l. to prevent the overfitting to pseudo-labels, we resample the candidate for labeling samples in each step. we set the algorithm 1 iter denotes the iteration of training. the function labeling means the method of labeling. we assign pseudo-labels to samples when the predictions of f 1 and f 2 agree and at least one of them is confident of their predictions. number of the initial candidates n init as 5, 000. we gradually increase the number of the candidates n t= k/ 20* n, where n denotes the number of all target samples and k denotes the number of steps, and we set the maximum number of pseudo-labeled candidates as 40, 000. after the pseudo-labeled training set t l is composed, f, f 1, f 2 are updated by the objective eq. (1) on the labeled training set l= s\u222a t l. then, f, f t are simply optimized by the category loss for t l. discriminative representations will be learned by constructing a target-specific network trained only on target samples. however, if only noisy pseudo-labeled samples are used for training, the network may not learn useful representations. then, we use both source samples and pseudo-labeled samples for training f, f 1, f 2 to ensure the accuracy. also, as the learning proceeds, f will learn target-discriminative representations, resulting in an improvement in accuracy in f 1, f 2. this cycle will gradually enhance the accuracy in the target domain. section: batch normalization for domain adaptation batch normalization (bn) [reference], which whitens the output of the hidden layer in a cnn, is an effective technique to accelerate training speed and enhance the accuracy of the model. in addition, in domain adaptation, whitening the hidden layer's output is effective for improving the performance, which make the distribution in different domains similar [reference][reference]. input samples of f 1, f 2 include both pseudo-labeled target samples and source samples. introducing bn will be useful for matching the distribution and improves the performance. we add the bn layer in the last layer in f. section: analysis in this section, we provide a theoretical analysis to our approach. first, we provide an insight into existing theory, then we introduce a simple expansion of the theory related to our method. in [reference], an equation was introduced showing that the upper bound of the expected error in the target domain depends on three terms, which include the divergence between different domains and the error of an ideal joint hypothesis. the divergence between source and target domain, h\u2206h-distance, is defined as follows: this distance is frequently used to measure the adaptability between different domains. section: the ideal joint hypothesis is defined as, and its corresponding error, where r denotes the expected error on each hypothesis. the theorem is as follows. this theorem means that the expected error on the target domain is upper bounded by three terms, the expected error on the source domain, the domain divergence measured by the disagreement of the hypothesis, and the error of the ideal joint hypothesis. in the existing work [reference][reference], c was disregarded because it was considered to be negligibly small. if we are provided with fixed features, we do not need to consider the term because the term is also fixed. however, if we assume that x s\u223c s, x t\u223c t are obtained from the last fully connected layer of deep models, we note that c is determined by the output of the layer, and further note the necessity of considering this term. we consider the pseudo-labeled target samples set given false labels at the ratio of\u03c1. the shared error of h* on s, t l is denoted as c\u2032. then, the following inequality holds: we show a simple derivation of the inequality in the supplementary material. in theorem 1, we can not measure c in the absence of labeled target samples. we can approximately evaluate and minimize it by using pseudo-labels. furthermore, when we consider the second term on the right-hand side, our method is expected to reduce this term. this term intuitively denotes the discrepancy between different domains in the disagreement of two classifiers. if we regard certain h and h\u2032 as f 1 and f 2, respectively,] is expected to be low, although we use the training set t l instead of genuine labeled target samples. thus, our method will consider both the second and the third term in theorem 1. section: experiment and evaluation we perform extensive evaluations of our method on image datasets and a sentiment analysis dataset. we evaluate the accuracy of target-specific networks in all experiments. visual domain adaptation for visual domain adaptation, we perform our evaluation on the digits datasets and traffic signs datasets. digits datasets include mnist [reference], [reference], street view house numbers (svhn) [reference], and synthetic digits (syn digits) [reference]. we further evaluate our method on traffic sign datasets including synthetic traffic signs (syn signs) [reference] and german traffic signs recognition benchmark [reference]. in total, five adaptation scenarios are evaluated in this experiment. as the datasets used for evaluation are varied in previous works, we extensively evaluate our method on the five scenarios. we do not evaluate our method on office [reference], which is the most commonly used dataset for visual domain adaptation. as pointed out by [reference], some labels in that dataset are noisy and some images contain other classes' objects. furthermore, many previous studies have evaluated the fine-tuning of pretrained networks using imagenet. this protocol assumes the existence of another source domain. in our work, we want to evaluate the situation where we have access to only one source domain and one target domain. section: adaptation in amazon reviews to investigate the behavior on language datasets, we also evaluated our method on the amazon reviews dataset [reference] with the same preprocessing as used by [reference][reference]. the dataset contains reviews on four types of products: books, dvds, electronics, and kitchen appliances. we evaluate our method on 12 domain adaptation scenarios. the results are shown in table 1. baseline methods we compare our method with five methods for unsupervised domain adaptation including state-of-the art methods in visual domain adaptation; maximum mean discrepancy (mmd) [reference], domain adversarial neural network (dann) [reference], deep reconstruction classification network (drcn) [reference], domain separation networks (dsn) [reference], and k-nearest neighbor based adaptation (knn-ad) [reference]. we cite the results of mmd from [reference]. in addition, we compare our method with cnn trained only on source samples. we compare our method with variational fair autoencoder (vfae) [reference] and dann [reference] in the amazon reviews experiment. section: implementation detail in experiments on image datasets, we employ the architecture of cnn used in [reference]. for a fair comparison, we separate the network at the hidden layer from which [reference] constructed discriminator networks. therefore, when considering one classifier, for example, f 1\u2022 f, the architecture is identical to previous work. we also follow [reference] in the other protocols. we set the threshold value for the labeling method as 0.95 in mnist\u2192svhn. in other scenarios, we set it as 0.9. we use momentumsgd for optimization and set the momentum as 0.9, while the learning rate is determined on validation splits and uses either [0.01, 0.05].\u03bb is set 0.01 in all scenarios. in our supplementary material, we provide details of the network architecture and hyper-parameters. for experiments on the amazon reviews dataset, we use a similar architecture to that used in [reference]: with sigmoid activated, one dense hidden layer with 50 hidden units, and softmax output. we extend the architecture to our method similarly in the architecture of cnn.\u03bb is set as 0.001 based on the validation. since the input is sparse, we use adagrad [reference] for optimization. we repeat this evaluation 10 times and report mean accuracy. section: experimental result in tables 1 and 3, we show the main results of the experiments. when training only on source samples, the effect of the bn is not clear as in tables 1. however, in all image recognition experiments, the effect of bn in our method is clear; at the same time, the effect of our method is also clear when we do not use bn in the network architecture. the effect of the weight constraint is obvious in mnist\u2192svhn. [reference] and [reference] in parentheses. mnist\u2192mnist-m: last pooling layer mnist\u2192mnist-m first, we evaluate the adaptation scenario between the hand-written digits dataset mnist and its transformed dataset mnist-m. mnist-m is composed by merging the clip of the background from bsds500 datasets [reference]. a patch is randomly taken from the images in bsds500, merged to mnist digits. even with this simple domain shift, the adaptation performance of cnn is much worse than the case where it was trained on target samples. from 59, 001 target training samples, we randomly select 1, 000 labeled target samples as a validation split and tuned hyper-parameters. our method outperforms the other existing method by about 7%. visualization of features in the last pooling layer is shown in fig. 3 (a)(b). we can observe that the red target samples are more dispersed when adaptation is achieved. we show the comparison of the accuracy between the actual labeling accuracy on target samples during training and the test accuracy in fig. 4. the test accuracy is very low at first, but as the steps increase, the accuracy becomes closer to that of the labeling accuracy. in this adaptation, we can clearly see that the actual labeling accuracy gradually improves with the accuracy of the network. svhn\u2194mnist we increase the gap between distributions in this experiment. we evaluate adaptation between svhn [reference] and mnist in a ten-class classification problem. svhn and mnist have distinct appearance, thus this adaptation is a challenging scenario especially in mnist\u2192svhn. svhn is colored and some images contain multiple digits. therefore, a classifier trained on svhn is expected to perform well on mnist, but the reverse is not true. mnist does not include any samples containing multiple digits and most samples are centered in images, thus adaptation from mnist to svhn is rather difficult. in both settings, we use 1, 000 labeled target samples to find the optimal hyperparameters. we evaluate our method on both adaptation scenarios and achieved state-of-the-art performance on both datasets. in particular, for the adaptation mnist\u2192svhn, we outperformed other methods by more than 10%. in fig. 3 (c)(d), we visualize the representations in mnist\u2192svhn. although the distributions seem to be separated between domains, the red svhn samples become more discriminative using our method compared with non-adapted embedding. we also show the comparison between actual labeling method accuracy and testing accuracy in fig. 4 (b)(c). in this figure, we can see that the labeling accuracy rapidly drops in the initial adaptation stage. on the other hand, testing accuracy continues to improve, and finally exceeds the labeling accuracy. there are two questions about this interesting phenomenon. the first question is why does the labeling method continue to decrease despite the increase in the test accuracy? target samples given pseudo-labels always include mistakenly labeled samples whereas those given no labels are ignored in our method. therefore, the error will be reinforced in the target samples that are included in training set. the second question is why does the test accuracy continue to increase despite the lower labeling accuracy? the assumed reasons are that the network already acquires target discriminative representations in this phase and they can improve the accuracy using source samples and correctly labeled target samples. in fig. 4 (f), we also show the comparison of accuracy of the three networks f 1, f 2, f t in svhn\u2192mnist. the accuracy of three networks is nearly the same in every step. the same thing is observed in other scenarios. from this result, we can state that the target-discriminative representations are shared in all three networks. syn digits\u2192svhn in this experiment, we aimed to address a common adaptation scenario from synthetic images to real images. the datasets of synthetic numbers [reference] consist of 500, 000 images generated from windows fonts by varying the text, positioning, orientation, background and stroke colors, and the amount of blur. we use 479, 400 source samples and 73, 257 target samples for training, and 26, 032 target samples for testing. we use 1, 000 svhn samples as a validation set. our method also outperforms other methods in this experiment. in this experiment, the effect of bn is not clear compared with other scenarios. the domain gap is considered small in this scenario as the performance of the source-only classifier shows. in fig. 4 (d), although the labeling accuracy is dropping, the accuracy of the learned network's prediction is improving as in mnist\u2194svhn. syn signs\u2192gtsrb this setting is similar to the pre-table 2. results of gradient stop experiment. when stopping gradients from ft, we do not use backward gradients from ft to f, and f learns only from f1, f2. when stopping gradients from f1, f2, we do not use backward gradients from f1, f2 to f, and f learns from ft. none denotes our proposed method, we backward all gradients from all branches to f. in these three adaptation scenarios, our method shows stable performance. vious setting, adaptation from synthetic images to real images, but we have a larger number of classes, namely 43 classes instead of 10. we use the syn signs dataset [reference] for the source dataset and the gtsrb dataset [reference] for the target dataset, which consist of real traffic sign images. we select randomly 31, 367 samples for target training samples and evaluate accuracy on the rest of the samples. a total of 3, 000 labeled target samples are used for validation. in this scenario, our method outperforms other methods. this result shows that our method is effective for the adaptation from synthesized images to real images, which have diverse classes. in fig. 4 (e), the same tendency as in mnist\u2194svhn is observed in this adaptation scenario. section: gradient stop experiment we evaluate the effect of the target-specific network in our method. we stop the gradient from upper layer networks f 1, f 2, and f t to examine the effect of f t. table 2 shows three scenarios including the case where we stop the gradient from f 1, f 2, and f t. in all scenarios, when we backward all gradients from f 1, f 2, f t, we obtain clear performance improvements. in the experiment mnist\u2192mnist-m, we can assume that only the backpropagation from f 1, f 2 can not construct discriminative representations for target samples and confirm the effect of f t. for the adaptation mnist\u2192svhn, the best performance is realized when f receives all gradients from upper networks. backwarding all gradients will ensure both target-specific discriminative representations in difficult adaptations. in syn signs\u2192gtsrb, backwarding only from f t produces the worst performance because these domains are similar and noisy pseudo-labeled target samples worsen the performance. section: a-distance from the theoretical results in [reference], a-distance is usually used as a measure of domain discrepancy. the way of estimating empirical a-distance is simple, in which we train a classifier to classify a domain from each domains' feature. then, the approximate distance is calculated table 3. amazon reviews experimental results. the accuracy (%) of the proposed method is shown with the result of vfae [reference] and dann [reference]. asd a= 2 (1\u2212 2\u01eb), where\u01eb is the generalization error of the classifier. in fig. 4 (g), we show the a-distance calculated from each cnn features. we used linear svm to calculate the distance. from this graph, we can see that our method certainly reduces the a-distance compared with the cnn trained on only source samples. in addition, when comparing dann and our method, although dann reduces a-distance much more than our method, our method shows superior performance. this indicates that minimizing the domain discrepancy is not necessarily an appropriate way to achieve better performance. amazon reviews reviews are encoded in 5, 000 dimensional vectors of bag-of-words unigrams and bigrams with binary labels. negative labels are attached to the samples if they are ranked with 1-3 stars. positive labels are attached if they are ranked with 4 or 5 stars. we have 2, 000 labeled source samples and 2, 000 unlabeled target samples for training, and between 3, 000 and 6, 000 samples for testing. we use 200 of labeled target samples for validation. from the results in table 3, our method performs better than vfae [reference] and dann [reference] in nine settings out of twelve. our method is effective in learning a shallow network on different domains. section: conclusion in this paper, we have proposed a novel asymmetric tri-training method for unsupervised domain adaptation, which is simply implemented. we aimed to learn discriminative representations by utilizing pseudo-labels assigned to unlabeled target samples. we utilized three classifiers, two networks assign pseudo-labels to unlabeled target samples and the remaining network learns from them. we evaluated our method both on domain adaptation on a visual recognition task and a sentiment analysis task, outperforming other methods. in particular, our method outperformed the other methods by more than 10% in the mnist\u2192svhn adaptation task. section: acknowledgement proof of theorem we introduce the derivation of theorem of the main paper. the ideal joint hypothesis is defined as h*= arg min, and its corresponding error, where r denotes the expected error on each hypothesis. we consider the pseudo-labeled target samples set given false labels at the ratio of\u03c1. the minimum shared error on s, t l is denoted as c\u2032. then, the following inequality holds: proof. the probabiliy of false labels in the pseudo-labeled set t l is\u03c1. when we consider 0-1 loss function for l, the difference between the error based on the true labeled set and pseudo-labeled set is then, the difference in the expected error is, from the characteritic of the loss function, the triangle inequality will hold, then from this result, the main inequality holds. section: cnn architectures and training detail four types of architectures are used for our method, which is based on [reference]. the network topology is shown in figs 6, 7 and 8. the other hyperparameters are decided on the validation splits. the learning rate is set to 0.05 in svhn\u2194mnist. in the other scenarios, it is set to 0.01. the batchsize for training f t, f is set as 128, the batchsize for training f 1, f 2, f is set as 64 in all scenarios. in mnist\u2192mnist-m, the dropout rate used in the experiment is 0.2 for training f t, 0.5 for training f 1, f 2. in mnist\u2192svhn, we did not use dropout. we decreased learning rate to 0.001 after step 10. in svhn\u2192mnist, the dropout rate used in the experiment is 0.5. in syndigits\u2192svhn, the dropout rate used in the experiment is 0.5. in synsigns\u2192gtsrb, the dropout rate used in the experiment is 0.5. section: supplementary experiments on mnist\u2192mnist-m we observe the behavior of our model when increasing the number of steps up to one hundred. we show the result in fig. 5. our model's accuracy gets about 97%. in our main experiments, we set the number of steps thirty, but from this experiment, further improvements can be expected. figure 6. the architecture used for mnist\u2192mnist-m. we added bn layer in the last convolution layer and fc layers in f1, f2. we also used dropout in our experiment. figure 8. the architecture used in the adaptation synthetic signs\u2192gtsrb. we added a bn layer after the last convolution layer in f and also used dropout. section: section: this work was funded by impact program of council for science, technology and innovation (cabinet office, government of japan) and supported by crest, jst. section:",
    "templates": [
        {
            "Material": [],
            "Method": [
                [
                    [
                        "asymmetric tri-training",
                        0
                    ],
                    [
                        "tri-training",
                        795
                    ],
                    [
                        "tritraining",
                        9277
                    ],
                    [
                        "tritraining method",
                        10184
                    ]
                ]
            ],
            "Metric": [],
            "Task": [
                [
                    [
                        "sentiment analysis task",
                        5758
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "44fcb91c6c436902edb35b4c453c34fb353b14ad-63",
    "doctext": "a convolutional encoder model for neural machine translation section: abstract the prevalent approach to neural machine translation relies on bi-directional lstms to encode the source sentence. we present a faster and simpler architecture based on a succession of convolutional layers. this allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. on wmt'16 englishromanian translation we achieve competitive accuracy to the state-of-the-art and on wmt'15 english-german we outperform several recently published results. our models obtain almost the same accuracy as a very deep lstm setup on wmt'14 english-french translation. we speed up cpu decoding by more than two times at the same or higher accuracy as a strong bidirectional lstm. 1 section: introduction neural machine translation (nmt) is an end-to-end approach to machine translation. the most successful approach to date encodes the source sentence with a bi-directional recurrent neural network (rnn) into a variable length representation and then generates the translation left-to-right with another rnn where both components interface via a soft-attention mechanism [reference][reference][reference][reference]. recurrent networks are typically parameterized as long short term memory networks (lstm; [reference] or gated recurrent units [reference][reference], often with residual or skip connections [reference][reference] to enable stacking of several layers (\u00a7 2). there have been several attempts to use convolutional encoder models for neural machine trans-lation in the past but they were either only applied to rescoring n-best lists of classical systems [reference] or were not competitive to recurrent alternatives [reference]. this is despite several attractive properties of convolutional networks. for example, convolutional networks operate over a fixed-size window of the input sequence which enables the simultaneous computation of all features for a source sentence. this contrasts to rnns which maintain a hidden state of the entire past that prevents parallel computation within a sequence. a succession of convolutional layers provides a shorter path to capture relationships between elements of a sequence compared to rnns. [reference] this also eases learning because the resulting tree-structure applies a fixed number of non-linearities compared to a recurrent neural network for which the number of non-linearities vary depending on the time-step. because processing is bottom-up, all words undergo the same number of transformations, whereas for rnns the first word is over-processed and the last word is transformed only once. in this paper we show that an architecture based on convolutional layers is very competitive to recurrent encoders. we investigate simple average pooling as well as parameterized convolutions as an alternative to recurrent encoders and enable very deep convolutional encoders by using residual connections [reference][reference]. we experiment on several standard datasets and compare our approach to variants of recurrent encoders such as uni-directional and bi-directional lstms. on wmt'16 english-romanian translation we achieve accuracy that is very competitive to the current state-of-the-art result. we perform competitively on wmt'15 english-german, and nearly match the performance of the best wmt'14 english-french system based on a deep lstm setup when comparing on a commonly used subset 2 for kernel width k and sequence length n we require max 1, forwards on a succession of stacked convolutional layers compared to n forwards with an rnn. section: arxiv:1611.02344v3 [cs. cl] 25 jul 2017 of the training data [reference][reference]. section: recurrent neural machine translation the general architecture of the models in this work follows the encoder-decoder approach with soft attention first introduced in [reference]. a source sentence x= (x 1,..., x m) of m words is processed by an encoder which outputs a sequence of states z= (z 1...., z m). the decoder is an rnn network that computes a new hidden state s i+ 1 based on the previous state s i, an embedding g i of the previous target language word y i, as well as a conditional input c i derived from the encoder output z. we use lstms (hochreiter and schmidhuber, 1997) for all decoder networks whose state s i comprises of a cell vector and a hidden vector h i which is output by the lstm at each time step. we input c i into the lstm by concatenating it to g i. the translation model computes a distribution over the v possible target words y i+ 1 by transforming the lstm output h i via a linear layer with weights w o and bias b o: the conditional input c i at time i is computed via a simple dot-product style attention mechanism [reference]. specifically, we transform the decoder hidden state h i by a linear layer with weights w d and b d to match the size of the embedding of the previous target word g i and then sum the two representations to yield d i. conditional input c i is a weighted sum of attention scores a i\u2208 r m and encoder outputs z. the attention scores a i are determined by a dot product between h i with each z j, followed by a softmax over the source sequence: in preliminary experiments, we did not find the mlp attention of [reference] to perform significantly better in terms of bleu nor perplexity. however, we found the dot-product attention to be more favorable in terms of training and evaluation speed. we use bi-directional lstms to implement recurrent encoders similar to [reference] which achieved some of the best wmt14 englishfrench results reported to date. first, each word of the input sequence x is embedded in distributional space resulting in e= (e 1,..., e m). the embeddings are input to two stacks of uni-directional rnns where the output of each layer is reversed before being fed into the next layer. the first stack takes the original sequence while the second takes the reversed input sequence; the output of the second stack is reversed so that the final outputs of the stacks align. finally, the top-level hidden states of the two stacks are concatenated and fed into a linear layer to yield z. we denote this encoder architecture as bilstm. 3 non-recurrent encoders section: pooling encoder a simple baseline for non-recurrent encoders is the pooling model described in [reference] which simply averages the embeddings of k consecutive words. averaging word embeddings does not convey positional information besides that the words in the input are somewhat close to each other. as a remedy, we add position embeddings to encode the absolute position of each source word within a sentence. each source embedding e j therefore contains a position embedding l j as well as the word embedding w j. position embeddings have also been found helpful in memory networks for question-answering and language modeling [reference]. similar to the recurrent encoder (\u00a7 2), the attention scores a ij are computed from the pooled representations z j, however, the conditional input c i is a weighted sum of the embeddings e j, not z j, i.e., the input sequence is padded prior to pooling such that the encoder output matches the input length|z|=|x|. we set k to 5 in all experiments as [reference]. section: convolutional encoder a straightforward extension of pooling is to learn the kernel in a convolutional neural network (cnn). the encoder output z j contains information about a fixed-sized context depending on the kernel width k but the desired context width may vary. this can be addressed by stacking several layers of convolutions followed by non-linearities: additional layers increase the total context size while non-linearities can modulate the effective size of the context as needed. for instance, stacking 5 convolutions with kernel width k= 3 results in an input field of 11 words, i.e., each output depends on 11 input words, and the non-linearities allow the encoder to exploit the full input field, or to concentrate on fewer words as needed. to ease learning for deep encoders, we add residual connections from the input of each convolution to the output and then apply the non-linear activation function to the output (tanh; [reference]; the non-linearities are therefore not' bypassed'. multi-layer cnns are constructed by stacking several blocks on top of each other. the cnns do not contain pooling layers which are commonly used for down-sampling, i.e., the full source sequence length will be retained after the network has been applied. similar to the pooling model, the convolutional encoder uses position embeddings. the final encoder consists of two stacked convolutional networks (figure 1): cnn-a produces the encoder output z j to compute the attention scores a i, while the conditional input c i to the decoder is computed by summing the outputs of cnn-c, in practice, we found that two different cnns resulted in better perplexity as well as bleu compared to using a single one (\u00a7 5.3). we also found this to perform better than directly summing the e i without transformation as for the pooling model. section: related work there are several past attempts to use convolutional encoders for neural machine translation, however, to our knowledge none of them were able to match the performance of recurrent encoders. (kalchbrenner and blunsom, 2013) introduce a convolutional sentence encoder in which a multi-layer cnn generates a fixed sized embedding for a source sentence, or an n-gram representation followed by transposed convolutions for directly generating a per-token decoder input. the latter requires the length of the translation prior to generation and both models were evaluated by rescoring the output of an existing translation system. [reference] propose a gated recursive cnn which is repeatedly applied until a fixed-size representation is obtained but the recurrent encoder achieves higher accuracy. in follow-up work, the authors improved the model via a soft-attention mechanism but did not reconsider convolutional encoder models [reference]. concurrently to our work, [reference] have introduced convolutional translation models without an explicit attention mechanism but their approach does not yet result in state-ofthe-art accuracy. [reference]) also proposed a multi-layer cnn to generate a fixed-size encoder representation but their work lacks quantitative evaluation in terms of bleu. [reference] and [reference] applied convolutional models to score phrase-pairs of traditional phrasebased and dependency-based translation models. convolutional architectures have also been successful in language modeling but so far failed to outperform lstms [reference]. section: experimental setup section: datasets we evaluate different encoders and ablate architectural choices on a small dataset from the germanenglish machine translation track of iwslt 2014 (cettolo et al., 2014) with a similar setting to [reference]. unless otherwise stated, we restrict training sentences to have no more than 175 words; test sentences are not filtered. this is a higher threshold compared to other publications but ensures proper training of the position embeddings for non-recurrent encoders; the length threshold did not significantly effect recurrent encoders. length filtering results in 167 k sentence pairs and we test on the concatenation of tst2010, tst2011, tst2012, tst2013 and dev2010 comprising 6948 sentence pairs. [reference] our final results are on three major wmt tasks: wmt'16 english-romanian. we use the same data and pre-processing as [reference] and train on 2.8 m sentence pairs. [reference] our model is word-based instead of relying on byte-pair encoding [reference]. we evaluate on newstest2016. wmt'15 english-german. we use all available parallel training data, namely europarl v7, com-mon crawl and news commentary v10 and apply the standard moses tokenization to obtain 3.9 m sentence pairs [reference]. we report results on newstest2015. wmt'14 english-french. we use a commonly used subset of 12 m sentence pairs [reference], and remove sentences longer than 150 words. this results in 10.7 m sentence-pairs for training. results are reported on ntst14. a small subset of the training data serves as validation set (5% for iwslt'14 and 1% for wmt) for early stopping and learning rate annealing (\u00a7 4.3). for iwslt'14, we replace words that occur fewer than 3 times with a< unk> symbol, which results in a vocabulary of 24158 english and 35882 german word types. for wmt datasets, we retain 200 k source and 80 k target words. for english-french only, we set the target vocabulary to 30 k types to be comparable with previous work. section: model parameters we use 512 hidden units for both recurrent encoders and decoders. we reset the decoder hidden states to zero between sentences. for the convolutional encoder, 512 hidden units are used for each layer in cnn-a, while layers in cnn-c contain 256 units each. all embeddings, including the output produced by the decoder before the final linear layer, are of 256 dimensions. on the wmt corpora, we find that we can improve the performance of the bidirectional lstm models (bilstm) by using 512-dimensional word embeddings. model weights are initialized from a uniform distribution within [\u22120.05, 0.05]. for convolutional layers, we use a uniform distribution of\u2212kd\u22120.5, kd\u22120.5, where k is the kernel width (we use 3 throughout this work) and d is the input size for the first layer and the number of hidden units for subsequent layers [reference]. for cnn-c, we transform the input and output with a linear layer each to match the smaller embedding size. the model parameters were tuned on iwslt'14 and cross-validated on the larger wmt corpora. section: optimization recurrent models are trained with adam as we found them to benefit from aggressive optimization. we use a step width of 3.125\u00b7 10\u22124 and early stopping based on validation perplexity (kingma and ba, 2014). for non-recurrent encoders, we obtain best results with stochastic gradient descent (sgd) and annealing: we use a learning rate of 0.1 and once the validation perplexity stops improving, we reduce the learning rate by an order of magnitude each epoch until it falls below 10\u22124. for all models, we use mini-batches of 32 sentences for iwslt'14 and 64 for wmt. we use truncated back-propagation through time to limit the length of target sequences per mini-batch to 25 words. gradients are normalized by the mini-batch size. we re-normalize the gradients if their norm exceeds 25 [reference]. gradients of convolutional layers are scaled by sqrt (dim (input))\u22121 similar to [reference]. we use dropout on the embeddings and decoder outputs h i with a rate of 0.2 for iwslt'14 and 0.1 for wmt [reference]. all models are implemented in torch [reference] and trained on a single gpu. section: evaluation we report accuracy of single systems by training several identical models with different random seeds (5 for iwslt'14, 3 for wmt) and pick the one with the best validation perplexity for final bleu evaluation. translations are generated by a beam search and we normalize log-likelihood scores by sentence length. on iwslt'14 we use a beam width of 10 and for wmt models we tune beam width and word penalty on a separate test set, that is newsdev2016 for wmt'16 english-romanian, newstest2014 for wmt'15 english-german and ntst1213 for wmt'14 english-french. [reference] the word penalty adds a constant factor to log-likelihoods, except for the end-of-sentence token. prior to scoring the generated translations against the respective references, we perform unknown word replacement based on attention scores [reference]. unknown words are replaced by looking up the source word with the maximum attention score in a pre-computed dictionary. if the dictionary contains no translation, then we simply copy the source word. dictionaries were extracted from the aligned training data that was aligned with fast align [reference]. each source word is mapped to the target word it is most frequently aligned to. for convolutional encoders with stacked cnn-c layers we noticed for some models that the attention maxima were consistently shifted by one word. we determine this per-model offset on the abovementioned development sets and correct for it. finally, we compute case-sensitive tokenized bleu, except for wmt'16 english-romanian where we use detokenized bleu to be comparable with [reference] 5 results section: recurrent vs. non-recurrent encoders we first compare recurrent and non-recurrent encoders in terms of perplexity and bleu on iwslt'14 with and without position embeddings (\u00a7 3.1) and include a phrase-based system [reference]. table 1 shows that a single-layer convolutional model with position embeddings (convolutional) can outperform both a uni-directional lstm encoder (lstm) as well as a bi-directional lstm encoder (bilstm). next, we increase the depth of the convolutional encoder. we choose a 5 specifically, we select a beam from {5, 10} and a word penalty from {0,\u22120.5,\u22121,\u22121.5} 6 https:// github.com/ moses-smt/ mosesdecoder/ blob/ 617e8c8ed1630fb1d1/ scripts/ generic/ {multi-bleu.perl, mteval-v13a.pl} good setting by independently varying the number of layers in cnn-a and cnn-c between 1 and 10 and obtained best validation set perplexity with six layers for cnn-a and three layers for cnn-c. this configuration outperforms bilstm by 0.7 bleu (deep convolutional 6/ 3). we investigate depth in the convolutional encoder more in\u00a7 5.3. among recurrent encoders, the bilstm is 2.3 bleu better than the uni-directional version. the simple pooling encoder which does not contain any parameters is only 1.3 bleu lower than a unidirectional lstm encoder and 3.6 bleu lower than bilstm. the results without position embeddings (words) show that position information is crucial for convolutional encoders. in particular for shallow models (pooling and convolutional), whereas deeper models are less effected. recurrent encoders do not benefit from explicit position information because this information can be naturally extracted through the sequential computation. when tuning model settings, we generally observe good correlation between perplexity and bleu. however, for convolutional encoders perplexity gains translate to smaller bleu improvements compared to recurrent counterparts (table 1). we observe a similar trend on larger datasets. section: evaluation on wmt corpora next, we evaluate the bilstm encoder and the convolutional encoder architecture on three larger tasks and compare against previously published results. on wmt'16 english-romanian translation we compare to [reference], the winning single system entry for this language pair. their model consists of a bi-directional gru encoder, a gru decoder and mlp-based attention. table 2: accuracy on three wmt tasks, including results published in previous work. for deep convolutional encoders, we include the number of layers in cnn-a and cnn-c, respectively. they use byte pair encoding (bpe) to achieve openvocabulary translation and dropout in all components of the neural network to achieve 28.1 bleu; we use the same pre-processing but no bpe (\u00a7 4). the results (table 2) show that a deep convolutional encoder can perform competitively to the state of the art on this dataset [reference]. our bi-directional lstm encoder baseline is 0.6 bleu lower than the state of the art but uses only 512 hidden units compared to 1024. a singlelayer convolutional encoder with embedding size 256 performs at 27.1 bleu. increasing the number of convolutional layers to 8 in cnn-a and 4 in cnn-c achieves 27.8 bleu which outperforms our baseline and is competitive to the state of the art. on wmt'15 english to german, we compare to a bilstm baseline and prior work: [reference] introduce a large output vocabulary; the decoder of (chung et al., 2016) operates on the character-level; [reference] uses lstms instead of grus and feeds the conditional input to the output layer as well as to the decoder. our single-layer bilstm baseline is competitive to prior work and a two-layer bilstm encoder performs 0.6 bleu better at 24.1 bleu. previous work also used multi-layer setups, e.g., [reference] has two layers both in the encoder and the decoder with 1024 hidden units, and [reference] use 1000 hidden units per lstm. we use 512 hidden units for both lstm and convolutional encoders. our convolutional model with 15 layers in cnn-a and 5 layers in cnn-c outperforms the bilstm encoder with both a single decoder layer or two decoder layers. finally, we evaluate on the larger wmt'14 english-french corpus. on this dataset the recurrent architectures benefit from an additional layer both in the encoder and the decoder. for a singlelayer decoder, a deep convolutional encoder outperforms the bilstm accuracy by 0.3 bleu and for a two-layer decoder, our very deep convolutional encoder with up to 20 layers outperforms the bilstm by 0.4 bleu. it has 40% fewer parameters than the bilstm due to the smaller embedding sizes. we also outperform several previous systems, including the very deep encoder-decoder model proposed by [reference]. our best result is just 0.2 bleu below [reference] who use a very deep lstm setup with a 9-layer encoder, a 7-layer decoder, shortcut connections and extensive regularization with dropout and l2 regularization. section: convolutional encoder architecture details we next motivate our design of the convolutional encoder (\u00a7 3.2). we use the smaller iwslt'14 german-english setup without unknown word replacement to enable fast experimental turn-around. bleu results are averaged over three training runs initialized with different seeds. figure 2 shows accuracy for a different number of layers of both cnns with and without residual connections. our first observation is that computing the conditional input c i directly over embeddings e (line\" without cnn-c\") is already working well at 28.3 bleu with a single cnn-a layer and at 29.1 bleu for cnn-a with 7 layers (figure 2a). increasing the number of cnn-c layers is beneficial up to three layers and beyond this we did not observe further improvements. similarly, increasing the number of layers in cnn-a beyond six does not increase accuracy on this relatively small dataset. in general, choosing two to three times as many layers in cnn-a as in cnn-c is a good rule of thumb. without residual connections, the model fails to utilize the increase in modeling power from additional layers, and performance drops significantly for deeper encoders (figure 2b). our convolutional architecture relies on two sets of networks, cnn-a for attention score computation a i and cnn-c for the conditional input c i to be fed to the decoder. we found that using the same network for both tasks, similar to recurrent encoders, resulted in poor accuracy of 22.9 bleu. this compares to 28.5 bleu for separate singlelayer networks, or 28.3 bleu when aggregating embeddings for c i. increasing the number of layers in the single network setup did not help. figure 2 (a) suggests that the attention weights (cnn-a) need to integrate information from a wide context which can be done with a deep stack. at the same time, the vectors which are averaged (cnn-c) seem to benefit from a shallower, more local representation closer to the input words. two stacks are an easy way to achieve these contradicting requirements. in appendix a we visualize attention scores and find that alignments for cnn encoders are less sharp compared to bilstms, however, this does not affect the effectiveness of unknown word replacement once we adjust for shifted maxima. in appendix b we investigate whether deep convolutional encoders are required for translating long sentences and observe that even relatively shallow encoders perform well on long sentences. section: training and generation speed for training, we use the fast cudnn lstm implementation for layers without attention and experiment on iwslt'14 with batch size 32. the single-layer bilstm model trains at 4300 target words/ second, while the 6/ 3 deep convolutional encoder compares at 6400 words/ second on an nvidia tesla m40 gpu. we do not observe shorter overall training time since sgd converges slower than adam which we use for bilstm models. we measure generation speed on an intel haswell cpu clocked at 2.50ghz with a single thread for blas operations. we use vocabulary selection which can speed up generation by up to a factor of ten at no cost in accuracy via making the time to compute the final output layer negligible [reference][reference]. this shifts the focus from the efficiency of the encoder to the efficiency of the decoder. on iwslt'14 (table 3a) the convolutional encoder increases the speed of the overall model by a factor of 1.35 compared to the bilstm encoder while improving accuracy by 0.7 bleu. in this setup both encoders models have the same hidden layer and embedding sizes. on the larger wmt'15 english-german task (table 3b) the convolutional encoder speeds up generation by 2.1 times compared to a two-layer bil-stm. this corresponds to 231 source words/ second with beam size 5. our best model on this dataset generates 203 words/ second but at slightly lower accuracy compared to the full vocabulary setting in table 2. the recurrent encoder uses larger embeddings than the convolutional encoder which were required for the models to match in accuracy. the smaller embedding size is not the only reason for the speed-up. in table 3a (a), we compare a conv 6/ 3 encoder and a bilstm with equal embedding sizes. the convolutional encoder is still 1.34x faster (at 0.7 higher bleu) although it requires roughly 1.6x as many flops. we believe that this is likely due to better cache locality for convolutional layers on cpus: an lstm with fused gates 7 requires two big matrix multiplications with different weights as well as additions, multiplications and non-linearities for each source word, while the output of each convolutional layer can be computed as whole with a single matrix multiply. for comparison, the quantized deep lstm-7 our bi-directional lstm implementation is based on torch rnnlib which uses fused lstm gates (https:// github.com/ facebookresearch/ torch-rnnlib/) and which we consider an efficient implementation. section: conclusion we introduced a simple encoder model for neural machine translation based on convolutional networks. this approach is more parallelizable than recurrent networks and provides a shorter path to capture long-range dependencies in the source. we find it essential to use source position embeddings as well as different cnns for attention score computation and conditional input aggregation. our experiments show that convolutional encoders perform on par or better than baselines based on bi-directional lstm encoders. in comparison to other recent work, our deep convolutional encoder is competitive to the best published results to date (wmt'16 english-romanian) which are obtained with significantly more complex models (wmt'14 english-french) or stem from improvements that are orthogonal to our work (wmt'15 english-german). our architecture also leads to large generation speed improvements: translation models with our convolutional encoder can translate twice as fast as strong baselines with bi-directional recurrent encoders. future work includes better training to enable faster convergence with the convolutional encoder to better leverage the higher processing speed. our fast architecture is interesting for character level encoders where the input is significantly longer than for words. also, we plan to investigate the effectiveness of our architecture on other sequence-tosequence tasks, e.g. summarization, constituency parsing, dialog modeling. section: a alignment visualization in figure 4 and figure 5, we plot attention scores for a sample wmt'15 english-german and wmt'14 english-french translation with bilstm and deep convolutional encoders. the translation is on the x-axis and the source sentence on the y-axis. the attention scores of the bilstm output are sharp but do not necessarily represent a correct alignment. for cnn encoders the scores are less focused but still indicate an approximate source location, e.g., in figure 4b, when moving the clause\" over 1, 000 people were taken hostage\" to the back of the translation. for some models, attention maxima are consistently shifted by one token as both in figure 4b and figure 5b. interestingly, convolutional encoders tend to focus on the last token (figure 4b) or both the first and last tokens (figure 5b). motivated by the hypothesis that the this may be due to the decoder depending on the length of the source sentence (which it can not determine without position embeddings), we explicitly provided a distributed representation of the input length to the decoder and attention module. however, this did not cause a change in attention patterns nor did it improve translation accuracy. one characteristic of our convolutional encoder architecture is that the context over which outputs are computed depends on the number of layers. with bi-directional rnns, every encoder output depends on the entire source sentence. in figure 3, we evaluate whether limited context affects the translation quality on longer sentences of wmt'15 english-german which often requires moving verbs over long distances. we sort the newstest2015 test set by source length, partition it into 15 equallysized buckets, and compare the bleu scores of models listed in table 2 on a per-bucket basis. section: b performance by sentence length there is no clear evidence for sub-par translations on sentences that are longer than the observable context per encoder output. we include a small encoder with a 6-layer cnn-c and a 3-layer cnn-a in the comparison which performs worse than a 2-layer bilstm (23.3 bleu vs23.6). with 6 convolutional layers at kernel width 3, each encoder output contains information of 13 adjacent source words. looking at the accuracy for sentences with 15 words or more, this relatively shallow cnn is either on par or better than the bilstm for 5 out of 10 buckets; the bilstm has access to the entire source context. similar observations can be made for the deeper convolutional encoders. section:",
    "templates": [
        {
            "Material": [
                [
                    [
                        "wmt'15 english-german",
                        538
                    ],
                    [
                        "iwslt 2014",
                        10931
                    ],
                    [
                        "wmt'15 english to german",
                        19804
                    ]
                ]
            ],
            "Method": [],
            "Metric": [
                [
                    [
                        "bleu",
                        5391
                    ],
                    [
                        "bleu evaluation",
                        15122
                    ],
                    [
                        "attention scores",
                        27960
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "neural machine translation",
                        34
                    ],
                    [
                        "nmt",
                        881
                    ],
                    [
                        "machine translation",
                        914
                    ],
                    [
                        "neural machine trans-lation",
                        1596
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "wmt14 englishfrench",
                        5635
                    ],
                    [
                        "wmt'14 english-french",
                        12041
                    ],
                    [
                        "iwslt'14",
                        12327
                    ],
                    [
                        "english-french",
                        12634
                    ],
                    [
                        "newstest2014",
                        15408
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "pooling",
                        7384
                    ],
                    [
                        "layers of convolutions",
                        7642
                    ],
                    [
                        "convolutional encoders",
                        9225
                    ],
                    [
                        "encoders",
                        10818
                    ],
                    [
                        "decoders",
                        12817
                    ],
                    [
                        "single-layer convolutional model",
                        16792
                    ],
                    [
                        "singlelayer convolutional encoder",
                        19551
                    ],
                    [
                        "convolutional layer",
                        26093
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "bleu",
                        5391
                    ],
                    [
                        "bleu evaluation",
                        15122
                    ],
                    [
                        "attention scores",
                        27960
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "neural machine translation",
                        34
                    ],
                    [
                        "nmt",
                        881
                    ],
                    [
                        "machine translation",
                        914
                    ],
                    [
                        "neural machine trans-lation",
                        1596
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "wmt'16 english-romanian",
                        11560
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "bi-directional lstms",
                        142
                    ],
                    [
                        "bidirectional lstm",
                        808
                    ],
                    [
                        "bilstm",
                        6271
                    ],
                    [
                        "bilstms",
                        23618
                    ],
                    [
                        "layer bil-stm",
                        25178
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "bleu",
                        5391
                    ],
                    [
                        "bleu evaluation",
                        15122
                    ],
                    [
                        "attention scores",
                        27960
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "neural machine translation",
                        34
                    ],
                    [
                        "nmt",
                        881
                    ],
                    [
                        "machine translation",
                        914
                    ],
                    [
                        "neural machine trans-lation",
                        1596
                    ]
                ]
            ]
        },
        {
            "Material": [
                [
                    [
                        "wmt'16 english-romanian",
                        11560
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "pooling",
                        7384
                    ],
                    [
                        "layers of convolutions",
                        7642
                    ],
                    [
                        "convolutional encoders",
                        9225
                    ],
                    [
                        "encoders",
                        10818
                    ],
                    [
                        "decoders",
                        12817
                    ],
                    [
                        "single-layer convolutional model",
                        16792
                    ],
                    [
                        "singlelayer convolutional encoder",
                        19551
                    ],
                    [
                        "convolutional layer",
                        26093
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "bleu",
                        5391
                    ],
                    [
                        "bleu evaluation",
                        15122
                    ],
                    [
                        "attention scores",
                        27960
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "neural machine translation",
                        34
                    ],
                    [
                        "nmt",
                        881
                    ],
                    [
                        "machine translation",
                        914
                    ],
                    [
                        "neural machine trans-lation",
                        1596
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "4592364a6c1eb227ffb3cbb6bb9f4e6431e89f2f-64",
    "doctext": "document: learning to compare: relation network for few-shot learning we present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. our method, called the relation network (rn), is trained end-to-end from scratch. during meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. once trained, a rn is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks. section: introduction deep learning models have achieved great success in visual recognition tasks. however, these supervised learning models need large amounts of labelled data and many iterations to train their large number of parameters. this severely limits their scalability to new classes due to annotation cost, but more fundamentally limits their applicability to newly emerging (eg. new consumer devices) or rare (eg. rare animals) categories where numerous annotated images may simply never exist. in contrast, humans are very good at recognising objects with very little direct supervision, or none at all, few-shot or zero-shot learning. for example, children have no problem generalising the concept of\" zebra\" from a single picture in a book, or hearing its description as looking like a stripy horse. motivated by the failure of conventional deep learning methods to work well on one or few examples per class, and inspired by the few-and zero-shot learning ability of humans, there has been a recent resurgence of interest in machine one/ few-shot and zero-shot learning. few-shot learning aims to recognise novel visual categories from very few labelled examples. the availability of only one or very few examples challenges the standard 'fine-tuning' practice in deep learning. data augmentation and regularisation techniques can alleviate overfitting in such a limited-data regime, but they do not solve it. therefore contemporary approaches to few-shot learning often decompose training into an auxiliary meta learning phase where transferrable knowledge is learned in the form of good initial conditions, embeddings or optimisation strategies. the target few-shot learning problem is then learned by fine-tuning with the learned optimisation strategy or computed in a feed-forward pass without updating network weights. zero-shot learning also suffers from a related challenge. recognisers are trained by a single example in the form of a class description (c.f., single exemplar image in one-shot), making data insufficiency for gradient-based learning a challenge. while promising, most existing few-shot learning approaches either require complex inference mechanisms, complex recurrent neural network (rnn) architectures, or fine-tuning the target problem. our approach is most related to others that aim to train an effective metric for one-shot learning. where they focus on the learning of the transferrable embedding and pre-define a fixed metric (e.g., as euclidean), we further aim to learn a transferrable deep metric for comparing the relation between images (few-shot learning), or between images and class descriptions (zero-shot learning). by expressing the inductive bias of a deeper solution (multiple non-linear learned stages at both embedding and relation modules), we make it easier to learn a generalisable solution to the problem. specifically, we propose a two-branch relation network (rn) that performs few-shot recognition by learning to compare query images against few-shot labeled sample images. first an embedding module generates representations of the query and training images. then these embeddings are compared by a relation module that determines if they are from matching categories or not. defining an episode-based strategy inspired by, the embedding and relation modules are meta-learned end-to-end to support few-shot learning. this can be seen as extending the strategy of to include a learnable non-linear comparator, instead of a fixed linear comparator. our approach outperforms prior approaches, while being simpler (no rnns) and faster (no fine-tuning). our proposed strategy also directly generalises to zero-shot learning. in this case the sample branch embeds a single-shot category description rather than a single exemplar training image, and the relation module learns to compare query image and category description embeddings. overall our contribution is to provide a clean framework that elegantly encompasses both few and zero-shot learning. our evaluation on four benchmarks show that it provides compelling performance across the board while being simpler and faster than the alternatives. section: related work the study of one or few-shot object recognition has been of interest for some time. earlier work on few-shot learning tended to involve generative models with complex iterative inference strategies. with the success of discriminative deep learning-based approaches in the data-rich many-shot setting, there has been a surge of interest in generalising such deep learning approaches to the few-shot learning setting. many of these approaches use a meta-learning or learning-to-learn strategy in the sense that they extract some transferrable knowledge from a set of auxiliary tasks (meta-learning, learning-to-learn), which then helps them to learn the target few-shot problem well without suffering from the overfitting that might be expected when applying deep models to sparse data problems. learning to fine-tune the successful maml approach aimed to meta-learn an initial condition (set of neural network weights) that is good for fine-tuning on few-shot problems. the strategy here is to search for the weight configuration of a given neural network such that it can be effectively fine-tuned on a sparse data problem within a few gradient-descent update steps. many distinct target problems are sampled from a multiple task training set; the base neural network model is then fine-tuned to solve each of them, and the success at each target problem after fine-tuning drives updates in the base model- thus driving the production of an easy to fine-tune initial condition. the few-shot optimisation approach goes further in meta-learning not only a good initial condition but an lstm-based optimizer that is trained to be specifically effective for fine-tuning. however both of these approaches suffer from the need to fine-tune on the target problem. in contrast, our approach solves target problems in an entirely feed-forward manner with no model updates required, making it more convenient for low-latency or low-power applications. rnn memory based another category of approaches leverage recurrent neural networks with memories. here the idea is typically that an rnn iterates over an examples of given problem and accumulates the knowledge required to solve that problem in its hidden activations, or external memory. new examples can be classified, for example by comparing them to historic information stored in the memory. so 'learning' a single target problem can occur in unrolling the rnn, while learning-to-learn means training the weights of the rnn by learning many distinct problems. while appealing, these architectures face issues in ensuring that they reliably store all the, potentially long term, historical information of relevance without forgetting. in our approach we avoid the complexity of recurrent networks, and the issues involved in ensuring the adequacy of their memory. instead our learning-to-learn approach is defined entirely with simple and fast feed forward cnns. embedding and metric learning approaches the prior approaches entail some complexity when learning the target few-shot problem. another category of approach aims to learn a set of projection functions that take query and sample images from the target problem and classify them in a feed forward manner. one approach is to parameterise the weights of a feed-forward classifier in terms of the sample set. the meta-learning here is to train the auxiliary parameterisation net that learns how to paramaterise a given feed-forward classification problem in terms of a few-shot sample set. metric-learning based approaches aim to learn a set of projection functions such that when represented in this embedding, images are easy to recognise using simple nearest neighbour or linear classifiers. in this case the meta-learned transferrable knowledge are the projection functions and the target problem is a simple feed-forward computation. the most related methodologies to ours are the prototypical networks of and the siamese networks of. these approaches focus on learning embeddings that transform the data such that it can be recognised with a fixed nearest-neighbour or linear classifier. in contrast, our framework further defines a relation classifier cnn, in the style of (while focuses on reasoning about relation between two objects in a same image which is to address a different problem.). compared to, this can be seen as providing a learnable rather than fixed metric, or non-linear rather than linear classifier. compared to we benefit from an episodic training strategy with an end-to-end manner from scratch, and compared to we avoid the complexity of set-to-set rnn embedding of the sample-set, and simply rely on pooling. zero-shot learning our approach is designed for few-shot learning, but elegantly spans the space into zero-shot learning (zsl) by modifying the sample branch to input a single category description rather than single training image. when applied to zsl our architecture is related to methods that learn to align images and category embeddings and perform recognition by predicting if an image and category embedding pair match. similarly to the case with the prior metric-based few-shot approaches, most of these apply a fixed manually defined similarity metric or linear classifier after combining the image and category embedding. in contrast, we again benefit from a deeper end-to-end architecture including a learned non-linear metric in the form of our learned convolutional relation network; as well as from an episode-based training strategy. section: methodology subsection: problem definition we consider the task of few-shot classifier learning. formally, we have three datasets: a training set, a support set, and a testing set. the support set and testing set share the same label space, but the training set has its own label space that is disjoint with support/ testing set. if the support set contains labelled examples for each of unique classes, the target few-shot problem is called-way-shot. with the support set only, we can in principle train a classifier to assign a class label to each sample in the test set. however, due to the lack of labelled samples in the support set, the performance of such a classifier is usually not satisfactory. therefore we aim to perform meta-learning on the training set, in order to extract transferrable knowledge that will allow us to perform better few-shot learning on the support set and thus classify the test set more successfully. an effective way to exploit the training set is to mimic the few-shot learning setting via episode based training, as proposed in. in each training iteration, an episode is formed by randomly selecting classes from the training set with labelled samples from each of the classes to act as the sample set (), as well as a fraction of the remainder of those classes' samples to serve as the query set. this sample/ query set split is designed to simulate the support/ test set that will be encountered at test time. a model trained from sample/ query set can be further fine-tuned using the support set, if desired. in this work we adopt such an episode-based training strategy. in our few-shot experiments (see section [reference]) we consider one-shot (, figure [reference]) and five-shot () settings. we also address the zero-shot learning case as explained in section [reference]. subsection: model one-shot our relation network (rn) consists of two modules: an embedding module and a relation module, as illustrated in figure [reference]. samples in the query set, and samples in the sample set are fed through the embedding module, which produces feature maps and. the feature maps and are combined with operator. in this work we assume to be concatenation of feature maps in depth, although other choices are possible. the combined feature map of the sample and query are fed into the relation module, which eventually produces a scalar in range of to representing the similarity between and, which is called relation score. thus, in the-way one-shot setting, we generate relation scores for the relation between one query input and training sample set examples, k-shot for-shot where, we element-wise sum over the embedding module outputs of all samples from each training class to form this class' feature map. this pooled class-level feature map is combined with the query image feature map as above. thus, the number of relation scores for one query is always in both one-shot or few-shot setting. objective function we use mean square error (mse) loss (eq. ([reference])) to train our model, regressing the relation score to the ground truth: matched pairs have similarity and the mismatched pair have similarity. the choice of mse is somewhat non-standard. our problem may seem to be a classification problem with a label space. however conceptually we are predicting relation scores, which can be considered a regression problem despite that for ground-truth we can only automatically generate targets. subsection: zero-shot learning zero-shot learning is analogous to one-shot learning in that one datum is given to define each class to recognise. however instead of being given a support set with one-shot image for each of training classes, it contains a semantic class embedding vector for each. modifying our framework to deal with the zero-shot case is straightforward: as a different modality of semantic vectors is used for the support set (attribute vectors instead of images), we use a second heterogeneous embedding module besides the embedding module used for the image query set. then the relation net is applied as before. therefore, the relation score for each query input will be: the objective function for zero-shot learning is the same as that for few-shot learning. subsection: network architecture as most few-shot learning models utilise four convolutional blocks for embedding module, we follow the same architecture setting for fair comparison, see figure [reference]. more concretely, each convolutional block contains a 64-filter convolution, a batch normalisation and a relu nonlinearity layer respectively. the first two blocks also contain a max-pooling layer while the latter two do not. we do so because we need the output feature maps for further convolutional layers in the relation module. the relation module consists of two convolutional blocks and two fully-connected layers. each of convolutional block is a convolution with 64 filters followed by batch normalisation, relu non-linearity and max-pooling. the output size of last max pooling layer is and for omniglot and mini imagenet respectively. the two fully-connected layers are 8 and 1 dimensional, respectively. all fully-connected layers are relu except the output layer is sigmoid in order to generate relation scores in a reasonable range for all versions of our network architecture. the zero-shot learning architecture is shown in figure [reference]. in this architecture, the dnn subnet is an existing network (e.g., inception or resnet) pretrained on imagenet. section: experiments we evaluate our approach on two related tasks: few-shot classification on omniglot and mini imagenet, and zero-shot classification on animals with attributes (awa) and caltech-ucsd birds-200-2011 (cub). all the experiments are implemented based on pytorch. subsection: few-shot recognition settings few-shot learning in all experiments uses adam with initial learning rate, annealed by half for every 100, 000 episodes. all our models are end-to-end trained from scratch with no additional dataset. baselines we compare against various state of the art baselines for few-shot recognition, including neural statistician, matching nets with and without fine-tuning, mann, siamese nets with memory, convolutional siamese nets, maml, meta nets, prototypical nets and meta-learner lstm. subsubsection: omniglot dataset omniglot contains 1623 characters (classes) from 50 different alphabets. each class contains 20 samples drawn by different people. following, we augment new classes through, and rotations of existing data and use 1200 original classes plus rotations for training and remaining 423 classes plus rotations for testing. all input images are resized to. training besides the sample images, the 5-way 1-shot contains 19 query images, the 5-way 5-shot has 15 query images, the 20-way 1-shot has 10 query images and the 20-way 5-shot has 5 query images for each of the sampled classes in each training episode. this means for example that there are images in one training episode/ mini-batch for the 5-way 1-shot experiments. results following, we computed few-shot classification accuracies on omniglot by averaging over 1000 randomly generated episodes from the testing set. for the 1-shot and 5-shot experiments, we batch one and five query images per class respectively for evaluation during testing. the results are shown in table [reference]. we achieved state-of-the-art performance under all experiments setting with higher averaged accuracies and lower standard deviations, except 5-way 5-shot where our model is 0.1% lower in accuracy than. this is despite that many alternatives have significantly more complicated machinery, or fine-tune on the target problem, while we do not. subsubsection: mini imagenet paragraph: dataset the mini imagenet dataset, originally proposed by, consists of 60, 000 colour images with 100 classes, each having 600 examples. we followed the split introduced by, with 64, 16, and 20 classes for training, validation and testing, respectively. the 16 validation classes is used for monitoring generalisation performance only. training following the standard setting adopted by most existing few-shot learning work, we conducted 5 way 1-shot and 5-shot classification. beside the sample images, the 5-way 1-shot contains 15 query images, and the 5-way 5-shot has 10 query images for each of the sampled classes in each training episode. this means for example that there are images in one training episode/ mini-batch for 5-way 1-shot experiments. we resize input images to. our model is trained end-to-end from scratch, with random initialisation, and no additional training set. results following, we batch 15 query images per class in each episode for evaluation in both 1-shot and 5-shot scenarios and the few-shot classification accuracies are computed by averaging over 600 randomly generated episodes from the test set. from table [reference], we can see that our model achieved state-of-the-art performance on 5-way 1-shot settings and competitive results on 5-way 5-shot. however, the 1-shot result reported by prototypical networks reqired to be trained on 30-way 15 queries per training episode, and 5-shot result was trained on 20-way 15 queries per training episode. when trained with 5-way 15 query per training episode, only got for 1-shot evaluation, clearly weaker than ours. in contrast, all our models are trained on 5-way, 1 query for 1-shot and 5 queries for 5-shot per training episode, with much less training queries than. subsection: zero-shot recognition datasets and settings we follow two zsl settings: the old setting and the new gbu setting provided by for training/ test splits. under the old setting, adopted by most existing zsl works before, some of the test classes also appear in the imagenet 1 k classes, which have been used to pretrain the image embedding network, thus violating the zero-shot assumption. in contrast, the new gbu setting ensures that none of the test classes of the datasets appear in the imagenet 1 k classes. under both settings, the test set can comprise only the unseen class samples (conventional test set setting) or a mixture of seen and unseen class samples. the latter, termed generalised zero-shot learning (gzsl), is more realistic in practice. two widely used zsl benchmarks are selected for the old setting: awa (animals with attributes) consists of 30, 745 images of 50 classes of animals. it has a fixed split for evaluation with 40 training classes and 10 test classes. cub (caltech-ucsd birds-200-2011) contains 11, 788 images of 200 bird species with 150 seen classes and 50 disjoint unseen classes. three datasets are selected for gbu setting: awa1, awa2 and cub. the newly released awa2 consists of 37, 322 images of 50 classes which is an extension of awa while awa1 is same as awa but under the gbu setting. semantic representation for awa, we use the continuous 85-dimension class-level attribute vector from, which has been used by all recent works. for cub, a continuous 312-dimension class-level attribute vector is used. implementation details two different embedding modules are used for the two input modalities in zero-shot learning. unless otherwise specified, we use inception-v2 as the query image embedding dnn in the old and conventional setting and resnet101 for the gbu and generalised setting, taking the top pooling units as image embedding with dimension and respectively. this dnn is pre-trained on ilsvrc 2012 1 k classification without fine-tuning, as in recent deep zsl works. a mlp network is used for embedding semantic attribute vectors. the size of hidden layer fc1 (figure [reference]) is set to 1024 and 1200 for awa and cub respectively, and the output size fc2 is set to the same dimension as the image embedding for both datasets. for the relation module, the image and semantic embeddings are concatenated before being fed into mlps with hidden layer fc3 size 400 and 1200 for awa and cub, respectively. we add weight decay (l2 regularisation) in fc1& 2 as there is a hubness problem in cross-modal mapping for zsl which can be best solved by mapping the semantic feature vector to the visual feature space with regularisation. after that, fc3& 4 (relation module) are used to compute the relation between the semantic representation (in the visual feature space) and the visual representation. since the hubness problem does not existing in this step, no l2 regularisation/ weight decay is needed. all the zsl models are trained with weight decay in the embedding network. the learning rate is initialised to with adam and then annealed by half every 200, 000 iterations. results under the old setting the conventional evaluation for zsl followed by the majority of prior work is to assume that the test data all comes from unseen classes. we evaluate this setting first. we compare 15 alternative approaches in table [reference]. with only the attribute vector used as the sample class embedding, our model achieves competitive result on awa and state-of-the-art performance on the more challenging cub dataset, outperforming the most related alternative prototypical networks by a big margin. note that only inductive methods are considered. some recent methods are tranductive in that they use all test data at once for model training, which gives them a big advantage at the cost of making a very strong assumption that may not be met in practical applications, so we do not compare with them here. results under the gbu setting we follow the evaluation setting of. we compare our model with 11 alternative zsl models in table [reference]. the 10 shallow models\u00e2\u0080\u0099 results are from and the result of the state-of-the-art method dem is from the authors' github page. we can see that on awa2 and cub, our model is particularly strong under the more realistic gzsl setting measured using the harmonic mean (h) metric. while on awa1, our method is only outperformed by dem. section: why does relation network work? subsection: relationship to existing models related prior few-shot work uses fixed pre-specified distance metrics such as euclidean or cosine distance to perform classification. these studies can be seen as distance metric learning, but where all the learning occurs in the feature embedding, and a fixed metric is used given the learned embedding. also related are conventional metric learning approaches that focus on learning a shallow (linear) mahalanobis metric for a fixed feature representation. in contrast to prior work's fixed metric or fixed features and shallow learned metric, relation network can be seen as both learning a deep embedding and learning a deep non-linear metric (similarity function). these are mutually tuned end-to-end to support each other in few short learning. why might this be particularly useful? by using a flexible function approximator to learn similarity, we learn a good metric in a data driven way and do not have to manually choose the right metric (euclidean, cosine, mahalanobis). fixed metrics like assume that features are solely compared element-wise, and the most related assumes linear separability after the embedding. these are thus critically dependent on the efficacy of the learned embedding network, and hence limited by the extent to which the embedding networks generate inadequately discriminative representations. in contrast, by deep learning a non-linear similarity metric jointly with the embedding, relation network can better identify matching/ mismatching pairs. [b] 0.5 [b] 0.5 [b] 0.5 [b] 0.5 subsection: visualisation to illustrate the previous point about adequacy of learned input embeddings, we show a synthetic example where existing approaches definitely fail and our relation network can succeed due to using a deep relation module. assuming 2d query and sample input embeddings to a relation module, fig. [reference] (a) shows the space of 2d sample inputs for a fixed 2d query input. each sample input (pixel) is colored according to whether it matches the fixed query or not. this represents a case where the output of the embedding modules is not discriminative enough for trivial (euclidean nn) comparison between query and sample set. in fig. [reference] (c) we attempt to learn matching via a mahalanobis metric learning relation module, and we can see the result is inadequate. in fig. [reference] (d) we learn a further 2-hidden layer mlp embedding of query and sample inputs as well as the subsequent mahalanobis metric, which is also not adequate. only by learning the full deep relation module for similarity can we solve this problem in fig. [reference] (b). in a real problem the difficulty of comparing embeddings may not be this extreme, but it can still be challenging. we qualitatively illustrate the challenge of matching two example omniglot query images (embeddings projected to 2d, figure [reference] (left)) by showing an analogous plot of real sample images colored by match (cyan) or mismatch (magenta) to two example queries (yellow). under standard assumptions the cyan matching samples should be nearest neighbours to the yellow query image with some metric (euclidean, cosine, mahalanobis). but we can see that the match relation is more complex than this. in figure [reference] (right), we instead plot the same two example queries in terms of a 2d pca representation of each query-sample pair, as represented by the relation module's penultimate layer. we can see that the relation network has mapped the data into a space where the (mis) matched pairs are linearly separable. section: conclusion we proposed a simple method called the relation network for few-shot and zero-shot learning. relation network learns an embedding and a deep non-linear distance metric for comparing query and sample items. training the network end-to-end with episodic training tunes the embedding and distance metric for effective few-shot learning. this approach is far simpler and more efficient than recent few-shot meta-learning approaches, and produces state-of-the-art results. it further proves effective at both conventional and generalised zero-shot settings. acknowledgements this work was supported by the erc grant erc-2012-adg 321162-helios, epsrc grant seebibyte ep/ m013774/ 1, epsrc/ muri grant ep/ n019474/ 1, epsrc grant ep/ r026173/ 1, and the european union's horizon 2020 research and innovation program (grant agreement no. 640891). we gratefully acknowledge the support of nvidia corporation with the donation of the titan xp gpu and the esprc funded tier 2 facility, jade used for this research. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "mini imagenet",
                        15713
                    ],
                    [
                        "imagenet",
                        16157
                    ],
                    [
                        "1-shot",
                        20089
                    ],
                    [
                        "imagenet 1 k classes",
                        20454
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "relation network",
                        31
                    ],
                    [
                        "rn",
                        291
                    ],
                    [
                        "relation net",
                        14706
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "averaged accuracies",
                        18127
                    ],
                    [
                        "accuracy",
                        18231
                    ],
                    [
                        "awa",
                        21464
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "learning",
                        10
                    ],
                    [
                        "few-shot learning",
                        52
                    ],
                    [
                        "few-shot setting",
                        490
                    ],
                    [
                        "few-shot",
                        1580
                    ],
                    [
                        "few",
                        1908
                    ],
                    [
                        "few short learning",
                        25436
                    ]
                ]
            ]
        }
    ]
}
{
    "docid": "4682e5d4654b6040f213475c8b75ca039661b5f6-65",
    "doctext": "document: east: an efficient and accurate scene text detector previous approaches for scene text detection have already achieved promising performances across various benchmarks. however, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. in this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. the pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. the simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. experiments on standard datasets including icdar 2015, coco-text and msra-td500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. on the icdar 2015 dataset, the proposed algorithm achieves an f-score of 0.7820 at 13.2fps at 720p resolution. megvii technology inc., beijing, china {zxy, yaocong, wenhe, wangyuzhi, zsc, hwr, liangjiajun}@megvii.com section: introduction recently, extracting and understanding textual information embodied in natural scenes have become increasingly important and popular, which is evidenced by the unprecedented large numbers of participants of the icdar series contests and the launch of the trait 2016 evaluation by nist. text detection, as a prerequisite of the subsequent processes, plays a critical role in the whole procedure of textual information extraction and understanding. previous text detection approaches have already obtained promising performances on various benchmarks in this field. the core of text detection is the design of features to distinguish text from backgrounds. traditionally, features are manually designed to capture the properties of scene text, while in deep learning based methods effective features are directly learned from training data. compat=1.8 every tick label/.append style= font= grid style= dotted, gray [xmin=0, xmax=17, ymin=0.49, ymax=0.85, grid= both, axis lines= middle, line width=.8pt, width=1.1height=.7xlabel= speed (fps), ylabel= f-score] [only marks, draw= black, fill= blue, mark size=3pt] table x y+ 7.14+ 0.6085+ 1.61+ 0.6477+ 0.476+ 0.5358; [only marks, draw= black, fill= red, mark size=3pt] table x y+ 13.245e+ 00+ 0.7820+ 16.779e+ 00+ 0.7571; at (axis cs:7.3, 0.58) [anchor= base west, text= black, rotate=0.0, align= left] tian (0.609@7.14fps); at (axis cs:1.3, 0.6677) [anchor= base west, text= black, rotate=0.0, align= left] yao (0.648@1.61fps); at (axis cs:0.6, 0.505) [anchor= base west, text= black, rotate=0.0, align= left] zhang (0.532@0.476fps); at (axis cs:13.0, 0.78) [anchor= base east, text= black, rotate=0.0, align= right] ours+ pvanet2x (0.782@13.2fps); at (axis cs:16.8, 0.69) [anchor= base east, text= black, rotate=0.0, align= right] ours+ pvanet (0.757@16.8fps); however, existing methods, either conventional or deep neural network based, mostly consist of several stages and components, which are probably sub-optimal and time-consuming. therefore, the accuracy and efficiency of such methods are still far from satisfactory. in this paper, we propose a fast and accurate scene text detection pipeline that has only two stages. the pipeline utilizes a fully convolutional network (fcn) model that directly produces word or text-line level predictions, excluding redundant and slow intermediate steps. the produced text predictions, which can be either rotated rectangles or quadrangles, are sent to non-maximum suppression to yield final results. compared with existing methods, the proposed algorithm achieves significantly enhanced performance, while running much faster, according to the qualitative and quantitative experiments on standard benchmarks. specifically, the proposed algorithm achieves an f-score of 0.7820 on icdar 2015 (0.8072 when tested in multi-scale), 0.7608 on msra-td500 and 0.3945 on coco-text, outperforming previous state-of-the-art algorithms in performance while taking much less time on average (13.2fps at 720p resolution on a titan-x gpu for our best performing model, 16.8fps for our fastest model). the contributions of this work are three-fold: we propose a scene text detection method that consists of two stages: a fully convolutional network and an nms merging stage. the fcn directly produces text regions, excluding redundant and time-consuming intermediate steps. the pipeline is flexible to produce either word level or line level predictions, whose geometric shapes can be rotated boxes or quadrangles, depending on specific applications. the proposed algorithm significantly outperforms state-of-the-art methods in both accuracy and speed. section: related work scene text detection and recognition have been active research topics in computer vision for a long period of time. numerous inspiring ideas and effective approaches have been investigated. comprehensive reviews and detailed analyses can be found in survey papers. this section will focus on works that are mostly relevant to the proposed algorithm. conventional approaches rely on manually designed features. stroke width transform (swt) and maximally stable extremal regions (mser) based methods generally seek character candidates via edge detection or extremal region extraction. zhang made use of the local symmetry property of text and designed various features for text region detection. fastext is a fast text detection system that adapted and modified the well-known fast key point detector for stroke extraction. however, these methods fall behind of those based on deep neural networks, in terms of both accuracy and adaptability, especially when dealing with challenging scenarios, such as low resolution and geometric distortion. recently, the area of scene text detection has entered a new era that deep neural network based algorithms have gradually become the mainstream. huang first found candidates using mser and then employed a deep convolutional network as a strong classifier to prune false positives. the method of jaderberg scanned the image in a sliding-window fashion and produced a dense heatmap for each scale with a convolutional neural network model. later, jaderberg employed both a cnn and an acf to hunt word candidates and further refined them using regression. tian developed vertical anchors and constructed a cnn-rnn joint model to detect horizontal text lines. different from these methods, zhang proposed to utilize fcn for heatmap generation and to use component projection for orientation estimation. these methods obtained excellent performance on standard benchmarks. however, as illustrated in fig. [reference] (a-d), they mostly consist of multiple stages and components, such as false positive removal by post filtering, candidate aggregation, line formation and word partition. the multitude of stages and components may require exhaustive tuning, leading to sub-optimal performance, and add to processing time of the whole pipeline. in this paper, we devise a deep fcn-based pipeline that directly targets the final goal of text detection: word or text-line level detection. as depicted in fig. [reference] (e), the model abandons unnecessary intermediate components and steps, and allows for end-to-end training and optimization. the resultant system, equipped with a single, light-weighted neural network, surpasses all previous methods by an obvious margin in both performance and speed. section: methodology the key component of the proposed algorithm is a neural network model, which is trained to directly predict the existence of text instances and their geometries from full images. the model is a fully-convolutional neural network adapted for text detection that outputs dense per-pixel predictions of words or text lines. this eliminates intermediate steps such as candidate proposal, text region formation and word partition. the post-processing steps only include thresholding and nms on predicted geometric shapes. the detector is named as east, since it is an e fficient and a ccuracy s cene t ext detection pipeline. subsection: pipeline a high-level overview of our pipeline is illustrated in fig. [reference] (e). the algorithm follows the general design of densebox, in which an image is fed into the fcn and multiple channels of pixel-level text score map and geometry are generated. one of the predicted channels is a score map whose pixel values are in the range of. the remaining channels represent geometries that encloses the word from the view of each pixel. the score stands for the confidence of the geometry shape predicted at the same location. we have experimented with two geometry shapes for text regions, rotated box (rbox) and quadrangle (quad), and designed different loss functions for each geometry. thresholding is then applied to each predicted region, where the geometries whose scores are over the predefined threshold is considered valid and saved for later non-maximum-suppression. results after nms are considered the final output of the pipeline. subsection: network design several factors must be taken into account when designing neural networks for text detection. since the sizes of word regions, as shown in fig. [reference], vary tremendously, determining the existence of large words would require features from late-stage of a neural network, while predicting accurate geometry enclosing a small word regions need low-level information in early stages. therefore the network must use features from different levels to fulfill these requirements. hypernet meets these conditions on features maps, but merging a large number of channels on large feature maps would significantly increase the computation overhead for later stages. in remedy of this, we adopt the idea from u-shape to merge feature maps gradually, while keeping the up-sampling branches small. together we end up with a network that can both utilize different levels of features and keep a small computation cost. a schematic view of our model is depicted in fig. [reference]. the model can be decomposed in to three parts: feature extractor stem, feature-merging branch and output layer. the stem can be a convolutional network pre-trained on imagenet dataset, with interleaving convolution and pooling layers. four levels of feature maps, denoted as, are extracted from the stem, whose sizes are,, and of the input image, respectively. in fig. [reference], pvanet is depicted. in our experiments, we also adopted the well-known vgg16 model, where feature maps after pooling-2 to pooling-5 are extracted. in the feature-merging branch, we gradually merge them: where is the merge base, and is the merged feature map, and the operator represents concatenation along the channel axis. in each merging stage, the feature map from the last stage is first fed to an unpooling layer to double its size, and then concatenated with the current feature map. next, a bottleneck cuts down the number of channels and reduces computation, followed by a that fuses the information to finally produce the output of this merging stage. following the last merging stage, a layer produces the final feature map of the merging branch and feed it to the output layer. the number of output channels for each convolution is shown in fig. [reference]. we keep the number of channels for convolutions in branch small, which adds only a fraction of computation overhead over the stem, making the network computation-efficient. the final output layer contains several operations to project 32 channels of feature maps into 1 channel of score map and a multi-channel geometry map. the geometry output can be either one of rbox or quad, summarized in tab. [reference] for rbox, the geometry is represented by 4 channels of axis-aligned bounding box (aabb) and 1 channel rotation angle. the formulation of is the same as that in, where the 4 channels represents 4 distances from the pixel location to the top, right, bottom, left boundaries of the rectangle respectively. for quad, we use 8 numbers to denote the coordinate shift from four corner vertices of the quadrangle to the pixel location. as each distance offset contains two numbers, the geometry output contains 8 channels. subsection: label generation subsubsection: score map generation for quadrangle without loss of generality, we only consider the case where the geometry is a quadrangle. the positive area of the quadrangle on the score map is designed to be roughly a shrunk version of the original one, illustrated in fig. [reference] (a). for a quadrangle, where are vertices on the quadrangle in clockwise order. to shrink, we first compute a reference length for each vertex as where is the distance between and. we first shrink the two longer edges of a quadrangle, and then the two shorter ones. for each pair of two opposing edges, we determine the\" longer\" pair by comparing the mean of their lengths. for each edge, we shrink it by moving its two endpoints inward along the edge by and respectively. subsubsection: geometry map generation as discussed in sec. [reference], the geometry map is either one of rbox or quad. the generation process for rbox is illustrated in fig. [reference] (c-e). for those datasets whose text regions are annotated in quad style (, icdar 2015), we first generate a rotated rectangle that covers the region with minimal area. then for each pixel which has positive score, we calculate its distances to the 4 boundaries of the text box, and put them to the 4 channels of rbox ground truth. for the quad ground truth, the value of each pixel with positive score in the 8-channel geometry map is its coordinate shift from the 4 vertices of the quadrangle. subsection: loss functions the loss can be formulated as where and represents the losses for the score map and the geometry, respectively, and weighs the importance between two losses. in our experiment, we set to 1. subsubsection: loss for score map in most state-of-the-art detection pipelines, training images are carefully processed by balanced sampling and hard negative mining to tackle with the imbalanced distribution of target objects. doing so would potentially improve the network performance. however, using such techniques inevitably introduces a non-differentiable stage and more parameters to tune and a more complicated pipeline, which contradicts our design principle. to facilitate a simpler training procedure, we use class-balanced cross-entropy introduced in, given by where is the prediction of the score map, and is the ground truth. the parameter is the balancing factor between positive and negative samples, given by this balanced cross-entropy loss is first adopted in text detection by yao as the objective function for score map prediction. we find it works well in practice. subsubsection: loss for geometries one challenge for text detection is that the sizes of text in natural scene images vary tremendously. directly using l1 or l2 loss for regression would guide the loss bias towards larger and longer text regions. as we need to generate accurate text geometry prediction for both large and small text regions, the regression loss should be scale-invariant. therefore, we adopt the iou loss in the aabb part of rbox regression, and a scale-normalized smoothed-l1 loss for quad regression. paragraph: rbox for the aabb part, we adopt iou loss in, since it is invariant against objects of different scales. where represents the predicted aabb geometry and is its corresponding ground truth. it is easy to see that the width and height of the intersected rectangle are where,, and represents the distance from a pixel to the top, right, bottom and left boundary of its corresponding rectangle, respectively. the union area is given by therefore, both the intersection/ union area can be computed easily. next, the loss of rotation angle is computed as where is the prediction to the rotation angle and represents the ground truth. finally, the overall geometry loss is the weighted sum of aabb loss and angle loss, given by where is set to in our experiments. note that we compute regardless of rotation angle. this can be seen as an approximation of quadrangle iou when the angle is perfectly predicted. although it is not the case during training, it could still impose the correct gradient for the network to learn to predict. paragraph: quad we extend the smoothed-l1 loss proposed in by adding an extra normalization term designed for word quadrangles, which is typically longer in one direction. let all coordinate values of be an ordered set then the loss can be written as where the normalization term is the shorted edge length of the quadrangle, given by and is the set of all equivalent quadrangles of with different vertices ordering. this ordering permutation is required since the annotations of quadrangles in the public training datasets are inconsistent. subsection: training the network is trained end-to-end using adam optimizer. to speed up learning, we uniformly sample 512x512 crops from images to form a minibatch of size 24. learning rate of adam starts from 1e-3, decays to one-tenth every 27300 minibatches, and stops at 1e-5. the network is trained until performance stops improving. subsection: locality-aware nms to form the final results, the geometries survived after thresholding should be merged by nms. a na\u00efve nms algorithm runs in\u00ef\u00bc\u008c where is the number of candidate geometries, which is unacceptable as we are facing tens of thousands of geometries from dense predictions. under the assumption that the geometries from nearby pixels tend to be highly correlated, we proposed to merge the geometries row by row, and while merging geometries in the same row, we will iteratively merge the geometry currently encountered with the last merged one. this improved technique runs in in best scenarios. even though its worst case is the same as the na\u00efve one, as long as the locality assumption holds, the algorithm runs sufficiently fast in practice. the procedure is summarized in algorithm [reference] it is worth mentioning that, in, the coordinates of merged quadrangle are weight-averaged by the scores of two given quadrangles. to be specific, if, then and, where is one of the coordinates of subscripted by, and is the score of geometry. in fact, there is a subtle difference that we are\" averaging\" rather than\" selecting\" geometries, as in a standard nms procedure will do, acting as a voting mechanism, which in turn introduces a stabilization effect when feeding videos. nonetheless, we still adopt the word\" nms\" for functional description. [t] locality-aware nms [1] nmslocality, in row first order section: experiments to compare the proposed algorithm with existing methods, we conducted qualitative and quantitative experiments on three public benchmarks: icdar2015, coco-text and msra-td500. subsection: benchmark datasets icdar 2015 is used in challenge 4 of icdar 2015 robust reading competition. it includes a total of 1500 pictures, 1000 of which are used for training and the remaining are for testing. the text regions are annotated by 4 vertices of the quadrangle, corresponding to the quad geometry in this paper. we also generate rbox output by fitting a rotated rectangle which has the minimum area. these images are taken by google glass in an incidental way. therefore text in the scene can be in arbitrary orientations, or suffer from motion blur and low resolution. we also used the 229 training images from icdar 2013. coco-text is the largest text detection dataset to date. it reuses the images from ms-coco dataset. a total of 63, 686 images are annotated, in which 43, 686 are chosen to be the training set and the rest 20, 000 for testing. word regions are annotated in the form of axis-aligned bounding box (aabb), which is a special case of rbox. for this dataset, we set angle to zero. we use the same data processing and test method as in icdar 2015. msra-td500 is a dataset comprises of 300 training images and 200 test images. text regions are of arbitrary orientations and annotated at sentence level. different from the other datasets, it contains text in both english and chinese. the text regions are annotated in rbox format. since the number of training images is too few to learn a deep model, we also harness 400 images from hust-tr400 dataset as training data. subsection: base networks as except for coco-text, all text detection datasets are relatively small compared to the datasets for general object detection, therefore if a single network is adopted for all the benchmarks, it may suffer from either over-fitting or under-fitting. we experimented with three different base networks, with different output geometries, on all the datasets to evaluate the proposed framework. these networks are summarized in tab. [reference]. vgg16 is widely used as base network in many tasks to support subsequent task-specific fine-tuning, including text detection. there are two drawbacks of this network: (1). the receptive field for this network is small. each pixel in output of conv5_3 only has a receptive field of 196. (2). it is a rather large network. pvanet is a light weight network introduced in, aiming as a substitution of the feature extractor in faster-rcnn framework. since it is too small for gpu to fully utilizes computation parallelism, we also adopt pvanet2x that doubles the channels of the original pvanet, exploiting more computation parallelism while running slightly slower than pvanet. this is detailed in sec. [reference]. the receptive field of the output of the last convolution layer is 809, which is much larger than vgg16. the models are pre-trained on the imagenet dataset. subsection: qualitative results (a) (b) (c) (a) (c) (b) (d) fig. [reference] depicts several detection examples by the proposed algorithm. it is able to handle various challenging scenarios, such as non-uniform illumination, low resolution, varying orientation and perspective distortion. moreover, due to the voting mechanism in the nms procedure, the proposed method shows a high level of stability on videos with various forms of text instances. the intermediate results of the proposed method are illustrated in fig. [reference]. as can be seen, the trained model produces highly accurate geometry maps and score map, in which detections of text instances in varying orientations are easily formed. subsection: quantitative results as shown in tab. [reference] and tab. [reference], our approach outperforms previous state-of-the-art methods by a large margin on icdar 2015 and coco-text. in icdar 2015 challenge 4, when images are fed at their original scale, the proposed method achieves an f-score of 0.7820. when tested at multiple scales using the same network, our method reaches 0.8072 in f-score, which is nearly 0.16 higher than the best method in terms of absolute value (0.8072 vs. 0.6477). comparing the results using vgg16 network, the proposed method also outperforms best previous work by 0.0924 when using quad output, 0.116 when using rbox output. meanwhile these networks are quite efficient, as will be shown in sec. [reference]. in coco-text, all of the three settings of the proposed algorithm result in higher accuracy than previous top performer. specifically, the improvement over in f-score is 0.0614 while that in recall is 0.053, which confirm the advantage of the proposed algorithm, considering that coco-text is the largest and most challenging benchmark to date. note that we also included the results from as reference, but these results are actually not valid baselines, since the methods (a, b and c) are used in data annotation. the improvements of the proposed algorithm over previous methods prove that a simple text detection pipeline, which directly targets the final goal and eliminating redundant processes, can beat elaborated pipelines, even those integrated with large neural network models. as shown in tab. [reference], on msra-td500 all of the three settings of our method achieve excellent results. the f-score of the best performer (ours+ pvanet2x) is slightly higher than that of. compared with the method of zhang, the previous published state-of-the-art system, the best performer (ours+ pvanet2x) obtains an improvement of 0.0208 in f-score and 0.0428 in precision. note that on msra-td500 our algorithm equipped with vgg16 performs much poorer than that with pvanet and pvanet2x (0.7023 vs. 0.7445 and 0.7608), the main reason is that the effective receptive field of vgg16 is smaller than that of pvanet and pvanet2x, while the evaluation protocol of msra-td500 requires text detection algorithms output line level instead of word level predictions. in addition, we also evaluated ours+ pvanet2x on the icdar 2013 benchmark. it achieves 0.8267, 0.9264 and 0.8737 in recall, precision and f-score, which are comparable with the previous state-of-the-art method, which obtains 0.8298, 0.9298 and 0.8769 in recall, precision and f-score, respectively. subsection: speed comparison the overall speed comparison is demonstrated in tab. [reference]. the numbers we reported are averages from running through 500 test images from the icdar 2015 dataset at their original resolution (1280x720) using our best performing networks. these experiments were conducted on a server using a single nvidia titan x graphic card with maxwell architecture and an intel e5-2670 v3@ 2.30ghz cpu. for the proposed method, the post-processing includes thresholding and nms, while others should refer to their original paper. while the proposed method significantly outperforms state-of-the-art methods, the computation cost is kept very low, attributing to the simple and efficient pipeline. as can be observed from tab. [reference], the fastest setting of our method runs at a speed of 16.8 fps, while slowest setting runs at 6.52 fps. even the best performing model ours+ pvanet2x runs at a speed of 13.2 fps. this confirm that our method is among the most efficient text detectors that achieve state-of-the-art performance on benchmarks. subsection: limitations the maximal size of text instances the detector can handle is proportional to the receptive field of the network. this limits the capability of the network to predict even longer text regions like text lines running across the images. also, the algorithm might miss or give imprecise predictions for vertical text instances as they take only a small portion of text regions in the icdar 2015 training set. section: conclusion and future work we have presented a scene text detector that directly produces word or line level predictions from full images with a single neural network. by incorporating proper loss functions, the detector can predict either rotated rectangles or quadrangles for text regions, depending on specific applications. the experiments on standard benchmarks confirm that the proposed algorithm substantially outperforms previous methods in terms of both accuracy and efficiency. possible directions for future research include: (1) adapting the geometry formulation to allow direct detection of curved text; (2) integrating the detector with a text recognizer; (3) extending the idea to general object detection. bibliography: references",
    "templates": [
        {
            "Material": [
                [
                    [
                        "icdar 2015",
                        948
                    ],
                    [
                        "icdar2015",
                        19147
                    ]
                ]
            ],
            "Method": [
                [
                    [
                        "east",
                        10
                    ],
                    [
                        "efficient and accurate scene text detector",
                        19
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "f-score",
                        1180
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "scene text detection",
                        86
                    ],
                    [
                        "text detection in natural scenes",
                        506
                    ],
                    [
                        "text region detection",
                        5685
                    ],
                    [
                        "word or text-line level detection",
                        7401
                    ],
                    [
                        "general object detection",
                        20817
                    ],
                    [
                        "object detection",
                        27546
                    ]
                ]
            ]
        },
        {
            "Material": [],
            "Method": [
                [
                    [
                        "east",
                        10
                    ],
                    [
                        "efficient and accurate scene text detector",
                        19
                    ]
                ]
            ],
            "Metric": [
                [
                    [
                        "f-score",
                        1180
                    ]
                ]
            ],
            "Task": [
                [
                    [
                        "text detection",
                        1643
                    ],
                    [
                        "direct detection of curved text",
                        27426
                    ]
                ]
            ]
        }
    ]
}
